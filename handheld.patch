From 30a0418cccddb4b7d874ead5f68ca6229686c5a9 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:02:51 +0200
Subject: [PATCH v1.4 001/120] [BEGIN] workaround for i915

getting stuck during async page flips on Nvidia PRIME systems
Link: https://gitlab.freedesktop.org/xorg/xserver/-/issues/24
Link: https://gitlab.freedesktop.org/xorg/driver/xf86-video-intel/-/issues/208
-- 
2.47.0


From dbafca510374176d4f497164a36f53912ad237b3 Mon Sep 17 00:00:00 2001
From: Jan200101 <sentrycraft123@gmail.com>
Date: Mon, 14 Nov 2022 20:13:53 +0100
Subject: [PATCH v1.4 002/120] drm/i915/quirks: disable async flipping on
 specific devices

Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/gpu/drm/i915/display/intel_quirks.c | 20 ++++++++++++++++++++
 1 file changed, 20 insertions(+)

diff --git a/drivers/gpu/drm/i915/display/intel_quirks.c b/drivers/gpu/drm/i915/display/intel_quirks.c
index dfd8b4960e6d..a63b3d513075 100644
--- a/drivers/gpu/drm/i915/display/intel_quirks.c
+++ b/drivers/gpu/drm/i915/display/intel_quirks.c
@@ -78,6 +78,12 @@ static void quirk_fw_sync_len(struct intel_dp *intel_dp)
 	drm_info(display->drm, "Applying Fast Wake sync pulse count quirk\n");
 }
 
+static void quirk_async_page_flips_force_disable(struct intel_display *display)
+{
+	display->drm->mode_config.async_page_flip = false;
+	drm_info(display->drm, "Applying async flip disable quirk\n");
+}
+
 struct intel_quirk {
 	int device;
 	int subsystem_vendor;
@@ -164,6 +170,20 @@ static const struct intel_dmi_quirk intel_dmi_quirks[] = {
 		},
 		.hook = quirk_no_pps_backlight_power_hook,
 	},
+	{
+		.dmi_id_list = &(const struct dmi_system_id[]) {
+			{
+				.callback = NULL,
+				.ident = "ASUS TUF DASH F15",
+				.matches = {
+					DMI_MATCH(DMI_SYS_VENDOR, "ASUSTeK COMPUTER INC."),
+					DMI_MATCH(DMI_PRODUCT_NAME, "ASUS TUF Dash F15 FX516PC"),
+				},
+			},
+			{ }
+		},
+		.hook = quirk_async_page_flips_force_disable,
+	},
 };
 
 static struct intel_quirk intel_quirks[] = {
-- 
2.47.0


From c4406e53903eb90915e25416f070bb20ae5c1cfa Mon Sep 17 00:00:00 2001
From: Jan200101 <sentrycraft123@gmail.com>
Date: Sat, 30 Mar 2024 19:20:48 +0100
Subject: [PATCH v1.4 003/120] drm/i915: add kernel parameter to disable async
 page flipping

Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/gpu/drm/i915/display/intel_display_driver.c | 2 +-
 drivers/gpu/drm/i915/i915_params.c                  | 4 ++++
 drivers/gpu/drm/i915/i915_params.h                  | 1 +
 drivers/gpu/drm/xe/xe_device_types.h                | 3 +++
 4 files changed, 9 insertions(+), 1 deletion(-)

diff --git a/drivers/gpu/drm/i915/display/intel_display_driver.c b/drivers/gpu/drm/i915/display/intel_display_driver.c
index 794b4af38055..58edd0a9e537 100644
--- a/drivers/gpu/drm/i915/display/intel_display_driver.c
+++ b/drivers/gpu/drm/i915/display/intel_display_driver.c
@@ -127,7 +127,7 @@ static void intel_mode_config_init(struct drm_i915_private *i915)
 	mode_config->funcs = &intel_mode_funcs;
 	mode_config->helper_private = &intel_mode_config_funcs;
 
-	mode_config->async_page_flip = HAS_ASYNC_FLIPS(i915);
+	mode_config->async_page_flip = HAS_ASYNC_FLIPS(i915) && !i915->params.disable_async_page_flip;
 
 	/*
 	 * Maximum framebuffer dimensions, chosen to match
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index 316e55f3e87b..7da7dc413469 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -130,6 +130,10 @@ i915_param_named_unsafe(lmem_size, uint, 0400,
 			"Set the lmem size(in MiB) for each region. (default: 0, all memory)");
 i915_param_named_unsafe(lmem_bar_size, uint, 0400,
 			"Set the lmem bar size(in MiB).");
+ 
+i915_param_named_unsafe(disable_async_page_flip, bool, 0400,
+			"Disable async page flipping"
+			"(0=disabled [default], 1=enabled)");
 
 #if IS_ENABLED(CONFIG_DRM_I915_REPLAY_GPU_HANGS_API)
 i915_param_named(enable_debug_only_api, bool, 0400,
diff --git a/drivers/gpu/drm/i915/i915_params.h b/drivers/gpu/drm/i915/i915_params.h
index 0fbcb5b6d7bf..8e9aa9131569 100644
--- a/drivers/gpu/drm/i915/i915_params.h
+++ b/drivers/gpu/drm/i915/i915_params.h
@@ -64,6 +64,7 @@ struct drm_printer;
 	param(bool, enable_hangcheck, true, 0600) \
 	param(bool, error_capture, true, IS_ENABLED(CONFIG_DRM_I915_CAPTURE_ERROR) ? 0600 : 0) \
 	param(bool, enable_gvt, false, IS_ENABLED(CONFIG_DRM_I915_GVT) ? 0400 : 0) \
+	param(bool, disable_async_page_flip, false, 0400) \
 	param(bool, enable_debug_only_api, false, IS_ENABLED(CONFIG_DRM_I915_REPLAY_GPU_HANGS_API) ? 0400 : 0)
 
 #define MEMBER(T, member, ...) T member;
diff --git a/drivers/gpu/drm/xe/xe_device_types.h b/drivers/gpu/drm/xe/xe_device_types.h
index a7c7812d5791..51ca0d6aea26 100644
--- a/drivers/gpu/drm/xe/xe_device_types.h
+++ b/drivers/gpu/drm/xe/xe_device_types.h
@@ -539,6 +539,9 @@ struct xe_device {
 		unsigned int czclk_freq;
 		unsigned int fsb_freq, mem_freq, is_ddr3;
 	};
+	struct {
+		bool disable_async_page_flip;
+	} params;
 
 	void *pxp;
 #endif
-- 
2.47.0


From 607838c74e6d067b746c1ce85b0b001df8cd8825 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:22:43 +0200
Subject: [PATCH v1.4 004/120] [BEGIN] Gamescope Framerate fixups by Valve

also fixes https://gitlab.freedesktop.org/drm/amd/-/issues/2733
-- 
2.47.0


From 67476a9852570718c6f4585add774c7aa622cf69 Mon Sep 17 00:00:00 2001
From: Thomas Crider <gloriouseggroll@gmail.com>
Date: Mon, 18 Dec 2023 03:36:09 -0500
Subject: [PATCH v1.4 005/120] revert 1101185bc50f5e45b8b89300914d9aa35a0c8cbe

---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index 4f19e9736a67..029457a3e258 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -6673,6 +6673,8 @@ create_stream_for_sink(struct drm_connector *connector,
 
 	if (recalculate_timing)
 		drm_mode_set_crtcinfo(&saved_mode, 0);
+	else if (!old_stream)
+		drm_mode_set_crtcinfo(&mode, 0);
 
 	/*
 	 * If scaling is enabled and refresh rate didn't change
@@ -7370,8 +7372,6 @@ enum drm_mode_status amdgpu_dm_connector_mode_valid(struct drm_connector *connec
 		goto fail;
 	}
 
-	drm_mode_set_crtcinfo(mode, 0);
-
 	stream = create_validate_stream_for_sink(aconnector, mode,
 						 to_dm_connector_state(connector->state),
 						 NULL);
-- 
2.47.0


From 92874c7b4f288fc2ff30015f05813f255b092f95 Mon Sep 17 00:00:00 2001
From: Simon Ser <contact@emersion.fr>
Date: Tue, 30 Aug 2022 17:29:26 +0000
Subject: [PATCH v1.4 006/120] drm: introduce
 drm_mode_config.atomic_async_page_flip_not_supported
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This new field indicates whether the driver has the necessary logic
to support async page-flips via the atomic uAPI. This is leveraged by
the next commit to allow user-space to use this functionality.

All atomic drivers setting drm_mode_config.async_page_flip are updated
to also set drm_mode_config.atomic_async_page_flip_not_supported. We
will gradually check and update these drivers to properly handle
drm_crtc_state.async_flip in their atomic logic.

The goal of this negative flag is the same as
fb_modifiers_not_supported: we want to eventually get rid of all
drivers missing atomic support for async flips. New drivers should not
set this flag, instead they should support atomic async flips (if
they support async flips at all). IOW, we don't want more drivers
with async flip support for legacy but not atomic.

v2: only set the flag on atomic drivers (remove it on amdgpu DCE and
on radeon)

Signed-off-by: Simon Ser <contact@emersion.fr>
Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
Cc: Joshua Ashton <joshua@froggi.es>
Cc: Melissa Wen <mwen@igalia.com>
Cc: Alex Deucher <alexander.deucher@amd.com>
Cc: Harry Wentland <hwentlan@amd.com>
Cc: Nicholas Kazlauskas <nicholas.kazlauskas@amd.com>
Cc: André Almeida <andrealmeid@igalia.com>
Cc: Ville Syrjälä <ville.syrjala@linux.intel.com>
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Link: https://lore.kernel.org/r/20220830172851.269402-4-contact@emersion.fr
---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c   |  1 +
 drivers/gpu/drm/atmel-hlcdc/atmel_hlcdc_dc.c        |  1 +
 drivers/gpu/drm/i915/display/intel_display_driver.c |  1 +
 drivers/gpu/drm/nouveau/nouveau_display.c           |  1 +
 drivers/gpu/drm/vc4/vc4_kms.c                       |  1 +
 include/drm/drm_mode_config.h                       | 11 +++++++++++
 6 files changed, 16 insertions(+)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index 029457a3e258..fb0e63e6cdf2 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -4422,6 +4422,7 @@ static int amdgpu_dm_mode_config_init(struct amdgpu_device *adev)
 		adev_to_drm(adev)->mode_config.prefer_shadow = 1;
 	/* indicates support for immediate flip */
 	adev_to_drm(adev)->mode_config.async_page_flip = true;
+	adev_to_drm(adev)->mode_config.atomic_async_page_flip_not_supported = true;
 
 	state = kzalloc(sizeof(*state), GFP_KERNEL);
 	if (!state)
diff --git a/drivers/gpu/drm/atmel-hlcdc/atmel_hlcdc_dc.c b/drivers/gpu/drm/atmel-hlcdc/atmel_hlcdc_dc.c
index 9ce429f889ca..aa8700eefadb 100644
--- a/drivers/gpu/drm/atmel-hlcdc/atmel_hlcdc_dc.c
+++ b/drivers/gpu/drm/atmel-hlcdc/atmel_hlcdc_dc.c
@@ -744,6 +744,7 @@ static int atmel_hlcdc_dc_modeset_init(struct drm_device *dev)
 	dev->mode_config.max_height = dc->desc->max_height;
 	dev->mode_config.funcs = &mode_config_funcs;
 	dev->mode_config.async_page_flip = true;
+	dev->mode_config.atomic_async_page_flip_not_supported = true;
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/i915/display/intel_display_driver.c b/drivers/gpu/drm/i915/display/intel_display_driver.c
index 58edd0a9e537..7490afcbcde9 100644
--- a/drivers/gpu/drm/i915/display/intel_display_driver.c
+++ b/drivers/gpu/drm/i915/display/intel_display_driver.c
@@ -128,6 +128,7 @@ static void intel_mode_config_init(struct drm_i915_private *i915)
 	mode_config->helper_private = &intel_mode_config_funcs;
 
 	mode_config->async_page_flip = HAS_ASYNC_FLIPS(i915) && !i915->params.disable_async_page_flip;
+	mode_config->atomic_async_page_flip_not_supported = true;
 
 	/*
 	 * Maximum framebuffer dimensions, chosen to match
diff --git a/drivers/gpu/drm/nouveau/nouveau_display.c b/drivers/gpu/drm/nouveau/nouveau_display.c
index d4725a968827..5cdcc4e79ba3 100644
--- a/drivers/gpu/drm/nouveau/nouveau_display.c
+++ b/drivers/gpu/drm/nouveau/nouveau_display.c
@@ -723,6 +723,7 @@ nouveau_display_create(struct drm_device *dev)
 		dev->mode_config.async_page_flip = false;
 	else
 		dev->mode_config.async_page_flip = true;
+	dev->mode_config.atomic_async_page_flip_not_supported = true;
 
 	drm_kms_helper_poll_init(dev);
 	drm_kms_helper_poll_disable(dev);
diff --git a/drivers/gpu/drm/vc4/vc4_kms.c b/drivers/gpu/drm/vc4/vc4_kms.c
index 5495f2a94fa9..5b6b311e70fd 100644
--- a/drivers/gpu/drm/vc4/vc4_kms.c
+++ b/drivers/gpu/drm/vc4/vc4_kms.c
@@ -1068,6 +1068,7 @@ int vc4_kms_load(struct drm_device *dev)
 	dev->mode_config.helper_private = &vc4_mode_config_helpers;
 	dev->mode_config.preferred_depth = 24;
 	dev->mode_config.async_page_flip = true;
+	dev->mode_config.atomic_async_page_flip_not_supported = true;
 	dev->mode_config.normalize_zpos = true;
 
 	ret = vc4_ctm_obj_init(vc4);
diff --git a/include/drm/drm_mode_config.h b/include/drm/drm_mode_config.h
index ab0f167474b1..5f311be7bcf2 100644
--- a/include/drm/drm_mode_config.h
+++ b/include/drm/drm_mode_config.h
@@ -928,6 +928,17 @@ struct drm_mode_config {
 	 */
 	bool async_page_flip;
 
+	/**
+	 * @atomic_async_page_flip_not_supported:
+	 *
+	 * If true, the driver does not support async page-flips with the
+	 * atomic uAPI. This is only used by old drivers which haven't yet
+	 * accomodated for &drm_crtc_state.async_flip in their atomic logic,
+	 * even if they have &drm_mode_config.async_page_flip set to true.
+	 * New drivers shall not set this flag.
+	 */
+	bool atomic_async_page_flip_not_supported;
+
 	/**
 	 * @fb_modifiers_not_supported:
 	 *
-- 
2.47.0


From 0e225450fdc683d8833c1bd0dadee405cec385ee Mon Sep 17 00:00:00 2001
From: Simon Ser <contact@emersion.fr>
Date: Tue, 30 Aug 2022 17:29:52 +0000
Subject: [PATCH v1.4 007/120] amd/display: indicate support for atomic async
 page-flips on DC
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

amdgpu_dm_commit_planes() already sets the flip_immediate flag for
async page-flips. This flag is used to set the UNP_FLIP_CONTROL
register. Thus, no additional change is required to handle async
page-flips with the atomic uAPI.

v2: make it clear this commit is about DC and not only DCN

Signed-off-by: Simon Ser <contact@emersion.fr>
Cc: Joshua Ashton <joshua@froggi.es>
Cc: Melissa Wen <mwen@igalia.com>
Cc: Alex Deucher <alexander.deucher@amd.com>
Cc: Harry Wentland <hwentlan@amd.com>
Cc: Nicholas Kazlauskas <nicholas.kazlauskas@amd.com>
Cc: André Almeida <andrealmeid@igalia.com>
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Link: https://lore.kernel.org/r/20220830172851.269402-7-contact@emersion.fr
---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c | 1 -
 1 file changed, 1 deletion(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index fb0e63e6cdf2..029457a3e258 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -4422,7 +4422,6 @@ static int amdgpu_dm_mode_config_init(struct amdgpu_device *adev)
 		adev_to_drm(adev)->mode_config.prefer_shadow = 1;
 	/* indicates support for immediate flip */
 	adev_to_drm(adev)->mode_config.async_page_flip = true;
-	adev_to_drm(adev)->mode_config.atomic_async_page_flip_not_supported = true;
 
 	state = kzalloc(sizeof(*state), GFP_KERNEL);
 	if (!state)
-- 
2.47.0


From 47acd19b5739128b7598f91b4bb356f624f60f9b Mon Sep 17 00:00:00 2001
From: Joshua Ashton <joshua@froggi.es>
Date: Mon, 14 Nov 2022 19:52:30 +0000
Subject: [PATCH v1.4 008/120] drm/amd/display: Always set crtcinfo from
 create_stream_for_sink

Given that we always pass dm_state into here now, this won't ever
trigger anymore.

This is needed for we will always fail mode validation with invalid
clocks or link bandwidth errors.

Signed-off-by: Joshua Ashton <joshua@froggi.es>
Signed-off-by: Harry Wentland <harry.wentland@amd.com>
Reviewed-by: Harry Wentland <harry.wentland@amd.com>

Cc: Pekka Paalanen <ppaalanen@gmail.com>
Cc: Sebastian Wick <sebastian.wick@redhat.com>
Cc: Vitaly.Prosyak@amd.com
Cc: Joshua Ashton <joshua@froggi.es>
Cc: Simon Ser <contact@emersion.fr>
Cc: Melissa Wen <mwen@igalia.com>
Cc: dri-devel@lists.freedesktop.org
Cc: amd-gfx@lists.freedesktop.org
Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index 029457a3e258..5266424ce15b 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -6673,7 +6673,7 @@ create_stream_for_sink(struct drm_connector *connector,
 
 	if (recalculate_timing)
 		drm_mode_set_crtcinfo(&saved_mode, 0);
-	else if (!old_stream)
+	else
 		drm_mode_set_crtcinfo(&mode, 0);
 
 	/*
-- 
2.47.0


From 7d067ceab97e7951b62f7fded6e613ccfa5b429d Mon Sep 17 00:00:00 2001
From: Hamza Mahfooz <hamza.mahfooz@amd.com>
Date: Fri, 4 Aug 2023 11:13:04 -0400
Subject: [PATCH v1.4 009/120] drm/amd/display: ensure async flips are only
 accepted for fast updates

We should be checking to see if async flips are supported in
amdgpu_dm_atomic_check() (i.e. not dm_crtc_helper_atomic_check()). Also,
async flipping isn't supported if a plane's framebuffer changes memory
domains during an atomic commit. So, move the check from
dm_crtc_helper_atomic_check() to amdgpu_dm_atomic_check() and check if
the memory domain has changed in amdgpu_dm_atomic_check().

Cc: stable@vger.kernel.org
Link: https://gitlab.freedesktop.org/drm/amd/-/issues/2733
Fixes: c1e18c44dc7f ("drm/amd/display: only accept async flips for fast updates")
Reviewed-by: Harry Wentland <harry.wentland@amd.com>
Signed-off-by: Hamza Mahfooz <hamza.mahfooz@amd.com>
Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
(cherry picked from commit a7c0cad0dc060bb77e9c9d235d68441b0fc69507)
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
---
 .../gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c   | 12 ------------
 1 file changed, 12 deletions(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c
index 99014339aaa3..1726bd1f3de4 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c
@@ -603,18 +603,6 @@ static int amdgpu_dm_crtc_helper_atomic_check(struct drm_crtc *crtc,
 		return -EINVAL;
 	}
 
-	/*
-	 * Only allow async flips for fast updates that don't change the FB
-	 * pitch, the DCC state, rotation, etc.
-	 */
-	if (crtc_state->async_flip &&
-	    dm_crtc_state->update_type != UPDATE_TYPE_FAST) {
-		drm_dbg_atomic(crtc->dev,
-			       "[CRTC:%d:%s] async flips are only supported for fast updates\n",
-			       crtc->base.id, crtc->name);
-		return -EINVAL;
-	}
-
 	/* In some use cases, like reset, no stream is attached */
 	if (!dm_crtc_state->stream)
 		return 0;
-- 
2.47.0


From f7a0a931e78deac3127f39a21f7f531bd188c017 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 20:36:57 +0200
Subject: [PATCH v1.4 010/120] [BEGIN] linux-surface patches

-- 
2.47.0


From 4ac927361a015fc748b585226cad384b024d5282 Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sun, 9 Jun 2024 19:48:58 +0200
Subject: [PATCH v1.4 011/120] Revert "efi/x86: Set the PE/COFF header's NX
 compat flag unconditionally"

This reverts commit 891f8890a4a3663da7056542757022870b499bc1.

Revert because of compatibility issues of MS Surface devices and GRUB
with NX. In short, these devices get stuck on boot with NX advertised.
So to not advertise it, add the respective option back in.

Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: secureboot
---
 arch/x86/boot/header.S | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/arch/x86/boot/header.S b/arch/x86/boot/header.S
index b5c79f43359b..a1bbedd989e4 100644
--- a/arch/x86/boot/header.S
+++ b/arch/x86/boot/header.S
@@ -111,7 +111,11 @@ extra_header_fields:
 	.long	salign				# SizeOfHeaders
 	.long	0				# CheckSum
 	.word	IMAGE_SUBSYSTEM_EFI_APPLICATION	# Subsystem (EFI application)
+#ifdef CONFIG_EFI_DXE_MEM_ATTRIBUTES
 	.word	IMAGE_DLL_CHARACTERISTICS_NX_COMPAT	# DllCharacteristics
+#else
+	.word	0				# DllCharacteristics
+#endif
 #ifdef CONFIG_X86_32
 	.long	0				# SizeOfStackReserve
 	.long	0				# SizeOfStackCommit
-- 
2.47.0


From 3f9b1aaf6cc182419291909ff6eea81daea997b2 Mon Sep 17 00:00:00 2001
From: "J. Eduardo" <j.eduardo@gmail.com>
Date: Sun, 25 Aug 2024 14:17:45 +0200
Subject: [PATCH v1.4 012/120] PM: hibernate: Add a lockdown_hibernate
 parameter

This allows the user to tell the kernel that they know better (namely,
they secured their swap properly), and that it can enable hibernation.

Signed-off-by: Kelvie Wong <kelvie@kelvie.ca>
Link: https://github.com/linux-surface/kernel/pull/158
Link: https://gist.github.com/brknkfr/95d1925ccdbb7a2d18947c168dfabbee
Patchset: secureboot
---
 Documentation/admin-guide/kernel-parameters.txt |  5 +++++
 kernel/power/hibernate.c                        | 10 +++++++++-
 2 files changed, 14 insertions(+), 1 deletion(-)

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index be010fec7654..cc6e7ae5786e 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -3020,6 +3020,11 @@
 			to extract confidential information from the kernel
 			are also disabled.
 
+	lockdown_hibernate	[HIBERNATION]
+			Enable hibernation even if lockdown is enabled. Enable this only if
+			your swap is encrypted and secured properly, as an attacker can
+			modify the kernel offline during hibernation.
+
 	locktorture.acq_writer_lim= [KNL]
 			Set the time limit in jiffies for a lock
 			acquisition.  Acquisitions exceeding this limit
diff --git a/kernel/power/hibernate.c b/kernel/power/hibernate.c
index 0a213f69a9e4..8e4f9dcc9f4c 100644
--- a/kernel/power/hibernate.c
+++ b/kernel/power/hibernate.c
@@ -37,6 +37,7 @@
 #include "power.h"
 
 
+static int lockdown_hibernate;
 static int nocompress;
 static int noresume;
 static int nohibernate;
@@ -92,7 +93,7 @@ void hibernate_release(void)
 bool hibernation_available(void)
 {
 	return nohibernate == 0 &&
-		!security_locked_down(LOCKDOWN_HIBERNATION) &&
+		(lockdown_hibernate || !security_locked_down(LOCKDOWN_HIBERNATION)) &&
 		!secretmem_active() && !cxl_mem_active();
 }
 
@@ -1422,6 +1423,12 @@ static int __init nohibernate_setup(char *str)
 	return 1;
 }
 
+static int __init lockdown_hibernate_setup(char *str)
+{
+	lockdown_hibernate = 1;
+	return 1;
+}
+
 static const char * const comp_alg_enabled[] = {
 #if IS_ENABLED(CONFIG_CRYPTO_LZO)
 	COMPRESSION_ALGO_LZO,
@@ -1480,3 +1487,4 @@ __setup("hibernate=", hibernate_setup);
 __setup("resumewait", resumewait_setup);
 __setup("resumedelay=", resumedelay_setup);
 __setup("nohibernate", nohibernate_setup);
+__setup("lockdown_hibernate", lockdown_hibernate_setup);
-- 
2.47.0


From 8390c622de67ea334445d2a701e8f797858a0829 Mon Sep 17 00:00:00 2001
From: Tsuchiya Yuto <kitakar@gmail.com>
Date: Sun, 18 Oct 2020 16:42:44 +0900
Subject: [PATCH v1.4 013/120] (surface3-oemb) add DMI matches for Surface 3
 with broken DMI table

On some Surface 3, the DMI table gets corrupted for unknown reasons
and breaks existing DMI matching used for device-specific quirks.

This commit adds the (broken) DMI data into dmi_system_id tables used
for quirks so that each driver can enable quirks even on the affected
systems.

On affected systems, DMI data will look like this:
    $ grep . /sys/devices/virtual/dmi/id/{bios_vendor,board_name,board_vendor,\
    chassis_vendor,product_name,sys_vendor}
    /sys/devices/virtual/dmi/id/bios_vendor:American Megatrends Inc.
    /sys/devices/virtual/dmi/id/board_name:OEMB
    /sys/devices/virtual/dmi/id/board_vendor:OEMB
    /sys/devices/virtual/dmi/id/chassis_vendor:OEMB
    /sys/devices/virtual/dmi/id/product_name:OEMB
    /sys/devices/virtual/dmi/id/sys_vendor:OEMB

Expected:
    $ grep . /sys/devices/virtual/dmi/id/{bios_vendor,board_name,board_vendor,\
    chassis_vendor,product_name,sys_vendor}
    /sys/devices/virtual/dmi/id/bios_vendor:American Megatrends Inc.
    /sys/devices/virtual/dmi/id/board_name:Surface 3
    /sys/devices/virtual/dmi/id/board_vendor:Microsoft Corporation
    /sys/devices/virtual/dmi/id/chassis_vendor:Microsoft Corporation
    /sys/devices/virtual/dmi/id/product_name:Surface 3
    /sys/devices/virtual/dmi/id/sys_vendor:Microsoft Corporation

Signed-off-by: Tsuchiya Yuto <kitakar@gmail.com>
Patchset: surface3-oemb
---
 drivers/platform/surface/surface3-wmi.c           | 7 +++++++
 sound/soc/codecs/rt5645.c                         | 9 +++++++++
 sound/soc/intel/common/soc-acpi-intel-cht-match.c | 8 ++++++++
 3 files changed, 24 insertions(+)

diff --git a/drivers/platform/surface/surface3-wmi.c b/drivers/platform/surface/surface3-wmi.c
index c15ed7a12784..1ec8edb5aafa 100644
--- a/drivers/platform/surface/surface3-wmi.c
+++ b/drivers/platform/surface/surface3-wmi.c
@@ -37,6 +37,13 @@ static const struct dmi_system_id surface3_dmi_table[] = {
 			DMI_MATCH(DMI_PRODUCT_NAME, "Surface 3"),
 		},
 	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BIOS_VENDOR, "American Megatrends Inc."),
+			DMI_MATCH(DMI_SYS_VENDOR, "OEMB"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "OEMB"),
+		},
+	},
 #endif
 	{ }
 };
diff --git a/sound/soc/codecs/rt5645.c b/sound/soc/codecs/rt5645.c
index 51187b1e0ed2..bfb83ce8d8f8 100644
--- a/sound/soc/codecs/rt5645.c
+++ b/sound/soc/codecs/rt5645.c
@@ -3790,6 +3790,15 @@ static const struct dmi_system_id dmi_platform_data[] = {
 		},
 		.driver_data = (void *)&intel_braswell_platform_data,
 	},
+	{
+		.ident = "Microsoft Surface 3",
+		.matches = {
+			DMI_MATCH(DMI_BIOS_VENDOR, "American Megatrends Inc."),
+			DMI_MATCH(DMI_SYS_VENDOR, "OEMB"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "OEMB"),
+		},
+		.driver_data = (void *)&intel_braswell_platform_data,
+	},
 	{
 		/*
 		 * Match for the GPDwin which unfortunately uses somewhat
diff --git a/sound/soc/intel/common/soc-acpi-intel-cht-match.c b/sound/soc/intel/common/soc-acpi-intel-cht-match.c
index e4c3492a0c28..0b930c91bccb 100644
--- a/sound/soc/intel/common/soc-acpi-intel-cht-match.c
+++ b/sound/soc/intel/common/soc-acpi-intel-cht-match.c
@@ -27,6 +27,14 @@ static const struct dmi_system_id cht_table[] = {
 			DMI_MATCH(DMI_PRODUCT_NAME, "Surface 3"),
 		},
 	},
+	{
+		.callback = cht_surface_quirk_cb,
+		.matches = {
+			DMI_MATCH(DMI_BIOS_VENDOR, "American Megatrends Inc."),
+			DMI_MATCH(DMI_SYS_VENDOR, "OEMB"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "OEMB"),
+		},
+	},
 	{ }
 };
 
-- 
2.47.0


From 2b3fc38533b401d791d7ab5fbbb83c4d22b79af9 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Jonas=20Dre=C3=9Fler?= <verdre@v0yd.nl>
Date: Tue, 3 Nov 2020 13:28:04 +0100
Subject: [PATCH v1.4 014/120] mwifiex: Add quirk resetting the PCI bridge on
 MS Surface devices

The most recent firmware of the 88W8897 card reports a hardcoded LTR
value to the system during initialization, probably as an (unsuccessful)
attempt of the developers to fix firmware crashes. This LTR value
prevents most of the Microsoft Surface devices from entering deep
powersaving states (either platform C-State 10 or S0ix state), because
the exit latency of that state would be higher than what the card can
tolerate.

Turns out the card works just the same (including the firmware crashes)
no matter if that hardcoded LTR value is reported or not, so it's kind
of useless and only prevents us from saving power.

To get rid of those hardcoded LTR reports, it's possible to reset the
PCI bridge device after initializing the cards firmware. I'm not exactly
sure why that works, maybe the power management subsystem of the PCH
resets its stored LTR values when doing a function level reset of the
bridge device. Doing the reset once after starting the wifi firmware
works very well, probably because the firmware only reports that LTR
value a single time during firmware startup.

Patchset: mwifiex
---
 drivers/net/wireless/marvell/mwifiex/pcie.c   | 12 +++++++++
 .../wireless/marvell/mwifiex/pcie_quirks.c    | 26 +++++++++++++------
 .../wireless/marvell/mwifiex/pcie_quirks.h    |  1 +
 3 files changed, 31 insertions(+), 8 deletions(-)

diff --git a/drivers/net/wireless/marvell/mwifiex/pcie.c b/drivers/net/wireless/marvell/mwifiex/pcie.c
index 5f997becdbaa..9a9929424513 100644
--- a/drivers/net/wireless/marvell/mwifiex/pcie.c
+++ b/drivers/net/wireless/marvell/mwifiex/pcie.c
@@ -1702,9 +1702,21 @@ mwifiex_pcie_send_boot_cmd(struct mwifiex_adapter *adapter, struct sk_buff *skb)
 static void mwifiex_pcie_init_fw_port(struct mwifiex_adapter *adapter)
 {
 	struct pcie_service_card *card = adapter->card;
+	struct pci_dev *pdev = card->dev;
+	struct pci_dev *parent_pdev = pci_upstream_bridge(pdev);
 	const struct mwifiex_pcie_card_reg *reg = card->pcie.reg;
 	int tx_wrap = card->txbd_wrptr & reg->tx_wrap_mask;
 
+	/* Trigger a function level reset of the PCI bridge device, this makes
+	 * the firmware of PCIe 88W8897 cards stop reporting a fixed LTR value
+	 * that prevents the system from entering package C10 and S0ix powersaving
+	 * states.
+	 * We need to do it here because it must happen after firmware
+	 * initialization and this function is called after that is done.
+	 */
+	if (card->quirks & QUIRK_DO_FLR_ON_BRIDGE)
+		pci_reset_function(parent_pdev);
+
 	/* Write the RX ring read pointer in to reg->rx_rdptr */
 	mwifiex_write_reg(adapter, reg->rx_rdptr, card->rxbd_rdptr | tx_wrap);
 }
diff --git a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c
index dd6d21f1dbfd..f46b06f8d643 100644
--- a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c
+++ b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c
@@ -13,7 +13,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Pro 4"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{
 		.ident = "Surface Pro 5",
@@ -22,7 +23,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Pro_1796"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{
 		.ident = "Surface Pro 5 (LTE)",
@@ -31,7 +33,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Pro_1807"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{
 		.ident = "Surface Pro 6",
@@ -39,7 +42,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Pro 6"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{
 		.ident = "Surface Book 1",
@@ -47,7 +51,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Book"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{
 		.ident = "Surface Book 2",
@@ -55,7 +60,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Book 2"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{
 		.ident = "Surface Laptop 1",
@@ -63,7 +69,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Laptop"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{
 		.ident = "Surface Laptop 2",
@@ -71,7 +78,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Laptop 2"),
 		},
-		.driver_data = (void *)QUIRK_FW_RST_D3COLD,
+		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
+					QUIRK_DO_FLR_ON_BRIDGE),
 	},
 	{}
 };
@@ -89,6 +97,8 @@ void mwifiex_initialize_quirks(struct pcie_service_card *card)
 		dev_info(&pdev->dev, "no quirks enabled\n");
 	if (card->quirks & QUIRK_FW_RST_D3COLD)
 		dev_info(&pdev->dev, "quirk reset_d3cold enabled\n");
+	if (card->quirks & QUIRK_DO_FLR_ON_BRIDGE)
+		dev_info(&pdev->dev, "quirk do_flr_on_bridge enabled\n");
 }
 
 static void mwifiex_pcie_set_power_d3cold(struct pci_dev *pdev)
diff --git a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h
index d6ff964aec5b..5d30ae39d65e 100644
--- a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h
+++ b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h
@@ -4,6 +4,7 @@
 #include "pcie.h"
 
 #define QUIRK_FW_RST_D3COLD	BIT(0)
+#define QUIRK_DO_FLR_ON_BRIDGE	BIT(1)
 
 void mwifiex_initialize_quirks(struct pcie_service_card *card);
 int mwifiex_pcie_reset_d3cold_quirk(struct pci_dev *pdev);
-- 
2.47.0


From 18b0a7424127eabd44a136e7abe2c8123d5da6e0 Mon Sep 17 00:00:00 2001
From: Tsuchiya Yuto <kitakar@gmail.com>
Date: Sun, 4 Oct 2020 00:11:49 +0900
Subject: [PATCH v1.4 015/120] mwifiex: pcie: disable bridge_d3 for Surface
 gen4+

Currently, mwifiex fw will crash after suspend on recent kernel series.
On Windows, it seems that the root port of wifi will never enter D3 state
(stay on D0 state). And on Linux, disabling the D3 state for the
bridge fixes fw crashing after suspend.

This commit disables the D3 state of root port on driver initialization
and fixes fw crashing after suspend.

Signed-off-by: Tsuchiya Yuto <kitakar@gmail.com>
Patchset: mwifiex
---
 drivers/net/wireless/marvell/mwifiex/pcie.c   |  7 +++++
 .../wireless/marvell/mwifiex/pcie_quirks.c    | 27 +++++++++++++------
 .../wireless/marvell/mwifiex/pcie_quirks.h    |  1 +
 3 files changed, 27 insertions(+), 8 deletions(-)

diff --git a/drivers/net/wireless/marvell/mwifiex/pcie.c b/drivers/net/wireless/marvell/mwifiex/pcie.c
index 9a9929424513..2273e3029776 100644
--- a/drivers/net/wireless/marvell/mwifiex/pcie.c
+++ b/drivers/net/wireless/marvell/mwifiex/pcie.c
@@ -377,6 +377,7 @@ static int mwifiex_pcie_probe(struct pci_dev *pdev,
 					const struct pci_device_id *ent)
 {
 	struct pcie_service_card *card;
+	struct pci_dev *parent_pdev = pci_upstream_bridge(pdev);
 	int ret;
 
 	pr_debug("info: vendor=0x%4.04X device=0x%4.04X rev=%d\n",
@@ -418,6 +419,12 @@ static int mwifiex_pcie_probe(struct pci_dev *pdev,
 		return -1;
 	}
 
+	/* disable bridge_d3 for Surface gen4+ devices to fix fw crashing
+	 * after suspend
+	 */
+	if (card->quirks & QUIRK_NO_BRIDGE_D3)
+		parent_pdev->bridge_d3 = false;
+
 	return 0;
 }
 
diff --git a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c
index f46b06f8d643..99b024ecbade 100644
--- a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c
+++ b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.c
@@ -14,7 +14,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Pro 4"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{
 		.ident = "Surface Pro 5",
@@ -24,7 +25,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Pro_1796"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{
 		.ident = "Surface Pro 5 (LTE)",
@@ -34,7 +36,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Pro_1807"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{
 		.ident = "Surface Pro 6",
@@ -43,7 +46,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Pro 6"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{
 		.ident = "Surface Book 1",
@@ -52,7 +56,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Book"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{
 		.ident = "Surface Book 2",
@@ -61,7 +66,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Book 2"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{
 		.ident = "Surface Laptop 1",
@@ -70,7 +76,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Laptop"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{
 		.ident = "Surface Laptop 2",
@@ -79,7 +86,8 @@ static const struct dmi_system_id mwifiex_quirk_table[] = {
 			DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Surface Laptop 2"),
 		},
 		.driver_data = (void *)(QUIRK_FW_RST_D3COLD |
-					QUIRK_DO_FLR_ON_BRIDGE),
+					QUIRK_DO_FLR_ON_BRIDGE |
+					QUIRK_NO_BRIDGE_D3),
 	},
 	{}
 };
@@ -99,6 +107,9 @@ void mwifiex_initialize_quirks(struct pcie_service_card *card)
 		dev_info(&pdev->dev, "quirk reset_d3cold enabled\n");
 	if (card->quirks & QUIRK_DO_FLR_ON_BRIDGE)
 		dev_info(&pdev->dev, "quirk do_flr_on_bridge enabled\n");
+	if (card->quirks & QUIRK_NO_BRIDGE_D3)
+		dev_info(&pdev->dev,
+			 "quirk no_brigde_d3 enabled\n");
 }
 
 static void mwifiex_pcie_set_power_d3cold(struct pci_dev *pdev)
diff --git a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h
index 5d30ae39d65e..c14eb56eb911 100644
--- a/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h
+++ b/drivers/net/wireless/marvell/mwifiex/pcie_quirks.h
@@ -5,6 +5,7 @@
 
 #define QUIRK_FW_RST_D3COLD	BIT(0)
 #define QUIRK_DO_FLR_ON_BRIDGE	BIT(1)
+#define QUIRK_NO_BRIDGE_D3	BIT(2)
 
 void mwifiex_initialize_quirks(struct pcie_service_card *card);
 int mwifiex_pcie_reset_d3cold_quirk(struct pci_dev *pdev);
-- 
2.47.0


From 1413f113fe5ee0697f94a44ed2081b3d550d278b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Jonas=20Dre=C3=9Fler?= <verdre@v0yd.nl>
Date: Thu, 25 Mar 2021 11:33:02 +0100
Subject: [PATCH v1.4 016/120] Bluetooth: btusb: Lower passive lescan interval
 on Marvell 88W8897

The Marvell 88W8897 combined wifi and bluetooth card (pcie+usb version)
is used in a lot of Microsoft Surface devices, and all those devices
suffer from very low 2.4GHz wifi connection speeds while bluetooth is
enabled. The reason for that is that the default passive scanning
interval for Bluetooth Low Energy devices is quite high in Linux
(interval of 60 msec and scan window of 30 msec, see hci_core.c), and
the Marvell chip is known for its bad bt+wifi coexisting performance.

So decrease that passive scan interval and make the scan window shorter
on this particular device to allow for spending more time transmitting
wifi signals: The new scan interval is 250 msec (0x190 * 0.625 msec) and
the new scan window is 6.25 msec (0xa * 0,625 msec).

This change has a very large impact on the 2.4GHz wifi speeds and gets
it up to performance comparable with the Windows driver, which seems to
apply a similar quirk.

The interval and window length were tested and found to work very well
with a lot of Bluetooth Low Energy devices, including the Surface Pen, a
Bluetooth Speaker and two modern Bluetooth headphones. All devices were
discovered immediately after turning them on. Even lower values were
also tested, but they introduced longer delays until devices get
discovered.

Patchset: mwifiex
---
 drivers/bluetooth/btusb.c | 15 +++++++++++++++
 1 file changed, 15 insertions(+)

diff --git a/drivers/bluetooth/btusb.c b/drivers/bluetooth/btusb.c
index 2408e50743ca..9d6139a24982 100644
--- a/drivers/bluetooth/btusb.c
+++ b/drivers/bluetooth/btusb.c
@@ -65,6 +65,7 @@ static struct usb_driver btusb_driver;
 #define BTUSB_INTEL_BROKEN_INITIAL_NCMD BIT(25)
 #define BTUSB_INTEL_NO_WBS_SUPPORT	BIT(26)
 #define BTUSB_ACTIONS_SEMI		BIT(27)
+#define BTUSB_LOWER_LESCAN_INTERVAL	BIT(28)
 
 static const struct usb_device_id btusb_table[] = {
 	/* Generic Bluetooth USB device */
@@ -468,6 +469,7 @@ static const struct usb_device_id quirks_table[] = {
 	{ USB_DEVICE(0x1286, 0x2044), .driver_info = BTUSB_MARVELL },
 	{ USB_DEVICE(0x1286, 0x2046), .driver_info = BTUSB_MARVELL },
 	{ USB_DEVICE(0x1286, 0x204e), .driver_info = BTUSB_MARVELL },
+	{ USB_DEVICE(0x1286, 0x204c), .driver_info = BTUSB_LOWER_LESCAN_INTERVAL },
 
 	/* Intel Bluetooth devices */
 	{ USB_DEVICE(0x8087, 0x0025), .driver_info = BTUSB_INTEL_COMBINED },
@@ -3866,6 +3868,19 @@ static int btusb_probe(struct usb_interface *intf,
 	if (id->driver_info & BTUSB_MARVELL)
 		hdev->set_bdaddr = btusb_set_bdaddr_marvell;
 
+	/* The Marvell 88W8897 combined wifi and bluetooth card is known for
+	 * very bad bt+wifi coexisting performance.
+	 *
+	 * Decrease the passive BT Low Energy scan interval a bit
+	 * (0x0190 * 0.625 msec = 250 msec) and make the scan window shorter
+	 * (0x000a * 0,625 msec = 6.25 msec). This allows for significantly
+	 * higher wifi throughput while passively scanning for BT LE devices.
+	 */
+	if (id->driver_info & BTUSB_LOWER_LESCAN_INTERVAL) {
+		hdev->le_scan_interval = 0x0190;
+		hdev->le_scan_window = 0x000a;
+	}
+
 	if (IS_ENABLED(CONFIG_BT_HCIBTUSB_MTK) &&
 	    (id->driver_info & BTUSB_MEDIATEK)) {
 		hdev->setup = btusb_mtk_setup;
-- 
2.47.0


From 1533ca4b3f4cd34f7feb51dd11e2a11355e7438e Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sat, 27 Feb 2021 00:45:52 +0100
Subject: [PATCH v1.4 017/120] ath10k: Add module parameters to override board
 files

Some Surface devices, specifically the Surface Go and AMD version of the
Surface Laptop 3 (wich both come with QCA6174 WiFi chips), work better
with a different board file, as it seems that the firmeware included
upstream is buggy.

As it is generally not a good idea to randomly overwrite files, let
alone doing so via packages, we add module parameters to override those
file names in the driver. This allows us to package/deploy the override
via a modprobe.d config.

Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: ath10k
---
 drivers/net/wireless/ath/ath10k/core.c | 57 ++++++++++++++++++++++++++
 1 file changed, 57 insertions(+)

diff --git a/drivers/net/wireless/ath/ath10k/core.c b/drivers/net/wireless/ath/ath10k/core.c
index b3294287bce1..2936fdae823c 100644
--- a/drivers/net/wireless/ath/ath10k/core.c
+++ b/drivers/net/wireless/ath/ath10k/core.c
@@ -40,6 +40,9 @@ static bool fw_diag_log;
 /* frame mode values are mapped as per enum ath10k_hw_txrx_mode */
 unsigned int ath10k_frame_mode = ATH10K_HW_TXRX_NATIVE_WIFI;
 
+static char *override_board = "";
+static char *override_board2 = "";
+
 unsigned long ath10k_coredump_mask = BIT(ATH10K_FW_CRASH_DUMP_REGISTERS) |
 				     BIT(ATH10K_FW_CRASH_DUMP_CE_DATA);
 
@@ -52,6 +55,9 @@ module_param(fw_diag_log, bool, 0644);
 module_param_named(frame_mode, ath10k_frame_mode, uint, 0644);
 module_param_named(coredump_mask, ath10k_coredump_mask, ulong, 0444);
 
+module_param(override_board, charp, 0644);
+module_param(override_board2, charp, 0644);
+
 MODULE_PARM_DESC(debug_mask, "Debugging mask");
 MODULE_PARM_DESC(uart_print, "Uart target debugging");
 MODULE_PARM_DESC(skip_otp, "Skip otp failure for calibration in testmode");
@@ -61,6 +67,9 @@ MODULE_PARM_DESC(frame_mode,
 MODULE_PARM_DESC(coredump_mask, "Bitfield of what to include in firmware crash file");
 MODULE_PARM_DESC(fw_diag_log, "Diag based fw log debugging");
 
+MODULE_PARM_DESC(override_board, "Override for board.bin file");
+MODULE_PARM_DESC(override_board2, "Override for board-2.bin file");
+
 static const struct ath10k_hw_params ath10k_hw_params_list[] = {
 	{
 		.id = QCA988X_HW_2_0_VERSION,
@@ -931,6 +940,42 @@ static int ath10k_init_configure_target(struct ath10k *ar)
 	return 0;
 }
 
+static const char *ath10k_override_board_fw_file(struct ath10k *ar,
+						 const char *file)
+{
+	if (strcmp(file, "board.bin") == 0) {
+		if (strcmp(override_board, "") == 0)
+			return file;
+
+		if (strcmp(override_board, "none") == 0) {
+			dev_info(ar->dev, "firmware override: pretending 'board.bin' does not exist\n");
+			return NULL;
+		}
+
+		dev_info(ar->dev, "firmware override: replacing 'board.bin' with '%s'\n",
+			 override_board);
+
+		return override_board;
+	}
+
+	if (strcmp(file, "board-2.bin") == 0) {
+		if (strcmp(override_board2, "") == 0)
+			return file;
+
+		if (strcmp(override_board2, "none") == 0) {
+			dev_info(ar->dev, "firmware override: pretending 'board-2.bin' does not exist\n");
+			return NULL;
+		}
+
+		dev_info(ar->dev, "firmware override: replacing 'board-2.bin' with '%s'\n",
+			 override_board2);
+
+		return override_board2;
+	}
+
+	return file;
+}
+
 static const struct firmware *ath10k_fetch_fw_file(struct ath10k *ar,
 						   const char *dir,
 						   const char *file)
@@ -945,6 +990,18 @@ static const struct firmware *ath10k_fetch_fw_file(struct ath10k *ar,
 	if (dir == NULL)
 		dir = ".";
 
+	/* HACK: Override board.bin and board-2.bin files if specified.
+	 *
+	 * Some Surface devices perform better with a different board
+	 * configuration. To this end, one would need to replace the board.bin
+	 * file with the modified config and remove the board-2.bin file.
+	 * Unfortunately, that's not a solution that we can easily package. So
+	 * we add module options to perform these overrides here.
+	 */
+	file = ath10k_override_board_fw_file(ar, file);
+	if (!file)
+		return ERR_PTR(-ENOENT);
+
 	if (ar->board_name) {
 		snprintf(filename, sizeof(filename), "%s/%s/%s",
 			 dir, ar->board_name, file);
-- 
2.47.0


From 767fceac9f0f70c832d2d519ac47cc2fdce46cd1 Mon Sep 17 00:00:00 2001
From: Dorian Stoll <dorian.stoll@tmsp.io>
Date: Thu, 30 Jul 2020 13:21:53 +0200
Subject: [PATCH v1.4 018/120] mei: me: Add Icelake device ID for iTouch

Signed-off-by: Dorian Stoll <dorian.stoll@tmsp.io>
Patchset: ipts
---
 drivers/misc/mei/hw-me-regs.h | 1 +
 drivers/misc/mei/pci-me.c     | 1 +
 2 files changed, 2 insertions(+)

diff --git a/drivers/misc/mei/hw-me-regs.h b/drivers/misc/mei/hw-me-regs.h
index c3a6657dcd4a..82eef2f4eb0a 100644
--- a/drivers/misc/mei/hw-me-regs.h
+++ b/drivers/misc/mei/hw-me-regs.h
@@ -92,6 +92,7 @@
 #define MEI_DEV_ID_CDF        0x18D3  /* Cedar Fork */
 
 #define MEI_DEV_ID_ICP_LP     0x34E0  /* Ice Lake Point LP */
+#define MEI_DEV_ID_ICP_LP_3   0x34E4  /* Ice Lake Point LP 3 (iTouch) */
 #define MEI_DEV_ID_ICP_N      0x38E0  /* Ice Lake Point N */
 
 #define MEI_DEV_ID_JSP_N      0x4DE0  /* Jasper Lake Point N */
diff --git a/drivers/misc/mei/pci-me.c b/drivers/misc/mei/pci-me.c
index 6589635f8ba3..a1df48a434e2 100644
--- a/drivers/misc/mei/pci-me.c
+++ b/drivers/misc/mei/pci-me.c
@@ -97,6 +97,7 @@ static const struct pci_device_id mei_me_pci_tbl[] = {
 	{MEI_PCI_DEVICE(MEI_DEV_ID_CMP_H_3, MEI_ME_PCH8_ITOUCH_CFG)},
 
 	{MEI_PCI_DEVICE(MEI_DEV_ID_ICP_LP, MEI_ME_PCH12_CFG)},
+	{MEI_PCI_DEVICE(MEI_DEV_ID_ICP_LP_3, MEI_ME_PCH12_CFG)},
 	{MEI_PCI_DEVICE(MEI_DEV_ID_ICP_N, MEI_ME_PCH12_CFG)},
 
 	{MEI_PCI_DEVICE(MEI_DEV_ID_TGP_LP, MEI_ME_PCH15_CFG)},
-- 
2.47.0


From 4e8e14b792a60c26327a40bddb8ab0328d3e938f Mon Sep 17 00:00:00 2001
From: Liban Hannan <liban.p@gmail.com>
Date: Tue, 12 Apr 2022 23:31:12 +0100
Subject: [PATCH v1.4 019/120] iommu: Use IOMMU passthrough mode for IPTS

Adds a quirk so that IOMMU uses passthrough mode for the IPTS device.
Otherwise, when IOMMU is enabled, IPTS produces DMAR errors like:

DMAR: [DMA Read NO_PASID] Request device [00:16.4] fault addr
0x104ea3000 [fault reason 0x06] PTE Read access is not set

This is very similar to the bug described at:
https://bugs.launchpad.net/bugs/1958004

Fixed with the following patch which this patch basically copies:
https://launchpadlibrarian.net/586396847/43255ca.diff

Signed-off-by: Dorian Stoll <dorian.stoll@tmsp.io>
Patchset: ipts
---
 drivers/iommu/intel/iommu.c | 29 +++++++++++++++++++++++++++++
 1 file changed, 29 insertions(+)

diff --git a/drivers/iommu/intel/iommu.c b/drivers/iommu/intel/iommu.c
index dda6dea7cce0..85b48ba6e0af 100644
--- a/drivers/iommu/intel/iommu.c
+++ b/drivers/iommu/intel/iommu.c
@@ -40,6 +40,11 @@
 #define IS_ISA_DEVICE(pdev) ((pdev->class >> 8) == PCI_CLASS_BRIDGE_ISA)
 #define IS_AZALIA(pdev) ((pdev)->vendor == 0x8086 && (pdev)->device == 0x3a3e)
 
+#define IS_IPTS(pdev) ( \
+		((pdev)->vendor == PCI_VENDOR_ID_INTEL && (pdev)->device == 0x9D3E) || \
+		((pdev)->vendor == PCI_VENDOR_ID_INTEL && (pdev)->device == 0x34E4) \
+	)
+
 #define IOAPIC_RANGE_START	(0xfee00000)
 #define IOAPIC_RANGE_END	(0xfeefffff)
 #define IOVA_START_ADDR		(0x1000)
@@ -217,12 +222,14 @@ int intel_iommu_sm = IS_ENABLED(CONFIG_INTEL_IOMMU_SCALABLE_MODE_DEFAULT_ON);
 int intel_iommu_enabled = 0;
 EXPORT_SYMBOL_GPL(intel_iommu_enabled);
 
+static int dmar_map_ipts = 1;
 static int intel_iommu_superpage = 1;
 static int iommu_identity_mapping;
 static int iommu_skip_te_disable;
 static int disable_igfx_iommu;
 
 #define IDENTMAP_AZALIA		4
+#define IDENTMAP_IPTS		16
 
 const struct iommu_ops intel_iommu_ops;
 static const struct iommu_dirty_ops intel_dirty_ops;
@@ -2156,6 +2163,9 @@ static int device_def_domain_type(struct device *dev)
 
 		if ((iommu_identity_mapping & IDENTMAP_AZALIA) && IS_AZALIA(pdev))
 			return IOMMU_DOMAIN_IDENTITY;
+
+		if ((iommu_identity_mapping & IDENTMAP_IPTS) && IS_IPTS(pdev))
+			return IOMMU_DOMAIN_IDENTITY;
 	}
 
 	return 0;
@@ -2456,6 +2466,9 @@ static int __init init_dmars(void)
 		iommu_set_root_entry(iommu);
 	}
 
+	if (!dmar_map_ipts)
+		iommu_identity_mapping |= IDENTMAP_IPTS;
+
 	check_tylersburg_isoch();
 
 	ret = si_domain_init(hw_pass_through);
@@ -4701,6 +4714,18 @@ static void quirk_iommu_igfx(struct pci_dev *dev)
 	disable_igfx_iommu = 1;
 }
 
+static void quirk_iommu_ipts(struct pci_dev *dev)
+{
+	if (!IS_IPTS(dev))
+		return;
+
+	if (risky_device(dev))
+		return;
+
+	pci_info(dev, "Disabling IOMMU for IPTS\n");
+	dmar_map_ipts = 0;
+}
+
 /* G4x/GM45 integrated gfx dmar support is totally busted. */
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2a40, quirk_iommu_igfx);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x2e00, quirk_iommu_igfx);
@@ -4736,6 +4761,10 @@ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1632, quirk_iommu_igfx);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163A, quirk_iommu_igfx);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163D, quirk_iommu_igfx);
 
+/* disable IPTS dmar support */
+DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x9D3E, quirk_iommu_ipts);
+DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x34E4, quirk_iommu_ipts);
+
 static void quirk_iommu_rwbf(struct pci_dev *dev)
 {
 	if (risky_device(dev))
-- 
2.47.0


From 9478679e77270e999d085f18ce1d1c197b7d6809 Mon Sep 17 00:00:00 2001
From: Dorian Stoll <dorian.stoll@tmsp.io>
Date: Sun, 11 Dec 2022 12:00:59 +0100
Subject: [PATCH v1.4 020/120] hid: Add support for Intel Precise Touch and
 Stylus

Based on linux-surface/intel-precise-touch@8abe268

Signed-off-by: Dorian Stoll <dorian.stoll@tmsp.io>
Patchset: ipts
---
 drivers/hid/Kconfig            |   2 +
 drivers/hid/Makefile           |   2 +
 drivers/hid/ipts/Kconfig       |  14 +
 drivers/hid/ipts/Makefile      |  16 ++
 drivers/hid/ipts/cmd.c         |  61 +++++
 drivers/hid/ipts/cmd.h         |  60 ++++
 drivers/hid/ipts/context.h     |  52 ++++
 drivers/hid/ipts/control.c     | 486 +++++++++++++++++++++++++++++++++
 drivers/hid/ipts/control.h     | 126 +++++++++
 drivers/hid/ipts/desc.h        |  80 ++++++
 drivers/hid/ipts/eds1.c        | 104 +++++++
 drivers/hid/ipts/eds1.h        |  35 +++
 drivers/hid/ipts/eds2.c        | 145 ++++++++++
 drivers/hid/ipts/eds2.h        |  35 +++
 drivers/hid/ipts/hid.c         | 225 +++++++++++++++
 drivers/hid/ipts/hid.h         |  24 ++
 drivers/hid/ipts/main.c        | 126 +++++++++
 drivers/hid/ipts/mei.c         | 188 +++++++++++++
 drivers/hid/ipts/mei.h         |  66 +++++
 drivers/hid/ipts/receiver.c    | 251 +++++++++++++++++
 drivers/hid/ipts/receiver.h    |  16 ++
 drivers/hid/ipts/resources.c   | 131 +++++++++
 drivers/hid/ipts/resources.h   |  41 +++
 drivers/hid/ipts/spec-data.h   | 100 +++++++
 drivers/hid/ipts/spec-device.h | 290 ++++++++++++++++++++
 drivers/hid/ipts/spec-hid.h    |  34 +++
 drivers/hid/ipts/thread.c      |  84 ++++++
 drivers/hid/ipts/thread.h      |  59 ++++
 28 files changed, 2853 insertions(+)
 create mode 100644 drivers/hid/ipts/Kconfig
 create mode 100644 drivers/hid/ipts/Makefile
 create mode 100644 drivers/hid/ipts/cmd.c
 create mode 100644 drivers/hid/ipts/cmd.h
 create mode 100644 drivers/hid/ipts/context.h
 create mode 100644 drivers/hid/ipts/control.c
 create mode 100644 drivers/hid/ipts/control.h
 create mode 100644 drivers/hid/ipts/desc.h
 create mode 100644 drivers/hid/ipts/eds1.c
 create mode 100644 drivers/hid/ipts/eds1.h
 create mode 100644 drivers/hid/ipts/eds2.c
 create mode 100644 drivers/hid/ipts/eds2.h
 create mode 100644 drivers/hid/ipts/hid.c
 create mode 100644 drivers/hid/ipts/hid.h
 create mode 100644 drivers/hid/ipts/main.c
 create mode 100644 drivers/hid/ipts/mei.c
 create mode 100644 drivers/hid/ipts/mei.h
 create mode 100644 drivers/hid/ipts/receiver.c
 create mode 100644 drivers/hid/ipts/receiver.h
 create mode 100644 drivers/hid/ipts/resources.c
 create mode 100644 drivers/hid/ipts/resources.h
 create mode 100644 drivers/hid/ipts/spec-data.h
 create mode 100644 drivers/hid/ipts/spec-device.h
 create mode 100644 drivers/hid/ipts/spec-hid.h
 create mode 100644 drivers/hid/ipts/thread.c
 create mode 100644 drivers/hid/ipts/thread.h

diff --git a/drivers/hid/Kconfig b/drivers/hid/Kconfig
index 08446c89eff6..ccddfba86004 100644
--- a/drivers/hid/Kconfig
+++ b/drivers/hid/Kconfig
@@ -1367,4 +1367,6 @@ source "drivers/hid/amd-sfh-hid/Kconfig"
 
 source "drivers/hid/surface-hid/Kconfig"
 
+source "drivers/hid/ipts/Kconfig"
+
 endif # HID_SUPPORT
diff --git a/drivers/hid/Makefile b/drivers/hid/Makefile
index e40f1ddebbb7..bdb17cffca2f 100644
--- a/drivers/hid/Makefile
+++ b/drivers/hid/Makefile
@@ -169,3 +169,5 @@ obj-$(INTEL_ISH_FIRMWARE_DOWNLOADER)	+= intel-ish-hid/
 obj-$(CONFIG_AMD_SFH_HID)       += amd-sfh-hid/
 
 obj-$(CONFIG_SURFACE_HID_CORE)  += surface-hid/
+
+obj-$(CONFIG_HID_IPTS)          += ipts/
diff --git a/drivers/hid/ipts/Kconfig b/drivers/hid/ipts/Kconfig
new file mode 100644
index 000000000000..297401bd388d
--- /dev/null
+++ b/drivers/hid/ipts/Kconfig
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0-or-later
+
+config HID_IPTS
+	tristate "Intel Precise Touch & Stylus"
+	depends on INTEL_MEI
+	depends on HID
+	help
+	  Say Y here if your system has a touchscreen using Intels
+	  Precise Touch & Stylus (IPTS) technology.
+
+	  If unsure say N.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called ipts.
diff --git a/drivers/hid/ipts/Makefile b/drivers/hid/ipts/Makefile
new file mode 100644
index 000000000000..883896f68e6a
--- /dev/null
+++ b/drivers/hid/ipts/Makefile
@@ -0,0 +1,16 @@
+# SPDX-License-Identifier: GPL-2.0-or-later
+#
+# Makefile for the IPTS touchscreen driver
+#
+
+obj-$(CONFIG_HID_IPTS) += ipts.o
+ipts-objs := cmd.o
+ipts-objs += control.o
+ipts-objs += eds1.o
+ipts-objs += eds2.o
+ipts-objs += hid.o
+ipts-objs += main.o
+ipts-objs += mei.o
+ipts-objs += receiver.o
+ipts-objs += resources.o
+ipts-objs += thread.o
diff --git a/drivers/hid/ipts/cmd.c b/drivers/hid/ipts/cmd.c
new file mode 100644
index 000000000000..63a4934bbc5f
--- /dev/null
+++ b/drivers/hid/ipts/cmd.c
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/errno.h>
+#include <linux/types.h>
+
+#include "cmd.h"
+#include "context.h"
+#include "mei.h"
+#include "spec-device.h"
+
+int ipts_cmd_recv_timeout(struct ipts_context *ipts, enum ipts_command_code code,
+			  struct ipts_response *rsp, u64 timeout)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!rsp)
+		return -EFAULT;
+
+	/*
+	 * In a response, the command code will have the most significant bit flipped to 1.
+	 * If code is passed to ipts_mei_recv as is, no messages will be received.
+	 */
+	ret = ipts_mei_recv(&ipts->mei, code | IPTS_RSP_BIT, rsp, timeout);
+	if (ret < 0)
+		return ret;
+
+	dev_dbg(ipts->dev, "Received 0x%02X with status 0x%02X\n", code, rsp->status);
+
+	/*
+	 * Some devices will always return this error.
+	 * It is allowed to ignore it and to try continuing.
+	 */
+	if (rsp->status == IPTS_STATUS_COMPAT_CHECK_FAIL)
+		rsp->status = IPTS_STATUS_SUCCESS;
+
+	return 0;
+}
+
+int ipts_cmd_send(struct ipts_context *ipts, enum ipts_command_code code, void *data, size_t size)
+{
+	struct ipts_command cmd = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	cmd.cmd = code;
+
+	if (data && size > 0)
+		memcpy(cmd.payload, data, size);
+
+	dev_dbg(ipts->dev, "Sending 0x%02X with %ld bytes payload\n", code, size);
+	return ipts_mei_send(&ipts->mei, &cmd, sizeof(cmd.cmd) + size);
+}
diff --git a/drivers/hid/ipts/cmd.h b/drivers/hid/ipts/cmd.h
new file mode 100644
index 000000000000..2b4079075b64
--- /dev/null
+++ b/drivers/hid/ipts/cmd.h
@@ -0,0 +1,60 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_CMD_H
+#define IPTS_CMD_H
+
+#include <linux/types.h>
+
+#include "context.h"
+#include "spec-device.h"
+
+/*
+ * The default timeout for receiving responses
+ */
+#define IPTS_CMD_DEFAULT_TIMEOUT 1000
+
+/**
+ * ipts_cmd_recv_timeout() - Receives a response to a command.
+ * @ipts: The IPTS driver context.
+ * @code: The type of the command / response.
+ * @rsp: The address that the received response will be copied to.
+ * @timeout: How many milliseconds the function will wait at most.
+ *
+ * A negative timeout means to wait forever.
+ *
+ * Returns: 0 on success, <0 on error, -EAGAIN if no response has been received.
+ */
+int ipts_cmd_recv_timeout(struct ipts_context *ipts, enum ipts_command_code code,
+			  struct ipts_response *rsp, u64 timeout);
+
+/**
+ * ipts_cmd_recv() - Receives a response to a command.
+ * @ipts: The IPTS driver context.
+ * @code: The type of the command / response.
+ * @rsp: The address that the received response will be copied to.
+ *
+ * Returns: 0 on success, <0 on error, -EAGAIN if no response has been received.
+ */
+static inline int ipts_cmd_recv(struct ipts_context *ipts, enum ipts_command_code code,
+				struct ipts_response *rsp)
+{
+	return ipts_cmd_recv_timeout(ipts, code, rsp, IPTS_CMD_DEFAULT_TIMEOUT);
+}
+
+/**
+ * ipts_cmd_send() - Executes a command on the device.
+ * @ipts: The IPTS driver context.
+ * @code: The type of the command to execute.
+ * @data: The payload containing parameters for the command.
+ * @size: The size of the payload.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_cmd_send(struct ipts_context *ipts, enum ipts_command_code code, void *data, size_t size);
+
+#endif /* IPTS_CMD_H */
diff --git a/drivers/hid/ipts/context.h b/drivers/hid/ipts/context.h
new file mode 100644
index 000000000000..ba33259f1f7c
--- /dev/null
+++ b/drivers/hid/ipts/context.h
@@ -0,0 +1,52 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_CONTEXT_H
+#define IPTS_CONTEXT_H
+
+#include <linux/completion.h>
+#include <linux/device.h>
+#include <linux/hid.h>
+#include <linux/mei_cl_bus.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>
+#include <linux/types.h>
+
+#include "mei.h"
+#include "resources.h"
+#include "spec-device.h"
+#include "thread.h"
+
+struct ipts_context {
+	struct device *dev;
+	struct ipts_mei mei;
+
+	enum ipts_mode mode;
+
+	/*
+	 * Prevents concurrent GET_FEATURE reports.
+	 */
+	struct mutex feature_lock;
+	struct completion feature_event;
+
+	/*
+	 * These are not inside of struct ipts_resources
+	 * because they don't own the memory they point to.
+	 */
+	struct ipts_buffer feature_report;
+	struct ipts_buffer descriptor;
+
+	bool hid_active;
+	struct hid_device *hid;
+
+	struct ipts_device_info info;
+	struct ipts_resources resources;
+
+	struct ipts_thread receiver_loop;
+};
+
+#endif /* IPTS_CONTEXT_H */
diff --git a/drivers/hid/ipts/control.c b/drivers/hid/ipts/control.c
new file mode 100644
index 000000000000..5360842d260b
--- /dev/null
+++ b/drivers/hid/ipts/control.c
@@ -0,0 +1,486 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/delay.h>
+#include <linux/dev_printk.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/types.h>
+
+#include "cmd.h"
+#include "context.h"
+#include "control.h"
+#include "desc.h"
+#include "hid.h"
+#include "receiver.h"
+#include "resources.h"
+#include "spec-data.h"
+#include "spec-device.h"
+
+static int ipts_control_get_device_info(struct ipts_context *ipts, struct ipts_device_info *info)
+{
+	int ret = 0;
+	struct ipts_response rsp = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!info)
+		return -EFAULT;
+
+	ret = ipts_cmd_send(ipts, IPTS_CMD_GET_DEVICE_INFO, NULL, 0);
+	if (ret) {
+		dev_err(ipts->dev, "GET_DEVICE_INFO: send failed: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_cmd_recv(ipts, IPTS_CMD_GET_DEVICE_INFO, &rsp);
+	if (ret) {
+		dev_err(ipts->dev, "GET_DEVICE_INFO: recv failed: %d\n", ret);
+		return ret;
+	}
+
+	if (rsp.status != IPTS_STATUS_SUCCESS) {
+		dev_err(ipts->dev, "GET_DEVICE_INFO: cmd failed: %d\n", rsp.status);
+		return -EBADR;
+	}
+
+	memcpy(info, rsp.payload, sizeof(*info));
+	return 0;
+}
+
+static int ipts_control_set_mode(struct ipts_context *ipts, enum ipts_mode mode)
+{
+	int ret = 0;
+	struct ipts_set_mode cmd = { 0 };
+	struct ipts_response rsp = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	cmd.mode = mode;
+
+	ret = ipts_cmd_send(ipts, IPTS_CMD_SET_MODE, &cmd, sizeof(cmd));
+	if (ret) {
+		dev_err(ipts->dev, "SET_MODE: send failed: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_cmd_recv(ipts, IPTS_CMD_SET_MODE, &rsp);
+	if (ret) {
+		dev_err(ipts->dev, "SET_MODE: recv failed: %d\n", ret);
+		return ret;
+	}
+
+	if (rsp.status != IPTS_STATUS_SUCCESS) {
+		dev_err(ipts->dev, "SET_MODE: cmd failed: %d\n", rsp.status);
+		return -EBADR;
+	}
+
+	return 0;
+}
+
+static int ipts_control_set_mem_window(struct ipts_context *ipts, struct ipts_resources *res)
+{
+	int i = 0;
+	int ret = 0;
+	struct ipts_mem_window cmd = { 0 };
+	struct ipts_response rsp = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!res)
+		return -EFAULT;
+
+	for (i = 0; i < IPTS_BUFFERS; i++) {
+		cmd.data_addr_lower[i] = lower_32_bits(res->data[i].dma_address);
+		cmd.data_addr_upper[i] = upper_32_bits(res->data[i].dma_address);
+		cmd.feedback_addr_lower[i] = lower_32_bits(res->feedback[i].dma_address);
+		cmd.feedback_addr_upper[i] = upper_32_bits(res->feedback[i].dma_address);
+	}
+
+	cmd.workqueue_addr_lower = lower_32_bits(res->workqueue.dma_address);
+	cmd.workqueue_addr_upper = upper_32_bits(res->workqueue.dma_address);
+
+	cmd.doorbell_addr_lower = lower_32_bits(res->doorbell.dma_address);
+	cmd.doorbell_addr_upper = upper_32_bits(res->doorbell.dma_address);
+
+	cmd.hid2me_addr_lower = lower_32_bits(res->hid2me.dma_address);
+	cmd.hid2me_addr_upper = upper_32_bits(res->hid2me.dma_address);
+
+	cmd.workqueue_size = IPTS_WORKQUEUE_SIZE;
+	cmd.workqueue_item_size = IPTS_WORKQUEUE_ITEM_SIZE;
+
+	ret = ipts_cmd_send(ipts, IPTS_CMD_SET_MEM_WINDOW, &cmd, sizeof(cmd));
+	if (ret) {
+		dev_err(ipts->dev, "SET_MEM_WINDOW: send failed: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_cmd_recv(ipts, IPTS_CMD_SET_MEM_WINDOW, &rsp);
+	if (ret) {
+		dev_err(ipts->dev, "SET_MEM_WINDOW: recv failed: %d\n", ret);
+		return ret;
+	}
+
+	if (rsp.status != IPTS_STATUS_SUCCESS) {
+		dev_err(ipts->dev, "SET_MEM_WINDOW: cmd failed: %d\n", rsp.status);
+		return -EBADR;
+	}
+
+	return 0;
+}
+
+static int ipts_control_get_descriptor(struct ipts_context *ipts)
+{
+	int ret = 0;
+	struct ipts_data_header *header = NULL;
+	struct ipts_get_descriptor cmd = { 0 };
+	struct ipts_response rsp = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!ipts->resources.descriptor.address)
+		return -EFAULT;
+
+	memset(ipts->resources.descriptor.address, 0, ipts->resources.descriptor.size);
+
+	cmd.addr_lower = lower_32_bits(ipts->resources.descriptor.dma_address);
+	cmd.addr_upper = upper_32_bits(ipts->resources.descriptor.dma_address);
+	cmd.magic = 8;
+
+	ret = ipts_cmd_send(ipts, IPTS_CMD_GET_DESCRIPTOR, &cmd, sizeof(cmd));
+	if (ret) {
+		dev_err(ipts->dev, "GET_DESCRIPTOR: send failed: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_cmd_recv(ipts, IPTS_CMD_GET_DESCRIPTOR, &rsp);
+	if (ret) {
+		dev_err(ipts->dev, "GET_DESCRIPTOR: recv failed: %d\n", ret);
+		return ret;
+	}
+
+	if (rsp.status != IPTS_STATUS_SUCCESS) {
+		dev_err(ipts->dev, "GET_DESCRIPTOR: cmd failed: %d\n", rsp.status);
+		return -EBADR;
+	}
+
+	header = (struct ipts_data_header *)ipts->resources.descriptor.address;
+
+	if (header->type == IPTS_DATA_TYPE_DESCRIPTOR) {
+		ipts->descriptor.address = &header->data[8];
+		ipts->descriptor.size = header->size - 8;
+
+		return 0;
+	}
+
+	return -ENODATA;
+}
+
+int ipts_control_request_flush(struct ipts_context *ipts)
+{
+	int ret = 0;
+	struct ipts_quiesce_io cmd = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	ret = ipts_cmd_send(ipts, IPTS_CMD_QUIESCE_IO, &cmd, sizeof(cmd));
+	if (ret)
+		dev_err(ipts->dev, "QUIESCE_IO: send failed: %d\n", ret);
+
+	return ret;
+}
+
+int ipts_control_wait_flush(struct ipts_context *ipts)
+{
+	int ret = 0;
+	struct ipts_response rsp = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	ret = ipts_cmd_recv(ipts, IPTS_CMD_QUIESCE_IO, &rsp);
+	if (ret) {
+		dev_err(ipts->dev, "QUIESCE_IO: recv failed: %d\n", ret);
+		return ret;
+	}
+
+	if (rsp.status == IPTS_STATUS_TIMEOUT)
+		return -EAGAIN;
+
+	if (rsp.status != IPTS_STATUS_SUCCESS) {
+		dev_err(ipts->dev, "QUIESCE_IO: cmd failed: %d\n", rsp.status);
+		return -EBADR;
+	}
+
+	return 0;
+}
+
+int ipts_control_request_data(struct ipts_context *ipts)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	ret = ipts_cmd_send(ipts, IPTS_CMD_READY_FOR_DATA, NULL, 0);
+	if (ret)
+		dev_err(ipts->dev, "READY_FOR_DATA: send failed: %d\n", ret);
+
+	return ret;
+}
+
+int ipts_control_wait_data(struct ipts_context *ipts, bool shutdown)
+{
+	int ret = 0;
+	struct ipts_response rsp = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!shutdown)
+		ret = ipts_cmd_recv_timeout(ipts, IPTS_CMD_READY_FOR_DATA, &rsp, 0);
+	else
+		ret = ipts_cmd_recv(ipts, IPTS_CMD_READY_FOR_DATA, &rsp);
+
+	if (ret) {
+		if (ret != -EAGAIN)
+			dev_err(ipts->dev, "READY_FOR_DATA: recv failed: %d\n", ret);
+
+		return ret;
+	}
+
+	/*
+	 * During shutdown, it is possible that the sensor has already been disabled.
+	 */
+	if (rsp.status == IPTS_STATUS_SENSOR_DISABLED)
+		return 0;
+
+	if (rsp.status == IPTS_STATUS_TIMEOUT)
+		return -EAGAIN;
+
+	if (rsp.status != IPTS_STATUS_SUCCESS) {
+		dev_err(ipts->dev, "READY_FOR_DATA: cmd failed: %d\n", rsp.status);
+		return -EBADR;
+	}
+
+	return 0;
+}
+
+int ipts_control_send_feedback(struct ipts_context *ipts, u32 buffer)
+{
+	int ret = 0;
+	struct ipts_feedback cmd = { 0 };
+	struct ipts_response rsp = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	cmd.buffer = buffer;
+
+	ret = ipts_cmd_send(ipts, IPTS_CMD_FEEDBACK, &cmd, sizeof(cmd));
+	if (ret) {
+		dev_err(ipts->dev, "FEEDBACK: send failed: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_cmd_recv(ipts, IPTS_CMD_FEEDBACK, &rsp);
+	if (ret) {
+		dev_err(ipts->dev, "FEEDBACK: recv failed: %d\n", ret);
+		return ret;
+	}
+
+	/*
+	 * We don't know what feedback data looks like so we are sending zeros.
+	 * See also ipts_control_refill_buffer.
+	 */
+	if (rsp.status == IPTS_STATUS_INVALID_PARAMS)
+		return 0;
+
+	if (rsp.status != IPTS_STATUS_SUCCESS) {
+		dev_err(ipts->dev, "FEEDBACK: cmd failed: %d\n", rsp.status);
+		return -EBADR;
+	}
+
+	return 0;
+}
+
+int ipts_control_hid2me_feedback(struct ipts_context *ipts, enum ipts_feedback_cmd_type cmd,
+				 enum ipts_feedback_data_type type, void *data, size_t size)
+{
+	struct ipts_feedback_header *header = NULL;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!ipts->resources.hid2me.address)
+		return -EFAULT;
+
+	memset(ipts->resources.hid2me.address, 0, ipts->resources.hid2me.size);
+	header = (struct ipts_feedback_header *)ipts->resources.hid2me.address;
+
+	header->cmd_type = cmd;
+	header->data_type = type;
+	header->size = size;
+	header->buffer = IPTS_HID2ME_BUFFER;
+
+	if (size + sizeof(*header) > ipts->resources.hid2me.size)
+		return -EINVAL;
+
+	if (data && size > 0)
+		memcpy(header->payload, data, size);
+
+	return ipts_control_send_feedback(ipts, IPTS_HID2ME_BUFFER);
+}
+
+int ipts_control_start(struct ipts_context *ipts)
+{
+	int ret = 0;
+	struct ipts_device_info info = { 0 };
+
+	if (!ipts)
+		return -EFAULT;
+
+	dev_info(ipts->dev, "Starting IPTS\n");
+
+	ret = ipts_control_get_device_info(ipts, &info);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to get device info: %d\n", ret);
+		return ret;
+	}
+
+	ipts->info = info;
+
+	ret = ipts_resources_init(&ipts->resources, ipts->dev, info.data_size, info.feedback_size);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to allocate buffers: %d", ret);
+		return ret;
+	}
+
+	dev_info(ipts->dev, "IPTS EDS Version: %d\n", info.intf_eds);
+
+	/*
+	 * Handle newer devices
+	 */
+	if (info.intf_eds > 1) {
+		/*
+		 * Fetching the descriptor will only work on newer devices.
+		 * For older devices, a fallback descriptor will be used.
+		 */
+		ret = ipts_control_get_descriptor(ipts);
+		if (ret) {
+			dev_err(ipts->dev, "Failed to fetch HID descriptor: %d\n", ret);
+			return ret;
+		}
+
+		/*
+		 * Newer devices can be directly initialized in polling mode.
+		 */
+		ipts->mode = IPTS_MODE_POLL;
+	}
+
+	ret = ipts_control_set_mode(ipts, ipts->mode);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to set mode: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_control_set_mem_window(ipts, &ipts->resources);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to set memory window: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_receiver_start(ipts);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to start receiver: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_control_request_data(ipts);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to request data: %d\n", ret);
+		return ret;
+	}
+
+	ipts_hid_enable(ipts);
+
+	ret = ipts_hid_init(ipts, info);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to initialize HID device: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int _ipts_control_stop(struct ipts_context *ipts)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	ipts_hid_disable(ipts);
+	dev_info(ipts->dev, "Stopping IPTS\n");
+
+	ret = ipts_receiver_stop(ipts);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to stop receiver: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_resources_free(&ipts->resources);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to free resources: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+int ipts_control_stop(struct ipts_context *ipts)
+{
+	int ret = 0;
+
+	ret = _ipts_control_stop(ipts);
+	if (ret)
+		return ret;
+
+	ret = ipts_hid_free(ipts);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to free HID device: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+int ipts_control_restart(struct ipts_context *ipts)
+{
+	int ret = 0;
+
+	ret = _ipts_control_stop(ipts);
+	if (ret)
+		return ret;
+
+	/*
+	 * Wait a second to give the sensor time to fully shut down.
+	 */
+	msleep(1000);
+
+	ret = ipts_control_start(ipts);
+	if (ret)
+		return ret;
+
+	return 0;
+}
diff --git a/drivers/hid/ipts/control.h b/drivers/hid/ipts/control.h
new file mode 100644
index 000000000000..26629c5144ed
--- /dev/null
+++ b/drivers/hid/ipts/control.h
@@ -0,0 +1,126 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_CONTROL_H
+#define IPTS_CONTROL_H
+
+#include <linux/types.h>
+
+#include "context.h"
+#include "spec-data.h"
+#include "spec-device.h"
+
+/**
+ * ipts_control_request_flush() - Stop the data flow.
+ * @ipts: The IPTS driver context.
+ *
+ * Runs the command to stop the data flow on the device.
+ * All outstanding data needs to be acknowledged using feedback before the command will return.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_request_flush(struct ipts_context *ipts);
+
+/**
+ * ipts_control_wait_flush() - Wait until data flow has been stopped.
+ * @ipts: The IPTS driver context.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_wait_flush(struct ipts_context *ipts);
+
+/**
+ * ipts_control_wait_flush() - Notify the device that the driver can receive new data.
+ * @ipts: The IPTS driver context.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_request_data(struct ipts_context *ipts);
+
+/**
+ * ipts_control_wait_data() - Wait until new data is available.
+ * @ipts: The IPTS driver context.
+ * @block: Whether to block execution until data is available.
+ *
+ * In poll mode, this function will never return while the data flow is active. Instead,
+ * the poll will be incremented when new data is available.
+ *
+ * Returns: 0 on success, <0 on error, -EAGAIN if no data is available.
+ */
+int ipts_control_wait_data(struct ipts_context *ipts, bool block);
+
+/**
+ * ipts_control_send_feedback() - Submits a feedback buffer to the device.
+ * @ipts: The IPTS driver context.
+ * @buffer: The ID of the buffer containing feedback data.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_send_feedback(struct ipts_context *ipts, u32 buffer);
+
+/**
+ * ipts_control_hid2me_feedback() - Sends HID2ME feedback, a special type of feedback.
+ * @ipts: The IPTS driver context.
+ * @cmd: The command that will be run on the device.
+ * @type: The type of the payload that is sent to the device.
+ * @data: The payload of the feedback command.
+ * @size: The size of the payload.
+ *
+ * HID2ME feedback is a special type of feedback, because it allows interfacing with
+ * the HID API of the device at any moment, without requiring a buffer that has to
+ * be acknowledged.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_hid2me_feedback(struct ipts_context *ipts, enum ipts_feedback_cmd_type cmd,
+				 enum ipts_feedback_data_type type, void *data, size_t size);
+
+/**
+ * ipts_control_refill_buffer() - Acknowledges that data in a buffer has been processed.
+ * @ipts: The IPTS driver context.
+ * @buffer: The buffer that has been processed and can be refilled.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+static inline int ipts_control_refill_buffer(struct ipts_context *ipts, u32 buffer)
+{
+	/*
+	 * IPTS expects structured data in the feedback buffer matching the buffer that will be
+	 * refilled. We don't know what that data looks like, so we just keep the buffer empty.
+	 * This results in an INVALID_PARAMS error, but the buffer gets refilled without an issue.
+	 * Sending a minimal structure with the buffer ID fixes the error, but breaks refilling
+	 * the buffers on some devices.
+	 */
+
+	return ipts_control_send_feedback(ipts, buffer);
+}
+
+/**
+ * ipts_control_start() - Initialized the device and starts the data flow.
+ * @ipts: The IPTS driver context.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_start(struct ipts_context *ipts);
+
+/**
+ * ipts_control_stop() - Stops the data flow and resets the device.
+ * @ipts: The IPTS driver context.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_stop(struct ipts_context *ipts);
+
+/**
+ * ipts_control_restart() - Stops the device and starts it again.
+ * @ipts: The IPTS driver context.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_control_restart(struct ipts_context *ipts);
+
+#endif /* IPTS_CONTROL_H */
diff --git a/drivers/hid/ipts/desc.h b/drivers/hid/ipts/desc.h
new file mode 100644
index 000000000000..307438c7c80c
--- /dev/null
+++ b/drivers/hid/ipts/desc.h
@@ -0,0 +1,80 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2022-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_DESC_H
+#define IPTS_DESC_H
+
+#include <linux/types.h>
+
+#define IPTS_HID_REPORT_SINGLETOUCH 64
+#define IPTS_HID_REPORT_DATA	    65
+#define IPTS_HID_REPORT_SET_MODE    66
+
+#define IPTS_HID_REPORT_DATA_SIZE 7485
+
+/*
+ * HID descriptor for singletouch data.
+ * This descriptor should be present on all IPTS devices.
+ */
+static const u8 ipts_singletouch_descriptor[] = {
+	0x05, 0x0D,	  /*  Usage Page (Digitizer),            */
+	0x09, 0x04,	  /*  Usage (Touchscreen),               */
+	0xA1, 0x01,	  /*  Collection (Application),          */
+	0x85, 0x40,	  /*      Report ID (64),                */
+	0x09, 0x42,	  /*      Usage (Tip Switch),            */
+	0x15, 0x00,	  /*      Logical Minimum (0),           */
+	0x25, 0x01,	  /*      Logical Maximum (1),           */
+	0x75, 0x01,	  /*      Report Size (1),               */
+	0x95, 0x01,	  /*      Report Count (1),              */
+	0x81, 0x02,	  /*      Input (Variable),              */
+	0x95, 0x07,	  /*      Report Count (7),              */
+	0x81, 0x03,	  /*      Input (Constant, Variable),    */
+	0x05, 0x01,	  /*      Usage Page (Desktop),          */
+	0x09, 0x30,	  /*      Usage (X),                     */
+	0x75, 0x10,	  /*      Report Size (16),              */
+	0x95, 0x01,	  /*      Report Count (1),              */
+	0xA4,		  /*      Push,                          */
+	0x55, 0x0E,	  /*      Unit Exponent (14),            */
+	0x65, 0x11,	  /*      Unit (Centimeter),             */
+	0x46, 0x76, 0x0B, /*      Physical Maximum (2934),       */
+	0x26, 0xFF, 0x7F, /*      Logical Maximum (32767),       */
+	0x81, 0x02,	  /*      Input (Variable),              */
+	0x09, 0x31,	  /*      Usage (Y),                     */
+	0x46, 0x74, 0x06, /*      Physical Maximum (1652),       */
+	0x26, 0xFF, 0x7F, /*      Logical Maximum (32767),       */
+	0x81, 0x02,	  /*      Input (Variable),              */
+	0xB4,		  /*      Pop,                           */
+	0xC0,		  /*  End Collection                     */
+};
+
+/*
+ * Fallback HID descriptor for older devices that do not have
+ * the ability to query their HID descriptor.
+ */
+static const u8 ipts_fallback_descriptor[] = {
+	0x05, 0x0D,	  /*  Usage Page (Digitizer),            */
+	0x09, 0x0F,	  /*  Usage (Capacitive Hm Digitizer),   */
+	0xA1, 0x01,	  /*  Collection (Application),          */
+	0x85, 0x41,	  /*      Report ID (65),                */
+	0x09, 0x56,	  /*      Usage (Scan Time),             */
+	0x95, 0x01,	  /*      Report Count (1),              */
+	0x75, 0x10,	  /*      Report Size (16),              */
+	0x81, 0x02,	  /*      Input (Variable),              */
+	0x09, 0x61,	  /*      Usage (Gesture Char Quality),  */
+	0x75, 0x08,	  /*      Report Size (8),               */
+	0x96, 0x3D, 0x1D, /*      Report Count (7485),           */
+	0x81, 0x03,	  /*      Input (Constant, Variable),    */
+	0x85, 0x42,	  /*      Report ID (66),                */
+	0x06, 0x00, 0xFF, /*      Usage Page (FF00h),            */
+	0x09, 0xC8,	  /*      Usage (C8h),                   */
+	0x75, 0x08,	  /*      Report Size (8),               */
+	0x95, 0x01,	  /*      Report Count (1),              */
+	0xB1, 0x02,	  /*      Feature (Variable),            */
+	0xC0,		  /*  End Collection,                    */
+};
+
+#endif /* IPTS_DESC_H */
diff --git a/drivers/hid/ipts/eds1.c b/drivers/hid/ipts/eds1.c
new file mode 100644
index 000000000000..7b9f54388a9f
--- /dev/null
+++ b/drivers/hid/ipts/eds1.c
@@ -0,0 +1,104 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/err.h>
+#include <linux/gfp.h>
+#include <linux/hid.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include "context.h"
+#include "control.h"
+#include "desc.h"
+#include "eds1.h"
+#include "spec-device.h"
+
+int ipts_eds1_get_descriptor(struct ipts_context *ipts, u8 **desc_buffer, size_t *desc_size)
+{
+	size_t size = 0;
+	u8 *buffer = NULL;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!desc_buffer)
+		return -EFAULT;
+
+	if (!desc_size)
+		return -EFAULT;
+
+	size = sizeof(ipts_singletouch_descriptor) + sizeof(ipts_fallback_descriptor);
+
+	buffer = kzalloc(size, GFP_KERNEL);
+	if (!buffer)
+		return -ENOMEM;
+
+	memcpy(buffer, ipts_singletouch_descriptor, sizeof(ipts_singletouch_descriptor));
+	memcpy(&buffer[sizeof(ipts_singletouch_descriptor)], ipts_fallback_descriptor,
+	       sizeof(ipts_fallback_descriptor));
+
+	*desc_size = size;
+	*desc_buffer = buffer;
+
+	return 0;
+}
+
+static int ipts_eds1_switch_mode(struct ipts_context *ipts, enum ipts_mode mode)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (ipts->mode == mode)
+		return 0;
+
+	ipts->mode = mode;
+
+	ret = ipts_control_restart(ipts);
+	if (ret)
+		dev_err(ipts->dev, "Failed to switch modes: %d\n", ret);
+
+	return ret;
+}
+
+int ipts_eds1_raw_request(struct ipts_context *ipts, u8 *buffer, size_t size, u8 report_id,
+			  enum hid_report_type report_type, enum hid_class_request request_type)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!buffer)
+		return -EFAULT;
+
+	if (report_id != IPTS_HID_REPORT_SET_MODE)
+		return -EIO;
+
+	if (report_type != HID_FEATURE_REPORT)
+		return -EIO;
+
+	if (size != 2)
+		return -EINVAL;
+
+	/*
+	 * Implement mode switching report for older devices without native HID support.
+	 */
+
+	if (request_type == HID_REQ_GET_REPORT) {
+		memset(buffer, 0, size);
+		buffer[0] = report_id;
+		buffer[1] = ipts->mode;
+	} else if (request_type == HID_REQ_SET_REPORT) {
+		return ipts_eds1_switch_mode(ipts, buffer[1]);
+	} else {
+		return -EIO;
+	}
+
+	return ret;
+}
diff --git a/drivers/hid/ipts/eds1.h b/drivers/hid/ipts/eds1.h
new file mode 100644
index 000000000000..eeeb6575e3e8
--- /dev/null
+++ b/drivers/hid/ipts/eds1.h
@@ -0,0 +1,35 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/hid.h>
+#include <linux/types.h>
+
+#include "context.h"
+
+/**
+ * ipts_eds1_get_descriptor() - Assembles the HID descriptor of the device.
+ * @ipts: The IPTS driver context.
+ * @desc_buffer: A pointer to the location where the address of the allocated buffer is stored.
+ * @desc_size: A pointer to the location where the size of the allocated buffer is stored.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_eds1_get_descriptor(struct ipts_context *ipts, u8 **desc_buffer, size_t *desc_size);
+
+/**
+ * ipts_eds1_raw_request() - Executes an output or feature report on the device.
+ * @ipts: The IPTS driver context.
+ * @buffer: The buffer containing the report.
+ * @size: The size of the buffer.
+ * @report_id: The HID report ID.
+ * @report_type: Whether this report is an output or a feature report.
+ * @request_type: Whether this report requests or sends data.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_eds1_raw_request(struct ipts_context *ipts, u8 *buffer, size_t size, u8 report_id,
+			  enum hid_report_type report_type, enum hid_class_request request_type);
diff --git a/drivers/hid/ipts/eds2.c b/drivers/hid/ipts/eds2.c
new file mode 100644
index 000000000000..639940794615
--- /dev/null
+++ b/drivers/hid/ipts/eds2.c
@@ -0,0 +1,145 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/completion.h>
+#include <linux/err.h>
+#include <linux/gfp.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include "context.h"
+#include "control.h"
+#include "desc.h"
+#include "eds2.h"
+#include "spec-data.h"
+
+int ipts_eds2_get_descriptor(struct ipts_context *ipts, u8 **desc_buffer, size_t *desc_size)
+{
+	size_t size = 0;
+	u8 *buffer = NULL;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!desc_buffer)
+		return -EFAULT;
+
+	if (!desc_size)
+		return -EFAULT;
+
+	size = sizeof(ipts_singletouch_descriptor) + ipts->descriptor.size;
+
+	buffer = kzalloc(size, GFP_KERNEL);
+	if (!buffer)
+		return -ENOMEM;
+
+	memcpy(buffer, ipts_singletouch_descriptor, sizeof(ipts_singletouch_descriptor));
+	memcpy(&buffer[sizeof(ipts_singletouch_descriptor)], ipts->descriptor.address,
+	       ipts->descriptor.size);
+
+	*desc_size = size;
+	*desc_buffer = buffer;
+
+	return 0;
+}
+
+static int ipts_eds2_get_feature(struct ipts_context *ipts, u8 *buffer, size_t size, u8 report_id,
+				 enum ipts_feedback_data_type type)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!buffer)
+		return -EFAULT;
+
+	mutex_lock(&ipts->feature_lock);
+
+	memset(buffer, 0, size);
+	buffer[0] = report_id;
+
+	memset(&ipts->feature_report, 0, sizeof(ipts->feature_report));
+	reinit_completion(&ipts->feature_event);
+
+	ret = ipts_control_hid2me_feedback(ipts, IPTS_FEEDBACK_CMD_TYPE_NONE, type, buffer, size);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to send hid2me feedback: %d\n", ret);
+		goto out;
+	}
+
+	ret = wait_for_completion_timeout(&ipts->feature_event, msecs_to_jiffies(5000));
+	if (ret == 0) {
+		dev_warn(ipts->dev, "GET_FEATURES timed out!\n");
+		ret = -EIO;
+		goto out;
+	}
+
+	if (!ipts->feature_report.address) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	if (ipts->feature_report.size > size) {
+		ret = -ETOOSMALL;
+		goto out;
+	}
+
+	ret = ipts->feature_report.size;
+	memcpy(buffer, ipts->feature_report.address, ipts->feature_report.size);
+
+out:
+	mutex_unlock(&ipts->feature_lock);
+	return ret;
+}
+
+static int ipts_eds2_set_feature(struct ipts_context *ipts, u8 *buffer, size_t size, u8 report_id,
+				 enum ipts_feedback_data_type type)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!buffer)
+		return -EFAULT;
+
+	buffer[0] = report_id;
+
+	ret = ipts_control_hid2me_feedback(ipts, IPTS_FEEDBACK_CMD_TYPE_NONE, type, buffer, size);
+	if (ret)
+		dev_err(ipts->dev, "Failed to send hid2me feedback: %d\n", ret);
+
+	return ret;
+}
+
+int ipts_eds2_raw_request(struct ipts_context *ipts, u8 *buffer, size_t size, u8 report_id,
+			  enum hid_report_type report_type, enum hid_class_request request_type)
+{
+	enum ipts_feedback_data_type feedback_type = IPTS_FEEDBACK_DATA_TYPE_VENDOR;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!buffer)
+		return -EFAULT;
+
+	if (report_type == HID_OUTPUT_REPORT && request_type == HID_REQ_SET_REPORT)
+		feedback_type = IPTS_FEEDBACK_DATA_TYPE_OUTPUT_REPORT;
+	else if (report_type == HID_FEATURE_REPORT && request_type == HID_REQ_GET_REPORT)
+		feedback_type = IPTS_FEEDBACK_DATA_TYPE_GET_FEATURES;
+	else if (report_type == HID_FEATURE_REPORT && request_type == HID_REQ_SET_REPORT)
+		feedback_type = IPTS_FEEDBACK_DATA_TYPE_SET_FEATURES;
+	else
+		return -EIO;
+
+	if (request_type == HID_REQ_GET_REPORT)
+		return ipts_eds2_get_feature(ipts, buffer, size, report_id, feedback_type);
+	else
+		return ipts_eds2_set_feature(ipts, buffer, size, report_id, feedback_type);
+}
diff --git a/drivers/hid/ipts/eds2.h b/drivers/hid/ipts/eds2.h
new file mode 100644
index 000000000000..064e3716907a
--- /dev/null
+++ b/drivers/hid/ipts/eds2.h
@@ -0,0 +1,35 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/hid.h>
+#include <linux/types.h>
+
+#include "context.h"
+
+/**
+ * ipts_eds2_get_descriptor() - Assembles the HID descriptor of the device.
+ * @ipts: The IPTS driver context.
+ * @desc_buffer: A pointer to the location where the address of the allocated buffer is stored.
+ * @desc_size: A pointer to the location where the size of the allocated buffer is stored.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_eds2_get_descriptor(struct ipts_context *ipts, u8 **desc_buffer, size_t *desc_size);
+
+/**
+ * ipts_eds2_raw_request() - Executes an output or feature report on the device.
+ * @ipts: The IPTS driver context.
+ * @buffer: The buffer containing the report.
+ * @size: The size of the buffer.
+ * @report_id: The HID report ID.
+ * @report_type: Whether this report is an output or a feature report.
+ * @request_type: Whether this report requests or sends data.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_eds2_raw_request(struct ipts_context *ipts, u8 *buffer, size_t size, u8 report_id,
+			  enum hid_report_type report_type, enum hid_class_request request_type);
diff --git a/drivers/hid/ipts/hid.c b/drivers/hid/ipts/hid.c
new file mode 100644
index 000000000000..e34a1a4f9fa7
--- /dev/null
+++ b/drivers/hid/ipts/hid.c
@@ -0,0 +1,225 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2022-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/completion.h>
+#include <linux/err.h>
+#include <linux/gfp.h>
+#include <linux/hid.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include "context.h"
+#include "desc.h"
+#include "eds1.h"
+#include "eds2.h"
+#include "hid.h"
+#include "spec-data.h"
+#include "spec-hid.h"
+
+void ipts_hid_enable(struct ipts_context *ipts)
+{
+	WRITE_ONCE(ipts->hid_active, true);
+}
+
+void ipts_hid_disable(struct ipts_context *ipts)
+{
+	WRITE_ONCE(ipts->hid_active, false);
+}
+
+static int ipts_hid_start(struct hid_device *hid)
+{
+	return 0;
+}
+
+static void ipts_hid_stop(struct hid_device *hid)
+{
+}
+
+static int ipts_hid_parse(struct hid_device *hid)
+{
+	int ret = 0;
+	struct ipts_context *ipts = NULL;
+
+	u8 *buffer = NULL;
+	size_t size = 0;
+
+	if (!hid)
+		return -ENODEV;
+
+	ipts = hid->driver_data;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!READ_ONCE(ipts->hid_active))
+		return -ENODEV;
+
+	if (ipts->info.intf_eds == 1)
+		ret = ipts_eds1_get_descriptor(ipts, &buffer, &size);
+	else
+		ret = ipts_eds2_get_descriptor(ipts, &buffer, &size);
+
+	if (ret) {
+		dev_err(ipts->dev, "Failed to allocate HID descriptor: %d\n", ret);
+		return ret;
+	}
+
+	ret = hid_parse_report(hid, buffer, size);
+	kfree(buffer);
+
+	if (ret) {
+		dev_err(ipts->dev, "Failed to parse HID descriptor: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int ipts_hid_raw_request(struct hid_device *hid, unsigned char report_id, __u8 *buffer,
+				size_t size, unsigned char report_type, int request_type)
+{
+	struct ipts_context *ipts = NULL;
+
+	if (!hid)
+		return -ENODEV;
+
+	ipts = hid->driver_data;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!READ_ONCE(ipts->hid_active))
+		return -ENODEV;
+
+	if (ipts->info.intf_eds == 1) {
+		return ipts_eds1_raw_request(ipts, buffer, size, report_id, report_type,
+					     request_type);
+	} else {
+		return ipts_eds2_raw_request(ipts, buffer, size, report_id, report_type,
+					     request_type);
+	}
+}
+
+static struct hid_ll_driver ipts_hid_driver = {
+	.start = ipts_hid_start,
+	.stop = ipts_hid_stop,
+	.open = ipts_hid_start,
+	.close = ipts_hid_stop,
+	.parse = ipts_hid_parse,
+	.raw_request = ipts_hid_raw_request,
+};
+
+int ipts_hid_input_data(struct ipts_context *ipts, u32 buffer)
+{
+	u8 *temp = NULL;
+	struct ipts_hid_header *frame = NULL;
+	struct ipts_data_header *header = NULL;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (!ipts->hid)
+		return -ENODEV;
+
+	if (!READ_ONCE(ipts->hid_active))
+		return -ENODEV;
+
+	header = (struct ipts_data_header *)ipts->resources.data[buffer].address;
+
+	temp = ipts->resources.report.address;
+	memset(temp, 0, ipts->resources.report.size);
+
+	if (!header)
+		return -EFAULT;
+
+	if (header->size == 0)
+		return 0;
+
+	if (header->type == IPTS_DATA_TYPE_HID)
+		return hid_input_report(ipts->hid, HID_INPUT_REPORT, header->data, header->size, 1);
+
+	if (header->type == IPTS_DATA_TYPE_GET_FEATURES) {
+		ipts->feature_report.address = header->data;
+		ipts->feature_report.size = header->size;
+
+		complete_all(&ipts->feature_event);
+		return 0;
+	}
+
+	if (header->type != IPTS_DATA_TYPE_FRAME)
+		return 0;
+
+	if (header->size + 3 + sizeof(struct ipts_hid_header) > IPTS_HID_REPORT_DATA_SIZE)
+		return -ERANGE;
+
+	/*
+	 * Synthesize a HID report matching the devices that natively send HID reports
+	 */
+	temp[0] = IPTS_HID_REPORT_DATA;
+
+	frame = (struct ipts_hid_header *)&temp[3];
+	frame->type = IPTS_HID_FRAME_TYPE_RAW;
+	frame->size = header->size + sizeof(*frame);
+
+	memcpy(frame->data, header->data, header->size);
+
+	return hid_input_report(ipts->hid, HID_INPUT_REPORT, temp, IPTS_HID_REPORT_DATA_SIZE, 1);
+}
+
+int ipts_hid_init(struct ipts_context *ipts, struct ipts_device_info info)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (ipts->hid)
+		return 0;
+
+	ipts->hid = hid_allocate_device();
+	if (IS_ERR(ipts->hid)) {
+		int err = PTR_ERR(ipts->hid);
+
+		dev_err(ipts->dev, "Failed to allocate HID device: %d\n", err);
+		return err;
+	}
+
+	ipts->hid->driver_data = ipts;
+	ipts->hid->dev.parent = ipts->dev;
+	ipts->hid->ll_driver = &ipts_hid_driver;
+
+	ipts->hid->vendor = info.vendor;
+	ipts->hid->product = info.product;
+	ipts->hid->group = HID_GROUP_GENERIC;
+
+	snprintf(ipts->hid->name, sizeof(ipts->hid->name), "IPTS %04X:%04X", info.vendor,
+		 info.product);
+
+	ret = hid_add_device(ipts->hid);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to add HID device: %d\n", ret);
+		ipts_hid_free(ipts);
+		return ret;
+	}
+
+	return 0;
+}
+
+int ipts_hid_free(struct ipts_context *ipts)
+{
+	if (!ipts)
+		return -EFAULT;
+
+	if (!ipts->hid)
+		return 0;
+
+	hid_destroy_device(ipts->hid);
+	ipts->hid = NULL;
+
+	return 0;
+}
diff --git a/drivers/hid/ipts/hid.h b/drivers/hid/ipts/hid.h
new file mode 100644
index 000000000000..1ebe77447903
--- /dev/null
+++ b/drivers/hid/ipts/hid.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2022-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_HID_H
+#define IPTS_HID_H
+
+#include <linux/types.h>
+
+#include "context.h"
+#include "spec-device.h"
+
+void ipts_hid_enable(struct ipts_context *ipts);
+void ipts_hid_disable(struct ipts_context *ipts);
+
+int ipts_hid_input_data(struct ipts_context *ipts, u32 buffer);
+
+int ipts_hid_init(struct ipts_context *ipts, struct ipts_device_info info);
+int ipts_hid_free(struct ipts_context *ipts);
+
+#endif /* IPTS_HID_H */
diff --git a/drivers/hid/ipts/main.c b/drivers/hid/ipts/main.c
new file mode 100644
index 000000000000..fb5b5c13ee3e
--- /dev/null
+++ b/drivers/hid/ipts/main.c
@@ -0,0 +1,126 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/completion.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/mei_cl_bus.h>
+#include <linux/mod_devicetable.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/stddef.h>
+#include <linux/types.h>
+
+#include "context.h"
+#include "control.h"
+#include "mei.h"
+#include "receiver.h"
+#include "spec-device.h"
+
+/*
+ * The MEI client ID for IPTS functionality.
+ */
+#define IPTS_ID UUID_LE(0x3e8d0870, 0x271a, 0x4208, 0x8e, 0xb5, 0x9a, 0xcb, 0x94, 0x02, 0xae, 0x04)
+
+static int ipts_set_dma_mask(struct mei_cl_device *cldev)
+{
+	if (!cldev)
+		return -EFAULT;
+
+	if (!dma_coerce_mask_and_coherent(&cldev->dev, DMA_BIT_MASK(64)))
+		return 0;
+
+	return dma_coerce_mask_and_coherent(&cldev->dev, DMA_BIT_MASK(32));
+}
+
+static int ipts_probe(struct mei_cl_device *cldev, const struct mei_cl_device_id *id)
+{
+	int ret = 0;
+	struct ipts_context *ipts = NULL;
+
+	if (!cldev)
+		return -EFAULT;
+
+	ret = ipts_set_dma_mask(cldev);
+	if (ret) {
+		dev_err(&cldev->dev, "Failed to set DMA mask for IPTS: %d\n", ret);
+		return ret;
+	}
+
+	ret = mei_cldev_enable(cldev);
+	if (ret) {
+		dev_err(&cldev->dev, "Failed to enable MEI device: %d\n", ret);
+		return ret;
+	}
+
+	ipts = devm_kzalloc(&cldev->dev, sizeof(*ipts), GFP_KERNEL);
+	if (!ipts) {
+		mei_cldev_disable(cldev);
+		return -ENOMEM;
+	}
+
+	ret = ipts_mei_init(&ipts->mei, cldev);
+	if (ret) {
+		dev_err(&cldev->dev, "Failed to init MEI bus logic: %d\n", ret);
+		return ret;
+	}
+
+	ipts->dev = &cldev->dev;
+	ipts->mode = IPTS_MODE_EVENT;
+
+	mutex_init(&ipts->feature_lock);
+	init_completion(&ipts->feature_event);
+
+	mei_cldev_set_drvdata(cldev, ipts);
+
+	ret = ipts_control_start(ipts);
+	if (ret) {
+		dev_err(&cldev->dev, "Failed to start IPTS: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void ipts_remove(struct mei_cl_device *cldev)
+{
+	int ret = 0;
+	struct ipts_context *ipts = NULL;
+
+	if (!cldev) {
+		pr_err("MEI device is NULL!");
+		return;
+	}
+
+	ipts = mei_cldev_get_drvdata(cldev);
+
+	ret = ipts_control_stop(ipts);
+	if (ret)
+		dev_err(&cldev->dev, "Failed to stop IPTS: %d\n", ret);
+
+	mei_cldev_disable(cldev);
+}
+
+static struct mei_cl_device_id ipts_device_id_table[] = {
+	{ .uuid = IPTS_ID, .version = MEI_CL_VERSION_ANY },
+	{},
+};
+MODULE_DEVICE_TABLE(mei, ipts_device_id_table);
+
+static struct mei_cl_driver ipts_driver = {
+	.id_table = ipts_device_id_table,
+	.name = "ipts",
+	.probe = ipts_probe,
+	.remove = ipts_remove,
+};
+module_mei_cl_driver(ipts_driver);
+
+MODULE_DESCRIPTION("IPTS touchscreen driver");
+MODULE_AUTHOR("Dorian Stoll <dorian.stoll@tmsp.io>");
+MODULE_LICENSE("GPL");
diff --git a/drivers/hid/ipts/mei.c b/drivers/hid/ipts/mei.c
new file mode 100644
index 000000000000..1e0395ceae4a
--- /dev/null
+++ b/drivers/hid/ipts/mei.c
@@ -0,0 +1,188 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/jiffies.h>
+#include <linux/list.h>
+#include <linux/mei_cl_bus.h>
+#include <linux/printk.h>
+#include <linux/rwsem.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+
+#include "context.h"
+#include "mei.h"
+
+static void locked_list_add(struct list_head *new, struct list_head *head,
+			    struct rw_semaphore *lock)
+{
+	down_write(lock);
+	list_add(new, head);
+	up_write(lock);
+}
+
+static void locked_list_del(struct list_head *entry, struct rw_semaphore *lock)
+{
+	down_write(lock);
+	list_del(entry);
+	up_write(lock);
+}
+
+static void ipts_mei_incoming(struct mei_cl_device *cldev)
+{
+	ssize_t ret = 0;
+	struct ipts_mei_message *entry = NULL;
+	struct ipts_context *ipts = NULL;
+
+	if (!cldev) {
+		pr_err("MEI device is NULL!");
+		return;
+	}
+
+	ipts = mei_cldev_get_drvdata(cldev);
+	if (!ipts) {
+		pr_err("IPTS driver context is NULL!");
+		return;
+	}
+
+	entry = devm_kzalloc(ipts->dev, sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return;
+
+	INIT_LIST_HEAD(&entry->list);
+
+	do {
+		ret = mei_cldev_recv(cldev, (u8 *)&entry->rsp, sizeof(entry->rsp));
+	} while (ret == -EINTR);
+
+	if (ret < 0) {
+		dev_err(ipts->dev, "Error while reading response: %ld\n", ret);
+		return;
+	}
+
+	if (ret == 0) {
+		dev_err(ipts->dev, "Received empty response\n");
+		return;
+	}
+
+	locked_list_add(&entry->list, &ipts->mei.messages, &ipts->mei.message_lock);
+	wake_up_all(&ipts->mei.message_queue);
+}
+
+static int ipts_mei_search(struct ipts_mei *mei, enum ipts_command_code code,
+			   struct ipts_response *rsp)
+{
+	struct ipts_mei_message *entry = NULL;
+
+	if (!mei)
+		return -EFAULT;
+
+	if (!rsp)
+		return -EFAULT;
+
+	down_read(&mei->message_lock);
+
+	/*
+	 * Iterate over the list of received messages, and check if there is one
+	 * matching the requested command code.
+	 */
+	list_for_each_entry(entry, &mei->messages, list) {
+		if (entry->rsp.cmd == code)
+			break;
+	}
+
+	up_read(&mei->message_lock);
+
+	/*
+	 * If entry is not the list head, this means that the loop above has been stopped early,
+	 * and that we found a matching element. We drop the message from the list and return it.
+	 */
+	if (!list_entry_is_head(entry, &mei->messages, list)) {
+		locked_list_del(&entry->list, &mei->message_lock);
+
+		*rsp = entry->rsp;
+		devm_kfree(&mei->cldev->dev, entry);
+
+		return 0;
+	}
+
+	return -EAGAIN;
+}
+
+int ipts_mei_recv(struct ipts_mei *mei, enum ipts_command_code code, struct ipts_response *rsp,
+		  u64 timeout)
+{
+	int ret = 0;
+
+	if (!mei)
+		return -EFAULT;
+
+	/*
+	 * A timeout of 0 means check and return immideately.
+	 */
+	if (timeout == 0)
+		return ipts_mei_search(mei, code, rsp);
+
+	/*
+	 * A timeout of less than 0 means to wait forever.
+	 */
+	if (timeout < 0) {
+		wait_event(mei->message_queue, ipts_mei_search(mei, code, rsp) == 0);
+		return 0;
+	}
+
+	ret = wait_event_timeout(mei->message_queue, ipts_mei_search(mei, code, rsp) == 0,
+				 msecs_to_jiffies(timeout));
+
+	if (ret > 0)
+		return 0;
+
+	return -EAGAIN;
+}
+
+int ipts_mei_send(struct ipts_mei *mei, void *data, size_t length)
+{
+	int ret = 0;
+
+	if (!mei)
+		return -EFAULT;
+
+	if (!mei->cldev)
+		return -EFAULT;
+
+	if (!data)
+		return -EFAULT;
+
+	do {
+		ret = mei_cldev_send(mei->cldev, (u8 *)data, length);
+	} while (ret == -EINTR);
+
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+int ipts_mei_init(struct ipts_mei *mei, struct mei_cl_device *cldev)
+{
+	if (!mei)
+		return -EFAULT;
+
+	if (!cldev)
+		return -EFAULT;
+
+	mei->cldev = cldev;
+
+	INIT_LIST_HEAD(&mei->messages);
+	init_waitqueue_head(&mei->message_queue);
+	init_rwsem(&mei->message_lock);
+
+	mei_cldev_register_rx_cb(cldev, ipts_mei_incoming);
+
+	return 0;
+}
diff --git a/drivers/hid/ipts/mei.h b/drivers/hid/ipts/mei.h
new file mode 100644
index 000000000000..973bade6b0fd
--- /dev/null
+++ b/drivers/hid/ipts/mei.h
@@ -0,0 +1,66 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_MEI_H
+#define IPTS_MEI_H
+
+#include <linux/list.h>
+#include <linux/mei_cl_bus.h>
+#include <linux/rwsem.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+
+#include "spec-device.h"
+
+struct ipts_mei_message {
+	struct list_head list;
+	struct ipts_response rsp;
+};
+
+struct ipts_mei {
+	struct mei_cl_device *cldev;
+
+	struct list_head messages;
+
+	wait_queue_head_t message_queue;
+	struct rw_semaphore message_lock;
+};
+
+/**
+ * ipts_mei_recv() - Receive data from a MEI device.
+ * @mei: The IPTS MEI device context.
+ * @code: The IPTS command code to look for.
+ * @rsp: The address that the received data will be copied to.
+ * @timeout: How many milliseconds the function will wait at most.
+ *
+ * A negative timeout means to wait forever.
+ *
+ * Returns: 0 on success, <0 on error, -EAGAIN if no response has been received.
+ */
+int ipts_mei_recv(struct ipts_mei *mei, enum ipts_command_code code, struct ipts_response *rsp,
+		  u64 timeout);
+
+/**
+ * ipts_mei_send() - Send data to a MEI device.
+ * @ipts: The IPTS MEI device context.
+ * @data: The data to send.
+ * @size: The size of the data.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_mei_send(struct ipts_mei *mei, void *data, size_t length);
+
+/**
+ * ipts_mei_init() - Initialize the MEI device context.
+ * @mei: The MEI device context to initialize.
+ * @cldev: The MEI device the context will be bound to.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_mei_init(struct ipts_mei *mei, struct mei_cl_device *cldev);
+
+#endif /* IPTS_MEI_H */
diff --git a/drivers/hid/ipts/receiver.c b/drivers/hid/ipts/receiver.c
new file mode 100644
index 000000000000..977724c728c3
--- /dev/null
+++ b/drivers/hid/ipts/receiver.c
@@ -0,0 +1,251 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/delay.h>
+#include <linux/err.h>
+#include <linux/kthread.h>
+#include <linux/time64.h>
+#include <linux/timekeeping.h>
+#include <linux/types.h>
+
+#include "cmd.h"
+#include "context.h"
+#include "control.h"
+#include "hid.h"
+#include "receiver.h"
+#include "resources.h"
+#include "spec-device.h"
+#include "thread.h"
+
+static void ipts_receiver_next_doorbell(struct ipts_context *ipts)
+{
+	u32 *doorbell = (u32 *)ipts->resources.doorbell.address;
+	*doorbell = *doorbell + 1;
+}
+
+static u32 ipts_receiver_current_doorbell(struct ipts_context *ipts)
+{
+	u32 *doorbell = (u32 *)ipts->resources.doorbell.address;
+	return *doorbell;
+}
+
+static void ipts_receiver_backoff(time64_t last, u32 n)
+{
+	/*
+	 * If the last change was less than n seconds ago,
+	 * sleep for a shorter period so that new data can be
+	 * processed quickly. If there was no change for more than
+	 * n seconds, sleep longer to avoid wasting CPU cycles.
+	 */
+	if (last + n > ktime_get_seconds())
+		usleep_range(1 * USEC_PER_MSEC, 5 * USEC_PER_MSEC);
+	else
+		msleep(200);
+}
+
+static int ipts_receiver_event_loop(struct ipts_thread *thread)
+{
+	int ret = 0;
+	u32 buffer = 0;
+
+	struct ipts_context *ipts = NULL;
+	time64_t last = ktime_get_seconds();
+
+	if (!thread)
+		return -EFAULT;
+
+	ipts = thread->data;
+
+	if (!ipts)
+		return -EFAULT;
+
+	dev_info(ipts->dev, "IPTS running in event mode\n");
+
+	while (!ipts_thread_should_stop(thread)) {
+		int i = 0;
+
+		for (i = 0; i < IPTS_BUFFERS; i++) {
+			ret = ipts_control_wait_data(ipts, false);
+			if (ret == -EAGAIN)
+				break;
+
+			if (ret) {
+				dev_err(ipts->dev, "Failed to wait for data: %d\n", ret);
+				continue;
+			}
+
+			buffer = ipts_receiver_current_doorbell(ipts) % IPTS_BUFFERS;
+			ipts_receiver_next_doorbell(ipts);
+
+			ret = ipts_hid_input_data(ipts, buffer);
+			if (ret)
+				dev_err(ipts->dev, "Failed to process buffer: %d\n", ret);
+
+			ret = ipts_control_refill_buffer(ipts, buffer);
+			if (ret)
+				dev_err(ipts->dev, "Failed to send feedback: %d\n", ret);
+
+			ret = ipts_control_request_data(ipts);
+			if (ret)
+				dev_err(ipts->dev, "Failed to request data: %d\n", ret);
+
+			last = ktime_get_seconds();
+		}
+
+		ipts_receiver_backoff(last, 5);
+	}
+
+	ret = ipts_control_request_flush(ipts);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to request flush: %d\n", ret);
+		return ret;
+	}
+
+	ret = ipts_control_wait_data(ipts, true);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to wait for data: %d\n", ret);
+
+		if (ret != -EAGAIN)
+			return ret;
+		else
+			return 0;
+	}
+
+	ret = ipts_control_wait_flush(ipts);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to wait for flush: %d\n", ret);
+
+		if (ret != -EAGAIN)
+			return ret;
+		else
+			return 0;
+	}
+
+	return 0;
+}
+
+static int ipts_receiver_poll_loop(struct ipts_thread *thread)
+{
+	int ret = 0;
+	u32 buffer = 0;
+
+	u32 doorbell = 0;
+	u32 lastdb = 0;
+
+	struct ipts_context *ipts = NULL;
+	time64_t last = ktime_get_seconds();
+
+	if (!thread)
+		return -EFAULT;
+
+	ipts = thread->data;
+
+	if (!ipts)
+		return -EFAULT;
+
+	dev_info(ipts->dev, "IPTS running in poll mode\n");
+
+	while (true) {
+		if (ipts_thread_should_stop(thread)) {
+			ret = ipts_control_request_flush(ipts);
+			if (ret) {
+				dev_err(ipts->dev, "Failed to request flush: %d\n", ret);
+				return ret;
+			}
+		}
+
+		doorbell = ipts_receiver_current_doorbell(ipts);
+
+		/*
+		 * After filling up one of the data buffers, IPTS will increment
+		 * the doorbell. The value of the doorbell stands for the *next*
+		 * buffer that IPTS is going to fill.
+		 */
+		while (lastdb != doorbell) {
+			buffer = lastdb % IPTS_BUFFERS;
+
+			ret = ipts_hid_input_data(ipts, buffer);
+			if (ret)
+				dev_err(ipts->dev, "Failed to process buffer: %d\n", ret);
+
+			ret = ipts_control_refill_buffer(ipts, buffer);
+			if (ret)
+				dev_err(ipts->dev, "Failed to send feedback: %d\n", ret);
+
+			last = ktime_get_seconds();
+			lastdb++;
+		}
+
+		if (ipts_thread_should_stop(thread))
+			break;
+
+		ipts_receiver_backoff(last, 5);
+	}
+
+	ret = ipts_control_wait_data(ipts, true);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to wait for data: %d\n", ret);
+
+		if (ret != -EAGAIN)
+			return ret;
+		else
+			return 0;
+	}
+
+	ret = ipts_control_wait_flush(ipts);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to wait for flush: %d\n", ret);
+
+		if (ret != -EAGAIN)
+			return ret;
+		else
+			return 0;
+	}
+
+	return 0;
+}
+
+int ipts_receiver_start(struct ipts_context *ipts)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	if (ipts->mode == IPTS_MODE_EVENT) {
+		ret = ipts_thread_start(&ipts->receiver_loop, ipts_receiver_event_loop, ipts,
+					"ipts_event");
+	} else if (ipts->mode == IPTS_MODE_POLL) {
+		ret = ipts_thread_start(&ipts->receiver_loop, ipts_receiver_poll_loop, ipts,
+					"ipts_poll");
+	} else {
+		ret = -EINVAL;
+	}
+
+	if (ret) {
+		dev_err(ipts->dev, "Failed to start receiver loop: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+int ipts_receiver_stop(struct ipts_context *ipts)
+{
+	int ret = 0;
+
+	if (!ipts)
+		return -EFAULT;
+
+	ret = ipts_thread_stop(&ipts->receiver_loop);
+	if (ret) {
+		dev_err(ipts->dev, "Failed to stop receiver loop: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
diff --git a/drivers/hid/ipts/receiver.h b/drivers/hid/ipts/receiver.h
new file mode 100644
index 000000000000..3de7da62d40c
--- /dev/null
+++ b/drivers/hid/ipts/receiver.h
@@ -0,0 +1,16 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_RECEIVER_H
+#define IPTS_RECEIVER_H
+
+#include "context.h"
+
+int ipts_receiver_start(struct ipts_context *ipts);
+int ipts_receiver_stop(struct ipts_context *ipts);
+
+#endif /* IPTS_RECEIVER_H */
diff --git a/drivers/hid/ipts/resources.c b/drivers/hid/ipts/resources.c
new file mode 100644
index 000000000000..cc14653b2a9f
--- /dev/null
+++ b/drivers/hid/ipts/resources.c
@@ -0,0 +1,131 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/dma-mapping.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include "desc.h"
+#include "resources.h"
+#include "spec-device.h"
+
+static int ipts_resources_alloc_buffer(struct ipts_buffer *buffer, struct device *dev, size_t size)
+{
+	if (!buffer)
+		return -EFAULT;
+
+	if (buffer->address)
+		return 0;
+
+	buffer->address = dma_alloc_coherent(dev, size, &buffer->dma_address, GFP_KERNEL);
+
+	if (!buffer->address)
+		return -ENOMEM;
+
+	buffer->size = size;
+	buffer->device = dev;
+
+	return 0;
+}
+
+static void ipts_resources_free_buffer(struct ipts_buffer *buffer)
+{
+	if (!buffer->address)
+		return;
+
+	dma_free_coherent(buffer->device, buffer->size, buffer->address, buffer->dma_address);
+
+	buffer->address = NULL;
+	buffer->size = 0;
+
+	buffer->dma_address = 0;
+	buffer->device = NULL;
+}
+
+int ipts_resources_init(struct ipts_resources *res, struct device *dev, size_t ds, size_t fs)
+{
+	int ret = 0;
+
+	/*
+	 * Some compilers (AOSP clang) complain about a redefined
+	 * variable when this is declared inside of the for loop.
+	 */
+	int i = 0;
+
+	if (!res)
+		return -EFAULT;
+
+	for (i = 0; i < IPTS_BUFFERS; i++) {
+		ret = ipts_resources_alloc_buffer(&res->data[i], dev, ds);
+		if (ret)
+			goto err;
+	}
+
+	for (i = 0; i < IPTS_BUFFERS; i++) {
+		ret = ipts_resources_alloc_buffer(&res->feedback[i], dev, fs);
+		if (ret)
+			goto err;
+	}
+
+	ret = ipts_resources_alloc_buffer(&res->doorbell, dev, sizeof(u32));
+	if (ret)
+		goto err;
+
+	ret = ipts_resources_alloc_buffer(&res->workqueue, dev, sizeof(u32));
+	if (ret)
+		goto err;
+
+	ret = ipts_resources_alloc_buffer(&res->hid2me, dev, fs);
+	if (ret)
+		goto err;
+
+	ret = ipts_resources_alloc_buffer(&res->descriptor, dev, ds + 8);
+	if (ret)
+		goto err;
+
+	if (!res->report.address) {
+		res->report.size = IPTS_HID_REPORT_DATA_SIZE;
+		res->report.address = kzalloc(res->report.size, GFP_KERNEL);
+
+		if (!res->report.address) {
+			ret = -ENOMEM;
+			goto err;
+		}
+	}
+
+	return 0;
+
+err:
+
+	ipts_resources_free(res);
+	return ret;
+}
+
+int ipts_resources_free(struct ipts_resources *res)
+{
+	int i = 0;
+
+	if (!res)
+		return -EFAULT;
+
+	for (i = 0; i < IPTS_BUFFERS; i++)
+		ipts_resources_free_buffer(&res->data[i]);
+
+	for (i = 0; i < IPTS_BUFFERS; i++)
+		ipts_resources_free_buffer(&res->feedback[i]);
+
+	ipts_resources_free_buffer(&res->doorbell);
+	ipts_resources_free_buffer(&res->workqueue);
+	ipts_resources_free_buffer(&res->hid2me);
+	ipts_resources_free_buffer(&res->descriptor);
+
+	kfree(res->report.address);
+	res->report.address = NULL;
+	res->report.size = 0;
+
+	return 0;
+}
diff --git a/drivers/hid/ipts/resources.h b/drivers/hid/ipts/resources.h
new file mode 100644
index 000000000000..2068e13285f0
--- /dev/null
+++ b/drivers/hid/ipts/resources.h
@@ -0,0 +1,41 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_RESOURCES_H
+#define IPTS_RESOURCES_H
+
+#include <linux/device.h>
+#include <linux/types.h>
+
+#include "spec-device.h"
+
+struct ipts_buffer {
+	u8 *address;
+	size_t size;
+
+	dma_addr_t dma_address;
+	struct device *device;
+};
+
+struct ipts_resources {
+	struct ipts_buffer data[IPTS_BUFFERS];
+	struct ipts_buffer feedback[IPTS_BUFFERS];
+
+	struct ipts_buffer doorbell;
+	struct ipts_buffer workqueue;
+	struct ipts_buffer hid2me;
+
+	struct ipts_buffer descriptor;
+
+	// Buffer for synthesizing HID reports
+	struct ipts_buffer report;
+};
+
+int ipts_resources_init(struct ipts_resources *res, struct device *dev, size_t ds, size_t fs);
+int ipts_resources_free(struct ipts_resources *res);
+
+#endif /* IPTS_RESOURCES_H */
diff --git a/drivers/hid/ipts/spec-data.h b/drivers/hid/ipts/spec-data.h
new file mode 100644
index 000000000000..e8dd98895a7e
--- /dev/null
+++ b/drivers/hid/ipts/spec-data.h
@@ -0,0 +1,100 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2016 Intel Corporation
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_SPEC_DATA_H
+#define IPTS_SPEC_DATA_H
+
+#include <linux/build_bug.h>
+#include <linux/types.h>
+
+/**
+ * enum ipts_feedback_cmd_type - Commands that can be executed on the sensor through feedback.
+ */
+enum ipts_feedback_cmd_type {
+	IPTS_FEEDBACK_CMD_TYPE_NONE = 0,
+	IPTS_FEEDBACK_CMD_TYPE_SOFT_RESET = 1,
+	IPTS_FEEDBACK_CMD_TYPE_GOTO_ARMED = 2,
+	IPTS_FEEDBACK_CMD_TYPE_GOTO_SENSING = 3,
+	IPTS_FEEDBACK_CMD_TYPE_GOTO_SLEEP = 4,
+	IPTS_FEEDBACK_CMD_TYPE_GOTO_DOZE = 5,
+	IPTS_FEEDBACK_CMD_TYPE_HARD_RESET = 6,
+};
+
+/**
+ * enum ipts_feedback_data_type - Defines what data a feedback buffer contains.
+ * @IPTS_FEEDBACK_DATA_TYPE_VENDOR:        The buffer contains vendor specific feedback.
+ * @IPTS_FEEDBACK_DATA_TYPE_SET_FEATURES:  The buffer contains a HID set features report.
+ * @IPTS_FEEDBACK_DATA_TYPE_GET_FEATURES:  The buffer contains a HID get features report.
+ * @IPTS_FEEDBACK_DATA_TYPE_OUTPUT_REPORT: The buffer contains a HID output report.
+ * @IPTS_FEEDBACK_DATA_TYPE_STORE_DATA:    The buffer contains calibration data for the sensor.
+ */
+enum ipts_feedback_data_type {
+	IPTS_FEEDBACK_DATA_TYPE_VENDOR = 0,
+	IPTS_FEEDBACK_DATA_TYPE_SET_FEATURES = 1,
+	IPTS_FEEDBACK_DATA_TYPE_GET_FEATURES = 2,
+	IPTS_FEEDBACK_DATA_TYPE_OUTPUT_REPORT = 3,
+	IPTS_FEEDBACK_DATA_TYPE_STORE_DATA = 4,
+};
+
+/**
+ * struct ipts_feedback_header - Header that is prefixed to the data in a feedback buffer.
+ * @cmd_type:   A command that should be executed on the sensor.
+ * @size:       The size of the payload to be written.
+ * @buffer:     The ID of the buffer that contains this feedback data.
+ * @protocol:   The protocol version of the EDS.
+ * @data_type:  The type of data that the buffer contains.
+ * @spi_offset: The offset at which to write the payload data to the sensor.
+ * @payload:    Payload for the feedback command, or 0 if no payload is sent.
+ */
+struct ipts_feedback_header {
+	enum ipts_feedback_cmd_type cmd_type;
+	u32 size;
+	u32 buffer;
+	u32 protocol;
+	enum ipts_feedback_data_type data_type;
+	u32 spi_offset;
+	u8 reserved[40];
+	u8 payload[];
+} __packed;
+
+static_assert(sizeof(struct ipts_feedback_header) == 64);
+
+/**
+ * enum ipts_data_type - Defines what type of data a buffer contains.
+ * @IPTS_DATA_TYPE_FRAME:        Raw data frame.
+ * @IPTS_DATA_TYPE_ERROR:        Error data.
+ * @IPTS_DATA_TYPE_VENDOR:       Vendor specific data.
+ * @IPTS_DATA_TYPE_HID:          A HID report.
+ * @IPTS_DATA_TYPE_GET_FEATURES: The response to a GET_FEATURES HID2ME command.
+ */
+enum ipts_data_type {
+	IPTS_DATA_TYPE_FRAME = 0x00,
+	IPTS_DATA_TYPE_ERROR = 0x01,
+	IPTS_DATA_TYPE_VENDOR = 0x02,
+	IPTS_DATA_TYPE_HID = 0x03,
+	IPTS_DATA_TYPE_GET_FEATURES = 0x04,
+	IPTS_DATA_TYPE_DESCRIPTOR = 0x05,
+};
+
+/**
+ * struct ipts_data_header - Header that is prefixed to the data in a data buffer.
+ * @type: What data the buffer contains.
+ * @size: How much data the buffer contains.
+ * @buffer: Which buffer the data is in.
+ */
+struct ipts_data_header {
+	enum ipts_data_type type;
+	u32 size;
+	u32 buffer;
+	u8 reserved[52];
+	u8 data[];
+} __packed;
+
+static_assert(sizeof(struct ipts_data_header) == 64);
+
+#endif /* IPTS_SPEC_DATA_H */
diff --git a/drivers/hid/ipts/spec-device.h b/drivers/hid/ipts/spec-device.h
new file mode 100644
index 000000000000..41845f9d9025
--- /dev/null
+++ b/drivers/hid/ipts/spec-device.h
@@ -0,0 +1,290 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2016 Intel Corporation
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_SPEC_DEVICE_H
+#define IPTS_SPEC_DEVICE_H
+
+#include <linux/build_bug.h>
+#include <linux/types.h>
+
+/*
+ * The amount of buffers that IPTS can use for data transfer.
+ */
+#define IPTS_BUFFERS 16
+
+/*
+ * The buffer ID that is used for HID2ME feedback
+ */
+#define IPTS_HID2ME_BUFFER IPTS_BUFFERS
+
+/**
+ * enum ipts_command - Commands that can be sent to the IPTS hardware.
+ * @IPTS_CMD_GET_DEVICE_INFO:  Retrieves vendor information from the device.
+ * @IPTS_CMD_SET_MODE:         Changes the mode that the device will operate in.
+ * @IPTS_CMD_SET_MEM_WINDOW:   Configures memory buffers for passing data between device and driver.
+ * @IPTS_CMD_QUIESCE_IO:       Stops the data flow from the device to the driver.
+ * @IPTS_CMD_READY_FOR_DATA:   Informs the device that the driver is ready to receive data.
+ * @IPTS_CMD_FEEDBACK:         Informs the device that a buffer was processed and can be refilled.
+ * @IPTS_CMD_CLEAR_MEM_WINDOW: Stops the data flow and clears the buffer addresses on the device.
+ * @IPTS_CMD_RESET_SENSOR:     Resets the sensor to its default state.
+ * @IPTS_CMD_GET_DESCRIPTOR:   Retrieves the HID descriptor of the device.
+ */
+enum ipts_command_code {
+	IPTS_CMD_GET_DEVICE_INFO = 0x01,
+	IPTS_CMD_SET_MODE = 0x02,
+	IPTS_CMD_SET_MEM_WINDOW = 0x03,
+	IPTS_CMD_QUIESCE_IO = 0x04,
+	IPTS_CMD_READY_FOR_DATA = 0x05,
+	IPTS_CMD_FEEDBACK = 0x06,
+	IPTS_CMD_CLEAR_MEM_WINDOW = 0x07,
+	IPTS_CMD_RESET_SENSOR = 0x0B,
+	IPTS_CMD_GET_DESCRIPTOR = 0x0F,
+};
+
+/**
+ * enum ipts_status - Possible status codes returned by the IPTS device.
+ * @IPTS_STATUS_SUCCESS:                 Operation completed successfully.
+ * @IPTS_STATUS_INVALID_PARAMS:          Command contained an invalid payload.
+ * @IPTS_STATUS_ACCESS_DENIED:           ME could not validate a buffer address.
+ * @IPTS_STATUS_CMD_SIZE_ERROR:          Command contains an invalid payload.
+ * @IPTS_STATUS_NOT_READY:               Buffer addresses have not been set.
+ * @IPTS_STATUS_REQUEST_OUTSTANDING:     There is an outstanding command of the same type.
+ * @IPTS_STATUS_NO_SENSOR_FOUND:         No sensor could be found.
+ * @IPTS_STATUS_OUT_OF_MEMORY:           Not enough free memory for requested operation.
+ * @IPTS_STATUS_INTERNAL_ERROR:          An unexpected error occurred.
+ * @IPTS_STATUS_SENSOR_DISABLED:         The sensor has been disabled and must be reinitialized.
+ * @IPTS_STATUS_COMPAT_CHECK_FAIL:       Compatibility revision check between sensor and ME failed.
+ *                                       The host can ignore this error and attempt to continue.
+ * @IPTS_STATUS_SENSOR_EXPECTED_RESET:   The sensor went through a reset initiated by the driver.
+ * @IPTS_STATUS_SENSOR_UNEXPECTED_RESET: The sensor went through an unexpected reset.
+ * @IPTS_STATUS_RESET_FAILED:            Requested sensor reset failed to complete.
+ * @IPTS_STATUS_TIMEOUT:                 The operation timed out.
+ * @IPTS_STATUS_TEST_MODE_FAIL:          Test mode pattern did not match expected values.
+ * @IPTS_STATUS_SENSOR_FAIL_FATAL:       The sensor reported an error during reset sequence.
+ *                                       Further progress is not possible.
+ * @IPTS_STATUS_SENSOR_FAIL_NONFATAL:    The sensor reported an error during reset sequence.
+ *                                       The driver can attempt to continue.
+ * @IPTS_STATUS_INVALID_DEVICE_CAPS:     The device reported invalid capabilities.
+ * @IPTS_STATUS_QUIESCE_IO_IN_PROGRESS:  Command cannot be completed until Quiesce IO is done.
+ */
+enum ipts_status {
+	IPTS_STATUS_SUCCESS = 0x00,
+	IPTS_STATUS_INVALID_PARAMS = 0x01,
+	IPTS_STATUS_ACCESS_DENIED = 0x02,
+	IPTS_STATUS_CMD_SIZE_ERROR = 0x03,
+	IPTS_STATUS_NOT_READY = 0x04,
+	IPTS_STATUS_REQUEST_OUTSTANDING = 0x05,
+	IPTS_STATUS_NO_SENSOR_FOUND = 0x06,
+	IPTS_STATUS_OUT_OF_MEMORY = 0x07,
+	IPTS_STATUS_INTERNAL_ERROR = 0x08,
+	IPTS_STATUS_SENSOR_DISABLED = 0x09,
+	IPTS_STATUS_COMPAT_CHECK_FAIL = 0x0A,
+	IPTS_STATUS_SENSOR_EXPECTED_RESET = 0x0B,
+	IPTS_STATUS_SENSOR_UNEXPECTED_RESET = 0x0C,
+	IPTS_STATUS_RESET_FAILED = 0x0D,
+	IPTS_STATUS_TIMEOUT = 0x0E,
+	IPTS_STATUS_TEST_MODE_FAIL = 0x0F,
+	IPTS_STATUS_SENSOR_FAIL_FATAL = 0x10,
+	IPTS_STATUS_SENSOR_FAIL_NONFATAL = 0x11,
+	IPTS_STATUS_INVALID_DEVICE_CAPS = 0x12,
+	IPTS_STATUS_QUIESCE_IO_IN_PROGRESS = 0x13,
+};
+
+/**
+ * struct ipts_command - Message that is sent to the device for calling a command.
+ * @cmd:     The command that will be called.
+ * @payload: Payload containing parameters for the called command.
+ */
+struct ipts_command {
+	enum ipts_command_code cmd;
+	u8 payload[320];
+} __packed;
+
+static_assert(sizeof(struct ipts_command) == 324);
+
+/**
+ * enum ipts_mode - Configures what data the device produces and how its sent.
+ * @IPTS_MODE_EVENT: The device will send an event once a buffer was filled.
+ *                   Older devices will return singletouch data in this mode.
+ * @IPTS_MODE_POLL:  The device will notify the driver by incrementing the doorbell value.
+ *                   Older devices will return multitouch data in this mode.
+ */
+enum ipts_mode {
+	IPTS_MODE_EVENT = 0x00,
+	IPTS_MODE_POLL = 0x01,
+};
+
+/**
+ * struct ipts_set_mode - Payload for the SET_MODE command.
+ * @mode: Changes the mode that IPTS will operate in.
+ */
+struct ipts_set_mode {
+	enum ipts_mode mode;
+	u8 reserved[12];
+} __packed;
+
+static_assert(sizeof(struct ipts_set_mode) == 16);
+
+#define IPTS_WORKQUEUE_SIZE	 8192
+#define IPTS_WORKQUEUE_ITEM_SIZE 16
+
+/**
+ * struct ipts_mem_window - Payload for the SET_MEM_WINDOW command.
+ * @data_addr_lower:      Lower 32 bits of the data buffer addresses.
+ * @data_addr_upper:      Upper 32 bits of the data buffer addresses.
+ * @workqueue_addr_lower: Lower 32 bits of the workqueue buffer address.
+ * @workqueue_addr_upper: Upper 32 bits of the workqueue buffer address.
+ * @doorbell_addr_lower:  Lower 32 bits of the doorbell buffer address.
+ * @doorbell_addr_upper:  Upper 32 bits of the doorbell buffer address.
+ * @feedbackaddr_lower:   Lower 32 bits of the feedback buffer addresses.
+ * @feedbackaddr_upper:   Upper 32 bits of the feedback buffer addresses.
+ * @hid2me_addr_lower:    Lower 32 bits of the hid2me buffer address.
+ * @hid2me_addr_upper:    Upper 32 bits of the hid2me buffer address.
+ * @hid2me_size:          Size of the hid2me feedback buffer.
+ * @workqueue_item_size:  Magic value. Must be 16.
+ * @workqueue_size:       Magic value. Must be 8192.
+ *
+ * The workqueue related items in this struct are required for using
+ * GuC submission with binary processing firmware. Since this driver does
+ * not use GuC submission and instead exports raw data to userspace, these
+ * items are not actually used, but they need to be allocated and passed
+ * to the device, otherwise initialization will fail.
+ */
+struct ipts_mem_window {
+	u32 data_addr_lower[IPTS_BUFFERS];
+	u32 data_addr_upper[IPTS_BUFFERS];
+	u32 workqueue_addr_lower;
+	u32 workqueue_addr_upper;
+	u32 doorbell_addr_lower;
+	u32 doorbell_addr_upper;
+	u32 feedback_addr_lower[IPTS_BUFFERS];
+	u32 feedback_addr_upper[IPTS_BUFFERS];
+	u32 hid2me_addr_lower;
+	u32 hid2me_addr_upper;
+	u32 hid2me_size;
+	u8 reserved1;
+	u8 workqueue_item_size;
+	u16 workqueue_size;
+	u8 reserved[32];
+} __packed;
+
+static_assert(sizeof(struct ipts_mem_window) == 320);
+
+/**
+ * struct ipts_quiesce_io - Payload for the QUIESCE_IO command.
+ */
+struct ipts_quiesce_io {
+	u8 reserved[12];
+} __packed;
+
+static_assert(sizeof(struct ipts_quiesce_io) == 12);
+
+/**
+ * struct ipts_feedback - Payload for the FEEDBACK command.
+ * @buffer: The buffer that the device should refill.
+ */
+struct ipts_feedback {
+	u32 buffer;
+	u8 reserved[12];
+} __packed;
+
+static_assert(sizeof(struct ipts_feedback) == 16);
+
+/**
+ * enum ipts_reset_type - Possible ways of resetting the device.
+ * @IPTS_RESET_TYPE_HARD: Perform hardware reset using GPIO pin.
+ * @IPTS_RESET_TYPE_SOFT: Perform software reset using SPI command.
+ */
+enum ipts_reset_type {
+	IPTS_RESET_TYPE_HARD = 0x00,
+	IPTS_RESET_TYPE_SOFT = 0x01,
+};
+
+/**
+ * struct ipts_reset - Payload for the RESET_SENSOR command.
+ * @type: How the device should get reset.
+ */
+struct ipts_reset_sensor {
+	enum ipts_reset_type type;
+	u8 reserved[4];
+} __packed;
+
+static_assert(sizeof(struct ipts_reset_sensor) == 8);
+
+/**
+ * struct ipts_get_descriptor - Payload for the GET_DESCRIPTOR command.
+ * @addr_lower: The lower 32 bits of the descriptor buffer address.
+ * @addr_upper: The upper 32 bits of the descriptor buffer address.
+ * @magic:      A magic value. Must be 8.
+ */
+struct ipts_get_descriptor {
+	u32 addr_lower;
+	u32 addr_upper;
+	u32 magic;
+	u8 reserved[12];
+} __packed;
+
+static_assert(sizeof(struct ipts_get_descriptor) == 24);
+
+/*
+ * The type of a response is indicated by a
+ * command code, with the most significant bit flipped to 1.
+ */
+#define IPTS_RSP_BIT BIT(31)
+
+/**
+ * struct ipts_response - Data returned from the device in response to a command.
+ * @cmd:     The command that this response answers (IPTS_RSP_BIT will be 1).
+ * @status:  The return code of the command.
+ * @payload: The data that was produced by the command.
+ */
+struct ipts_response {
+	enum ipts_command_code cmd;
+	enum ipts_status status;
+	u8 payload[80];
+} __packed;
+
+static_assert(sizeof(struct ipts_response) == 88);
+
+/**
+ * struct ipts_device_info - Vendor information of the IPTS device.
+ * @vendor:         Vendor ID of this device.
+ * @product:        Product ID of this device.
+ * @hw_version:     Hardware revision of this device.
+ * @fw_version:     Firmware revision of this device.
+ * @data_size:      Requested size for a data buffer.
+ * @feedback_size:  Requested size for a feedback buffer.
+ * @mode:           Mode that the device currently operates in.
+ * @max_contacts:   Maximum amount of concurrent touches the sensor can process.
+ * @sensor_min_eds: The minimum EDS version supported by the sensor.
+ * @sensor_max_eds: The maximum EDS version supported by the sensor.
+ * @me_min_eds:     The minimum EDS version supported by the ME for communicating with the sensor.
+ * @me_max_eds:     The maximum EDS version supported by the ME for communicating with the sensor.
+ * @intf_eds:       The EDS version implemented by the interface between ME and host.
+ */
+struct ipts_device_info {
+	u16 vendor;
+	u16 product;
+	u32 hw_version;
+	u32 fw_version;
+	u32 data_size;
+	u32 feedback_size;
+	enum ipts_mode mode;
+	u8 max_contacts;
+	u8 reserved1[3];
+	u8 sensor_min_eds;
+	u8 sensor_maj_eds;
+	u8 me_min_eds;
+	u8 me_maj_eds;
+	u8 intf_eds;
+	u8 reserved2[11];
+} __packed;
+
+static_assert(sizeof(struct ipts_device_info) == 44);
+
+#endif /* IPTS_SPEC_DEVICE_H */
diff --git a/drivers/hid/ipts/spec-hid.h b/drivers/hid/ipts/spec-hid.h
new file mode 100644
index 000000000000..5a58d4a0a610
--- /dev/null
+++ b/drivers/hid/ipts/spec-hid.h
@@ -0,0 +1,34 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2020-2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_SPEC_HID_H
+#define IPTS_SPEC_HID_H
+
+#include <linux/build_bug.h>
+#include <linux/types.h>
+
+/*
+ * Made-up type for passing raw IPTS data in a HID report.
+ */
+#define IPTS_HID_FRAME_TYPE_RAW 0xEE
+
+/**
+ * struct ipts_hid_frame - Header that is prefixed to raw IPTS data wrapped in a HID report.
+ * @size: Size of the data inside the report, including this header.
+ * @type: What type of data does this report contain.
+ */
+struct ipts_hid_header {
+	u32 size;
+	u8 reserved1;
+	u8 type;
+	u8 reserved2;
+	u8 data[];
+} __packed;
+
+static_assert(sizeof(struct ipts_hid_header) == 7);
+
+#endif /* IPTS_SPEC_HID_H */
diff --git a/drivers/hid/ipts/thread.c b/drivers/hid/ipts/thread.c
new file mode 100644
index 000000000000..355e92bea26f
--- /dev/null
+++ b/drivers/hid/ipts/thread.c
@@ -0,0 +1,84 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#include <linux/completion.h>
+#include <linux/err.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+
+#include "thread.h"
+
+bool ipts_thread_should_stop(struct ipts_thread *thread)
+{
+	if (!thread)
+		return false;
+
+	return READ_ONCE(thread->should_stop);
+}
+
+static int ipts_thread_runner(void *data)
+{
+	int ret = 0;
+	struct ipts_thread *thread = data;
+
+	if (!thread)
+		return -EFAULT;
+
+	if (!thread->threadfn)
+		return -EFAULT;
+
+	ret = thread->threadfn(thread);
+	complete_all(&thread->done);
+
+	return ret;
+}
+
+int ipts_thread_start(struct ipts_thread *thread, int (*threadfn)(struct ipts_thread *thread),
+		      void *data, const char *name)
+{
+	if (!thread)
+		return -EFAULT;
+
+	if (!threadfn)
+		return -EFAULT;
+
+	init_completion(&thread->done);
+
+	thread->data = data;
+	thread->should_stop = false;
+	thread->threadfn = threadfn;
+
+	thread->thread = kthread_run(ipts_thread_runner, thread, name);
+	return PTR_ERR_OR_ZERO(thread->thread);
+}
+
+int ipts_thread_stop(struct ipts_thread *thread)
+{
+	int ret = 0;
+
+	if (!thread)
+		return -EFAULT;
+
+	if (!thread->thread)
+		return 0;
+
+	WRITE_ONCE(thread->should_stop, true);
+
+	/*
+	 * Make sure that the write has gone through before waiting.
+	 */
+	wmb();
+
+	wait_for_completion(&thread->done);
+	ret = kthread_stop(thread->thread);
+
+	thread->thread = NULL;
+	thread->data = NULL;
+	thread->threadfn = NULL;
+
+	return ret;
+}
diff --git a/drivers/hid/ipts/thread.h b/drivers/hid/ipts/thread.h
new file mode 100644
index 000000000000..1f966b8b32c4
--- /dev/null
+++ b/drivers/hid/ipts/thread.h
@@ -0,0 +1,59 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * Copyright (c) 2023 Dorian Stoll
+ *
+ * Linux driver for Intel Precise Touch & Stylus
+ */
+
+#ifndef IPTS_THREAD_H
+#define IPTS_THREAD_H
+
+#include <linux/completion.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>
+
+/*
+ * This wrapper over kthread is necessary, because calling kthread_stop makes it impossible
+ * to issue MEI commands from that thread while it shuts itself down. By using a custom
+ * boolean variable and a completion object, we can call kthread_stop only when the thread
+ * already finished all of its work and has returned.
+ */
+struct ipts_thread {
+	struct task_struct *thread;
+
+	bool should_stop;
+	struct completion done;
+
+	void *data;
+	int (*threadfn)(struct ipts_thread *thread);
+};
+
+/**
+ * ipts_thread_should_stop() - Returns true if the thread is asked to terminate.
+ * @thread: The current thread.
+ *
+ * Returns: true if the thread should stop, false if not.
+ */
+bool ipts_thread_should_stop(struct ipts_thread *thread);
+
+/**
+ * ipts_thread_start() - Starts an IPTS thread.
+ * @thread: The thread to initialize and start.
+ * @threadfn: The function to execute.
+ * @data: An argument that will be passed to threadfn.
+ * @name: The name of the new thread.
+ *
+ * Returns: 0 on success, <0 on error.
+ */
+int ipts_thread_start(struct ipts_thread *thread, int (*threadfn)(struct ipts_thread *thread),
+		      void *data, const char name[]);
+
+/**
+ * ipts_thread_stop() - Asks the thread to terminate and waits until it has finished.
+ * @thread: The thread that should stop.
+ *
+ * Returns: The return value of the thread function.
+ */
+int ipts_thread_stop(struct ipts_thread *thread);
+
+#endif /* IPTS_THREAD_H */
-- 
2.47.0


From 8ff8dcba900388c107713baacb9a02f5470821f8 Mon Sep 17 00:00:00 2001
From: Dorian Stoll <dorian.stoll@tmsp.io>
Date: Sun, 11 Dec 2022 12:03:38 +0100
Subject: [PATCH v1.4 021/120] iommu: intel: Disable source id verification for
 ITHC

Signed-off-by: Dorian Stoll <dorian.stoll@tmsp.io>
Patchset: ithc
---
 drivers/iommu/intel/irq_remapping.c | 16 ++++++++++++++++
 1 file changed, 16 insertions(+)

diff --git a/drivers/iommu/intel/irq_remapping.c b/drivers/iommu/intel/irq_remapping.c
index e090ca07364b..e575193615bf 100644
--- a/drivers/iommu/intel/irq_remapping.c
+++ b/drivers/iommu/intel/irq_remapping.c
@@ -389,6 +389,22 @@ static int set_msi_sid(struct irte *irte, struct pci_dev *dev)
 	data.busmatch_count = 0;
 	pci_for_each_dma_alias(dev, set_msi_sid_cb, &data);
 
+	/*
+	 * The Intel Touch Host Controller is at 00:10.6, but for some reason
+	 * the MSI interrupts have request id 01:05.0.
+	 * Disable id verification to work around this.
+	 * FIXME Find proper fix or turn this into a quirk.
+	 */
+	if (dev->vendor == PCI_VENDOR_ID_INTEL && (dev->class >> 8) == PCI_CLASS_INPUT_PEN) {
+		switch(dev->device) {
+		case 0x98d0: case 0x98d1: // LKF
+		case 0xa0d0: case 0xa0d1: // TGL LP
+		case 0x43d0: case 0x43d1: // TGL H
+			set_irte_sid(irte, SVT_NO_VERIFY, SQ_ALL_16, 0);
+			return 0;
+		}
+	}
+
 	/*
 	 * DMA alias provides us with a PCI device and alias.  The only case
 	 * where the it will return an alias on a different bus than the
-- 
2.47.0


From ab3514355e473a940c6e4ce638eb593d182c974d Mon Sep 17 00:00:00 2001
From: quo <tuple@list.ru>
Date: Sun, 11 Dec 2022 12:10:54 +0100
Subject: [PATCH v1.4 022/120] hid: Add support for Intel Touch Host Controller

Based on quo/ithc-linux@34539af4726d.

Signed-off-by: Maximilian Stoll <luzmaximilian@gmail.com>
Patchset: ithc
---
 drivers/hid/Kconfig              |   2 +
 drivers/hid/Makefile             |   1 +
 drivers/hid/ithc/Kbuild          |   6 +
 drivers/hid/ithc/Kconfig         |  12 +
 drivers/hid/ithc/ithc-debug.c    | 149 ++++++++
 drivers/hid/ithc/ithc-debug.h    |   7 +
 drivers/hid/ithc/ithc-dma.c      | 312 ++++++++++++++++
 drivers/hid/ithc/ithc-dma.h      |  47 +++
 drivers/hid/ithc/ithc-hid.c      | 207 +++++++++++
 drivers/hid/ithc/ithc-hid.h      |  32 ++
 drivers/hid/ithc/ithc-legacy.c   | 254 +++++++++++++
 drivers/hid/ithc/ithc-legacy.h   |   8 +
 drivers/hid/ithc/ithc-main.c     | 431 ++++++++++++++++++++++
 drivers/hid/ithc/ithc-quickspi.c | 607 +++++++++++++++++++++++++++++++
 drivers/hid/ithc/ithc-quickspi.h |  39 ++
 drivers/hid/ithc/ithc-regs.c     | 154 ++++++++
 drivers/hid/ithc/ithc-regs.h     | 211 +++++++++++
 drivers/hid/ithc/ithc.h          |  89 +++++
 18 files changed, 2568 insertions(+)
 create mode 100644 drivers/hid/ithc/Kbuild
 create mode 100644 drivers/hid/ithc/Kconfig
 create mode 100644 drivers/hid/ithc/ithc-debug.c
 create mode 100644 drivers/hid/ithc/ithc-debug.h
 create mode 100644 drivers/hid/ithc/ithc-dma.c
 create mode 100644 drivers/hid/ithc/ithc-dma.h
 create mode 100644 drivers/hid/ithc/ithc-hid.c
 create mode 100644 drivers/hid/ithc/ithc-hid.h
 create mode 100644 drivers/hid/ithc/ithc-legacy.c
 create mode 100644 drivers/hid/ithc/ithc-legacy.h
 create mode 100644 drivers/hid/ithc/ithc-main.c
 create mode 100644 drivers/hid/ithc/ithc-quickspi.c
 create mode 100644 drivers/hid/ithc/ithc-quickspi.h
 create mode 100644 drivers/hid/ithc/ithc-regs.c
 create mode 100644 drivers/hid/ithc/ithc-regs.h
 create mode 100644 drivers/hid/ithc/ithc.h

diff --git a/drivers/hid/Kconfig b/drivers/hid/Kconfig
index ccddfba86004..8e2ea8175bfb 100644
--- a/drivers/hid/Kconfig
+++ b/drivers/hid/Kconfig
@@ -1369,4 +1369,6 @@ source "drivers/hid/surface-hid/Kconfig"
 
 source "drivers/hid/ipts/Kconfig"
 
+source "drivers/hid/ithc/Kconfig"
+
 endif # HID_SUPPORT
diff --git a/drivers/hid/Makefile b/drivers/hid/Makefile
index bdb17cffca2f..8987177f8b81 100644
--- a/drivers/hid/Makefile
+++ b/drivers/hid/Makefile
@@ -171,3 +171,4 @@ obj-$(CONFIG_AMD_SFH_HID)       += amd-sfh-hid/
 obj-$(CONFIG_SURFACE_HID_CORE)  += surface-hid/
 
 obj-$(CONFIG_HID_IPTS)          += ipts/
+obj-$(CONFIG_HID_ITHC)          += ithc/
diff --git a/drivers/hid/ithc/Kbuild b/drivers/hid/ithc/Kbuild
new file mode 100644
index 000000000000..4937ba131297
--- /dev/null
+++ b/drivers/hid/ithc/Kbuild
@@ -0,0 +1,6 @@
+obj-$(CONFIG_HID_ITHC) := ithc.o
+
+ithc-objs := ithc-main.o ithc-regs.o ithc-dma.o ithc-hid.o ithc-legacy.o ithc-quickspi.o ithc-debug.o
+
+ccflags-y := -std=gnu11 -Wno-declaration-after-statement
+
diff --git a/drivers/hid/ithc/Kconfig b/drivers/hid/ithc/Kconfig
new file mode 100644
index 000000000000..ede713023609
--- /dev/null
+++ b/drivers/hid/ithc/Kconfig
@@ -0,0 +1,12 @@
+config HID_ITHC
+	tristate "Intel Touch Host Controller"
+	depends on PCI
+	depends on HID
+	help
+	  Say Y here if your system has a touchscreen using Intels
+	  Touch Host Controller (ITHC / IPTS) technology.
+
+	  If unsure say N.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called ithc.
diff --git a/drivers/hid/ithc/ithc-debug.c b/drivers/hid/ithc/ithc-debug.c
new file mode 100644
index 000000000000..2d8c6afe9966
--- /dev/null
+++ b/drivers/hid/ithc/ithc-debug.c
@@ -0,0 +1,149 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+
+#include "ithc.h"
+
+void ithc_log_regs(struct ithc *ithc)
+{
+	if (!ithc->prev_regs)
+		return;
+	u32 __iomem *cur = (__iomem void *)ithc->regs;
+	u32 *prev = (void *)ithc->prev_regs;
+	for (int i = 1024; i < sizeof(*ithc->regs) / 4; i++) {
+		u32 x = readl(cur + i);
+		if (x != prev[i]) {
+			pci_info(ithc->pci, "reg %04x: %08x -> %08x\n", i * 4, prev[i], x);
+			prev[i] = x;
+		}
+	}
+}
+
+static ssize_t ithc_debugfs_cmd_write(struct file *f, const char __user *buf, size_t len,
+	loff_t *offset)
+{
+	// Debug commands consist of a single letter followed by a list of numbers (decimal or
+	// hexadecimal, space-separated).
+	struct ithc *ithc = file_inode(f)->i_private;
+	char cmd[256];
+	if (!ithc || !ithc->pci)
+		return -ENODEV;
+	if (!len)
+		return -EINVAL;
+	if (len >= sizeof(cmd))
+		return -EINVAL;
+	if (copy_from_user(cmd, buf, len))
+		return -EFAULT;
+	cmd[len] = 0;
+	if (cmd[len-1] == '\n')
+		cmd[len-1] = 0;
+	pci_info(ithc->pci, "debug command: %s\n", cmd);
+
+	// Parse the list of arguments into a u32 array.
+	u32 n = 0;
+	const char *s = cmd + 1;
+	u32 a[32];
+	while (*s && *s != '\n') {
+		if (n >= ARRAY_SIZE(a))
+			return -EINVAL;
+		if (*s++ != ' ')
+			return -EINVAL;
+		char *e;
+		a[n++] = simple_strtoul(s, &e, 0);
+		if (e == s)
+			return -EINVAL;
+		s = e;
+	}
+	ithc_log_regs(ithc);
+
+	// Execute the command.
+	switch (cmd[0]) {
+	case 'x': // reset
+		ithc_reset(ithc);
+		break;
+	case 'w': // write register: offset mask value
+		if (n != 3 || (a[0] & 3))
+			return -EINVAL;
+		pci_info(ithc->pci, "debug write 0x%04x = 0x%08x (mask 0x%08x)\n",
+			a[0], a[2], a[1]);
+		bitsl(((__iomem u32 *)ithc->regs) + a[0] / 4, a[1], a[2]);
+		break;
+	case 'r': // read register: offset
+		if (n != 1 || (a[0] & 3))
+			return -EINVAL;
+		pci_info(ithc->pci, "debug read 0x%04x = 0x%08x\n", a[0],
+			readl(((__iomem u32 *)ithc->regs) + a[0] / 4));
+		break;
+	case 's': // spi command: cmd offset len data...
+		// read config: s 4 0 64 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
+		// set touch cfg: s 6 12 4 XX
+		if (n < 3 || a[2] > (n - 3) * 4)
+			return -EINVAL;
+		pci_info(ithc->pci, "debug spi command %u with %u bytes of data\n", a[0], a[2]);
+		if (!CHECK(ithc_spi_command, ithc, a[0], a[1], a[2], a + 3))
+			for (u32 i = 0; i < (a[2] + 3) / 4; i++)
+				pci_info(ithc->pci, "resp %u = 0x%08x\n", i, a[3+i]);
+		break;
+	case 'd': // dma command: cmd len data...
+		// get report descriptor: d 7 8 0 0
+		// enable multitouch: d 3 2 0x0105
+		if (n < 1)
+			return -EINVAL;
+		pci_info(ithc->pci, "debug dma command with %u bytes of data\n", n * 4);
+		struct ithc_data data = { .type = ITHC_DATA_RAW, .size = n * 4, .data = a };
+		if (ithc_dma_tx(ithc, &data))
+			pci_err(ithc->pci, "dma tx failed\n");
+		break;
+	default:
+		return -EINVAL;
+	}
+	ithc_log_regs(ithc);
+	return len;
+}
+
+static struct dentry *dbg_dir;
+
+void __init ithc_debug_init_module(void)
+{
+	struct dentry *d = debugfs_create_dir(DEVNAME, NULL);
+	if (IS_ERR(d))
+		pr_warn("failed to create debugfs dir (%li)\n", PTR_ERR(d));
+	else
+		dbg_dir = d;
+}
+
+void __exit ithc_debug_exit_module(void)
+{
+	debugfs_remove_recursive(dbg_dir);
+	dbg_dir = NULL;
+}
+
+static const struct file_operations ithc_debugfops_cmd = {
+	.owner = THIS_MODULE,
+	.write = ithc_debugfs_cmd_write,
+};
+
+static void ithc_debugfs_devres_release(struct device *dev, void *res)
+{
+	struct dentry **dbgm = res;
+	debugfs_remove_recursive(*dbgm);
+}
+
+int ithc_debug_init_device(struct ithc *ithc)
+{
+	if (!dbg_dir)
+		return -ENOENT;
+	struct dentry **dbgm = devres_alloc(ithc_debugfs_devres_release, sizeof(*dbgm), GFP_KERNEL);
+	if (!dbgm)
+		return -ENOMEM;
+	devres_add(&ithc->pci->dev, dbgm);
+	struct dentry *dbg = debugfs_create_dir(pci_name(ithc->pci), dbg_dir);
+	if (IS_ERR(dbg))
+		return PTR_ERR(dbg);
+	*dbgm = dbg;
+
+	struct dentry *cmd = debugfs_create_file("cmd", 0220, dbg, ithc, &ithc_debugfops_cmd);
+	if (IS_ERR(cmd))
+		return PTR_ERR(cmd);
+
+	return 0;
+}
+
diff --git a/drivers/hid/ithc/ithc-debug.h b/drivers/hid/ithc/ithc-debug.h
new file mode 100644
index 000000000000..38c53d916bdb
--- /dev/null
+++ b/drivers/hid/ithc/ithc-debug.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause */
+
+void ithc_debug_init_module(void);
+void ithc_debug_exit_module(void);
+int ithc_debug_init_device(struct ithc *ithc);
+void ithc_log_regs(struct ithc *ithc);
+
diff --git a/drivers/hid/ithc/ithc-dma.c b/drivers/hid/ithc/ithc-dma.c
new file mode 100644
index 000000000000..bf4eab33062b
--- /dev/null
+++ b/drivers/hid/ithc/ithc-dma.c
@@ -0,0 +1,312 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+
+#include "ithc.h"
+
+// The THC uses tables of PRDs (physical region descriptors) to describe the TX and RX data buffers.
+// Each PRD contains the DMA address and size of a block of DMA memory, and some status flags.
+// This allows each data buffer to consist of multiple non-contiguous blocks of memory.
+
+static int ithc_dma_prd_alloc(struct ithc *ithc, struct ithc_dma_prd_buffer *p,
+	unsigned int num_buffers, unsigned int num_pages, enum dma_data_direction dir)
+{
+	p->num_pages = num_pages;
+	p->dir = dir;
+	// We allocate enough space to have one PRD per data buffer page, however if the data
+	// buffer pages happen to be contiguous, we can describe the buffer using fewer PRDs, so
+	// some will remain unused (which is fine).
+	p->size = round_up(num_buffers * num_pages * sizeof(struct ithc_phys_region_desc), PAGE_SIZE);
+	p->addr = dmam_alloc_coherent(&ithc->pci->dev, p->size, &p->dma_addr, GFP_KERNEL);
+	if (!p->addr)
+		return -ENOMEM;
+	if (p->dma_addr & (PAGE_SIZE - 1))
+		return -EFAULT;
+	return 0;
+}
+
+// Devres managed sg_table wrapper.
+struct ithc_sg_table {
+	void *addr;
+	struct sg_table sgt;
+	enum dma_data_direction dir;
+};
+static void ithc_dma_sgtable_free(struct sg_table *sgt)
+{
+	struct scatterlist *sg;
+	int i;
+	for_each_sgtable_sg(sgt, sg, i) {
+		struct page *p = sg_page(sg);
+		if (p)
+			__free_page(p);
+	}
+	sg_free_table(sgt);
+}
+static void ithc_dma_data_devres_release(struct device *dev, void *res)
+{
+	struct ithc_sg_table *sgt = res;
+	if (sgt->addr)
+		vunmap(sgt->addr);
+	dma_unmap_sgtable(dev, &sgt->sgt, sgt->dir, 0);
+	ithc_dma_sgtable_free(&sgt->sgt);
+}
+
+static int ithc_dma_data_alloc(struct ithc *ithc, struct ithc_dma_prd_buffer *prds,
+	struct ithc_dma_data_buffer *b)
+{
+	// We don't use dma_alloc_coherent() for data buffers, because they don't have to be
+	// coherent (they are unidirectional) or contiguous (we can use one PRD per page).
+	// We could use dma_alloc_noncontiguous(), however this still always allocates a single
+	// DMA mapped segment, which is more restrictive than what we need.
+	// Instead we use an sg_table of individually allocated pages.
+	struct page *pages[16];
+	if (prds->num_pages == 0 || prds->num_pages > ARRAY_SIZE(pages))
+		return -EINVAL;
+	b->active_idx = -1;
+	struct ithc_sg_table *sgt = devres_alloc(
+		ithc_dma_data_devres_release, sizeof(*sgt), GFP_KERNEL);
+	if (!sgt)
+		return -ENOMEM;
+	sgt->dir = prds->dir;
+
+	if (!sg_alloc_table(&sgt->sgt, prds->num_pages, GFP_KERNEL)) {
+		struct scatterlist *sg;
+		int i;
+		bool ok = true;
+		for_each_sgtable_sg(&sgt->sgt, sg, i) {
+			// NOTE: don't need __GFP_DMA for PCI DMA
+			struct page *p = pages[i] = alloc_page(GFP_KERNEL | __GFP_ZERO);
+			if (!p) {
+				ok = false;
+				break;
+			}
+			sg_set_page(sg, p, PAGE_SIZE, 0);
+		}
+		if (ok && !dma_map_sgtable(&ithc->pci->dev, &sgt->sgt, prds->dir, 0)) {
+			devres_add(&ithc->pci->dev, sgt);
+			b->sgt = &sgt->sgt;
+			b->addr = sgt->addr = vmap(pages, prds->num_pages, 0, PAGE_KERNEL);
+			if (!b->addr)
+				return -ENOMEM;
+			return 0;
+		}
+		ithc_dma_sgtable_free(&sgt->sgt);
+	}
+	devres_free(sgt);
+	return -ENOMEM;
+}
+
+static int ithc_dma_data_buffer_put(struct ithc *ithc, struct ithc_dma_prd_buffer *prds,
+	struct ithc_dma_data_buffer *b, unsigned int idx)
+{
+	// Give a buffer to the THC.
+	struct ithc_phys_region_desc *prd = prds->addr;
+	prd += idx * prds->num_pages;
+	if (b->active_idx >= 0) {
+		pci_err(ithc->pci, "buffer already active\n");
+		return -EINVAL;
+	}
+	b->active_idx = idx;
+	if (prds->dir == DMA_TO_DEVICE) {
+		// TX buffer: Caller should have already filled the data buffer, so just fill
+		// the PRD and flush.
+		// (TODO: Support multi-page TX buffers. So far no device seems to use or need
+		// these though.)
+		if (b->data_size > PAGE_SIZE)
+			return -EINVAL;
+		prd->addr = sg_dma_address(b->sgt->sgl) >> 10;
+		prd->size = b->data_size | PRD_FLAG_END;
+		flush_kernel_vmap_range(b->addr, b->data_size);
+	} else if (prds->dir == DMA_FROM_DEVICE) {
+		// RX buffer: Reset PRDs.
+		struct scatterlist *sg;
+		int i;
+		for_each_sgtable_dma_sg(b->sgt, sg, i) {
+			prd->addr = sg_dma_address(sg) >> 10;
+			prd->size = sg_dma_len(sg);
+			prd++;
+		}
+		prd[-1].size |= PRD_FLAG_END;
+	}
+	dma_wmb(); // for the prds
+	dma_sync_sgtable_for_device(&ithc->pci->dev, b->sgt, prds->dir);
+	return 0;
+}
+
+static int ithc_dma_data_buffer_get(struct ithc *ithc, struct ithc_dma_prd_buffer *prds,
+	struct ithc_dma_data_buffer *b, unsigned int idx)
+{
+	// Take a buffer from the THC.
+	struct ithc_phys_region_desc *prd = prds->addr;
+	prd += idx * prds->num_pages;
+	// This is purely a sanity check. We don't strictly need the idx parameter for this
+	// function, because it should always be the same as active_idx, unless we have a bug.
+	if (b->active_idx != idx) {
+		pci_err(ithc->pci, "wrong buffer index\n");
+		return -EINVAL;
+	}
+	b->active_idx = -1;
+	if (prds->dir == DMA_FROM_DEVICE) {
+		// RX buffer: Calculate actual received data size from PRDs.
+		dma_rmb(); // for the prds
+		b->data_size = 0;
+		struct scatterlist *sg;
+		int i;
+		for_each_sgtable_dma_sg(b->sgt, sg, i) {
+			unsigned int size = prd->size;
+			b->data_size += size & PRD_SIZE_MASK;
+			if (size & PRD_FLAG_END)
+				break;
+			if ((size & PRD_SIZE_MASK) != sg_dma_len(sg)) {
+				pci_err(ithc->pci, "truncated prd\n");
+				break;
+			}
+			prd++;
+		}
+		invalidate_kernel_vmap_range(b->addr, b->data_size);
+	}
+	dma_sync_sgtable_for_cpu(&ithc->pci->dev, b->sgt, prds->dir);
+	return 0;
+}
+
+int ithc_dma_rx_init(struct ithc *ithc, u8 channel)
+{
+	struct ithc_dma_rx *rx = &ithc->dma_rx[channel];
+	mutex_init(&rx->mutex);
+
+	// Allocate buffers.
+	unsigned int num_pages = (ithc->max_rx_size + PAGE_SIZE - 1) / PAGE_SIZE;
+	pci_dbg(ithc->pci, "allocating rx buffers: num = %u, size = %u, pages = %u\n",
+		NUM_RX_BUF, ithc->max_rx_size, num_pages);
+	CHECK_RET(ithc_dma_prd_alloc, ithc, &rx->prds, NUM_RX_BUF, num_pages, DMA_FROM_DEVICE);
+	for (unsigned int i = 0; i < NUM_RX_BUF; i++)
+		CHECK_RET(ithc_dma_data_alloc, ithc, &rx->prds, &rx->bufs[i]);
+
+	// Init registers.
+	writeb(DMA_RX_CONTROL2_RESET, &ithc->regs->dma_rx[channel].control2);
+	lo_hi_writeq(rx->prds.dma_addr, &ithc->regs->dma_rx[channel].addr);
+	writeb(NUM_RX_BUF - 1, &ithc->regs->dma_rx[channel].num_bufs);
+	writeb(num_pages - 1, &ithc->regs->dma_rx[channel].num_prds);
+	u8 head = readb(&ithc->regs->dma_rx[channel].head);
+	if (head) {
+		pci_err(ithc->pci, "head is nonzero (%u)\n", head);
+		return -EIO;
+	}
+
+	// Init buffers.
+	for (unsigned int i = 0; i < NUM_RX_BUF; i++)
+		CHECK_RET(ithc_dma_data_buffer_put, ithc, &rx->prds, &rx->bufs[i], i);
+
+	writeb(head ^ DMA_RX_WRAP_FLAG, &ithc->regs->dma_rx[channel].tail);
+	return 0;
+}
+
+void ithc_dma_rx_enable(struct ithc *ithc, u8 channel)
+{
+	bitsb_set(&ithc->regs->dma_rx[channel].control,
+		DMA_RX_CONTROL_ENABLE | DMA_RX_CONTROL_IRQ_ERROR | DMA_RX_CONTROL_IRQ_DATA);
+	CHECK(waitl, ithc, &ithc->regs->dma_rx[channel].status,
+		DMA_RX_STATUS_ENABLED, DMA_RX_STATUS_ENABLED);
+}
+
+int ithc_dma_tx_init(struct ithc *ithc)
+{
+	struct ithc_dma_tx *tx = &ithc->dma_tx;
+	mutex_init(&tx->mutex);
+
+	// Allocate buffers.
+	unsigned int num_pages = (ithc->max_tx_size + PAGE_SIZE - 1) / PAGE_SIZE;
+	pci_dbg(ithc->pci, "allocating tx buffers: size = %u, pages = %u\n",
+		ithc->max_tx_size, num_pages);
+	CHECK_RET(ithc_dma_prd_alloc, ithc, &tx->prds, 1, num_pages, DMA_TO_DEVICE);
+	CHECK_RET(ithc_dma_data_alloc, ithc, &tx->prds, &tx->buf);
+
+	// Init registers.
+	lo_hi_writeq(tx->prds.dma_addr, &ithc->regs->dma_tx.addr);
+	writeb(num_pages - 1, &ithc->regs->dma_tx.num_prds);
+
+	// Init buffers.
+	CHECK_RET(ithc_dma_data_buffer_put, ithc, &ithc->dma_tx.prds, &ithc->dma_tx.buf, 0);
+	return 0;
+}
+
+static int ithc_dma_rx_unlocked(struct ithc *ithc, u8 channel)
+{
+	// Process all filled RX buffers from the ringbuffer.
+	struct ithc_dma_rx *rx = &ithc->dma_rx[channel];
+	unsigned int n = rx->num_received;
+	u8 head_wrap = readb(&ithc->regs->dma_rx[channel].head);
+	while (1) {
+		u8 tail = n % NUM_RX_BUF;
+		u8 tail_wrap = tail | ((n / NUM_RX_BUF) & 1 ? 0 : DMA_RX_WRAP_FLAG);
+		writeb(tail_wrap, &ithc->regs->dma_rx[channel].tail);
+		// ringbuffer is full if tail_wrap == head_wrap
+		// ringbuffer is empty if tail_wrap == head_wrap ^ WRAP_FLAG
+		if (tail_wrap == (head_wrap ^ DMA_RX_WRAP_FLAG))
+			return 0;
+
+		// take the buffer that the device just filled
+		struct ithc_dma_data_buffer *b = &rx->bufs[n % NUM_RX_BUF];
+		CHECK_RET(ithc_dma_data_buffer_get, ithc, &rx->prds, b, tail);
+		rx->num_received = ++n;
+
+		// process data
+		struct ithc_data d;
+		if ((ithc->use_quickspi ? ithc_quickspi_decode_rx : ithc_legacy_decode_rx)
+			(ithc, b->addr, b->data_size, &d) < 0) {
+			pci_err(ithc->pci, "invalid dma rx data! channel %u, buffer %u, size %u: %*ph\n",
+				channel, tail, b->data_size, min((int)b->data_size, 64), b->addr);
+			print_hex_dump_debug(DEVNAME " data: ", DUMP_PREFIX_OFFSET, 32, 1,
+				b->addr, min(b->data_size, 0x400u), 0);
+		} else {
+			ithc_hid_process_data(ithc, &d);
+		}
+
+		// give the buffer back to the device
+		CHECK_RET(ithc_dma_data_buffer_put, ithc, &rx->prds, b, tail);
+	}
+}
+int ithc_dma_rx(struct ithc *ithc, u8 channel)
+{
+	struct ithc_dma_rx *rx = &ithc->dma_rx[channel];
+	mutex_lock(&rx->mutex);
+	int ret = ithc_dma_rx_unlocked(ithc, channel);
+	mutex_unlock(&rx->mutex);
+	return ret;
+}
+
+static int ithc_dma_tx_unlocked(struct ithc *ithc, const struct ithc_data *data)
+{
+	// Send a single TX buffer to the THC.
+	pci_dbg(ithc->pci, "dma tx data type %u, size %u\n", data->type, data->size);
+	CHECK_RET(ithc_dma_data_buffer_get, ithc, &ithc->dma_tx.prds, &ithc->dma_tx.buf, 0);
+
+	// Fill the TX buffer with header and data.
+	ssize_t sz;
+	if (data->type == ITHC_DATA_RAW) {
+		sz = min(data->size, ithc->max_tx_size);
+		memcpy(ithc->dma_tx.buf.addr, data->data, sz);
+	} else {
+		sz = (ithc->use_quickspi ? ithc_quickspi_encode_tx : ithc_legacy_encode_tx)
+			(ithc, data, ithc->dma_tx.buf.addr, ithc->max_tx_size);
+	}
+	ithc->dma_tx.buf.data_size = sz < 0 ? 0 : sz;
+	CHECK_RET(ithc_dma_data_buffer_put, ithc, &ithc->dma_tx.prds, &ithc->dma_tx.buf, 0);
+	if (sz < 0) {
+		pci_err(ithc->pci, "failed to encode tx data type %i, size %u, error %i\n",
+			data->type, data->size, (int)sz);
+		return -EINVAL;
+	}
+
+	// Let the THC process the buffer.
+	bitsb_set(&ithc->regs->dma_tx.control, DMA_TX_CONTROL_SEND);
+	CHECK_RET(waitb, ithc, &ithc->regs->dma_tx.control, DMA_TX_CONTROL_SEND, 0);
+	writel(DMA_TX_STATUS_DONE, &ithc->regs->dma_tx.status);
+	return 0;
+}
+int ithc_dma_tx(struct ithc *ithc, const struct ithc_data *data)
+{
+	mutex_lock(&ithc->dma_tx.mutex);
+	int ret = ithc_dma_tx_unlocked(ithc, data);
+	mutex_unlock(&ithc->dma_tx.mutex);
+	return ret;
+}
+
diff --git a/drivers/hid/ithc/ithc-dma.h b/drivers/hid/ithc/ithc-dma.h
new file mode 100644
index 000000000000..1749a5819b3e
--- /dev/null
+++ b/drivers/hid/ithc/ithc-dma.h
@@ -0,0 +1,47 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause */
+
+#define PRD_SIZE_MASK            0xffffff
+#define PRD_FLAG_END             0x1000000
+#define PRD_FLAG_SUCCESS         0x2000000
+#define PRD_FLAG_ERROR           0x4000000
+
+struct ithc_phys_region_desc {
+	u64 addr; // physical addr/1024
+	u32 size; // num bytes, PRD_FLAG_END marks last prd for data split over multiple prds
+	u32 unused;
+};
+
+struct ithc_dma_prd_buffer {
+	void *addr;
+	dma_addr_t dma_addr;
+	u32 size;
+	u32 num_pages; // per data buffer
+	enum dma_data_direction dir;
+};
+
+struct ithc_dma_data_buffer {
+	void *addr;
+	struct sg_table *sgt;
+	int active_idx;
+	u32 data_size;
+};
+
+struct ithc_dma_tx {
+	struct mutex mutex;
+	struct ithc_dma_prd_buffer prds;
+	struct ithc_dma_data_buffer buf;
+};
+
+struct ithc_dma_rx {
+	struct mutex mutex;
+	u32 num_received;
+	struct ithc_dma_prd_buffer prds;
+	struct ithc_dma_data_buffer bufs[NUM_RX_BUF];
+};
+
+int ithc_dma_rx_init(struct ithc *ithc, u8 channel);
+void ithc_dma_rx_enable(struct ithc *ithc, u8 channel);
+int ithc_dma_tx_init(struct ithc *ithc);
+int ithc_dma_rx(struct ithc *ithc, u8 channel);
+int ithc_dma_tx(struct ithc *ithc, const struct ithc_data *data);
+
diff --git a/drivers/hid/ithc/ithc-hid.c b/drivers/hid/ithc/ithc-hid.c
new file mode 100644
index 000000000000..065646ab499e
--- /dev/null
+++ b/drivers/hid/ithc/ithc-hid.c
@@ -0,0 +1,207 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+
+#include "ithc.h"
+
+static int ithc_hid_start(struct hid_device *hdev) { return 0; }
+static void ithc_hid_stop(struct hid_device *hdev) { }
+static int ithc_hid_open(struct hid_device *hdev) { return 0; }
+static void ithc_hid_close(struct hid_device *hdev) { }
+
+static int ithc_hid_parse(struct hid_device *hdev)
+{
+	struct ithc *ithc = hdev->driver_data;
+	const struct ithc_data get_report_desc = { .type = ITHC_DATA_REPORT_DESCRIPTOR };
+	WRITE_ONCE(ithc->hid.parse_done, false);
+	for (int retries = 0; ; retries++) {
+		ithc_log_regs(ithc);
+		CHECK_RET(ithc_dma_tx, ithc, &get_report_desc);
+		if (wait_event_timeout(ithc->hid.wait_parse, READ_ONCE(ithc->hid.parse_done),
+				msecs_to_jiffies(200))) {
+			ithc_log_regs(ithc);
+			return 0;
+		}
+		if (retries > 5) {
+			ithc_log_regs(ithc);
+			pci_err(ithc->pci, "failed to read report descriptor\n");
+			return -ETIMEDOUT;
+		}
+		pci_warn(ithc->pci, "failed to read report descriptor, retrying\n");
+	}
+}
+
+static int ithc_hid_raw_request(struct hid_device *hdev, unsigned char reportnum, __u8 *buf,
+	size_t len, unsigned char rtype, int reqtype)
+{
+	struct ithc *ithc = hdev->driver_data;
+	if (!buf || !len)
+		return -EINVAL;
+
+	struct ithc_data d = { .size = len, .data = buf };
+	buf[0] = reportnum;
+
+	if (rtype == HID_OUTPUT_REPORT && reqtype == HID_REQ_SET_REPORT) {
+		d.type = ITHC_DATA_OUTPUT_REPORT;
+		CHECK_RET(ithc_dma_tx, ithc, &d);
+		return 0;
+	}
+
+	if (rtype == HID_FEATURE_REPORT && reqtype == HID_REQ_SET_REPORT) {
+		d.type = ITHC_DATA_SET_FEATURE;
+		CHECK_RET(ithc_dma_tx, ithc, &d);
+		return 0;
+	}
+
+	if (rtype == HID_FEATURE_REPORT && reqtype == HID_REQ_GET_REPORT) {
+		d.type = ITHC_DATA_GET_FEATURE;
+		d.data = &reportnum;
+		d.size = 1;
+
+		// Prepare for response.
+		mutex_lock(&ithc->hid.get_feature_mutex);
+		ithc->hid.get_feature_buf = buf;
+		ithc->hid.get_feature_size = len;
+		mutex_unlock(&ithc->hid.get_feature_mutex);
+
+		// Transmit 'get feature' request.
+		int r = CHECK(ithc_dma_tx, ithc, &d);
+		if (!r) {
+			r = wait_event_interruptible_timeout(ithc->hid.wait_get_feature,
+				!ithc->hid.get_feature_buf, msecs_to_jiffies(1000));
+			if (!r)
+				r = -ETIMEDOUT;
+			else if (r < 0)
+				r = -EINTR;
+			else
+				r = 0;
+		}
+
+		// If everything went ok, the buffer has been filled with the response data.
+		// Return the response size.
+		mutex_lock(&ithc->hid.get_feature_mutex);
+		ithc->hid.get_feature_buf = NULL;
+		if (!r)
+			r = ithc->hid.get_feature_size;
+		mutex_unlock(&ithc->hid.get_feature_mutex);
+		return r;
+	}
+
+	pci_err(ithc->pci, "unhandled hid request %i %i for report id %i\n",
+		rtype, reqtype, reportnum);
+	return -EINVAL;
+}
+
+// FIXME hid_input_report()/hid_parse_report() currently don't take const buffers, so we have to
+// cast away the const to avoid a compiler warning...
+#define NOCONST(x) ((void *)x)
+
+void ithc_hid_process_data(struct ithc *ithc, struct ithc_data *d)
+{
+	WARN_ON(!ithc->hid.dev);
+	if (!ithc->hid.dev)
+		return;
+
+	switch (d->type) {
+
+	case ITHC_DATA_IGNORE:
+		return;
+
+	case ITHC_DATA_ERROR:
+		CHECK(ithc_reset, ithc);
+		return;
+
+	case ITHC_DATA_REPORT_DESCRIPTOR:
+		// Response to the report descriptor request sent by ithc_hid_parse().
+		CHECK(hid_parse_report, ithc->hid.dev, NOCONST(d->data), d->size);
+		WRITE_ONCE(ithc->hid.parse_done, true);
+		wake_up(&ithc->hid.wait_parse);
+		return;
+
+	case ITHC_DATA_INPUT_REPORT:
+	{
+		// Standard HID input report.
+		int r = hid_input_report(ithc->hid.dev, HID_INPUT_REPORT, NOCONST(d->data), d->size, 1);
+		if (r < 0) {
+			pci_warn(ithc->pci, "hid_input_report failed with %i (size %u, report ID 0x%02x)\n",
+				r, d->size, d->size ? *(u8 *)d->data : 0);
+			print_hex_dump_debug(DEVNAME " report: ", DUMP_PREFIX_OFFSET, 32, 1,
+				d->data, min(d->size, 0x400u), 0);
+		}
+		return;
+	}
+
+	case ITHC_DATA_GET_FEATURE:
+	{
+		// Response to a 'get feature' request sent by ithc_hid_raw_request().
+		bool done = false;
+		mutex_lock(&ithc->hid.get_feature_mutex);
+		if (ithc->hid.get_feature_buf) {
+			if (d->size < ithc->hid.get_feature_size)
+				ithc->hid.get_feature_size = d->size;
+			memcpy(ithc->hid.get_feature_buf, d->data, ithc->hid.get_feature_size);
+			ithc->hid.get_feature_buf = NULL;
+			done = true;
+		}
+		mutex_unlock(&ithc->hid.get_feature_mutex);
+		if (done) {
+			wake_up(&ithc->hid.wait_get_feature);
+		} else {
+			// Received data without a matching request, or the request already
+			// timed out. (XXX What's the correct thing to do here?)
+			CHECK(hid_input_report, ithc->hid.dev, HID_FEATURE_REPORT,
+				NOCONST(d->data), d->size, 1);
+		}
+		return;
+	}
+
+	default:
+		pci_err(ithc->pci, "unhandled data type %i\n", d->type);
+		return;
+	}
+}
+
+static struct hid_ll_driver ithc_ll_driver = {
+	.start = ithc_hid_start,
+	.stop = ithc_hid_stop,
+	.open = ithc_hid_open,
+	.close = ithc_hid_close,
+	.parse = ithc_hid_parse,
+	.raw_request = ithc_hid_raw_request,
+};
+
+static void ithc_hid_devres_release(struct device *dev, void *res)
+{
+	struct hid_device **hidm = res;
+	if (*hidm)
+		hid_destroy_device(*hidm);
+}
+
+int ithc_hid_init(struct ithc *ithc)
+{
+	struct hid_device **hidm = devres_alloc(ithc_hid_devres_release, sizeof(*hidm), GFP_KERNEL);
+	if (!hidm)
+		return -ENOMEM;
+	devres_add(&ithc->pci->dev, hidm);
+	struct hid_device *hid = hid_allocate_device();
+	if (IS_ERR(hid))
+		return PTR_ERR(hid);
+	*hidm = hid;
+
+	strscpy(hid->name, DEVFULLNAME, sizeof(hid->name));
+	strscpy(hid->phys, ithc->phys, sizeof(hid->phys));
+	hid->ll_driver = &ithc_ll_driver;
+	hid->bus = BUS_PCI;
+	hid->vendor = ithc->vendor_id;
+	hid->product = ithc->product_id;
+	hid->version = 0x100;
+	hid->dev.parent = &ithc->pci->dev;
+	hid->driver_data = ithc;
+
+	ithc->hid.dev = hid;
+
+	init_waitqueue_head(&ithc->hid.wait_parse);
+	init_waitqueue_head(&ithc->hid.wait_get_feature);
+	mutex_init(&ithc->hid.get_feature_mutex);
+
+	return 0;
+}
+
diff --git a/drivers/hid/ithc/ithc-hid.h b/drivers/hid/ithc/ithc-hid.h
new file mode 100644
index 000000000000..599eb912c8c8
--- /dev/null
+++ b/drivers/hid/ithc/ithc-hid.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause */
+
+enum ithc_data_type {
+	ITHC_DATA_IGNORE,
+	ITHC_DATA_RAW,
+	ITHC_DATA_ERROR,
+	ITHC_DATA_REPORT_DESCRIPTOR,
+	ITHC_DATA_INPUT_REPORT,
+	ITHC_DATA_OUTPUT_REPORT,
+	ITHC_DATA_GET_FEATURE,
+	ITHC_DATA_SET_FEATURE,
+};
+
+struct ithc_data {
+	enum ithc_data_type type;
+	u32 size;
+	const void *data;
+};
+
+struct ithc_hid {
+	struct hid_device *dev;
+	bool parse_done;
+	wait_queue_head_t wait_parse;
+	wait_queue_head_t wait_get_feature;
+	struct mutex get_feature_mutex;
+	void *get_feature_buf;
+	size_t get_feature_size;
+};
+
+int ithc_hid_init(struct ithc *ithc);
+void ithc_hid_process_data(struct ithc *ithc, struct ithc_data *d);
+
diff --git a/drivers/hid/ithc/ithc-legacy.c b/drivers/hid/ithc/ithc-legacy.c
new file mode 100644
index 000000000000..8883987fb352
--- /dev/null
+++ b/drivers/hid/ithc/ithc-legacy.c
@@ -0,0 +1,254 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+
+#include "ithc.h"
+
+#define DEVCFG_DMA_RX_SIZE(x)          ((((x) & 0x3fff) + 1) << 6)
+#define DEVCFG_DMA_TX_SIZE(x)          (((((x) >> 14) & 0x3ff) + 1) << 6)
+
+#define DEVCFG_TOUCH_MASK              0x3f
+#define DEVCFG_TOUCH_ENABLE            BIT(0)
+#define DEVCFG_TOUCH_PROP_DATA_ENABLE  BIT(1)
+#define DEVCFG_TOUCH_HID_REPORT_ENABLE BIT(2)
+#define DEVCFG_TOUCH_POWER_STATE(x)    (((x) & 7) << 3)
+#define DEVCFG_TOUCH_UNKNOWN_6         BIT(6)
+
+#define DEVCFG_DEVICE_ID_TIC           0x43495424 // "$TIC"
+
+#define DEVCFG_SPI_CLKDIV(x)           (((x) >> 1) & 7)
+#define DEVCFG_SPI_CLKDIV_8            BIT(4)
+#define DEVCFG_SPI_SUPPORTS_SINGLE     BIT(5)
+#define DEVCFG_SPI_SUPPORTS_DUAL       BIT(6)
+#define DEVCFG_SPI_SUPPORTS_QUAD       BIT(7)
+#define DEVCFG_SPI_MAX_TOUCH_POINTS(x) (((x) >> 8) & 0x3f)
+#define DEVCFG_SPI_MIN_RESET_TIME(x)   (((x) >> 16) & 0xf)
+#define DEVCFG_SPI_NEEDS_HEARTBEAT     BIT(20) // TODO implement heartbeat
+#define DEVCFG_SPI_HEARTBEAT_INTERVAL(x) (((x) >> 21) & 7)
+#define DEVCFG_SPI_UNKNOWN_25          BIT(25)
+#define DEVCFG_SPI_UNKNOWN_26          BIT(26)
+#define DEVCFG_SPI_UNKNOWN_27          BIT(27)
+#define DEVCFG_SPI_DELAY(x)            (((x) >> 28) & 7) // TODO use this
+#define DEVCFG_SPI_USE_EXT_READ_CFG    BIT(31) // TODO use this?
+
+struct ithc_device_config { // (Example values are from an SP7+.)
+	u32 irq_cause;        // 00 = 0xe0000402 (0xe0000401 after DMA_RX_CODE_RESET)
+	u32 error;            // 04 = 0x00000000
+	u32 dma_buf_sizes;    // 08 = 0x000a00ff
+	u32 touch_cfg;        // 0c = 0x0000001c
+	u32 touch_state;      // 10 = 0x0000001c
+	u32 device_id;        // 14 = 0x43495424 = "$TIC"
+	u32 spi_config;       // 18 = 0xfda00a2e
+	u16 vendor_id;        // 1c = 0x045e = Microsoft Corp.
+	u16 product_id;       // 1e = 0x0c1a
+	u32 revision;         // 20 = 0x00000001
+	u32 fw_version;       // 24 = 0x05008a8b = 5.0.138.139 (this value looks more random on newer devices)
+	u32 command;          // 28 = 0x00000000
+	u32 fw_mode;          // 2c = 0x00000000 (for fw update?)
+	u32 _unknown_30;      // 30 = 0x00000000
+	u8 eds_minor_ver;     // 34 = 0x5e
+	u8 eds_major_ver;     // 35 = 0x03
+	u8 interface_rev;     // 36 = 0x04
+	u8 eu_kernel_ver;     // 37 = 0x04
+	u32 _unknown_38;      // 38 = 0x000001c0 (0x000001c1 after DMA_RX_CODE_RESET)
+	u32 _unknown_3c;      // 3c = 0x00000002
+};
+static_assert(sizeof(struct ithc_device_config) == 64);
+
+#define RX_CODE_INPUT_REPORT          3
+#define RX_CODE_FEATURE_REPORT        4
+#define RX_CODE_REPORT_DESCRIPTOR     5
+#define RX_CODE_RESET                 7
+
+#define TX_CODE_SET_FEATURE           3
+#define TX_CODE_GET_FEATURE           4
+#define TX_CODE_OUTPUT_REPORT         5
+#define TX_CODE_GET_REPORT_DESCRIPTOR 7
+
+static int ithc_set_device_enabled(struct ithc *ithc, bool enable)
+{
+	u32 x = ithc->legacy_touch_cfg =
+		(ithc->legacy_touch_cfg & ~(u32)DEVCFG_TOUCH_MASK) |
+		DEVCFG_TOUCH_HID_REPORT_ENABLE |
+		(enable ? DEVCFG_TOUCH_ENABLE | DEVCFG_TOUCH_POWER_STATE(3) : 0);
+	return ithc_spi_command(ithc, SPI_CMD_CODE_WRITE,
+		offsetof(struct ithc_device_config, touch_cfg), sizeof(x), &x);
+}
+
+int ithc_legacy_init(struct ithc *ithc)
+{
+	// Since we don't yet know which SPI config the device wants, use default speed and mode
+	// initially for reading config data.
+	CHECK(ithc_set_spi_config, ithc, 2, true, SPI_MODE_SINGLE, SPI_MODE_SINGLE);
+
+	// Setting the following bit seems to make reading the config more reliable.
+	bitsl_set(&ithc->regs->dma_rx[0].init_unknown, INIT_UNKNOWN_31);
+
+	// Setting this bit may be necessary on ADL devices.
+	switch (ithc->pci->device) {
+	case PCI_DEVICE_ID_INTEL_THC_ADL_S_PORT1:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_S_PORT2:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_P_PORT1:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_P_PORT2:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_M_PORT1:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_M_PORT2:
+		bitsl_set(&ithc->regs->dma_rx[0].init_unknown, INIT_UNKNOWN_5);
+		break;
+	}
+
+	// Take the touch device out of reset.
+	bitsl(&ithc->regs->control_bits, CONTROL_QUIESCE, 0);
+	CHECK_RET(waitl, ithc, &ithc->regs->control_bits, CONTROL_IS_QUIESCED, 0);
+	for (int retries = 0; ; retries++) {
+		ithc_log_regs(ithc);
+		bitsl_set(&ithc->regs->control_bits, CONTROL_NRESET);
+		if (!waitl(ithc, &ithc->regs->irq_cause, 0xf, 2))
+			break;
+		if (retries > 5) {
+			pci_err(ithc->pci, "failed to reset device, irq_cause = 0x%08x\n",
+				readl(&ithc->regs->irq_cause));
+			return -ETIMEDOUT;
+		}
+		pci_warn(ithc->pci, "invalid irq_cause, retrying reset\n");
+		bitsl(&ithc->regs->control_bits, CONTROL_NRESET, 0);
+		if (msleep_interruptible(1000))
+			return -EINTR;
+	}
+	ithc_log_regs(ithc);
+
+	CHECK(waitl, ithc, &ithc->regs->dma_rx[0].status, DMA_RX_STATUS_READY, DMA_RX_STATUS_READY);
+
+	// Read configuration data.
+	u32 spi_cfg;
+	for (int retries = 0; ; retries++) {
+		ithc_log_regs(ithc);
+		struct ithc_device_config config = { 0 };
+		CHECK_RET(ithc_spi_command, ithc, SPI_CMD_CODE_READ, 0, sizeof(config), &config);
+		u32 *p = (void *)&config;
+		pci_info(ithc->pci, "config: %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n",
+			p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7], p[8], p[9], p[10], p[11], p[12], p[13], p[14], p[15]);
+		if (config.device_id == DEVCFG_DEVICE_ID_TIC) {
+			spi_cfg = config.spi_config;
+			ithc->vendor_id = config.vendor_id;
+			ithc->product_id = config.product_id;
+			ithc->product_rev = config.revision;
+			ithc->max_rx_size = DEVCFG_DMA_RX_SIZE(config.dma_buf_sizes);
+			ithc->max_tx_size = DEVCFG_DMA_TX_SIZE(config.dma_buf_sizes);
+			ithc->legacy_touch_cfg = config.touch_cfg;
+			ithc->have_config = true;
+			break;
+		}
+		if (retries > 10) {
+			pci_err(ithc->pci, "failed to read config, unknown device ID 0x%08x\n",
+				config.device_id);
+			return -EIO;
+		}
+		pci_warn(ithc->pci, "failed to read config, retrying\n");
+		if (msleep_interruptible(100))
+			return -EINTR;
+	}
+	ithc_log_regs(ithc);
+
+	// Apply SPI config and enable touch device.
+	CHECK_RET(ithc_set_spi_config, ithc,
+		DEVCFG_SPI_CLKDIV(spi_cfg), (spi_cfg & DEVCFG_SPI_CLKDIV_8) != 0,
+		spi_cfg & DEVCFG_SPI_SUPPORTS_QUAD ? SPI_MODE_QUAD :
+		spi_cfg & DEVCFG_SPI_SUPPORTS_DUAL ? SPI_MODE_DUAL :
+		SPI_MODE_SINGLE,
+		SPI_MODE_SINGLE);
+	CHECK_RET(ithc_set_device_enabled, ithc, true);
+	ithc_log_regs(ithc);
+	return 0;
+}
+
+void ithc_legacy_exit(struct ithc *ithc)
+{
+	CHECK(ithc_set_device_enabled, ithc, false);
+}
+
+int ithc_legacy_decode_rx(struct ithc *ithc, const void *src, size_t len, struct ithc_data *dest)
+{
+	const struct {
+		u32 code;
+		u32 data_size;
+		u32 _unknown[14];
+	} *hdr = src;
+
+	if (len < sizeof(*hdr))
+		return -ENODATA;
+	// Note: RX data is not padded, even though TX data must be padded.
+	if (len != sizeof(*hdr) + hdr->data_size)
+		return -EMSGSIZE;
+
+	dest->data = hdr + 1;
+	dest->size = hdr->data_size;
+
+	switch (hdr->code) {
+	case RX_CODE_RESET:
+		// The THC sends a reset request when we need to reinitialize the device.
+		// This usually only happens if we send an invalid command or put the device
+		// in a bad state.
+		dest->type = ITHC_DATA_ERROR;
+		return 0;
+	case RX_CODE_REPORT_DESCRIPTOR:
+		// The descriptor is preceded by 8 nul bytes.
+		if (hdr->data_size < 8)
+			return -ENODATA;
+		dest->type = ITHC_DATA_REPORT_DESCRIPTOR;
+		dest->data = (char *)(hdr + 1) + 8;
+		dest->size = hdr->data_size - 8;
+		return 0;
+	case RX_CODE_INPUT_REPORT:
+		dest->type = ITHC_DATA_INPUT_REPORT;
+		return 0;
+	case RX_CODE_FEATURE_REPORT:
+		dest->type = ITHC_DATA_GET_FEATURE;
+		return 0;
+	default:
+		return -EINVAL;
+	}
+}
+
+ssize_t ithc_legacy_encode_tx(struct ithc *ithc, const struct ithc_data *src, void *dest,
+	size_t maxlen)
+{
+	struct {
+		u32 code;
+		u32 data_size;
+	} *hdr = dest;
+
+	size_t src_size = src->size;
+	const void *src_data = src->data;
+	const u64 get_report_desc_data = 0;
+	u32 code;
+
+	switch (src->type) {
+	case ITHC_DATA_SET_FEATURE:
+		code = TX_CODE_SET_FEATURE;
+		break;
+	case ITHC_DATA_GET_FEATURE:
+		code = TX_CODE_GET_FEATURE;
+		break;
+	case ITHC_DATA_OUTPUT_REPORT:
+		code = TX_CODE_OUTPUT_REPORT;
+		break;
+	case ITHC_DATA_REPORT_DESCRIPTOR:
+		code = TX_CODE_GET_REPORT_DESCRIPTOR;
+		src_size = sizeof(get_report_desc_data);
+		src_data = &get_report_desc_data;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	// Data must be padded to next 4-byte boundary.
+	size_t padded = round_up(src_size, 4);
+	if (sizeof(*hdr) + padded > maxlen)
+		return -EOVERFLOW;
+
+	// Fill the TX buffer with header and data.
+	hdr->code = code;
+	hdr->data_size = src_size;
+	memcpy_and_pad(hdr + 1, padded, src_data, src_size, 0);
+
+	return sizeof(*hdr) + padded;
+}
+
diff --git a/drivers/hid/ithc/ithc-legacy.h b/drivers/hid/ithc/ithc-legacy.h
new file mode 100644
index 000000000000..28d692462072
--- /dev/null
+++ b/drivers/hid/ithc/ithc-legacy.h
@@ -0,0 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause */
+
+int ithc_legacy_init(struct ithc *ithc);
+void ithc_legacy_exit(struct ithc *ithc);
+int ithc_legacy_decode_rx(struct ithc *ithc, const void *src, size_t len, struct ithc_data *dest);
+ssize_t ithc_legacy_encode_tx(struct ithc *ithc, const struct ithc_data *src, void *dest,
+	size_t maxlen);
+
diff --git a/drivers/hid/ithc/ithc-main.c b/drivers/hid/ithc/ithc-main.c
new file mode 100644
index 000000000000..ac56c253674b
--- /dev/null
+++ b/drivers/hid/ithc/ithc-main.c
@@ -0,0 +1,431 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+
+#include "ithc.h"
+
+MODULE_DESCRIPTION("Intel Touch Host Controller driver");
+MODULE_LICENSE("Dual BSD/GPL");
+
+static const struct pci_device_id ithc_pci_tbl[] = {
+	{
+		.vendor = PCI_VENDOR_ID_INTEL,
+		.device = PCI_ANY_ID,
+		.subvendor = PCI_ANY_ID,
+		.subdevice = PCI_ANY_ID,
+		.class = PCI_CLASS_INPUT_PEN << 8,
+		.class_mask = ~0,
+	},
+	{}
+};
+MODULE_DEVICE_TABLE(pci, ithc_pci_tbl);
+
+// Module parameters
+
+static bool ithc_use_polling = false;
+module_param_named(poll, ithc_use_polling, bool, 0);
+MODULE_PARM_DESC(poll, "Use polling instead of interrupts");
+
+// Since all known devices seem to use only channel 1, by default we disable channel 0.
+static bool ithc_use_rx0 = false;
+module_param_named(rx0, ithc_use_rx0, bool, 0);
+MODULE_PARM_DESC(rx0, "Use DMA RX channel 0");
+
+static bool ithc_use_rx1 = true;
+module_param_named(rx1, ithc_use_rx1, bool, 0);
+MODULE_PARM_DESC(rx1, "Use DMA RX channel 1");
+
+static int ithc_active_ltr_us = -1;
+module_param_named(activeltr, ithc_active_ltr_us, int, 0);
+MODULE_PARM_DESC(activeltr, "Active LTR value override (in microseconds)");
+
+static int ithc_idle_ltr_us = -1;
+module_param_named(idleltr, ithc_idle_ltr_us, int, 0);
+MODULE_PARM_DESC(idleltr, "Idle LTR value override (in microseconds)");
+
+static unsigned int ithc_idle_delay_ms = 1000;
+module_param_named(idledelay, ithc_idle_delay_ms, uint, 0);
+MODULE_PARM_DESC(idleltr, "Minimum idle time before applying idle LTR value (in milliseconds)");
+
+static bool ithc_log_regs_enabled = false;
+module_param_named(logregs, ithc_log_regs_enabled, bool, 0);
+MODULE_PARM_DESC(logregs, "Log changes in register values (for debugging)");
+
+// Interrupts/polling
+
+static void ithc_disable_interrupts(struct ithc *ithc)
+{
+	writel(0, &ithc->regs->error_control);
+	bitsb(&ithc->regs->spi_cmd.control, SPI_CMD_CONTROL_IRQ, 0);
+	bitsb(&ithc->regs->dma_rx[0].control, DMA_RX_CONTROL_IRQ_UNKNOWN_1 | DMA_RX_CONTROL_IRQ_ERROR | DMA_RX_CONTROL_IRQ_READY | DMA_RX_CONTROL_IRQ_DATA, 0);
+	bitsb(&ithc->regs->dma_rx[1].control, DMA_RX_CONTROL_IRQ_UNKNOWN_1 | DMA_RX_CONTROL_IRQ_ERROR | DMA_RX_CONTROL_IRQ_READY | DMA_RX_CONTROL_IRQ_DATA, 0);
+	bitsb(&ithc->regs->dma_tx.control, DMA_TX_CONTROL_IRQ, 0);
+}
+
+static void ithc_clear_dma_rx_interrupts(struct ithc *ithc, unsigned int channel)
+{
+	writel(DMA_RX_STATUS_ERROR | DMA_RX_STATUS_READY | DMA_RX_STATUS_HAVE_DATA,
+		&ithc->regs->dma_rx[channel].status);
+}
+
+static void ithc_clear_interrupts(struct ithc *ithc)
+{
+	writel(0xffffffff, &ithc->regs->error_flags);
+	writel(ERROR_STATUS_DMA | ERROR_STATUS_SPI, &ithc->regs->error_status);
+	writel(SPI_CMD_STATUS_DONE | SPI_CMD_STATUS_ERROR, &ithc->regs->spi_cmd.status);
+	ithc_clear_dma_rx_interrupts(ithc, 0);
+	ithc_clear_dma_rx_interrupts(ithc, 1);
+	writel(DMA_TX_STATUS_DONE | DMA_TX_STATUS_ERROR | DMA_TX_STATUS_UNKNOWN_2,
+		&ithc->regs->dma_tx.status);
+}
+
+static void ithc_idle_timer_callback(struct timer_list *t)
+{
+	struct ithc *ithc = container_of(t, struct ithc, idle_timer);
+	ithc_set_ltr_idle(ithc);
+}
+
+static void ithc_process(struct ithc *ithc)
+{
+	ithc_log_regs(ithc);
+
+	// The THC automatically transitions from LTR idle to active at the start of a DMA transfer.
+	// It does not appear to automatically go back to idle, so we switch it back after a delay.
+	mod_timer(&ithc->idle_timer, jiffies + msecs_to_jiffies(ithc_idle_delay_ms));
+
+	bool rx0 = ithc_use_rx0 && (readl(&ithc->regs->dma_rx[0].status) & (DMA_RX_STATUS_ERROR | DMA_RX_STATUS_HAVE_DATA)) != 0;
+	bool rx1 = ithc_use_rx1 && (readl(&ithc->regs->dma_rx[1].status) & (DMA_RX_STATUS_ERROR | DMA_RX_STATUS_HAVE_DATA)) != 0;
+
+	// Read and clear error bits
+	u32 err = readl(&ithc->regs->error_flags);
+	if (err) {
+		writel(err, &ithc->regs->error_flags);
+		if (err & ~ERROR_FLAG_DMA_RX_TIMEOUT)
+			pci_err(ithc->pci, "error flags: 0x%08x\n", err);
+		if (err & ERROR_FLAG_DMA_RX_TIMEOUT)
+			pci_err(ithc->pci, "DMA RX timeout/error (try decreasing activeltr/idleltr if this happens frequently)\n");
+	}
+
+	// Process DMA rx
+	if (ithc_use_rx0) {
+		ithc_clear_dma_rx_interrupts(ithc, 0);
+		if (rx0)
+			ithc_dma_rx(ithc, 0);
+	}
+	if (ithc_use_rx1) {
+		ithc_clear_dma_rx_interrupts(ithc, 1);
+		if (rx1)
+			ithc_dma_rx(ithc, 1);
+	}
+
+	ithc_log_regs(ithc);
+}
+
+static irqreturn_t ithc_interrupt_thread(int irq, void *arg)
+{
+	struct ithc *ithc = arg;
+	pci_dbg(ithc->pci, "IRQ! err=%08x/%08x/%08x, cmd=%02x/%08x, rx0=%02x/%08x, rx1=%02x/%08x, tx=%02x/%08x\n",
+		readl(&ithc->regs->error_control), readl(&ithc->regs->error_status), readl(&ithc->regs->error_flags),
+		readb(&ithc->regs->spi_cmd.control), readl(&ithc->regs->spi_cmd.status),
+		readb(&ithc->regs->dma_rx[0].control), readl(&ithc->regs->dma_rx[0].status),
+		readb(&ithc->regs->dma_rx[1].control), readl(&ithc->regs->dma_rx[1].status),
+		readb(&ithc->regs->dma_tx.control), readl(&ithc->regs->dma_tx.status));
+	ithc_process(ithc);
+	return IRQ_HANDLED;
+}
+
+static int ithc_poll_thread(void *arg)
+{
+	struct ithc *ithc = arg;
+	unsigned int sleep = 100;
+	while (!kthread_should_stop()) {
+		u32 n = ithc->dma_rx[1].num_received;
+		ithc_process(ithc);
+		// Decrease polling interval to 20ms if we received data, otherwise slowly
+		// increase it up to 200ms.
+		sleep = n != ithc->dma_rx[1].num_received ? 20
+			: min(200u, sleep + (sleep >> 4) + 1);
+		msleep_interruptible(sleep);
+	}
+	return 0;
+}
+
+// Device initialization and shutdown
+
+static void ithc_disable(struct ithc *ithc)
+{
+	bitsl_set(&ithc->regs->control_bits, CONTROL_QUIESCE);
+	CHECK(waitl, ithc, &ithc->regs->control_bits, CONTROL_IS_QUIESCED, CONTROL_IS_QUIESCED);
+	bitsl(&ithc->regs->control_bits, CONTROL_NRESET, 0);
+	bitsb(&ithc->regs->spi_cmd.control, SPI_CMD_CONTROL_SEND, 0);
+	bitsb(&ithc->regs->dma_tx.control, DMA_TX_CONTROL_SEND, 0);
+	bitsb(&ithc->regs->dma_rx[0].control, DMA_RX_CONTROL_ENABLE, 0);
+	bitsb(&ithc->regs->dma_rx[1].control, DMA_RX_CONTROL_ENABLE, 0);
+	ithc_disable_interrupts(ithc);
+	ithc_clear_interrupts(ithc);
+}
+
+static int ithc_init_device(struct ithc *ithc)
+{
+	// Read ACPI config for QuickSPI mode
+	struct ithc_acpi_config cfg = { 0 };
+	CHECK_RET(ithc_read_acpi_config, ithc, &cfg);
+	if (!cfg.has_config)
+		pci_info(ithc->pci, "no ACPI config, using legacy mode\n");
+	else
+		ithc_print_acpi_config(ithc, &cfg);
+	ithc->use_quickspi = cfg.has_config;
+
+	// Shut down device
+	ithc_log_regs(ithc);
+	bool was_enabled = (readl(&ithc->regs->control_bits) & CONTROL_NRESET) != 0;
+	ithc_disable(ithc);
+	CHECK_RET(waitl, ithc, &ithc->regs->control_bits, CONTROL_READY, CONTROL_READY);
+	ithc_log_regs(ithc);
+
+	// If the device was previously enabled, wait a bit to make sure it's fully shut down.
+	if (was_enabled)
+		if (msleep_interruptible(100))
+			return -EINTR;
+
+	// Set Latency Tolerance Reporting config. The device will automatically
+	// apply these values depending on whether it is active or idle.
+	// If active value is too high, DMA buffer data can become truncated.
+	// By default, we set the active LTR value to 50us, and idle to 100ms.
+	u64 active_ltr_ns = ithc_active_ltr_us >= 0 ? (u64)ithc_active_ltr_us * 1000
+		: cfg.has_config && cfg.has_active_ltr ? (u64)cfg.active_ltr << 10
+		: 50 * 1000;
+	u64 idle_ltr_ns = ithc_idle_ltr_us >= 0 ? (u64)ithc_idle_ltr_us * 1000
+		: cfg.has_config && cfg.has_idle_ltr ? (u64)cfg.idle_ltr << 10
+		: 100 * 1000 * 1000;
+	ithc_set_ltr_config(ithc, active_ltr_ns, idle_ltr_ns);
+
+	if (ithc->use_quickspi)
+		CHECK_RET(ithc_quickspi_init, ithc, &cfg);
+	else
+		CHECK_RET(ithc_legacy_init, ithc);
+
+	return 0;
+}
+
+int ithc_reset(struct ithc *ithc)
+{
+	// FIXME This should probably do devres_release_group()+ithc_start().
+	// But because this is called during DMA processing, that would have to be done
+	// asynchronously (schedule_work()?). And with extra locking?
+	pci_err(ithc->pci, "reset\n");
+	CHECK(ithc_init_device, ithc);
+	if (ithc_use_rx0)
+		ithc_dma_rx_enable(ithc, 0);
+	if (ithc_use_rx1)
+		ithc_dma_rx_enable(ithc, 1);
+	ithc_log_regs(ithc);
+	pci_dbg(ithc->pci, "reset completed\n");
+	return 0;
+}
+
+static void ithc_stop(void *res)
+{
+	struct ithc *ithc = res;
+	pci_dbg(ithc->pci, "stopping\n");
+	ithc_log_regs(ithc);
+
+	if (ithc->poll_thread)
+		CHECK(kthread_stop, ithc->poll_thread);
+	if (ithc->irq >= 0)
+		disable_irq(ithc->irq);
+	if (ithc->use_quickspi)
+		ithc_quickspi_exit(ithc);
+	else
+		ithc_legacy_exit(ithc);
+	ithc_disable(ithc);
+	del_timer_sync(&ithc->idle_timer);
+
+	// Clear DMA config.
+	for (unsigned int i = 0; i < 2; i++) {
+		CHECK(waitl, ithc, &ithc->regs->dma_rx[i].status, DMA_RX_STATUS_ENABLED, 0);
+		lo_hi_writeq(0, &ithc->regs->dma_rx[i].addr);
+		writeb(0, &ithc->regs->dma_rx[i].num_bufs);
+		writeb(0, &ithc->regs->dma_rx[i].num_prds);
+	}
+	lo_hi_writeq(0, &ithc->regs->dma_tx.addr);
+	writeb(0, &ithc->regs->dma_tx.num_prds);
+
+	ithc_log_regs(ithc);
+	pci_dbg(ithc->pci, "stopped\n");
+}
+
+static void ithc_clear_drvdata(void *res)
+{
+	struct pci_dev *pci = res;
+	pci_set_drvdata(pci, NULL);
+}
+
+static int ithc_start(struct pci_dev *pci)
+{
+	pci_dbg(pci, "starting\n");
+	if (pci_get_drvdata(pci)) {
+		pci_err(pci, "device already initialized\n");
+		return -EINVAL;
+	}
+	if (!devres_open_group(&pci->dev, ithc_start, GFP_KERNEL))
+		return -ENOMEM;
+
+	// Allocate/init main driver struct.
+	struct ithc *ithc = devm_kzalloc(&pci->dev, sizeof(*ithc), GFP_KERNEL);
+	if (!ithc)
+		return -ENOMEM;
+	ithc->irq = -1;
+	ithc->pci = pci;
+	snprintf(ithc->phys, sizeof(ithc->phys), "pci-%s/" DEVNAME, pci_name(pci));
+	pci_set_drvdata(pci, ithc);
+	CHECK_RET(devm_add_action_or_reset, &pci->dev, ithc_clear_drvdata, pci);
+	if (ithc_log_regs_enabled)
+		ithc->prev_regs = devm_kzalloc(&pci->dev, sizeof(*ithc->prev_regs), GFP_KERNEL);
+
+	// PCI initialization.
+	CHECK_RET(pcim_enable_device, pci);
+	pci_set_master(pci);
+	CHECK_RET(pcim_iomap_regions, pci, BIT(0), DEVNAME " regs");
+	CHECK_RET(dma_set_mask_and_coherent, &pci->dev, DMA_BIT_MASK(64));
+	CHECK_RET(pci_set_power_state, pci, PCI_D0);
+	ithc->regs = pcim_iomap_table(pci)[0];
+
+	// Allocate IRQ.
+	if (!ithc_use_polling) {
+		CHECK_RET(pci_alloc_irq_vectors, pci, 1, 1, PCI_IRQ_MSI | PCI_IRQ_MSIX);
+		ithc->irq = CHECK(pci_irq_vector, pci, 0);
+		if (ithc->irq < 0)
+			return ithc->irq;
+	}
+
+	// Initialize THC and touch device.
+	CHECK_RET(ithc_init_device, ithc);
+
+	// Initialize HID and DMA.
+	CHECK_RET(ithc_hid_init, ithc);
+	if (ithc_use_rx0)
+		CHECK_RET(ithc_dma_rx_init, ithc, 0);
+	if (ithc_use_rx1)
+		CHECK_RET(ithc_dma_rx_init, ithc, 1);
+	CHECK_RET(ithc_dma_tx_init, ithc);
+
+	timer_setup(&ithc->idle_timer, ithc_idle_timer_callback, 0);
+
+	// Add ithc_stop() callback AFTER setting up DMA buffers, so that polling/irqs/DMA are
+	// disabled BEFORE the buffers are freed.
+	CHECK_RET(devm_add_action_or_reset, &pci->dev, ithc_stop, ithc);
+
+	// Start polling/IRQ.
+	if (ithc_use_polling) {
+		pci_info(pci, "using polling instead of irq\n");
+		// Use a thread instead of simple timer because we want to be able to sleep.
+		ithc->poll_thread = kthread_run(ithc_poll_thread, ithc, DEVNAME "poll");
+		if (IS_ERR(ithc->poll_thread)) {
+			int err = PTR_ERR(ithc->poll_thread);
+			ithc->poll_thread = NULL;
+			return err;
+		}
+	} else {
+		CHECK_RET(devm_request_threaded_irq, &pci->dev, ithc->irq, NULL,
+			ithc_interrupt_thread, IRQF_TRIGGER_HIGH | IRQF_ONESHOT, DEVNAME, ithc);
+	}
+
+	if (ithc_use_rx0)
+		ithc_dma_rx_enable(ithc, 0);
+	if (ithc_use_rx1)
+		ithc_dma_rx_enable(ithc, 1);
+
+	// hid_add_device() can only be called after irq/polling is started and DMA is enabled,
+	// because it calls ithc_hid_parse() which reads the report descriptor via DMA.
+	CHECK_RET(hid_add_device, ithc->hid.dev);
+
+	CHECK(ithc_debug_init_device, ithc);
+
+	ithc_set_ltr_idle(ithc);
+
+	pci_dbg(pci, "started\n");
+	return 0;
+}
+
+static int ithc_probe(struct pci_dev *pci, const struct pci_device_id *id)
+{
+	pci_dbg(pci, "device probe\n");
+	return ithc_start(pci);
+}
+
+static void ithc_remove(struct pci_dev *pci)
+{
+	pci_dbg(pci, "device remove\n");
+	// all cleanup is handled by devres
+}
+
+// For suspend/resume, we just deinitialize and reinitialize everything.
+// TODO It might be cleaner to keep the HID device around, however we would then have to signal
+// to userspace that the touch device has lost state and userspace needs to e.g. resend 'set
+// feature' requests. Hidraw does not seem to have a facility to do that.
+static int ithc_suspend(struct device *dev)
+{
+	struct pci_dev *pci = to_pci_dev(dev);
+	pci_dbg(pci, "pm suspend\n");
+	devres_release_group(dev, ithc_start);
+	return 0;
+}
+
+static int ithc_resume(struct device *dev)
+{
+	struct pci_dev *pci = to_pci_dev(dev);
+	pci_dbg(pci, "pm resume\n");
+	return ithc_start(pci);
+}
+
+static int ithc_freeze(struct device *dev)
+{
+	struct pci_dev *pci = to_pci_dev(dev);
+	pci_dbg(pci, "pm freeze\n");
+	devres_release_group(dev, ithc_start);
+	return 0;
+}
+
+static int ithc_thaw(struct device *dev)
+{
+	struct pci_dev *pci = to_pci_dev(dev);
+	pci_dbg(pci, "pm thaw\n");
+	return ithc_start(pci);
+}
+
+static int ithc_restore(struct device *dev)
+{
+	struct pci_dev *pci = to_pci_dev(dev);
+	pci_dbg(pci, "pm restore\n");
+	return ithc_start(pci);
+}
+
+static struct pci_driver ithc_driver = {
+	.name = DEVNAME,
+	.id_table = ithc_pci_tbl,
+	.probe = ithc_probe,
+	.remove = ithc_remove,
+	.driver.pm = &(const struct dev_pm_ops) {
+		.suspend = ithc_suspend,
+		.resume = ithc_resume,
+		.freeze = ithc_freeze,
+		.thaw = ithc_thaw,
+		.restore = ithc_restore,
+	},
+	.driver.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+};
+
+static int __init ithc_init(void)
+{
+	ithc_debug_init_module();
+	return pci_register_driver(&ithc_driver);
+}
+
+static void __exit ithc_exit(void)
+{
+	pci_unregister_driver(&ithc_driver);
+	ithc_debug_exit_module();
+}
+
+module_init(ithc_init);
+module_exit(ithc_exit);
+
diff --git a/drivers/hid/ithc/ithc-quickspi.c b/drivers/hid/ithc/ithc-quickspi.c
new file mode 100644
index 000000000000..e2d1690b8cf8
--- /dev/null
+++ b/drivers/hid/ithc/ithc-quickspi.c
@@ -0,0 +1,607 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+
+// Some public THC/QuickSPI documentation can be found in:
+// - Intel Firmware Support Package repo: https://github.com/intel/FSP
+// - HID over SPI (HIDSPI) spec: https://www.microsoft.com/en-us/download/details.aspx?id=103325
+
+#include "ithc.h"
+
+static const guid_t guid_hidspi =
+	GUID_INIT(0x6e2ac436, 0x0fcf, 0x41af, 0xa2, 0x65, 0xb3, 0x2a, 0x22, 0x0d, 0xcf, 0xab);
+static const guid_t guid_thc_quickspi =
+	GUID_INIT(0x300d35b7, 0xac20, 0x413e, 0x8e, 0x9c, 0x92, 0xe4, 0xda, 0xfd, 0x0a, 0xfe);
+static const guid_t guid_thc_ltr =
+	GUID_INIT(0x84005682, 0x5b71, 0x41a4, 0x8d, 0x66, 0x81, 0x30, 0xf7, 0x87, 0xa1, 0x38);
+
+// TODO The HIDSPI spec says revision should be 3. Should we try both?
+#define DSM_REV 2
+
+struct hidspi_header {
+	u8 type;
+	u16 len;
+	u8 id;
+} __packed;
+static_assert(sizeof(struct hidspi_header) == 4);
+
+#define HIDSPI_INPUT_TYPE_DATA                        1
+#define HIDSPI_INPUT_TYPE_RESET_RESPONSE              3
+#define HIDSPI_INPUT_TYPE_COMMAND_RESPONSE            4
+#define HIDSPI_INPUT_TYPE_GET_FEATURE_RESPONSE        5
+#define HIDSPI_INPUT_TYPE_DEVICE_DESCRIPTOR           7
+#define HIDSPI_INPUT_TYPE_REPORT_DESCRIPTOR           8
+#define HIDSPI_INPUT_TYPE_SET_FEATURE_RESPONSE        9
+#define HIDSPI_INPUT_TYPE_OUTPUT_REPORT_RESPONSE      10
+#define HIDSPI_INPUT_TYPE_GET_INPUT_REPORT_RESPONSE   11
+
+#define HIDSPI_OUTPUT_TYPE_DEVICE_DESCRIPTOR_REQUEST  1
+#define HIDSPI_OUTPUT_TYPE_REPORT_DESCRIPTOR_REQUEST  2
+#define HIDSPI_OUTPUT_TYPE_SET_FEATURE                3
+#define HIDSPI_OUTPUT_TYPE_GET_FEATURE                4
+#define HIDSPI_OUTPUT_TYPE_OUTPUT_REPORT              5
+#define HIDSPI_OUTPUT_TYPE_INPUT_REPORT_REQUEST       6
+#define HIDSPI_OUTPUT_TYPE_COMMAND                    7
+
+struct hidspi_device_descriptor {
+	u16 wDeviceDescLength;
+	u16 bcdVersion;
+	u16 wReportDescLength;
+	u16 wMaxInputLength;
+	u16 wMaxOutputLength;
+	u16 wMaxFragmentLength;
+	u16 wVendorID;
+	u16 wProductID;
+	u16 wVersionID;
+	u16 wFlags;
+	u32 dwReserved;
+};
+static_assert(sizeof(struct hidspi_device_descriptor) == 24);
+
+static int read_acpi_u32(struct ithc *ithc, const guid_t *guid, u32 func, u32 *dest)
+{
+	acpi_handle handle = ACPI_HANDLE(&ithc->pci->dev);
+	union acpi_object *o = acpi_evaluate_dsm(handle, guid, DSM_REV, func, NULL);
+	if (!o)
+		return 0;
+	if (o->type != ACPI_TYPE_INTEGER) {
+		pci_err(ithc->pci, "DSM %pUl %u returned type %i instead of integer\n",
+			guid, func, o->type);
+		ACPI_FREE(o);
+		return -1;
+	}
+	pci_dbg(ithc->pci, "DSM %pUl %u = 0x%08x\n", guid, func, (u32)o->integer.value);
+	*dest = (u32)o->integer.value;
+	ACPI_FREE(o);
+	return 1;
+}
+
+static int read_acpi_buf(struct ithc *ithc, const guid_t *guid, u32 func, size_t len, u8 *dest)
+{
+	acpi_handle handle = ACPI_HANDLE(&ithc->pci->dev);
+	union acpi_object *o = acpi_evaluate_dsm(handle, guid, DSM_REV, func, NULL);
+	if (!o)
+		return 0;
+	if (o->type != ACPI_TYPE_BUFFER) {
+		pci_err(ithc->pci, "DSM %pUl %u returned type %i instead of buffer\n",
+			guid, func, o->type);
+		ACPI_FREE(o);
+		return -1;
+	}
+	if (o->buffer.length != len) {
+		pci_err(ithc->pci, "DSM %pUl %u returned len %u instead of %zu\n",
+			guid, func, o->buffer.length, len);
+		ACPI_FREE(o);
+		return -1;
+	}
+	memcpy(dest, o->buffer.pointer, len);
+	pci_dbg(ithc->pci, "DSM %pUl %u = 0x%02x\n", guid, func, dest[0]);
+	ACPI_FREE(o);
+	return 1;
+}
+
+int ithc_read_acpi_config(struct ithc *ithc, struct ithc_acpi_config *cfg)
+{
+	int r;
+	acpi_handle handle = ACPI_HANDLE(&ithc->pci->dev);
+
+	cfg->has_config = acpi_check_dsm(handle, &guid_hidspi, DSM_REV, BIT(0));
+	if (!cfg->has_config)
+		return 0;
+
+	// HIDSPI settings
+
+	r = read_acpi_u32(ithc, &guid_hidspi, 1, &cfg->input_report_header_address);
+	if (r < 0)
+		return r;
+	cfg->has_input_report_header_address = r > 0;
+	if (r > 0 && cfg->input_report_header_address > 0xffffff) {
+		pci_err(ithc->pci, "Invalid input report header address 0x%x\n",
+			cfg->input_report_header_address);
+		return -1;
+	}
+
+	r = read_acpi_u32(ithc, &guid_hidspi, 2, &cfg->input_report_body_address);
+	if (r < 0)
+		return r;
+	cfg->has_input_report_body_address = r > 0;
+	if (r > 0 && cfg->input_report_body_address > 0xffffff) {
+		pci_err(ithc->pci, "Invalid input report body address 0x%x\n",
+			cfg->input_report_body_address);
+		return -1;
+	}
+
+	r = read_acpi_u32(ithc, &guid_hidspi, 3, &cfg->output_report_body_address);
+	if (r < 0)
+		return r;
+	cfg->has_output_report_body_address = r > 0;
+	if (r > 0 && cfg->output_report_body_address > 0xffffff) {
+		pci_err(ithc->pci, "Invalid output report body address 0x%x\n",
+			cfg->output_report_body_address);
+		return -1;
+	}
+
+	r = read_acpi_buf(ithc, &guid_hidspi, 4, sizeof(cfg->read_opcode), &cfg->read_opcode);
+	if (r < 0)
+		return r;
+	cfg->has_read_opcode = r > 0;
+
+	r = read_acpi_buf(ithc, &guid_hidspi, 5, sizeof(cfg->write_opcode), &cfg->write_opcode);
+	if (r < 0)
+		return r;
+	cfg->has_write_opcode = r > 0;
+
+	u32 flags;
+	r = read_acpi_u32(ithc, &guid_hidspi, 6, &flags);
+	if (r < 0)
+		return r;
+	cfg->has_read_mode = cfg->has_write_mode = r > 0;
+	if (r > 0) {
+		cfg->read_mode = (flags >> 14) & 3;
+		cfg->write_mode = flags & BIT(13) ? cfg->read_mode : SPI_MODE_SINGLE;
+	}
+
+	// Quick SPI settings
+
+	r = read_acpi_u32(ithc, &guid_thc_quickspi, 1, &cfg->spi_frequency);
+	if (r < 0)
+		return r;
+	cfg->has_spi_frequency = r > 0;
+
+	r = read_acpi_u32(ithc, &guid_thc_quickspi, 2, &cfg->limit_packet_size);
+	if (r < 0)
+		return r;
+	cfg->has_limit_packet_size = r > 0;
+
+	r = read_acpi_u32(ithc, &guid_thc_quickspi, 3, &cfg->tx_delay);
+	if (r < 0)
+		return r;
+	cfg->has_tx_delay = r > 0;
+	if (r > 0)
+		cfg->tx_delay &= 0xffff;
+
+	// LTR settings
+
+	r = read_acpi_u32(ithc, &guid_thc_ltr, 1, &cfg->active_ltr);
+	if (r < 0)
+		return r;
+	cfg->has_active_ltr = r > 0;
+	if (r > 0 && (!cfg->active_ltr || cfg->active_ltr > 0x3ff)) {
+		if (cfg->active_ltr != 0xffffffff)
+			pci_warn(ithc->pci, "Ignoring invalid active LTR value 0x%x\n",
+				cfg->active_ltr);
+		cfg->active_ltr = 500;
+	}
+
+	r = read_acpi_u32(ithc, &guid_thc_ltr, 2, &cfg->idle_ltr);
+	if (r < 0)
+		return r;
+	cfg->has_idle_ltr = r > 0;
+	if (r > 0 && (!cfg->idle_ltr || cfg->idle_ltr > 0x3ff)) {
+		if (cfg->idle_ltr != 0xffffffff)
+			pci_warn(ithc->pci, "Ignoring invalid idle LTR value 0x%x\n",
+				cfg->idle_ltr);
+		cfg->idle_ltr = 500;
+		if (cfg->has_active_ltr && cfg->active_ltr > cfg->idle_ltr)
+			cfg->idle_ltr = cfg->active_ltr;
+	}
+
+	return 0;
+}
+
+void ithc_print_acpi_config(struct ithc *ithc, const struct ithc_acpi_config *cfg)
+{
+	if (!cfg->has_config) {
+		pci_info(ithc->pci, "No ACPI config");
+		return;
+	}
+
+	char input_report_header_address[16] = "-";
+	if (cfg->has_input_report_header_address)
+		sprintf(input_report_header_address, "0x%x", cfg->input_report_header_address);
+	char input_report_body_address[16] = "-";
+	if (cfg->has_input_report_body_address)
+		sprintf(input_report_body_address, "0x%x", cfg->input_report_body_address);
+	char output_report_body_address[16] = "-";
+	if (cfg->has_output_report_body_address)
+		sprintf(output_report_body_address, "0x%x", cfg->output_report_body_address);
+	char read_opcode[16] = "-";
+	if (cfg->has_read_opcode)
+		sprintf(read_opcode, "0x%02x", cfg->read_opcode);
+	char write_opcode[16] = "-";
+	if (cfg->has_write_opcode)
+		sprintf(write_opcode, "0x%02x", cfg->write_opcode);
+	char read_mode[16] = "-";
+	if (cfg->has_read_mode)
+		sprintf(read_mode, "%i", cfg->read_mode);
+	char write_mode[16] = "-";
+	if (cfg->has_write_mode)
+		sprintf(write_mode, "%i", cfg->write_mode);
+	char spi_frequency[16] = "-";
+	if (cfg->has_spi_frequency)
+		sprintf(spi_frequency, "%u", cfg->spi_frequency);
+	char limit_packet_size[16] = "-";
+	if (cfg->has_limit_packet_size)
+		sprintf(limit_packet_size, "%u", cfg->limit_packet_size);
+	char tx_delay[16] = "-";
+	if (cfg->has_tx_delay)
+		sprintf(tx_delay, "%u", cfg->tx_delay);
+	char active_ltr[16] = "-";
+	if (cfg->has_active_ltr)
+		sprintf(active_ltr, "%u", cfg->active_ltr);
+	char idle_ltr[16] = "-";
+	if (cfg->has_idle_ltr)
+		sprintf(idle_ltr, "%u", cfg->idle_ltr);
+
+	pci_info(ithc->pci, "ACPI config: InputHeaderAddr=%s InputBodyAddr=%s OutputBodyAddr=%s ReadOpcode=%s WriteOpcode=%s ReadMode=%s WriteMode=%s Frequency=%s LimitPacketSize=%s TxDelay=%s ActiveLTR=%s IdleLTR=%s\n",
+		input_report_header_address, input_report_body_address, output_report_body_address,
+		read_opcode, write_opcode, read_mode, write_mode,
+		spi_frequency, limit_packet_size, tx_delay, active_ltr, idle_ltr);
+}
+
+static void set_opcode(struct ithc *ithc, size_t i, u8 opcode)
+{
+	writeb(opcode, &ithc->regs->opcode[i].header);
+	writeb(opcode, &ithc->regs->opcode[i].single);
+	writeb(opcode, &ithc->regs->opcode[i].dual);
+	writeb(opcode, &ithc->regs->opcode[i].quad);
+}
+
+static int ithc_quickspi_init_regs(struct ithc *ithc, const struct ithc_acpi_config *cfg)
+{
+	pci_dbg(ithc->pci, "initializing QuickSPI registers\n");
+
+	// SPI frequency and mode
+	if (!cfg->has_spi_frequency || !cfg->spi_frequency) {
+		pci_err(ithc->pci, "Missing SPI frequency in configuration\n");
+		return -EINVAL;
+	}
+	unsigned int clkdiv = DIV_ROUND_UP(SPI_CLK_FREQ_BASE, cfg->spi_frequency);
+	bool clkdiv8 = clkdiv > 7;
+	if (clkdiv8)
+		clkdiv = min(7u, DIV_ROUND_UP(clkdiv, 8u));
+	if (!clkdiv)
+		clkdiv = 1;
+	CHECK_RET(ithc_set_spi_config, ithc, clkdiv, clkdiv8,
+		cfg->has_read_mode ? cfg->read_mode : SPI_MODE_SINGLE,
+		cfg->has_write_mode ? cfg->write_mode : SPI_MODE_SINGLE);
+
+	// SPI addresses and opcodes
+	if (cfg->has_input_report_header_address)
+		writel(cfg->input_report_header_address, &ithc->regs->spi_header_addr);
+	if (cfg->has_input_report_body_address) {
+		writel(cfg->input_report_body_address, &ithc->regs->dma_rx[0].spi_addr);
+		writel(cfg->input_report_body_address, &ithc->regs->dma_rx[1].spi_addr);
+	}
+	if (cfg->has_output_report_body_address)
+		writel(cfg->output_report_body_address, &ithc->regs->dma_tx.spi_addr);
+
+	switch (ithc->pci->device) {
+	// LKF/TGL don't support QuickSPI.
+	// For ADL, opcode layout is RX/TX/unused.
+	case PCI_DEVICE_ID_INTEL_THC_ADL_S_PORT1:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_S_PORT2:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_P_PORT1:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_P_PORT2:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_M_PORT1:
+	case PCI_DEVICE_ID_INTEL_THC_ADL_M_PORT2:
+		if (cfg->has_read_opcode) {
+			set_opcode(ithc, 0, cfg->read_opcode);
+		}
+		if (cfg->has_write_opcode) {
+			set_opcode(ithc, 1, cfg->write_opcode);
+		}
+		break;
+	// For MTL, opcode layout was changed to RX/RX/TX.
+	// (RPL layout is unknown.)
+	default:
+		if (cfg->has_read_opcode) {
+			set_opcode(ithc, 0, cfg->read_opcode);
+			set_opcode(ithc, 1, cfg->read_opcode);
+		}
+		if (cfg->has_write_opcode) {
+			set_opcode(ithc, 2, cfg->write_opcode);
+		}
+		break;
+	}
+
+	ithc_log_regs(ithc);
+
+	// The rest...
+	bitsl_set(&ithc->regs->dma_rx[0].init_unknown, INIT_UNKNOWN_31);
+
+	bitsl(&ithc->regs->quickspi_config1,
+		QUICKSPI_CONFIG1_UNKNOWN_0(0xff) | QUICKSPI_CONFIG1_UNKNOWN_5(0xff) |
+		QUICKSPI_CONFIG1_UNKNOWN_10(0xff) | QUICKSPI_CONFIG1_UNKNOWN_16(0xffff),
+		QUICKSPI_CONFIG1_UNKNOWN_0(4) | QUICKSPI_CONFIG1_UNKNOWN_5(4) |
+		QUICKSPI_CONFIG1_UNKNOWN_10(22) | QUICKSPI_CONFIG1_UNKNOWN_16(2));
+
+	bitsl(&ithc->regs->quickspi_config2,
+		QUICKSPI_CONFIG2_UNKNOWN_0(0xff) | QUICKSPI_CONFIG2_UNKNOWN_5(0xff) |
+		QUICKSPI_CONFIG2_UNKNOWN_12(0xff),
+		QUICKSPI_CONFIG2_UNKNOWN_0(8) | QUICKSPI_CONFIG2_UNKNOWN_5(14) |
+		QUICKSPI_CONFIG2_UNKNOWN_12(2));
+
+	u32 pktsize = cfg->has_limit_packet_size && cfg->limit_packet_size == 1 ? 4 : 0x80;
+	bitsl(&ithc->regs->spi_config,
+		SPI_CONFIG_READ_PACKET_SIZE(0xfff) | SPI_CONFIG_WRITE_PACKET_SIZE(0xfff),
+		SPI_CONFIG_READ_PACKET_SIZE(pktsize) | SPI_CONFIG_WRITE_PACKET_SIZE(pktsize));
+
+	bitsl_set(&ithc->regs->quickspi_config2,
+		QUICKSPI_CONFIG2_UNKNOWN_16 | QUICKSPI_CONFIG2_UNKNOWN_17);
+	bitsl(&ithc->regs->quickspi_config2,
+		QUICKSPI_CONFIG2_DISABLE_READ_ADDRESS_INCREMENT |
+		QUICKSPI_CONFIG2_DISABLE_WRITE_ADDRESS_INCREMENT |
+		QUICKSPI_CONFIG2_ENABLE_WRITE_STREAMING_MODE, 0);
+
+	return 0;
+}
+
+static int wait_for_report(struct ithc *ithc)
+{
+	CHECK_RET(waitl, ithc, &ithc->regs->dma_rx[0].status,
+		DMA_RX_STATUS_READY, DMA_RX_STATUS_READY);
+	writel(DMA_RX_STATUS_READY, &ithc->regs->dma_rx[0].status);
+
+	u32 h = readl(&ithc->regs->input_header);
+	ithc_log_regs(ithc);
+	if (INPUT_HEADER_SYNC(h) != INPUT_HEADER_SYNC_VALUE
+		|| INPUT_HEADER_VERSION(h) != INPUT_HEADER_VERSION_VALUE) {
+		pci_err(ithc->pci, "invalid input report frame header 0x%08x\n", h);
+		return -ENODATA;
+	}
+	return INPUT_HEADER_REPORT_LENGTH(h) * 4;
+}
+
+static int ithc_quickspi_init_hidspi(struct ithc *ithc, const struct ithc_acpi_config *cfg)
+{
+	pci_dbg(ithc->pci, "initializing HIDSPI\n");
+
+	// HIDSPI initialization sequence:
+	// "1. The host shall invoke the ACPI reset method to clear the device state."
+	acpi_status s = acpi_evaluate_object(ACPI_HANDLE(&ithc->pci->dev), "_RST", NULL, NULL);
+	if (ACPI_FAILURE(s)) {
+		pci_err(ithc->pci, "ACPI reset failed\n");
+		return -EIO;
+	}
+
+	bitsl(&ithc->regs->control_bits, CONTROL_QUIESCE, 0);
+
+	// "2. Within 1 second, the device shall signal an interrupt and make available to the host
+	// an input report containing a device reset response."
+	int size = wait_for_report(ithc);
+	if (size < 0)
+		return size;
+	if (size < sizeof(struct hidspi_header)) {
+		pci_err(ithc->pci, "SPI data size too small for reset response (%u)\n", size);
+		return -EMSGSIZE;
+	}
+
+	// "3. The host shall read the reset response from the device at the Input Report addresses
+	// specified in ACPI."
+	u32 in_addr = cfg->has_input_report_body_address ? cfg->input_report_body_address : 0x1000;
+	struct {
+		struct hidspi_header header;
+		union {
+			struct hidspi_device_descriptor device_desc;
+			u32 data[16];
+		};
+	} resp = { 0 };
+	if (size > sizeof(resp)) {
+		pci_err(ithc->pci, "SPI data size for reset response too big (%u)\n", size);
+		return -EMSGSIZE;
+	}
+	CHECK_RET(ithc_spi_command, ithc, SPI_CMD_CODE_READ, in_addr, size, &resp);
+	if (resp.header.type != HIDSPI_INPUT_TYPE_RESET_RESPONSE) {
+		pci_err(ithc->pci, "received type %i instead of reset response\n", resp.header.type);
+		return -ENOMSG;
+	}
+
+	// "4. The host shall then write an Output Report to the device at the Output Report Address
+	// specified in ACPI, requesting the Device Descriptor from the device."
+	u32 out_addr = cfg->has_output_report_body_address ? cfg->output_report_body_address : 0x1000;
+	struct hidspi_header req = { .type = HIDSPI_OUTPUT_TYPE_DEVICE_DESCRIPTOR_REQUEST };
+	CHECK_RET(ithc_spi_command, ithc, SPI_CMD_CODE_WRITE, out_addr, sizeof(req), &req);
+
+	// "5. Within 1 second, the device shall signal an interrupt and make available to the host
+	// an input report containing the Device Descriptor."
+	size = wait_for_report(ithc);
+	if (size < 0)
+		return size;
+	if (size < sizeof(resp.header) + sizeof(resp.device_desc)) {
+		pci_err(ithc->pci, "SPI data size too small for device descriptor (%u)\n", size);
+		return -EMSGSIZE;
+	}
+
+	// "6. The host shall read the Device Descriptor from the Input Report addresses specified
+	// in ACPI."
+	if (size > sizeof(resp)) {
+		pci_err(ithc->pci, "SPI data size for device descriptor too big (%u)\n", size);
+		return -EMSGSIZE;
+	}
+	memset(&resp, 0, sizeof(resp));
+	CHECK_RET(ithc_spi_command, ithc, SPI_CMD_CODE_READ, in_addr, size, &resp);
+	if (resp.header.type != HIDSPI_INPUT_TYPE_DEVICE_DESCRIPTOR) {
+		pci_err(ithc->pci, "received type %i instead of device descriptor\n",
+			resp.header.type);
+		return -ENOMSG;
+	}
+	struct hidspi_device_descriptor *d = &resp.device_desc;
+	if (resp.header.len < sizeof(*d)) {
+		pci_err(ithc->pci, "response too small for device descriptor (%u)\n",
+			resp.header.len);
+		return -EMSGSIZE;
+	}
+	if (d->wDeviceDescLength != sizeof(*d)) {
+		pci_err(ithc->pci, "invalid device descriptor length (%u)\n",
+			d->wDeviceDescLength);
+		return -EMSGSIZE;
+	}
+
+	pci_info(ithc->pci, "Device descriptor: bcdVersion=0x%04x wReportDescLength=%u wMaxInputLength=%u wMaxOutputLength=%u wMaxFragmentLength=%u wVendorID=0x%04x wProductID=0x%04x wVersionID=0x%04x wFlags=0x%04x dwReserved=0x%08x\n",
+		d->bcdVersion, d->wReportDescLength,
+		d->wMaxInputLength, d->wMaxOutputLength, d->wMaxFragmentLength,
+		d->wVendorID, d->wProductID, d->wVersionID,
+		d->wFlags, d->dwReserved);
+
+	ithc->vendor_id = d->wVendorID;
+	ithc->product_id = d->wProductID;
+	ithc->product_rev = d->wVersionID;
+	ithc->max_rx_size = max_t(u32, d->wMaxInputLength,
+		d->wReportDescLength + sizeof(struct hidspi_header));
+	ithc->max_tx_size = d->wMaxOutputLength;
+	ithc->have_config = true;
+
+	// "7. The device and host shall then enter their "Ready" states - where the device may
+	// begin sending Input Reports, and the device shall be prepared for Output Reports from
+	// the host."
+
+	return 0;
+}
+
+int ithc_quickspi_init(struct ithc *ithc, const struct ithc_acpi_config *cfg)
+{
+	bitsl_set(&ithc->regs->control_bits, CONTROL_QUIESCE);
+	CHECK_RET(waitl, ithc, &ithc->regs->control_bits, CONTROL_IS_QUIESCED, CONTROL_IS_QUIESCED);
+
+	ithc_log_regs(ithc);
+	CHECK_RET(ithc_quickspi_init_regs, ithc, cfg);
+	ithc_log_regs(ithc);
+	CHECK_RET(ithc_quickspi_init_hidspi, ithc, cfg);
+	ithc_log_regs(ithc);
+
+	// This value is set to 2 in ithc_quickspi_init_regs(). It needs to be set to 1 here,
+	// otherwise DMA will not work. Maybe selects between DMA and PIO mode?
+	bitsl(&ithc->regs->quickspi_config1,
+		QUICKSPI_CONFIG1_UNKNOWN_16(0xffff), QUICKSPI_CONFIG1_UNKNOWN_16(1));
+
+	// TODO Do we need to set any of the following bits here?
+	//bitsb_set(&ithc->regs->dma_rx[1].control2, DMA_RX_CONTROL2_UNKNOWN_4);
+	//bitsb_set(&ithc->regs->dma_rx[0].control2, DMA_RX_CONTROL2_UNKNOWN_5);
+	//bitsb_set(&ithc->regs->dma_rx[1].control2, DMA_RX_CONTROL2_UNKNOWN_5);
+	//bitsl_set(&ithc->regs->dma_rx[0].init_unknown, INIT_UNKNOWN_3);
+	//bitsl_set(&ithc->regs->dma_rx[0].init_unknown, INIT_UNKNOWN_31);
+
+	ithc_log_regs(ithc);
+
+	return 0;
+}
+
+void ithc_quickspi_exit(struct ithc *ithc)
+{
+	// TODO Should we send HIDSPI 'power off' command?
+	//struct hidspi_header h = { .type = HIDSPI_OUTPUT_TYPE_COMMAND, .id = 3, };
+	//struct ithc_data d = { .type = ITHC_DATA_RAW, .data = &h, .size = sizeof(h) };
+	//CHECK(ithc_dma_tx, ithc, &d); // or ithc_spi_command()
+}
+
+int ithc_quickspi_decode_rx(struct ithc *ithc, const void *src, size_t len, struct ithc_data *dest)
+{
+	const struct hidspi_header *hdr = src;
+
+	if (len < sizeof(*hdr))
+		return -ENODATA;
+	// TODO Do we need to handle HIDSPI packet fragmentation?
+	if (len < sizeof(*hdr) + hdr->len)
+		return -EMSGSIZE;
+	if (len > round_up(sizeof(*hdr) + hdr->len, 4))
+		return -EMSGSIZE;
+
+	switch (hdr->type) {
+	case HIDSPI_INPUT_TYPE_RESET_RESPONSE:
+		// TODO "When the device detects an error condition, it may interrupt and make
+		// available to the host an Input Report containing an unsolicited Reset Response.
+		// After receiving an unsolicited Reset Response, the host shall initiate the
+		// request procedure from step (4) in the [HIDSPI initialization] process."
+		dest->type = ITHC_DATA_ERROR;
+		return 0;
+	case HIDSPI_INPUT_TYPE_REPORT_DESCRIPTOR:
+		dest->type = ITHC_DATA_REPORT_DESCRIPTOR;
+		dest->data = hdr + 1;
+		dest->size = hdr->len;
+		return 0;
+	case HIDSPI_INPUT_TYPE_DATA:
+	case HIDSPI_INPUT_TYPE_GET_INPUT_REPORT_RESPONSE:
+		dest->type = ITHC_DATA_INPUT_REPORT;
+		dest->data = &hdr->id;
+		dest->size = hdr->len + 1;
+		return 0;
+	case HIDSPI_INPUT_TYPE_GET_FEATURE_RESPONSE:
+		dest->type = ITHC_DATA_GET_FEATURE;
+		dest->data = &hdr->id;
+		dest->size = hdr->len + 1;
+		return 0;
+	case HIDSPI_INPUT_TYPE_SET_FEATURE_RESPONSE:
+	case HIDSPI_INPUT_TYPE_OUTPUT_REPORT_RESPONSE:
+		dest->type = ITHC_DATA_IGNORE;
+		return 0;
+	default:
+		return -EINVAL;
+	}
+}
+
+ssize_t ithc_quickspi_encode_tx(struct ithc *ithc, const struct ithc_data *src, void *dest,
+	size_t maxlen)
+{
+	struct hidspi_header *hdr = dest;
+
+	size_t src_size = src->size;
+	const u8 *src_data = src->data;
+	u8 type;
+
+	switch (src->type) {
+	case ITHC_DATA_SET_FEATURE:
+		type = HIDSPI_OUTPUT_TYPE_SET_FEATURE;
+		break;
+	case ITHC_DATA_GET_FEATURE:
+		type = HIDSPI_OUTPUT_TYPE_GET_FEATURE;
+		break;
+	case ITHC_DATA_OUTPUT_REPORT:
+		type = HIDSPI_OUTPUT_TYPE_OUTPUT_REPORT;
+		break;
+	case ITHC_DATA_REPORT_DESCRIPTOR:
+		type = HIDSPI_OUTPUT_TYPE_REPORT_DESCRIPTOR_REQUEST;
+		src_size = 0;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	u8 id = 0;
+	if (src_size) {
+		id = *src_data++;
+		src_size--;
+	}
+
+	// Data must be padded to next 4-byte boundary.
+	size_t padded = round_up(src_size, 4);
+	if (sizeof(*hdr) + padded > maxlen)
+		return -EOVERFLOW;
+
+	// Fill the TX buffer with header and data.
+	hdr->type = type;
+	hdr->len = (u16)src_size;
+	hdr->id = id;
+	memcpy_and_pad(hdr + 1, padded, src_data, src_size, 0);
+
+	return sizeof(*hdr) + padded;
+}
+
diff --git a/drivers/hid/ithc/ithc-quickspi.h b/drivers/hid/ithc/ithc-quickspi.h
new file mode 100644
index 000000000000..74d882f6b2f0
--- /dev/null
+++ b/drivers/hid/ithc/ithc-quickspi.h
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause */
+
+struct ithc_acpi_config {
+	bool has_config: 1;
+	bool has_input_report_header_address: 1;
+	bool has_input_report_body_address: 1;
+	bool has_output_report_body_address: 1;
+	bool has_read_opcode: 1;
+	bool has_write_opcode: 1;
+	bool has_read_mode: 1;
+	bool has_write_mode: 1;
+	bool has_spi_frequency: 1;
+	bool has_limit_packet_size: 1;
+	bool has_tx_delay: 1;
+	bool has_active_ltr: 1;
+	bool has_idle_ltr: 1;
+	u32 input_report_header_address;
+	u32 input_report_body_address;
+	u32 output_report_body_address;
+	u8 read_opcode;
+	u8 write_opcode;
+	u8 read_mode;
+	u8 write_mode;
+	u32 spi_frequency;
+	u32 limit_packet_size;
+	u32 tx_delay; // us/10 // TODO use?
+	u32 active_ltr; // ns/1024
+	u32 idle_ltr; // ns/1024
+};
+
+int ithc_read_acpi_config(struct ithc *ithc, struct ithc_acpi_config *cfg);
+void ithc_print_acpi_config(struct ithc *ithc, const struct ithc_acpi_config *cfg);
+
+int ithc_quickspi_init(struct ithc *ithc, const struct ithc_acpi_config *cfg);
+void ithc_quickspi_exit(struct ithc *ithc);
+int ithc_quickspi_decode_rx(struct ithc *ithc, const void *src, size_t len, struct ithc_data *dest);
+ssize_t ithc_quickspi_encode_tx(struct ithc *ithc, const struct ithc_data *src, void *dest,
+	size_t maxlen);
+
diff --git a/drivers/hid/ithc/ithc-regs.c b/drivers/hid/ithc/ithc-regs.c
new file mode 100644
index 000000000000..c0f13506af20
--- /dev/null
+++ b/drivers/hid/ithc/ithc-regs.c
@@ -0,0 +1,154 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+
+#include "ithc.h"
+
+#define reg_num(r) (0x1fff & (u16)(__force u64)(r))
+
+void bitsl(__iomem u32 *reg, u32 mask, u32 val)
+{
+	if (val & ~mask)
+		pr_err("register 0x%x: invalid value 0x%x for bitmask 0x%x\n",
+			reg_num(reg), val, mask);
+	writel((readl(reg) & ~mask) | (val & mask), reg);
+}
+
+void bitsb(__iomem u8 *reg, u8 mask, u8 val)
+{
+	if (val & ~mask)
+		pr_err("register 0x%x: invalid value 0x%x for bitmask 0x%x\n",
+			reg_num(reg), val, mask);
+	writeb((readb(reg) & ~mask) | (val & mask), reg);
+}
+
+int waitl(struct ithc *ithc, __iomem u32 *reg, u32 mask, u32 val)
+{
+	ithc_log_regs(ithc);
+	pci_dbg(ithc->pci, "waiting for reg 0x%04x mask 0x%08x val 0x%08x\n",
+		reg_num(reg), mask, val);
+	u32 x;
+	if (readl_poll_timeout(reg, x, (x & mask) == val, 200, 1000*1000)) {
+		ithc_log_regs(ithc);
+		pci_err(ithc->pci, "timed out waiting for reg 0x%04x mask 0x%08x val 0x%08x\n",
+			reg_num(reg), mask, val);
+		return -ETIMEDOUT;
+	}
+	ithc_log_regs(ithc);
+	pci_dbg(ithc->pci, "done waiting\n");
+	return 0;
+}
+
+int waitb(struct ithc *ithc, __iomem u8 *reg, u8 mask, u8 val)
+{
+	ithc_log_regs(ithc);
+	pci_dbg(ithc->pci, "waiting for reg 0x%04x mask 0x%02x val 0x%02x\n",
+		reg_num(reg), mask, val);
+	u8 x;
+	if (readb_poll_timeout(reg, x, (x & mask) == val, 200, 1000*1000)) {
+		ithc_log_regs(ithc);
+		pci_err(ithc->pci, "timed out waiting for reg 0x%04x mask 0x%02x val 0x%02x\n",
+			reg_num(reg), mask, val);
+		return -ETIMEDOUT;
+	}
+	ithc_log_regs(ithc);
+	pci_dbg(ithc->pci, "done waiting\n");
+	return 0;
+}
+
+static void calc_ltr(u64 *ns, unsigned int *val, unsigned int *scale)
+{
+	unsigned int s = 0;
+	u64 v = *ns;
+	while (v > 0x3ff) {
+		s++;
+		v >>= 5;
+	}
+	if (s > 5) {
+		s = 5;
+		v = 0x3ff;
+	}
+	*val = v;
+	*scale = s;
+	*ns = v << (5 * s);
+}
+
+void ithc_set_ltr_config(struct ithc *ithc, u64 active_ltr_ns, u64 idle_ltr_ns)
+{
+	unsigned int active_val, active_scale, idle_val, idle_scale;
+	calc_ltr(&active_ltr_ns, &active_val, &active_scale);
+	calc_ltr(&idle_ltr_ns, &idle_val, &idle_scale);
+	pci_dbg(ithc->pci, "setting active LTR value to %llu ns, idle LTR value to %llu ns\n",
+		active_ltr_ns, idle_ltr_ns);
+	writel(LTR_CONFIG_ENABLE_ACTIVE | LTR_CONFIG_ENABLE_IDLE | LTR_CONFIG_APPLY |
+		LTR_CONFIG_ACTIVE_LTR_SCALE(active_scale) | LTR_CONFIG_ACTIVE_LTR_VALUE(active_val) |
+		LTR_CONFIG_IDLE_LTR_SCALE(idle_scale) | LTR_CONFIG_IDLE_LTR_VALUE(idle_val),
+		&ithc->regs->ltr_config);
+}
+
+void ithc_set_ltr_idle(struct ithc *ithc)
+{
+	u32 ltr = readl(&ithc->regs->ltr_config);
+	switch (ltr & (LTR_CONFIG_STATUS_ACTIVE | LTR_CONFIG_STATUS_IDLE)) {
+	case LTR_CONFIG_STATUS_IDLE:
+		break;
+	case LTR_CONFIG_STATUS_ACTIVE:
+		writel(ltr | LTR_CONFIG_TOGGLE | LTR_CONFIG_APPLY, &ithc->regs->ltr_config);
+		break;
+	default:
+		pci_err(ithc->pci, "invalid LTR state 0x%08x\n", ltr);
+		break;
+	}
+}
+
+int ithc_set_spi_config(struct ithc *ithc, u8 clkdiv, bool clkdiv8, u8 read_mode, u8 write_mode)
+{
+	if (clkdiv == 0 || clkdiv > 7 || read_mode > SPI_MODE_QUAD || write_mode > SPI_MODE_QUAD)
+		return -EINVAL;
+	static const char * const modes[] = { "single", "dual", "quad" };
+	pci_dbg(ithc->pci, "setting SPI frequency to %i Hz, %s read, %s write\n",
+		SPI_CLK_FREQ_BASE / (clkdiv * (clkdiv8 ? 8 : 1)),
+		modes[read_mode], modes[write_mode]);
+	bitsl(&ithc->regs->spi_config,
+		SPI_CONFIG_READ_MODE(0xff) | SPI_CONFIG_READ_CLKDIV(0xff) |
+		SPI_CONFIG_WRITE_MODE(0xff) | SPI_CONFIG_WRITE_CLKDIV(0xff) |
+		SPI_CONFIG_CLKDIV_8,
+		SPI_CONFIG_READ_MODE(read_mode) | SPI_CONFIG_READ_CLKDIV(clkdiv) |
+		SPI_CONFIG_WRITE_MODE(write_mode) | SPI_CONFIG_WRITE_CLKDIV(clkdiv) |
+		(clkdiv8 ? SPI_CONFIG_CLKDIV_8 : 0));
+	return 0;
+}
+
+int ithc_spi_command(struct ithc *ithc, u8 command, u32 offset, u32 size, void *data)
+{
+	pci_dbg(ithc->pci, "SPI command %u, size %u, offset 0x%x\n", command, size, offset);
+	if (size > sizeof(ithc->regs->spi_cmd.data))
+		return -EINVAL;
+
+	// Wait if the device is still busy.
+	CHECK_RET(waitl, ithc, &ithc->regs->spi_cmd.status, SPI_CMD_STATUS_BUSY, 0);
+	// Clear result flags.
+	writel(SPI_CMD_STATUS_DONE | SPI_CMD_STATUS_ERROR, &ithc->regs->spi_cmd.status);
+
+	// Init SPI command data.
+	writeb(command, &ithc->regs->spi_cmd.code);
+	writew(size, &ithc->regs->spi_cmd.size);
+	writel(offset, &ithc->regs->spi_cmd.offset);
+	u32 *p = data, n = (size + 3) / 4;
+	for (u32 i = 0; i < n; i++)
+		writel(p[i], &ithc->regs->spi_cmd.data[i]);
+
+	// Start transmission.
+	bitsb_set(&ithc->regs->spi_cmd.control, SPI_CMD_CONTROL_SEND);
+	CHECK_RET(waitl, ithc, &ithc->regs->spi_cmd.status, SPI_CMD_STATUS_BUSY, 0);
+
+	// Read response.
+	if ((readl(&ithc->regs->spi_cmd.status) & (SPI_CMD_STATUS_DONE | SPI_CMD_STATUS_ERROR)) != SPI_CMD_STATUS_DONE)
+		return -EIO;
+	if (readw(&ithc->regs->spi_cmd.size) != size)
+		return -EMSGSIZE;
+	for (u32 i = 0; i < n; i++)
+		p[i] = readl(&ithc->regs->spi_cmd.data[i]);
+
+	writel(SPI_CMD_STATUS_DONE | SPI_CMD_STATUS_ERROR, &ithc->regs->spi_cmd.status);
+	return 0;
+}
+
diff --git a/drivers/hid/ithc/ithc-regs.h b/drivers/hid/ithc/ithc-regs.h
new file mode 100644
index 000000000000..4f541fe533fa
--- /dev/null
+++ b/drivers/hid/ithc/ithc-regs.h
@@ -0,0 +1,211 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause */
+
+#define LTR_CONFIG_ENABLE_ACTIVE            BIT(0)
+#define LTR_CONFIG_TOGGLE                   BIT(1)
+#define LTR_CONFIG_ENABLE_IDLE              BIT(2)
+#define LTR_CONFIG_APPLY                    BIT(3)
+#define LTR_CONFIG_IDLE_LTR_SCALE(x)        (((x) & 7) << 4)
+#define LTR_CONFIG_IDLE_LTR_VALUE(x)        (((x) & 0x3ff) << 7)
+#define LTR_CONFIG_ACTIVE_LTR_SCALE(x)      (((x) & 7) << 17)
+#define LTR_CONFIG_ACTIVE_LTR_VALUE(x)      (((x) & 0x3ff) << 20)
+#define LTR_CONFIG_STATUS_ACTIVE            BIT(30)
+#define LTR_CONFIG_STATUS_IDLE              BIT(31)
+
+#define CONTROL_QUIESCE                     BIT(1)
+#define CONTROL_IS_QUIESCED                 BIT(2)
+#define CONTROL_NRESET                      BIT(3)
+#define CONTROL_UNKNOWN_24(x)               (((x) & 3) << 24)
+#define CONTROL_READY                       BIT(29)
+
+#define SPI_CONFIG_READ_MODE(x)             (((x) & 3) << 2)
+#define SPI_CONFIG_READ_CLKDIV(x)           (((x) & 7) << 4)
+#define SPI_CONFIG_READ_PACKET_SIZE(x)      (((x) & 0x1ff) << 7)
+#define SPI_CONFIG_WRITE_MODE(x)            (((x) & 3) << 18)
+#define SPI_CONFIG_WRITE_CLKDIV(x)          (((x) & 7) << 20)
+#define SPI_CONFIG_CLKDIV_8                 BIT(23) // additionally divide clk by 8, for both read and write
+#define SPI_CONFIG_WRITE_PACKET_SIZE(x)     (((x) & 0xff) << 24)
+
+#define SPI_CLK_FREQ_BASE                   125000000
+#define SPI_MODE_SINGLE                     0
+#define SPI_MODE_DUAL                       1
+#define SPI_MODE_QUAD                       2
+
+#define ERROR_CONTROL_UNKNOWN_0             BIT(0)
+#define ERROR_CONTROL_DISABLE_DMA           BIT(1) // clears DMA_RX_CONTROL_ENABLE when a DMA error occurs
+#define ERROR_CONTROL_UNKNOWN_2             BIT(2)
+#define ERROR_CONTROL_UNKNOWN_3             BIT(3)
+#define ERROR_CONTROL_IRQ_DMA_UNKNOWN_9     BIT(9)
+#define ERROR_CONTROL_IRQ_DMA_UNKNOWN_10    BIT(10)
+#define ERROR_CONTROL_IRQ_DMA_UNKNOWN_12    BIT(12)
+#define ERROR_CONTROL_IRQ_DMA_UNKNOWN_13    BIT(13)
+#define ERROR_CONTROL_UNKNOWN_16(x)         (((x) & 0xff) << 16) // spi error code irq?
+#define ERROR_CONTROL_SET_DMA_STATUS        BIT(29) // sets DMA_RX_STATUS_ERROR when a DMA error occurs
+
+#define ERROR_STATUS_DMA                    BIT(28)
+#define ERROR_STATUS_SPI                    BIT(30)
+
+#define ERROR_FLAG_DMA_UNKNOWN_9            BIT(9)
+#define ERROR_FLAG_DMA_UNKNOWN_10           BIT(10)
+#define ERROR_FLAG_DMA_RX_TIMEOUT           BIT(12) // set when we receive a truncated DMA message
+#define ERROR_FLAG_DMA_UNKNOWN_13           BIT(13)
+#define ERROR_FLAG_SPI_BUS_TURNAROUND       BIT(16)
+#define ERROR_FLAG_SPI_RESPONSE_TIMEOUT     BIT(17)
+#define ERROR_FLAG_SPI_INTRA_PACKET_TIMEOUT BIT(18)
+#define ERROR_FLAG_SPI_INVALID_RESPONSE     BIT(19)
+#define ERROR_FLAG_SPI_HS_RX_TIMEOUT        BIT(20)
+#define ERROR_FLAG_SPI_TOUCH_IC_INIT        BIT(21)
+
+#define SPI_CMD_CONTROL_SEND                BIT(0) // cleared by device when sending is complete
+#define SPI_CMD_CONTROL_IRQ                 BIT(1)
+
+#define SPI_CMD_CODE_READ                   4
+#define SPI_CMD_CODE_WRITE                  6
+
+#define SPI_CMD_STATUS_DONE                 BIT(0)
+#define SPI_CMD_STATUS_ERROR                BIT(1)
+#define SPI_CMD_STATUS_BUSY                 BIT(3)
+
+#define DMA_TX_CONTROL_SEND                 BIT(0) // cleared by device when sending is complete
+#define DMA_TX_CONTROL_IRQ                  BIT(3)
+
+#define DMA_TX_STATUS_DONE                  BIT(0)
+#define DMA_TX_STATUS_ERROR                 BIT(1)
+#define DMA_TX_STATUS_UNKNOWN_2             BIT(2)
+#define DMA_TX_STATUS_UNKNOWN_3             BIT(3) // busy?
+
+#define INPUT_HEADER_VERSION(x)             ((x) & 0xf)
+#define INPUT_HEADER_REPORT_LENGTH(x)       (((x) >> 8) & 0x3fff)
+#define INPUT_HEADER_SYNC(x)                ((x) >> 24)
+#define INPUT_HEADER_VERSION_VALUE          3
+#define INPUT_HEADER_SYNC_VALUE             0x5a
+
+#define QUICKSPI_CONFIG1_UNKNOWN_0(x)       (((x) & 0x1f) << 0)
+#define QUICKSPI_CONFIG1_UNKNOWN_5(x)       (((x) & 0x1f) << 5)
+#define QUICKSPI_CONFIG1_UNKNOWN_10(x)      (((x) & 0x1f) << 10)
+#define QUICKSPI_CONFIG1_UNKNOWN_16(x)      (((x) & 0xffff) << 16)
+
+#define QUICKSPI_CONFIG2_UNKNOWN_0(x)       (((x) & 0x1f) << 0)
+#define QUICKSPI_CONFIG2_UNKNOWN_5(x)       (((x) & 0x1f) << 5)
+#define QUICKSPI_CONFIG2_UNKNOWN_12(x)      (((x) & 0xf) << 12)
+#define QUICKSPI_CONFIG2_UNKNOWN_16         BIT(16)
+#define QUICKSPI_CONFIG2_UNKNOWN_17         BIT(17)
+#define QUICKSPI_CONFIG2_DISABLE_READ_ADDRESS_INCREMENT  BIT(24)
+#define QUICKSPI_CONFIG2_DISABLE_WRITE_ADDRESS_INCREMENT BIT(25)
+#define QUICKSPI_CONFIG2_ENABLE_WRITE_STREAMING_MODE     BIT(27)
+#define QUICKSPI_CONFIG2_IRQ_POLARITY       BIT(28)
+
+#define DMA_RX_CONTROL_ENABLE               BIT(0)
+#define DMA_RX_CONTROL_IRQ_UNKNOWN_1        BIT(1) // rx1 only?
+#define DMA_RX_CONTROL_IRQ_ERROR            BIT(3) // rx1 only?
+#define DMA_RX_CONTROL_IRQ_READY            BIT(4) // rx0 only
+#define DMA_RX_CONTROL_IRQ_DATA             BIT(5)
+
+#define DMA_RX_CONTROL2_UNKNOWN_4           BIT(4) // rx1 only?
+#define DMA_RX_CONTROL2_UNKNOWN_5           BIT(5) // rx0 only?
+#define DMA_RX_CONTROL2_RESET               BIT(7) // resets ringbuffer indices
+
+#define DMA_RX_WRAP_FLAG                    BIT(7)
+
+#define DMA_RX_STATUS_ERROR                 BIT(3)
+#define DMA_RX_STATUS_READY                 BIT(4) // set in rx0 after using CONTROL_NRESET when it becomes possible to read config (can take >100ms)
+#define DMA_RX_STATUS_HAVE_DATA             BIT(5)
+#define DMA_RX_STATUS_ENABLED               BIT(8)
+
+#define INIT_UNKNOWN_GUC_2                  BIT(2)
+#define INIT_UNKNOWN_3                      BIT(3)
+#define INIT_UNKNOWN_GUC_4                  BIT(4)
+#define INIT_UNKNOWN_5                      BIT(5)
+#define INIT_UNKNOWN_31                     BIT(31)
+
+// COUNTER_RESET can be written to counter registers to reset them to zero. However, in some cases this can mess up the THC.
+#define COUNTER_RESET                       BIT(31)
+
+struct ithc_registers {
+	/* 0000 */ u32 _unknown_0000[5];
+	/* 0014 */ u32 ltr_config;
+	/* 0018 */ u32 _unknown_0018[1018];
+	/* 1000 */ u32 _unknown_1000;
+	/* 1004 */ u32 _unknown_1004;
+	/* 1008 */ u32 control_bits;
+	/* 100c */ u32 _unknown_100c;
+	/* 1010 */ u32 spi_config;
+	struct {
+		/* 1014/1018/101c */ u8 header;
+		/* 1015/1019/101d */ u8 quad;
+		/* 1016/101a/101e */ u8 dual;
+		/* 1017/101b/101f */ u8 single;
+	} opcode[3];
+	/* 1020 */ u32 error_control;
+	/* 1024 */ u32 error_status; // write to clear
+	/* 1028 */ u32 error_flags; // write to clear
+	/* 102c */ u32 _unknown_102c[5];
+	struct {
+		/* 1040 */ u8 control;
+		/* 1041 */ u8 code;
+		/* 1042 */ u16 size;
+		/* 1044 */ u32 status; // write to clear
+		/* 1048 */ u32 offset;
+		/* 104c */ u32 data[16];
+		/* 108c */ u32 _unknown_108c;
+	} spi_cmd;
+	struct {
+		/* 1090 */ u64 addr; // cannot be written with writeq(), must use lo_hi_writeq()
+		/* 1098 */ u8 control;
+		/* 1099 */ u8 _unknown_1099;
+		/* 109a */ u8 _unknown_109a;
+		/* 109b */ u8 num_prds;
+		/* 109c */ u32 status; // write to clear
+		/* 10a0 */ u32 _unknown_10a0[5];
+		/* 10b4 */ u32 spi_addr;
+	} dma_tx;
+	/* 10b8 */ u32 spi_header_addr;
+	union {
+		/* 10bc */ u32 irq_cause; // in legacy THC mode
+		/* 10bc */ u32 input_header; // in QuickSPI mode (see HIDSPI spec)
+	};
+	/* 10c0 */ u32 _unknown_10c0[8];
+	/* 10e0 */ u32 _unknown_10e0_counters[3];
+	/* 10ec */ u32 quickspi_config1;
+	/* 10f0 */ u32 quickspi_config2;
+	/* 10f4 */ u32 _unknown_10f4[3];
+	struct {
+		/* 1100/1200 */ u64 addr; // cannot be written with writeq(), must use lo_hi_writeq()
+		/* 1108/1208 */ u8 num_bufs;
+		/* 1109/1209 */ u8 num_prds;
+		/* 110a/120a */ u16 _unknown_110a;
+		/* 110c/120c */ u8 control;
+		/* 110d/120d */ u8 head;
+		/* 110e/120e */ u8 tail;
+		/* 110f/120f */ u8 control2;
+		/* 1110/1210 */ u32 status; // write to clear
+		/* 1114/1214 */ u32 _unknown_1114;
+		/* 1118/1218 */ u64 _unknown_1118_guc_addr;
+		/* 1120/1220 */ u32 _unknown_1120_guc;
+		/* 1124/1224 */ u32 _unknown_1124_guc;
+		/* 1128/1228 */ u32 init_unknown;
+		/* 112c/122c */ u32 _unknown_112c;
+		/* 1130/1230 */ u64 _unknown_1130_guc_addr;
+		/* 1138/1238 */ u32 _unknown_1138_guc;
+		/* 113c/123c */ u32 _unknown_113c;
+		/* 1140/1240 */ u32 _unknown_1140_guc;
+		/* 1144/1244 */ u32 _unknown_1144[11];
+		/* 1170/1270 */ u32 spi_addr;
+		/* 1174/1274 */ u32 _unknown_1174[11];
+		/* 11a0/12a0 */ u32 _unknown_11a0_counters[6];
+		/* 11b8/12b8 */ u32 _unknown_11b8[18];
+	} dma_rx[2];
+};
+static_assert(sizeof(struct ithc_registers) == 0x1300);
+
+void bitsl(__iomem u32 *reg, u32 mask, u32 val);
+void bitsb(__iomem u8 *reg, u8 mask, u8 val);
+#define bitsl_set(reg, x) bitsl(reg, x, x)
+#define bitsb_set(reg, x) bitsb(reg, x, x)
+int waitl(struct ithc *ithc, __iomem u32 *reg, u32 mask, u32 val);
+int waitb(struct ithc *ithc, __iomem u8 *reg, u8 mask, u8 val);
+
+void ithc_set_ltr_config(struct ithc *ithc, u64 active_ltr_ns, u64 idle_ltr_ns);
+void ithc_set_ltr_idle(struct ithc *ithc);
+int ithc_set_spi_config(struct ithc *ithc, u8 clkdiv, bool clkdiv8, u8 read_mode, u8 write_mode);
+int ithc_spi_command(struct ithc *ithc, u8 command, u32 offset, u32 size, void *data);
+
diff --git a/drivers/hid/ithc/ithc.h b/drivers/hid/ithc/ithc.h
new file mode 100644
index 000000000000..aec320d4e945
--- /dev/null
+++ b/drivers/hid/ithc/ithc.h
@@ -0,0 +1,89 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause */
+
+#include <linux/acpi.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/hid.h>
+#include <linux/highmem.h>
+#include <linux/input.h>
+#include <linux/io-64-nonatomic-lo-hi.h>
+#include <linux/iopoll.h>
+#include <linux/kthread.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/poll.h>
+#include <linux/timer.h>
+#include <linux/vmalloc.h>
+
+#define DEVNAME "ithc"
+#define DEVFULLNAME "Intel Touch Host Controller"
+
+#undef pr_fmt
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#define CHECK(fn, ...) ({ int r = fn(__VA_ARGS__); if (r < 0) pci_err(ithc->pci, "%s: %s failed with %i\n", __func__, #fn, r); r; })
+#define CHECK_RET(...) do { int r = CHECK(__VA_ARGS__); if (r < 0) return r; } while (0)
+
+#define NUM_RX_BUF 16
+
+// PCI device IDs:
+// Lakefield
+#define PCI_DEVICE_ID_INTEL_THC_LKF_PORT1    0x98d0
+#define PCI_DEVICE_ID_INTEL_THC_LKF_PORT2    0x98d1
+// Tiger Lake
+#define PCI_DEVICE_ID_INTEL_THC_TGL_LP_PORT1 0xa0d0
+#define PCI_DEVICE_ID_INTEL_THC_TGL_LP_PORT2 0xa0d1
+#define PCI_DEVICE_ID_INTEL_THC_TGL_H_PORT1  0x43d0
+#define PCI_DEVICE_ID_INTEL_THC_TGL_H_PORT2  0x43d1
+// Alder Lake
+#define PCI_DEVICE_ID_INTEL_THC_ADL_S_PORT1  0x7ad8
+#define PCI_DEVICE_ID_INTEL_THC_ADL_S_PORT2  0x7ad9
+#define PCI_DEVICE_ID_INTEL_THC_ADL_P_PORT1  0x51d0
+#define PCI_DEVICE_ID_INTEL_THC_ADL_P_PORT2  0x51d1
+#define PCI_DEVICE_ID_INTEL_THC_ADL_M_PORT1  0x54d0
+#define PCI_DEVICE_ID_INTEL_THC_ADL_M_PORT2  0x54d1
+// Raptor Lake
+#define PCI_DEVICE_ID_INTEL_THC_RPL_S_PORT1  0x7a58
+#define PCI_DEVICE_ID_INTEL_THC_RPL_S_PORT2  0x7a59
+// Meteor Lake
+#define PCI_DEVICE_ID_INTEL_THC_MTL_S_PORT1  0x7f59
+#define PCI_DEVICE_ID_INTEL_THC_MTL_S_PORT2  0x7f5b
+#define PCI_DEVICE_ID_INTEL_THC_MTL_MP_PORT1 0x7e49
+#define PCI_DEVICE_ID_INTEL_THC_MTL_MP_PORT2 0x7e4b
+
+struct ithc;
+
+#include "ithc-regs.h"
+#include "ithc-hid.h"
+#include "ithc-dma.h"
+#include "ithc-legacy.h"
+#include "ithc-quickspi.h"
+#include "ithc-debug.h"
+
+struct ithc {
+	char phys[32];
+	struct pci_dev *pci;
+	int irq;
+	struct task_struct *poll_thread;
+	struct timer_list idle_timer;
+
+	struct ithc_registers __iomem *regs;
+	struct ithc_registers *prev_regs; // for debugging
+	struct ithc_dma_rx dma_rx[2];
+	struct ithc_dma_tx dma_tx;
+	struct ithc_hid hid;
+
+	bool use_quickspi;
+	bool have_config;
+	u16 vendor_id;
+	u16 product_id;
+	u32 product_rev;
+	u32 max_rx_size;
+	u32 max_tx_size;
+	u32 legacy_touch_cfg;
+};
+
+int ithc_reset(struct ithc *ithc);
+
-- 
2.47.0


From d3dc73097a95d90fa86a75787b56a5209e88ed0d Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sat, 30 Dec 2023 18:07:54 +0100
Subject: [PATCH v1.4 023/120] hwmon: Add thermal sensor driver for Surface
 Aggregator Module

Some of the newer Microsoft Surface devices (such as the Surface Book
3 and Pro 9) have thermal sensors connected via the Surface Aggregator
Module (the embedded controller on those devices). Add a basic driver
to read out the temperature values of those sensors.

Link: https://github.com/linux-surface/surface-aggregator-module/issues/59
Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: surface-sam
---
 drivers/hwmon/Kconfig        |  10 +++
 drivers/hwmon/Makefile       |   1 +
 drivers/hwmon/surface_temp.c | 165 +++++++++++++++++++++++++++++++++++
 3 files changed, 176 insertions(+)
 create mode 100644 drivers/hwmon/surface_temp.c

diff --git a/drivers/hwmon/Kconfig b/drivers/hwmon/Kconfig
index 778e584c3a75..b8d65292db83 100644
--- a/drivers/hwmon/Kconfig
+++ b/drivers/hwmon/Kconfig
@@ -2084,6 +2084,16 @@ config SENSORS_SURFACE_FAN
 
 	  Select M or Y here, if you want to be able to read the fan's speed.
 
+config SENSORS_SURFACE_TEMP
+	tristate "Microsoft Surface Thermal Sensor Driver"
+	depends on SURFACE_AGGREGATOR
+	help
+	  Driver for monitoring thermal sensors connected via the Surface
+	  Aggregator Module (embedded controller) on Microsoft Surface devices.
+
+	  This driver can also be built as a module. If so, the module
+	  will be called surface_temp.
+
 config SENSORS_ADC128D818
 	tristate "Texas Instruments ADC128D818"
 	depends on I2C
diff --git a/drivers/hwmon/Makefile b/drivers/hwmon/Makefile
index b1c7056c37db..3ce8d6a9202e 100644
--- a/drivers/hwmon/Makefile
+++ b/drivers/hwmon/Makefile
@@ -209,6 +209,7 @@ obj-$(CONFIG_SENSORS_SPARX5)	+= sparx5-temp.o
 obj-$(CONFIG_SENSORS_SPD5118)	+= spd5118.o
 obj-$(CONFIG_SENSORS_STTS751)	+= stts751.o
 obj-$(CONFIG_SENSORS_SURFACE_FAN)+= surface_fan.o
+obj-$(CONFIG_SENSORS_SURFACE_TEMP)+= surface_temp.o
 obj-$(CONFIG_SENSORS_SY7636A)	+= sy7636a-hwmon.o
 obj-$(CONFIG_SENSORS_AMC6821)	+= amc6821.o
 obj-$(CONFIG_SENSORS_TC74)	+= tc74.o
diff --git a/drivers/hwmon/surface_temp.c b/drivers/hwmon/surface_temp.c
new file mode 100644
index 000000000000..48c3e826713f
--- /dev/null
+++ b/drivers/hwmon/surface_temp.c
@@ -0,0 +1,165 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Thermal sensor subsystem driver for Surface System Aggregator Module (SSAM).
+ *
+ * Copyright (C) 2022-2023 Maximilian Luz <luzmaximilian@gmail.com>
+ */
+
+#include <linux/bitops.h>
+#include <linux/hwmon.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/types.h>
+
+#include <linux/surface_aggregator/controller.h>
+#include <linux/surface_aggregator/device.h>
+
+
+/* -- SAM interface. -------------------------------------------------------- */
+
+SSAM_DEFINE_SYNC_REQUEST_CL_R(__ssam_tmp_get_available_sensors, __le16, {
+	.target_category = SSAM_SSH_TC_TMP,
+	.command_id      = 0x04,
+});
+
+SSAM_DEFINE_SYNC_REQUEST_MD_R(__ssam_tmp_get_temperature, __le16, {
+	.target_category = SSAM_SSH_TC_TMP,
+	.command_id      = 0x01,
+});
+
+static int ssam_tmp_get_available_sensors(struct ssam_device *sdev, s16 *sensors)
+{
+	__le16 sensors_le;
+	int status;
+
+	status = __ssam_tmp_get_available_sensors(sdev, &sensors_le);
+	if (status)
+		return status;
+
+	*sensors = le16_to_cpu(sensors_le);
+	return 0;
+}
+
+static int ssam_tmp_get_temperature(struct ssam_device *sdev, u8 iid, long *temperature)
+{
+	__le16 temp_le;
+	int status;
+
+	status = __ssam_tmp_get_temperature(sdev->ctrl, sdev->uid.target, iid, &temp_le);
+	if (status)
+		return status;
+
+	/* Convert 1/10 °K to 1/1000 °C */
+	*temperature = (le16_to_cpu(temp_le) - 2731) * 100L;
+	return 0;
+}
+
+
+/* -- Driver.---------------------------------------------------------------- */
+
+struct ssam_temp {
+	struct ssam_device *sdev;
+	s16 sensors;
+};
+
+static umode_t ssam_temp_hwmon_is_visible(const void *data,
+					  enum hwmon_sensor_types type,
+					  u32 attr, int channel)
+{
+	const struct ssam_temp *ssam_temp = data;
+
+	if (!(ssam_temp->sensors & BIT(channel)))
+		return 0;
+
+	return 0444;
+}
+
+static int ssam_temp_hwmon_read(struct device *dev,
+				enum hwmon_sensor_types type,
+				u32 attr, int channel, long *value)
+{
+	const struct ssam_temp *ssam_temp = dev_get_drvdata(dev);
+
+	return ssam_tmp_get_temperature(ssam_temp->sdev, channel + 1, value);
+}
+
+static const struct hwmon_channel_info * const ssam_temp_hwmon_info[] = {
+	HWMON_CHANNEL_INFO(chip,
+			   HWMON_C_REGISTER_TZ),
+	/* We have at most 16 thermal sensor channels. */
+	HWMON_CHANNEL_INFO(temp,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT,
+			   HWMON_T_INPUT),
+	NULL
+};
+
+static const struct hwmon_ops ssam_temp_hwmon_ops = {
+	.is_visible = ssam_temp_hwmon_is_visible,
+	.read = ssam_temp_hwmon_read,
+};
+
+static const struct hwmon_chip_info ssam_temp_hwmon_chip_info = {
+	.ops = &ssam_temp_hwmon_ops,
+	.info = ssam_temp_hwmon_info,
+};
+
+static int ssam_temp_probe(struct ssam_device *sdev)
+{
+	struct ssam_temp *ssam_temp;
+	struct device *hwmon_dev;
+	s16 sensors;
+	int status;
+
+	status = ssam_tmp_get_available_sensors(sdev, &sensors);
+	if (status)
+		return status;
+
+	ssam_temp = devm_kzalloc(&sdev->dev, sizeof(*ssam_temp), GFP_KERNEL);
+	if (!ssam_temp)
+		return -ENOMEM;
+
+	ssam_temp->sdev = sdev;
+	ssam_temp->sensors = sensors;
+
+	hwmon_dev = devm_hwmon_device_register_with_info(&sdev->dev,
+			"surface_thermal", ssam_temp, &ssam_temp_hwmon_chip_info,
+			NULL);
+	if (IS_ERR(hwmon_dev))
+		return PTR_ERR(hwmon_dev);
+
+	return 0;
+}
+
+static const struct ssam_device_id ssam_temp_match[] = {
+	{ SSAM_SDEV(TMP, SAM, 0x00, 0x02) },
+	{ },
+};
+MODULE_DEVICE_TABLE(ssam, ssam_temp_match);
+
+static struct ssam_device_driver ssam_temp = {
+	.probe = ssam_temp_probe,
+	.match_table = ssam_temp_match,
+	.driver = {
+		.name = "surface_temp",
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+module_ssam_device_driver(ssam_temp);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Thermal sensor subsystem driver for Surface System Aggregator Module");
+MODULE_LICENSE("GPL");
-- 
2.47.0


From 30a1f280dc9268e67121c9f9b905db0778b985e7 Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sat, 30 Dec 2023 18:12:23 +0100
Subject: [PATCH v1.4 024/120] hwmon: surface_temp: Add support for sensor
 names

The thermal subsystem of the Surface Aggregator Module allows us to
query the names of the respective thermal sensors. Forward those to
userspace.

Signed-off-by: Ivor Wanders <ivor@iwanders.net>
Co-Developed-by: Maximilian Luz <luzmaximilian@gmail.com>
Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: surface-sam
---
 drivers/hwmon/surface_temp.c | 113 +++++++++++++++++++++++++++++------
 1 file changed, 96 insertions(+), 17 deletions(-)

diff --git a/drivers/hwmon/surface_temp.c b/drivers/hwmon/surface_temp.c
index 48c3e826713f..4c08926139db 100644
--- a/drivers/hwmon/surface_temp.c
+++ b/drivers/hwmon/surface_temp.c
@@ -17,6 +17,27 @@
 
 /* -- SAM interface. -------------------------------------------------------- */
 
+/*
+ * Available sensors are indicated by a 16-bit bitfield, where a 1 marks the
+ * presence of a sensor. So we have at most 16 possible sensors/channels.
+ */
+#define SSAM_TMP_SENSOR_MAX_COUNT 16
+
+/*
+ * All names observed so far are 6 characters long, but there's only
+ * zeros after the name, so perhaps they can be longer. This number reflects
+ * the maximum zero-padded space observed in the returned buffer.
+ */
+#define SSAM_TMP_SENSOR_NAME_LENGTH 18
+
+struct ssam_tmp_get_name_rsp {
+	__le16 unknown1;
+	char unknown2;
+	char name[SSAM_TMP_SENSOR_NAME_LENGTH];
+} __packed;
+
+static_assert(sizeof(struct ssam_tmp_get_name_rsp) == 21);
+
 SSAM_DEFINE_SYNC_REQUEST_CL_R(__ssam_tmp_get_available_sensors, __le16, {
 	.target_category = SSAM_SSH_TC_TMP,
 	.command_id      = 0x04,
@@ -27,6 +48,11 @@ SSAM_DEFINE_SYNC_REQUEST_MD_R(__ssam_tmp_get_temperature, __le16, {
 	.command_id      = 0x01,
 });
 
+SSAM_DEFINE_SYNC_REQUEST_MD_R(__ssam_tmp_get_name, struct ssam_tmp_get_name_rsp, {
+	.target_category = SSAM_SSH_TC_TMP,
+	.command_id      = 0x0e,
+});
+
 static int ssam_tmp_get_available_sensors(struct ssam_device *sdev, s16 *sensors)
 {
 	__le16 sensors_le;
@@ -54,12 +80,37 @@ static int ssam_tmp_get_temperature(struct ssam_device *sdev, u8 iid, long *temp
 	return 0;
 }
 
+static int ssam_tmp_get_name(struct ssam_device *sdev, u8 iid, char *buf, size_t buf_len)
+{
+	struct ssam_tmp_get_name_rsp name_rsp;
+	int status;
+
+	status =  __ssam_tmp_get_name(sdev->ctrl, sdev->uid.target, iid, &name_rsp);
+	if (status)
+		return status;
+
+	/*
+	 * This should not fail unless the name in the returned struct is not
+	 * null-terminated or someone changed something in the struct
+	 * definitions above, since our buffer and struct have the same
+	 * capacity by design. So if this fails blow this up with a warning.
+	 * Since the more likely cause is that the returned string isn't
+	 * null-terminated, we might have received garbage (as opposed to just
+	 * an incomplete string), so also fail the function.
+	 */
+	status = strscpy(buf, name_rsp.name, buf_len);
+	WARN_ON(status < 0);
+
+	return status < 0 ? status : 0;
+}
+
 
 /* -- Driver.---------------------------------------------------------------- */
 
 struct ssam_temp {
 	struct ssam_device *sdev;
 	s16 sensors;
+	char names[SSAM_TMP_SENSOR_MAX_COUNT][SSAM_TMP_SENSOR_NAME_LENGTH];
 };
 
 static umode_t ssam_temp_hwmon_is_visible(const void *data,
@@ -83,33 +134,47 @@ static int ssam_temp_hwmon_read(struct device *dev,
 	return ssam_tmp_get_temperature(ssam_temp->sdev, channel + 1, value);
 }
 
+static int ssam_temp_hwmon_read_string(struct device *dev,
+				       enum hwmon_sensor_types type,
+				       u32 attr, int channel, const char **str)
+{
+	const struct ssam_temp *ssam_temp = dev_get_drvdata(dev);
+
+	*str = ssam_temp->names[channel];
+	return 0;
+}
+
 static const struct hwmon_channel_info * const ssam_temp_hwmon_info[] = {
 	HWMON_CHANNEL_INFO(chip,
 			   HWMON_C_REGISTER_TZ),
-	/* We have at most 16 thermal sensor channels. */
+	/*
+	 * We have at most SSAM_TMP_SENSOR_MAX_COUNT = 16 thermal sensor
+	 * channels.
+	 */
 	HWMON_CHANNEL_INFO(temp,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT,
-			   HWMON_T_INPUT),
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL,
+			   HWMON_T_INPUT | HWMON_T_LABEL),
 	NULL
 };
 
 static const struct hwmon_ops ssam_temp_hwmon_ops = {
 	.is_visible = ssam_temp_hwmon_is_visible,
 	.read = ssam_temp_hwmon_read,
+	.read_string = ssam_temp_hwmon_read_string,
 };
 
 static const struct hwmon_chip_info ssam_temp_hwmon_chip_info = {
@@ -122,6 +187,7 @@ static int ssam_temp_probe(struct ssam_device *sdev)
 	struct ssam_temp *ssam_temp;
 	struct device *hwmon_dev;
 	s16 sensors;
+	int channel;
 	int status;
 
 	status = ssam_tmp_get_available_sensors(sdev, &sensors);
@@ -135,6 +201,19 @@ static int ssam_temp_probe(struct ssam_device *sdev)
 	ssam_temp->sdev = sdev;
 	ssam_temp->sensors = sensors;
 
+	/* Retrieve the name for each available sensor. */
+	for (channel = 0; channel < SSAM_TMP_SENSOR_MAX_COUNT; channel++)
+	{
+		if (!(sensors & BIT(channel)))
+			continue;
+
+		status = ssam_tmp_get_name(sdev, channel + 1,
+					   ssam_temp->names[channel],
+					   SSAM_TMP_SENSOR_NAME_LENGTH);
+		if (status)
+			return status;
+	}
+
 	hwmon_dev = devm_hwmon_device_register_with_info(&sdev->dev,
 			"surface_thermal", ssam_temp, &ssam_temp_hwmon_chip_info,
 			NULL);
-- 
2.47.0


From dbc76b6e948be7baa0995a3c18c8a85cc96539c7 Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sat, 25 Jul 2020 17:19:53 +0200
Subject: [PATCH v1.4 025/120] i2c: acpi: Implement RawBytes read access

Microsoft Surface Pro 4 and Book 1 devices access the MSHW0030 I2C
device via a generic serial bus operation region and RawBytes read
access. On the Surface Book 1, this access is required to turn on (and
off) the discrete GPU.

Multiple things are to note here:

a) The RawBytes access is device/driver dependent. The ACPI
   specification states:

   > Raw accesses assume that the writer has knowledge of the bus that
   > the access is made over and the device that is being accessed. The
   > protocol may only ensure that the buffer is transmitted to the
   > appropriate driver, but the driver must be able to interpret the
   > buffer to communicate to a register.

   Thus this implementation may likely not work on other devices
   accessing I2C via the RawBytes accessor type.

b) The MSHW0030 I2C device is an HID-over-I2C device which seems to
   serve multiple functions:

   1. It is the main access point for the legacy-type Surface Aggregator
      Module (also referred to as SAM-over-HID, as opposed to the newer
      SAM-over-SSH/UART). It has currently not been determined on how
      support for the legacy SAM should be implemented. Likely via a
      custom HID driver.

   2. It seems to serve as the HID device for the Integrated Sensor Hub.
      This might complicate matters with regards to implementing a
      SAM-over-HID driver required by legacy SAM.

In light of this, the simplest approach has been chosen for now.
However, it may make more sense regarding breakage and compatibility to
either provide functionality for replacing or enhancing the default
operation region handler via some additional API functions, or even to
completely blacklist MSHW0030 from the I2C core and provide a custom
driver for it.

Replacing/enhancing the default operation region handler would, however,
either require some sort of secondary driver and access point for it,
from which the new API functions would be called and the new handler
(part) would be installed, or hard-coding them via some sort of
quirk-like interface into the I2C core.

Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: surface-sam-over-hid
---
 drivers/i2c/i2c-core-acpi.c | 34 ++++++++++++++++++++++++++++++++++
 1 file changed, 34 insertions(+)

diff --git a/drivers/i2c/i2c-core-acpi.c b/drivers/i2c/i2c-core-acpi.c
index 14ae0cfc325e..6197c5252d2a 100644
--- a/drivers/i2c/i2c-core-acpi.c
+++ b/drivers/i2c/i2c-core-acpi.c
@@ -639,6 +639,27 @@ static int acpi_gsb_i2c_write_bytes(struct i2c_client *client,
 	return (ret == 1) ? 0 : -EIO;
 }
 
+static int acpi_gsb_i2c_write_raw_bytes(struct i2c_client *client,
+		u8 *data, u8 data_len)
+{
+	struct i2c_msg msgs[1];
+	int ret;
+
+	msgs[0].addr = client->addr;
+	msgs[0].flags = client->flags;
+	msgs[0].len = data_len + 1;
+	msgs[0].buf = data;
+
+	ret = i2c_transfer(client->adapter, msgs, ARRAY_SIZE(msgs));
+	if (ret < 0) {
+		dev_err(&client->adapter->dev, "i2c write failed: %d\n", ret);
+		return ret;
+	}
+
+	/* 1 transfer must have completed successfully */
+	return (ret == 1) ? 0 : -EIO;
+}
+
 static acpi_status
 i2c_acpi_space_handler(u32 function, acpi_physical_address command,
 			u32 bits, u64 *value64,
@@ -740,6 +761,19 @@ i2c_acpi_space_handler(u32 function, acpi_physical_address command,
 		}
 		break;
 
+	case ACPI_GSB_ACCESS_ATTRIB_RAW_BYTES:
+		if (action == ACPI_READ) {
+			dev_warn(&adapter->dev,
+				 "protocol 0x%02x not supported for client 0x%02x\n",
+				 accessor_type, client->addr);
+			ret = AE_BAD_PARAMETER;
+			goto err;
+		} else {
+			status = acpi_gsb_i2c_write_raw_bytes(client,
+					gsb->data, info->access_length);
+		}
+		break;
+
 	default:
 		dev_warn(&adapter->dev, "protocol 0x%02x not supported for client 0x%02x\n",
 			 accessor_type, client->addr);
-- 
2.47.0


From 29bcd20caaf9c7dcd39a14490f55e4a04f6226f4 Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sat, 13 Feb 2021 16:41:18 +0100
Subject: [PATCH v1.4 026/120] platform/surface: Add driver for Surface Book 1
 dGPU switch

Add driver exposing the discrete GPU power-switch of the  Microsoft
Surface Book 1 to user-space.

On the Surface Book 1, the dGPU power is controlled via the Surface
System Aggregator Module (SAM). The specific SAM-over-HID command for
this is exposed via ACPI. This module provides a simple driver exposing
the ACPI call via a sysfs parameter to user-space, so that users can
easily power-on/-off the dGPU.

Patchset: surface-sam-over-hid
---
 drivers/platform/surface/Kconfig              |   7 +
 drivers/platform/surface/Makefile             |   1 +
 .../surface/surfacebook1_dgpu_switch.c        | 136 ++++++++++++++++++
 3 files changed, 144 insertions(+)
 create mode 100644 drivers/platform/surface/surfacebook1_dgpu_switch.c

diff --git a/drivers/platform/surface/Kconfig b/drivers/platform/surface/Kconfig
index b629e82af97c..68656e8f309e 100644
--- a/drivers/platform/surface/Kconfig
+++ b/drivers/platform/surface/Kconfig
@@ -149,6 +149,13 @@ config SURFACE_AGGREGATOR_TABLET_SWITCH
 	  Select M or Y here, if you want to provide tablet-mode switch input
 	  events on the Surface Pro 8, Surface Pro X, and Surface Laptop Studio.
 
+config SURFACE_BOOK1_DGPU_SWITCH
+	tristate "Surface Book 1 dGPU Switch Driver"
+	depends on SYSFS
+	help
+	  This driver provides a sysfs switch to set the power-state of the
+	  discrete GPU found on the Microsoft Surface Book 1.
+
 config SURFACE_DTX
 	tristate "Surface DTX (Detachment System) Driver"
 	depends on SURFACE_AGGREGATOR
diff --git a/drivers/platform/surface/Makefile b/drivers/platform/surface/Makefile
index 53344330939b..7efcd0cdb532 100644
--- a/drivers/platform/surface/Makefile
+++ b/drivers/platform/surface/Makefile
@@ -12,6 +12,7 @@ obj-$(CONFIG_SURFACE_AGGREGATOR_CDEV)	+= surface_aggregator_cdev.o
 obj-$(CONFIG_SURFACE_AGGREGATOR_HUB)	+= surface_aggregator_hub.o
 obj-$(CONFIG_SURFACE_AGGREGATOR_REGISTRY) += surface_aggregator_registry.o
 obj-$(CONFIG_SURFACE_AGGREGATOR_TABLET_SWITCH) += surface_aggregator_tabletsw.o
+obj-$(CONFIG_SURFACE_BOOK1_DGPU_SWITCH) += surfacebook1_dgpu_switch.o
 obj-$(CONFIG_SURFACE_DTX)		+= surface_dtx.o
 obj-$(CONFIG_SURFACE_GPE)		+= surface_gpe.o
 obj-$(CONFIG_SURFACE_HOTPLUG)		+= surface_hotplug.o
diff --git a/drivers/platform/surface/surfacebook1_dgpu_switch.c b/drivers/platform/surface/surfacebook1_dgpu_switch.c
new file mode 100644
index 000000000000..68db237734a1
--- /dev/null
+++ b/drivers/platform/surface/surfacebook1_dgpu_switch.c
@@ -0,0 +1,136 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+#include <linux/module.h>
+#include <linux/acpi.h>
+#include <linux/platform_device.h>
+
+/* MSHW0040/VGBI DSM UUID: 6fd05c69-cde3-49f4-95ed-ab1665498035 */
+static const guid_t dgpu_sw_guid =
+	GUID_INIT(0x6fd05c69, 0xcde3, 0x49f4,
+		  0x95, 0xed, 0xab, 0x16, 0x65, 0x49, 0x80, 0x35);
+
+#define DGPUSW_ACPI_PATH_DSM	"\\_SB_.PCI0.LPCB.EC0_.VGBI"
+#define DGPUSW_ACPI_PATH_HGON	"\\_SB_.PCI0.RP05.HGON"
+#define DGPUSW_ACPI_PATH_HGOF	"\\_SB_.PCI0.RP05.HGOF"
+
+static int sb1_dgpu_sw_dsmcall(void)
+{
+	union acpi_object *obj;
+	acpi_handle handle;
+	acpi_status status;
+
+	status = acpi_get_handle(NULL, DGPUSW_ACPI_PATH_DSM, &handle);
+	if (status)
+		return -EINVAL;
+
+	obj = acpi_evaluate_dsm_typed(handle, &dgpu_sw_guid, 1, 1, NULL, ACPI_TYPE_BUFFER);
+	if (!obj)
+		return -EINVAL;
+
+	ACPI_FREE(obj);
+	return 0;
+}
+
+static int sb1_dgpu_sw_hgon(struct device *dev)
+{
+	struct acpi_buffer buf = {ACPI_ALLOCATE_BUFFER, NULL};
+	acpi_status status;
+
+	status = acpi_evaluate_object(NULL, DGPUSW_ACPI_PATH_HGON, NULL, &buf);
+	if (status) {
+		dev_err(dev, "failed to run HGON: %d\n", status);
+		return -EINVAL;
+	}
+
+	ACPI_FREE(buf.pointer);
+
+	dev_info(dev, "turned-on dGPU via HGON\n");
+	return 0;
+}
+
+static int sb1_dgpu_sw_hgof(struct device *dev)
+{
+	struct acpi_buffer buf = {ACPI_ALLOCATE_BUFFER, NULL};
+	acpi_status status;
+
+	status = acpi_evaluate_object(NULL, DGPUSW_ACPI_PATH_HGOF, NULL, &buf);
+	if (status) {
+		dev_err(dev, "failed to run HGOF: %d\n", status);
+		return -EINVAL;
+	}
+
+	ACPI_FREE(buf.pointer);
+
+	dev_info(dev, "turned-off dGPU via HGOF\n");
+	return 0;
+}
+
+static ssize_t dgpu_dsmcall_store(struct device *dev, struct device_attribute *attr,
+				  const char *buf, size_t len)
+{
+	bool value;
+	int status;
+
+	status = kstrtobool(buf, &value);
+	if (status < 0)
+		return status;
+
+	if (!value)
+		return 0;
+
+	status = sb1_dgpu_sw_dsmcall();
+
+	return status < 0 ? status : len;
+}
+static DEVICE_ATTR_WO(dgpu_dsmcall);
+
+static ssize_t dgpu_power_store(struct device *dev, struct device_attribute *attr,
+				const char *buf, size_t len)
+{
+	bool power;
+	int status;
+
+	status = kstrtobool(buf, &power);
+	if (status < 0)
+		return status;
+
+	if (power)
+		status = sb1_dgpu_sw_hgon(dev);
+	else
+		status = sb1_dgpu_sw_hgof(dev);
+
+	return status < 0 ? status : len;
+}
+static DEVICE_ATTR_WO(dgpu_power);
+
+static struct attribute *sb1_dgpu_sw_attrs[] = {
+	&dev_attr_dgpu_dsmcall.attr,
+	&dev_attr_dgpu_power.attr,
+	NULL
+};
+ATTRIBUTE_GROUPS(sb1_dgpu_sw);
+
+/*
+ * The dGPU power seems to be actually handled by MSHW0040. However, that is
+ * also the power-/volume-button device with a mainline driver. So let's use
+ * MSHW0041 instead for now, which seems to be the LTCH (latch/DTX) device.
+ */
+static const struct acpi_device_id sb1_dgpu_sw_match[] = {
+	{ "MSHW0041", },
+	{ }
+};
+MODULE_DEVICE_TABLE(acpi, sb1_dgpu_sw_match);
+
+static struct platform_driver sb1_dgpu_sw = {
+	.driver = {
+		.name = "surfacebook1_dgpu_switch",
+		.acpi_match_table = sb1_dgpu_sw_match,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+		.dev_groups = sb1_dgpu_sw_groups,
+	},
+};
+module_platform_driver(sb1_dgpu_sw);
+
+MODULE_AUTHOR("Maximilian Luz <luzmaximilian@gmail.com>");
+MODULE_DESCRIPTION("Discrete GPU Power-Switch for Surface Book 1");
+MODULE_LICENSE("GPL");
-- 
2.47.0


From 75a7adcd02ddea9353d4a97ae88fed39e8b6beef Mon Sep 17 00:00:00 2001
From: Sachi King <nakato@nakato.io>
Date: Tue, 5 Oct 2021 00:05:09 +1100
Subject: [PATCH v1.4 027/120] Input: soc_button_array - support AMD variant
 Surface devices

The power button on the AMD variant of the Surface Laptop uses the
same MSHW0040 device ID as the 5th and later generation of Surface
devices, however they report 0 for their OEM platform revision.  As the
_DSM does not exist on the devices requiring special casing, check for
the existance of the _DSM to determine if soc_button_array should be
loaded.

Fixes: c394159310d0 ("Input: soc_button_array - add support for newer surface devices")
Co-developed-by: Maximilian Luz <luzmaximilian@gmail.com>

Signed-off-by: Sachi King <nakato@nakato.io>
Patchset: surface-button
---
 drivers/input/misc/soc_button_array.c | 33 +++++++--------------------
 1 file changed, 8 insertions(+), 25 deletions(-)

diff --git a/drivers/input/misc/soc_button_array.c b/drivers/input/misc/soc_button_array.c
index 5c5d407fe965..4e1bfe90e730 100644
--- a/drivers/input/misc/soc_button_array.c
+++ b/drivers/input/misc/soc_button_array.c
@@ -540,8 +540,8 @@ static const struct soc_device_data soc_device_MSHW0028 = {
  * Both, the Surface Pro 4 (surfacepro3_button.c) and the above mentioned
  * devices use MSHW0040 for power and volume buttons, however the way they
  * have to be addressed differs. Make sure that we only load this drivers
- * for the correct devices by checking the OEM Platform Revision provided by
- * the _DSM method.
+ * for the correct devices by checking if the OEM Platform Revision DSM call
+ * exists.
  */
 #define MSHW0040_DSM_REVISION		0x01
 #define MSHW0040_DSM_GET_OMPR		0x02	// get OEM Platform Revision
@@ -552,31 +552,14 @@ static const guid_t MSHW0040_DSM_UUID =
 static int soc_device_check_MSHW0040(struct device *dev)
 {
 	acpi_handle handle = ACPI_HANDLE(dev);
-	union acpi_object *result;
-	u64 oem_platform_rev = 0;	// valid revisions are nonzero
-
-	// get OEM platform revision
-	result = acpi_evaluate_dsm_typed(handle, &MSHW0040_DSM_UUID,
-					 MSHW0040_DSM_REVISION,
-					 MSHW0040_DSM_GET_OMPR, NULL,
-					 ACPI_TYPE_INTEGER);
-
-	if (result) {
-		oem_platform_rev = result->integer.value;
-		ACPI_FREE(result);
-	}
-
-	/*
-	 * If the revision is zero here, the _DSM evaluation has failed. This
-	 * indicates that we have a Pro 4 or Book 1 and this driver should not
-	 * be used.
-	 */
-	if (oem_platform_rev == 0)
-		return -ENODEV;
+	bool exists;
 
-	dev_dbg(dev, "OEM Platform Revision %llu\n", oem_platform_rev);
+	// check if OEM platform revision DSM call exists
+	exists = acpi_check_dsm(handle, &MSHW0040_DSM_UUID,
+				MSHW0040_DSM_REVISION,
+				BIT(MSHW0040_DSM_GET_OMPR));
 
-	return 0;
+	return exists ? 0 : -ENODEV;
 }
 
 /*
-- 
2.47.0


From 81b36c3f488edae655fe54873d10c45a1c0cde9a Mon Sep 17 00:00:00 2001
From: Sachi King <nakato@nakato.io>
Date: Tue, 5 Oct 2021 00:22:57 +1100
Subject: [PATCH v1.4 028/120] platform/surface: surfacepro3_button: don't load
 on amd variant

The AMD variant of the Surface Laptop report 0 for their OEM platform
revision.  The Surface devices that require the surfacepro3_button
driver do not have the _DSM that gets the OEM platform revision.  If the
method does not exist, load surfacepro3_button.

Fixes: 64dd243d7356 ("platform/x86: surfacepro3_button: Fix device check")
Co-developed-by: Maximilian Luz <luzmaximilian@gmail.com>

Signed-off-by: Sachi King <nakato@nakato.io>
Patchset: surface-button
---
 drivers/platform/surface/surfacepro3_button.c | 30 ++++---------------
 1 file changed, 6 insertions(+), 24 deletions(-)

diff --git a/drivers/platform/surface/surfacepro3_button.c b/drivers/platform/surface/surfacepro3_button.c
index 2755601f979c..4240c98ca226 100644
--- a/drivers/platform/surface/surfacepro3_button.c
+++ b/drivers/platform/surface/surfacepro3_button.c
@@ -149,7 +149,8 @@ static int surface_button_resume(struct device *dev)
 /*
  * Surface Pro 4 and Surface Book 2 / Surface Pro 2017 use the same device
  * ID (MSHW0040) for the power/volume buttons. Make sure this is the right
- * device by checking for the _DSM method and OEM Platform Revision.
+ * device by checking for the _DSM method and OEM Platform Revision DSM
+ * function.
  *
  * Returns true if the driver should bind to this device, i.e. the device is
  * either MSWH0028 (Pro 3) or MSHW0040 on a Pro 4 or Book 1.
@@ -157,30 +158,11 @@ static int surface_button_resume(struct device *dev)
 static bool surface_button_check_MSHW0040(struct acpi_device *dev)
 {
 	acpi_handle handle = dev->handle;
-	union acpi_object *result;
-	u64 oem_platform_rev = 0;	// valid revisions are nonzero
-
-	// get OEM platform revision
-	result = acpi_evaluate_dsm_typed(handle, &MSHW0040_DSM_UUID,
-					 MSHW0040_DSM_REVISION,
-					 MSHW0040_DSM_GET_OMPR,
-					 NULL, ACPI_TYPE_INTEGER);
-
-	/*
-	 * If evaluating the _DSM fails, the method is not present. This means
-	 * that we have either MSHW0028 or MSHW0040 on Pro 4 or Book 1, so we
-	 * should use this driver. We use revision 0 indicating it is
-	 * unavailable.
-	 */
-
-	if (result) {
-		oem_platform_rev = result->integer.value;
-		ACPI_FREE(result);
-	}
-
-	dev_dbg(&dev->dev, "OEM Platform Revision %llu\n", oem_platform_rev);
 
-	return oem_platform_rev == 0;
+	// make sure that OEM platform revision DSM call does not exist
+	return !acpi_check_dsm(handle, &MSHW0040_DSM_UUID,
+			       MSHW0040_DSM_REVISION,
+			       BIT(MSHW0040_DSM_GET_OMPR));
 }
 
 
-- 
2.47.0


From f25282cc45eeea85187eba0e24e3aec625b5c1bb Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sat, 18 Feb 2023 01:02:49 +0100
Subject: [PATCH v1.4 029/120] USB: quirks: Add USB_QUIRK_DELAY_INIT for
 Surface Go 3 Type-Cover

The touchpad on the Type-Cover of the Surface Go 3 is sometimes not
being initialized properly. Apply USB_QUIRK_DELAY_INIT to fix this
issue.

More specifically, the device in question is a fairly standard modern
touchpad with pointer and touchpad input modes. During setup, the device
needs to be switched from pointer- to touchpad-mode (which is done in
hid-multitouch) to fully utilize it as intended. Unfortunately, however,
this seems to occasionally fail silently, leaving the device in
pointer-mode. Applying USB_QUIRK_DELAY_INIT seems to fix this.

Link: https://github.com/linux-surface/linux-surface/issues/1059
Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: surface-typecover
---
 drivers/usb/core/quirks.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/drivers/usb/core/quirks.c b/drivers/usb/core/quirks.c
index 13171454f959..a83beefd25f3 100644
--- a/drivers/usb/core/quirks.c
+++ b/drivers/usb/core/quirks.c
@@ -223,6 +223,9 @@ static const struct usb_device_id usb_quirk_list[] = {
 	/* Microsoft Surface Dock Ethernet (RTL8153 GigE) */
 	{ USB_DEVICE(0x045e, 0x07c6), .driver_info = USB_QUIRK_NO_LPM },
 
+	/* Microsoft Surface Go 3 Type-Cover */
+	{ USB_DEVICE(0x045e, 0x09b5), .driver_info = USB_QUIRK_DELAY_INIT },
+
 	/* Cherry Stream G230 2.0 (G85-231) and 3.0 (G85-232) */
 	{ USB_DEVICE(0x046a, 0x0023), .driver_info = USB_QUIRK_RESET_RESUME },
 
-- 
2.47.0


From 7456c9dc51fe6bcbc326b30175f78b0e12a68913 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Jonas=20Dre=C3=9Fler?= <verdre@v0yd.nl>
Date: Thu, 5 Nov 2020 13:09:45 +0100
Subject: [PATCH v1.4 030/120] hid/multitouch: Turn off Type Cover keyboard
 backlight when suspending

The Type Cover for Microsoft Surface devices supports a special usb
control request to disable or enable the built-in keyboard backlight.
On Windows, this request happens when putting the device into suspend or
resuming it, without it the backlight of the Type Cover will remain
enabled for some time even though the computer is suspended, which looks
weird to the user.

So add support for this special usb control request to hid-multitouch,
which is the driver that's handling the Type Cover.

The reason we have to use a pm_notifier for this instead of the usual
suspend/resume methods is that those won't get called in case the usb
device is already autosuspended.

Also, if the device is autosuspended, we have to briefly autoresume it
in order to send the request. Doing that should be fine, the usb-core
driver does something similar during suspend inside choose_wakeup().

To make sure we don't send that request to every device but only to
devices which support it, add a new quirk
MT_CLS_WIN_8_MS_SURFACE_TYPE_COVER to hid-multitouch. For now this quirk
is only enabled for the usb id of the Surface Pro 2017 Type Cover, which
is where I confirmed that it's working.

Patchset: surface-typecover
---
 drivers/hid/hid-multitouch.c | 100 ++++++++++++++++++++++++++++++++++-
 1 file changed, 98 insertions(+), 2 deletions(-)

diff --git a/drivers/hid/hid-multitouch.c b/drivers/hid/hid-multitouch.c
index 847462650549..eea3fdbe28e9 100644
--- a/drivers/hid/hid-multitouch.c
+++ b/drivers/hid/hid-multitouch.c
@@ -34,7 +34,10 @@
 #include <linux/device.h>
 #include <linux/hid.h>
 #include <linux/module.h>
+#include <linux/pm_runtime.h>
 #include <linux/slab.h>
+#include <linux/suspend.h>
+#include <linux/usb.h>
 #include <linux/input/mt.h>
 #include <linux/jiffies.h>
 #include <linux/string.h>
@@ -47,6 +50,7 @@ MODULE_DESCRIPTION("HID multitouch panels");
 MODULE_LICENSE("GPL");
 
 #include "hid-ids.h"
+#include "usbhid/usbhid.h"
 
 /* quirks to control the device */
 #define MT_QUIRK_NOT_SEEN_MEANS_UP	BIT(0)
@@ -72,12 +76,15 @@ MODULE_LICENSE("GPL");
 #define MT_QUIRK_FORCE_MULTI_INPUT	BIT(20)
 #define MT_QUIRK_DISABLE_WAKEUP		BIT(21)
 #define MT_QUIRK_ORIENTATION_INVERT	BIT(22)
+#define MT_QUIRK_HAS_TYPE_COVER_BACKLIGHT	BIT(23)
 
 #define MT_INPUTMODE_TOUCHSCREEN	0x02
 #define MT_INPUTMODE_TOUCHPAD		0x03
 
 #define MT_BUTTONTYPE_CLICKPAD		0
 
+#define MS_TYPE_COVER_FEATURE_REPORT_USAGE	0xff050086
+
 enum latency_mode {
 	HID_LATENCY_NORMAL = 0,
 	HID_LATENCY_HIGH = 1,
@@ -168,6 +175,8 @@ struct mt_device {
 
 	struct list_head applications;
 	struct list_head reports;
+
+	struct notifier_block pm_notifier;
 };
 
 static void mt_post_parse_default_settings(struct mt_device *td,
@@ -212,6 +221,7 @@ static void mt_post_parse(struct mt_device *td, struct mt_application *app);
 #define MT_CLS_GOOGLE				0x0111
 #define MT_CLS_RAZER_BLADE_STEALTH		0x0112
 #define MT_CLS_SMART_TECH			0x0113
+#define MT_CLS_WIN_8_MS_SURFACE_TYPE_COVER	0x0114
 
 #define MT_DEFAULT_MAXCONTACT	10
 #define MT_MAX_MAXCONTACT	250
@@ -396,6 +406,16 @@ static const struct mt_class mt_classes[] = {
 			MT_QUIRK_CONTACT_CNT_ACCURATE |
 			MT_QUIRK_SEPARATE_APP_REPORT,
 	},
+	{ .name = MT_CLS_WIN_8_MS_SURFACE_TYPE_COVER,
+		.quirks = MT_QUIRK_HAS_TYPE_COVER_BACKLIGHT |
+			MT_QUIRK_ALWAYS_VALID |
+			MT_QUIRK_IGNORE_DUPLICATES |
+			MT_QUIRK_HOVERING |
+			MT_QUIRK_CONTACT_CNT_ACCURATE |
+			MT_QUIRK_STICKY_FINGERS |
+			MT_QUIRK_WIN8_PTP_BUTTONS,
+		.export_all_inputs = true
+	},
 	{ }
 };
 
@@ -1745,6 +1765,69 @@ static void mt_expired_timeout(struct timer_list *t)
 	clear_bit_unlock(MT_IO_FLAGS_RUNNING, &td->mt_io_flags);
 }
 
+static void get_type_cover_backlight_field(struct hid_device *hdev,
+					   struct hid_field **field)
+{
+	struct hid_report_enum *rep_enum;
+	struct hid_report *rep;
+	struct hid_field *cur_field;
+	int i, j;
+
+	rep_enum = &hdev->report_enum[HID_FEATURE_REPORT];
+	list_for_each_entry(rep, &rep_enum->report_list, list) {
+		for (i = 0; i < rep->maxfield; i++) {
+			cur_field = rep->field[i];
+
+			for (j = 0; j < cur_field->maxusage; j++) {
+				if (cur_field->usage[j].hid
+				    == MS_TYPE_COVER_FEATURE_REPORT_USAGE) {
+					*field = cur_field;
+					return;
+				}
+			}
+		}
+	}
+}
+
+static void update_keyboard_backlight(struct hid_device *hdev, bool enabled)
+{
+	struct usb_device *udev = hid_to_usb_dev(hdev);
+	struct hid_field *field = NULL;
+
+	/* Wake up the device in case it's already suspended */
+	pm_runtime_get_sync(&udev->dev);
+
+	get_type_cover_backlight_field(hdev, &field);
+	if (!field) {
+		hid_err(hdev, "couldn't find backlight field\n");
+		goto out;
+	}
+
+	field->value[field->index] = enabled ? 0x01ff00ff : 0x00ff00ff;
+	hid_hw_request(hdev, field->report, HID_REQ_SET_REPORT);
+
+out:
+	pm_runtime_put_sync(&udev->dev);
+}
+
+static int mt_pm_notifier(struct notifier_block *notifier,
+			  unsigned long pm_event,
+			  void *unused)
+{
+	struct mt_device *td =
+		container_of(notifier, struct mt_device, pm_notifier);
+	struct hid_device *hdev = td->hdev;
+
+	if (td->mtclass.quirks & MT_QUIRK_HAS_TYPE_COVER_BACKLIGHT) {
+		if (pm_event == PM_SUSPEND_PREPARE)
+			update_keyboard_backlight(hdev, 0);
+		else if (pm_event == PM_POST_SUSPEND)
+			update_keyboard_backlight(hdev, 1);
+	}
+
+	return NOTIFY_DONE;
+}
+
 static int mt_probe(struct hid_device *hdev, const struct hid_device_id *id)
 {
 	int ret, i;
@@ -1768,6 +1851,9 @@ static int mt_probe(struct hid_device *hdev, const struct hid_device_id *id)
 	td->inputmode_value = MT_INPUTMODE_TOUCHSCREEN;
 	hid_set_drvdata(hdev, td);
 
+	td->pm_notifier.notifier_call = mt_pm_notifier;
+	register_pm_notifier(&td->pm_notifier);
+
 	INIT_LIST_HEAD(&td->applications);
 	INIT_LIST_HEAD(&td->reports);
 
@@ -1806,15 +1892,19 @@ static int mt_probe(struct hid_device *hdev, const struct hid_device_id *id)
 	timer_setup(&td->release_timer, mt_expired_timeout, 0);
 
 	ret = hid_parse(hdev);
-	if (ret != 0)
+	if (ret != 0) {
+		unregister_pm_notifier(&td->pm_notifier);
 		return ret;
+	}
 
 	if (mtclass->quirks & MT_QUIRK_FIX_CONST_CONTACT_ID)
 		mt_fix_const_fields(hdev, HID_DG_CONTACTID);
 
 	ret = hid_hw_start(hdev, HID_CONNECT_DEFAULT);
-	if (ret)
+	if (ret) {
+		unregister_pm_notifier(&td->pm_notifier);
 		return ret;
+	}
 
 	ret = sysfs_create_group(&hdev->dev.kobj, &mt_attribute_group);
 	if (ret)
@@ -1864,6 +1954,7 @@ static void mt_remove(struct hid_device *hdev)
 {
 	struct mt_device *td = hid_get_drvdata(hdev);
 
+	unregister_pm_notifier(&td->pm_notifier);
 	del_timer_sync(&td->release_timer);
 
 	sysfs_remove_group(&hdev->dev.kobj, &mt_attribute_group);
@@ -2277,6 +2368,11 @@ static const struct hid_device_id mt_devices[] = {
 		MT_USB_DEVICE(USB_VENDOR_ID_XIROKU,
 			USB_DEVICE_ID_XIROKU_CSR2) },
 
+	/* Microsoft Surface type cover */
+	{ .driver_data = MT_CLS_WIN_8_MS_SURFACE_TYPE_COVER,
+		HID_DEVICE(HID_BUS_ANY, HID_GROUP_ANY,
+			USB_VENDOR_ID_MICROSOFT, 0x09c0) },
+
 	/* Google MT devices */
 	{ .driver_data = MT_CLS_GOOGLE,
 		HID_DEVICE(HID_BUS_ANY, HID_GROUP_ANY, USB_VENDOR_ID_GOOGLE,
-- 
2.47.0


From 82ba82fa43e2c213242b654d7284aee2f5a80546 Mon Sep 17 00:00:00 2001
From: PJungkamp <p.jungkamp@gmail.com>
Date: Fri, 25 Feb 2022 12:04:25 +0100
Subject: [PATCH v1.4 031/120] hid/multitouch: Add support for surface pro type
 cover tablet switch

The Surface Pro Type Cover has several non standard HID usages in it's
hid report descriptor.
I noticed that, upon folding the typecover back, a vendor specific range
of 4 32 bit integer hid usages is transmitted.
Only the first byte of the message seems to convey reliable information
about the keyboard state.

0x22 => Normal (keys enabled)
0x33 => Folded back (keys disabled)
0x53 => Rotated left/right side up (keys disabled)
0x13 => Cover closed (keys disabled)
0x43 => Folded back and Tablet upside down (keys disabled)
This list may not be exhaustive.

The tablet mode switch will be disabled for a value of 0x22 and enabled
on any other value.

Patchset: surface-typecover
---
 drivers/hid/hid-multitouch.c | 148 +++++++++++++++++++++++++++++------
 1 file changed, 122 insertions(+), 26 deletions(-)

diff --git a/drivers/hid/hid-multitouch.c b/drivers/hid/hid-multitouch.c
index eea3fdbe28e9..382dd01beb82 100644
--- a/drivers/hid/hid-multitouch.c
+++ b/drivers/hid/hid-multitouch.c
@@ -77,6 +77,7 @@ MODULE_LICENSE("GPL");
 #define MT_QUIRK_DISABLE_WAKEUP		BIT(21)
 #define MT_QUIRK_ORIENTATION_INVERT	BIT(22)
 #define MT_QUIRK_HAS_TYPE_COVER_BACKLIGHT	BIT(23)
+#define MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH	BIT(24)
 
 #define MT_INPUTMODE_TOUCHSCREEN	0x02
 #define MT_INPUTMODE_TOUCHPAD		0x03
@@ -84,6 +85,8 @@ MODULE_LICENSE("GPL");
 #define MT_BUTTONTYPE_CLICKPAD		0
 
 #define MS_TYPE_COVER_FEATURE_REPORT_USAGE	0xff050086
+#define MS_TYPE_COVER_TABLET_MODE_SWITCH_USAGE	0xff050072
+#define MS_TYPE_COVER_APPLICATION	0xff050050
 
 enum latency_mode {
 	HID_LATENCY_NORMAL = 0,
@@ -408,6 +411,7 @@ static const struct mt_class mt_classes[] = {
 	},
 	{ .name = MT_CLS_WIN_8_MS_SURFACE_TYPE_COVER,
 		.quirks = MT_QUIRK_HAS_TYPE_COVER_BACKLIGHT |
+			MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH |
 			MT_QUIRK_ALWAYS_VALID |
 			MT_QUIRK_IGNORE_DUPLICATES |
 			MT_QUIRK_HOVERING |
@@ -1389,6 +1393,9 @@ static int mt_input_mapping(struct hid_device *hdev, struct hid_input *hi,
 	    field->application != HID_CP_CONSUMER_CONTROL &&
 	    field->application != HID_GD_WIRELESS_RADIO_CTLS &&
 	    field->application != HID_GD_SYSTEM_MULTIAXIS &&
+	    !(field->application == MS_TYPE_COVER_APPLICATION &&
+	      application->quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH &&
+	      usage->hid == MS_TYPE_COVER_TABLET_MODE_SWITCH_USAGE) &&
 	    !(field->application == HID_VD_ASUS_CUSTOM_MEDIA_KEYS &&
 	      application->quirks & MT_QUIRK_ASUS_CUSTOM_UP))
 		return -1;
@@ -1416,6 +1423,21 @@ static int mt_input_mapping(struct hid_device *hdev, struct hid_input *hi,
 		return 1;
 	}
 
+	/*
+	 * The Microsoft Surface Pro Typecover has a non-standard HID
+	 * tablet mode switch on a vendor specific usage page with vendor
+	 * specific usage.
+	 */
+	if (field->application == MS_TYPE_COVER_APPLICATION &&
+	    application->quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH &&
+	    usage->hid == MS_TYPE_COVER_TABLET_MODE_SWITCH_USAGE) {
+		usage->type = EV_SW;
+		usage->code = SW_TABLET_MODE;
+		*max = SW_MAX;
+		*bit = hi->input->swbit;
+		return 1;
+	}
+
 	if (rdata->is_mt_collection)
 		return mt_touch_input_mapping(hdev, hi, field, usage, bit, max,
 					      application);
@@ -1437,6 +1459,7 @@ static int mt_input_mapped(struct hid_device *hdev, struct hid_input *hi,
 {
 	struct mt_device *td = hid_get_drvdata(hdev);
 	struct mt_report_data *rdata;
+	struct input_dev *input;
 
 	rdata = mt_find_report_data(td, field->report);
 	if (rdata && rdata->is_mt_collection) {
@@ -1444,6 +1467,19 @@ static int mt_input_mapped(struct hid_device *hdev, struct hid_input *hi,
 		return -1;
 	}
 
+	/*
+	 * We own an input device which acts as a tablet mode switch for
+	 * the Surface Pro Typecover.
+	 */
+	if (field->application == MS_TYPE_COVER_APPLICATION &&
+	    rdata->application->quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH &&
+	    usage->hid == MS_TYPE_COVER_TABLET_MODE_SWITCH_USAGE) {
+		input = hi->input;
+		input_set_capability(input, EV_SW, SW_TABLET_MODE);
+		input_report_switch(input, SW_TABLET_MODE, 0);
+		return -1;
+	}
+
 	/* let hid-core decide for the others */
 	return 0;
 }
@@ -1453,11 +1489,21 @@ static int mt_event(struct hid_device *hid, struct hid_field *field,
 {
 	struct mt_device *td = hid_get_drvdata(hid);
 	struct mt_report_data *rdata;
+	struct input_dev *input;
 
 	rdata = mt_find_report_data(td, field->report);
 	if (rdata && rdata->is_mt_collection)
 		return mt_touch_event(hid, field, usage, value);
 
+	if (field->application == MS_TYPE_COVER_APPLICATION &&
+	    rdata->application->quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH &&
+	    usage->hid == MS_TYPE_COVER_TABLET_MODE_SWITCH_USAGE) {
+		input = field->hidinput->input;
+		input_report_switch(input, SW_TABLET_MODE, (value & 0xFF) != 0x22);
+		input_sync(input);
+		return 1;
+	}
+
 	return 0;
 }
 
@@ -1635,6 +1681,42 @@ static void mt_post_parse(struct mt_device *td, struct mt_application *app)
 		app->quirks &= ~MT_QUIRK_CONTACT_CNT_ACCURATE;
 }
 
+static int get_type_cover_field(struct hid_report_enum *rep_enum,
+				struct hid_field **field, int usage)
+{
+	struct hid_report *rep;
+	struct hid_field *cur_field;
+	int i, j;
+
+	list_for_each_entry(rep, &rep_enum->report_list, list) {
+		for (i = 0; i < rep->maxfield; i++) {
+			cur_field = rep->field[i];
+			if (cur_field->application != MS_TYPE_COVER_APPLICATION)
+				continue;
+			for (j = 0; j < cur_field->maxusage; j++) {
+				if (cur_field->usage[j].hid == usage) {
+					*field = cur_field;
+					return true;
+				}
+			}
+		}
+	}
+	return false;
+}
+
+static void request_type_cover_tablet_mode_switch(struct hid_device *hdev)
+{
+	struct hid_field *field;
+
+	if (get_type_cover_field(&hdev->report_enum[HID_INPUT_REPORT],
+				 &field,
+				 MS_TYPE_COVER_TABLET_MODE_SWITCH_USAGE)) {
+		hid_hw_request(hdev, field->report, HID_REQ_GET_REPORT);
+	} else {
+		hid_err(hdev, "couldn't find tablet mode field\n");
+	}
+}
+
 static int mt_input_configured(struct hid_device *hdev, struct hid_input *hi)
 {
 	struct mt_device *td = hid_get_drvdata(hdev);
@@ -1683,6 +1765,13 @@ static int mt_input_configured(struct hid_device *hdev, struct hid_input *hi)
 		/* force BTN_STYLUS to allow tablet matching in udev */
 		__set_bit(BTN_STYLUS, hi->input->keybit);
 		break;
+	case MS_TYPE_COVER_APPLICATION:
+		if (td->mtclass.quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH) {
+			suffix = "Tablet Mode Switch";
+			request_type_cover_tablet_mode_switch(hdev);
+			break;
+		}
+		fallthrough;
 	default:
 		suffix = "UNKNOWN";
 		break;
@@ -1765,30 +1854,6 @@ static void mt_expired_timeout(struct timer_list *t)
 	clear_bit_unlock(MT_IO_FLAGS_RUNNING, &td->mt_io_flags);
 }
 
-static void get_type_cover_backlight_field(struct hid_device *hdev,
-					   struct hid_field **field)
-{
-	struct hid_report_enum *rep_enum;
-	struct hid_report *rep;
-	struct hid_field *cur_field;
-	int i, j;
-
-	rep_enum = &hdev->report_enum[HID_FEATURE_REPORT];
-	list_for_each_entry(rep, &rep_enum->report_list, list) {
-		for (i = 0; i < rep->maxfield; i++) {
-			cur_field = rep->field[i];
-
-			for (j = 0; j < cur_field->maxusage; j++) {
-				if (cur_field->usage[j].hid
-				    == MS_TYPE_COVER_FEATURE_REPORT_USAGE) {
-					*field = cur_field;
-					return;
-				}
-			}
-		}
-	}
-}
-
 static void update_keyboard_backlight(struct hid_device *hdev, bool enabled)
 {
 	struct usb_device *udev = hid_to_usb_dev(hdev);
@@ -1797,8 +1862,9 @@ static void update_keyboard_backlight(struct hid_device *hdev, bool enabled)
 	/* Wake up the device in case it's already suspended */
 	pm_runtime_get_sync(&udev->dev);
 
-	get_type_cover_backlight_field(hdev, &field);
-	if (!field) {
+	if (!get_type_cover_field(&hdev->report_enum[HID_FEATURE_REPORT],
+				  &field,
+				  MS_TYPE_COVER_FEATURE_REPORT_USAGE)) {
 		hid_err(hdev, "couldn't find backlight field\n");
 		goto out;
 	}
@@ -1932,13 +1998,24 @@ static int mt_suspend(struct hid_device *hdev, pm_message_t state)
 
 static int mt_reset_resume(struct hid_device *hdev)
 {
+	struct mt_device *td = hid_get_drvdata(hdev);
+
 	mt_release_contacts(hdev);
 	mt_set_modes(hdev, HID_LATENCY_NORMAL, true, true);
+
+	/* Request an update on the typecover folding state on resume
+	 * after reset.
+	 */
+	if (td->mtclass.quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH)
+		request_type_cover_tablet_mode_switch(hdev);
+
 	return 0;
 }
 
 static int mt_resume(struct hid_device *hdev)
 {
+	struct mt_device *td = hid_get_drvdata(hdev);
+
 	/* Some Elan legacy devices require SET_IDLE to be set on resume.
 	 * It should be safe to send it to other devices too.
 	 * Tested on 3M, Stantum, Cypress, Zytronic, eGalax, and Elan panels. */
@@ -1947,12 +2024,31 @@ static int mt_resume(struct hid_device *hdev)
 
 	mt_set_modes(hdev, HID_LATENCY_NORMAL, true, true);
 
+	/* Request an update on the typecover folding state on resume. */
+	if (td->mtclass.quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH)
+		request_type_cover_tablet_mode_switch(hdev);
+
 	return 0;
 }
 
 static void mt_remove(struct hid_device *hdev)
 {
 	struct mt_device *td = hid_get_drvdata(hdev);
+	struct hid_field *field;
+	struct input_dev *input;
+
+	/* Reset tablet mode switch on disconnect. */
+	if (td->mtclass.quirks & MT_QUIRK_HAS_TYPE_COVER_TABLET_MODE_SWITCH) {
+		if (get_type_cover_field(&hdev->report_enum[HID_INPUT_REPORT],
+					 &field,
+					 MS_TYPE_COVER_TABLET_MODE_SWITCH_USAGE)) {
+			input = field->hidinput->input;
+			input_report_switch(input, SW_TABLET_MODE, 0);
+			input_sync(input);
+		} else {
+			hid_err(hdev, "couldn't find tablet mode field\n");
+		}
+	}
 
 	unregister_pm_notifier(&td->pm_notifier);
 	del_timer_sync(&td->release_timer);
-- 
2.47.0


From 5bb084d079f61afb0d64ae6a126583d9f2468054 Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sun, 19 Feb 2023 22:12:24 +0100
Subject: [PATCH v1.4 032/120] PCI: Add quirk to prevent calling shutdown
 mehtod

Work around buggy EFI firmware: On some Microsoft Surface devices
(Surface Pro 9 and Surface Laptop 5) the EFI ResetSystem call with
EFI_RESET_SHUTDOWN doesn't function properly. Instead of shutting the
system down, it returns and the system stays on.

It turns out that this only happens after PCI shutdown callbacks ran for
specific devices. Excluding those devices from the shutdown process
makes the ResetSystem call work as expected.

TODO: Maybe we can find a better way or the root cause of this?

Not-Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: surface-shutdown
---
 drivers/pci/pci-driver.c |  3 +++
 drivers/pci/quirks.c     | 36 ++++++++++++++++++++++++++++++++++++
 include/linux/pci.h      |  1 +
 3 files changed, 40 insertions(+)

diff --git a/drivers/pci/pci-driver.c b/drivers/pci/pci-driver.c
index f412ef73a6e4..9892cd72dd2c 100644
--- a/drivers/pci/pci-driver.c
+++ b/drivers/pci/pci-driver.c
@@ -505,6 +505,9 @@ static void pci_device_shutdown(struct device *dev)
 	struct pci_dev *pci_dev = to_pci_dev(dev);
 	struct pci_driver *drv = pci_dev->driver;
 
+	if (pci_dev->no_shutdown)
+		return;
+
 	pm_runtime_resume(dev);
 
 	if (drv && drv->shutdown)
diff --git a/drivers/pci/quirks.c b/drivers/pci/quirks.c
index c7ec62f076c3..69449d13ff21 100644
--- a/drivers/pci/quirks.c
+++ b/drivers/pci/quirks.c
@@ -6322,3 +6322,39 @@ static void pci_mask_replay_timer_timeout(struct pci_dev *pdev)
 DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_GLI, 0x9750, pci_mask_replay_timer_timeout);
 DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_GLI, 0x9755, pci_mask_replay_timer_timeout);
 #endif
+
+static const struct dmi_system_id no_shutdown_dmi_table[] = {
+	/*
+	 * Systems on which some devices should not be touched during shutdown.
+	 */
+	{
+		.ident = "Microsoft Surface Pro 9",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "Surface Pro 9"),
+		},
+	},
+	{
+		.ident = "Microsoft Surface Laptop 5",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_MATCH(DMI_PRODUCT_NAME, "Surface Laptop 5"),
+		},
+	},
+	{}
+};
+
+static void quirk_no_shutdown(struct pci_dev *dev)
+{
+	if (!dmi_check_system(no_shutdown_dmi_table))
+		return;
+
+	dev->no_shutdown = 1;
+	pci_info(dev, "disabling shutdown ops for [%04x:%04x]\n",
+		 dev->vendor, dev->device);
+}
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x461e, quirk_no_shutdown);  // Thunderbolt 4 USB Controller
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x461f, quirk_no_shutdown);  // Thunderbolt 4 PCI Express Root Port
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x462f, quirk_no_shutdown);  // Thunderbolt 4 PCI Express Root Port
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x466d, quirk_no_shutdown);  // Thunderbolt 4 NHI
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x46a8, quirk_no_shutdown);  // GPU
diff --git a/include/linux/pci.h b/include/linux/pci.h
index 37d97bef060f..056fce19477b 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -466,6 +466,7 @@ struct pci_dev {
 	unsigned int	no_command_memory:1;	/* No PCI_COMMAND_MEMORY */
 	unsigned int	rom_bar_overlap:1;	/* ROM BAR disable broken */
 	unsigned int	rom_attr_enabled:1;	/* Display of ROM attribute enabled? */
+	unsigned int	no_shutdown:1;		/* Do not touch device on shutdown */
 	pci_dev_flags_t dev_flags;
 	atomic_t	enable_cnt;	/* pci_enable_device has been called */
 
-- 
2.47.0


From edb4d51b7d4cb83989977028ff069bf6ec08ac37 Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Sun, 12 Mar 2023 01:41:57 +0100
Subject: [PATCH v1.4 033/120] platform/surface: gpe: Add support for Surface
 Pro 9

Add the lid GPE used by the Surface Pro 9.

Signed-off-by: Maximilian Luz <luzmaximilian@gmail.com>
Patchset: surface-gpe
---
 drivers/platform/surface/surface_gpe.c | 17 +++++++++++++++++
 1 file changed, 17 insertions(+)

diff --git a/drivers/platform/surface/surface_gpe.c b/drivers/platform/surface/surface_gpe.c
index 62fd4004db31..103fc4468262 100644
--- a/drivers/platform/surface/surface_gpe.c
+++ b/drivers/platform/surface/surface_gpe.c
@@ -41,6 +41,11 @@ static const struct property_entry lid_device_props_l4F[] = {
 	{},
 };
 
+static const struct property_entry lid_device_props_l52[] = {
+	PROPERTY_ENTRY_U32("gpe", 0x52),
+	{},
+};
+
 static const struct property_entry lid_device_props_l57[] = {
 	PROPERTY_ENTRY_U32("gpe", 0x57),
 	{},
@@ -107,6 +112,18 @@ static const struct dmi_system_id dmi_lid_device_table[] = {
 		},
 		.driver_data = (void *)lid_device_props_l4B,
 	},
+	{
+		/*
+		 * We match for SKU here due to product name clash with the ARM
+		 * version.
+		 */
+		.ident = "Surface Pro 9",
+		.matches = {
+			DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_EXACT_MATCH(DMI_PRODUCT_SKU, "Surface_Pro_9_2038"),
+		},
+		.driver_data = (void *)lid_device_props_l52,
+	},
 	{
 		.ident = "Surface Book 1",
 		.matches = {
-- 
2.47.0


From e7b4ef7e10478fa9ae687988a0eeb3aaafc38cfc Mon Sep 17 00:00:00 2001
From: Hans de Goede <hdegoede@redhat.com>
Date: Sun, 10 Oct 2021 20:56:57 +0200
Subject: [PATCH v1.4 034/120] ACPI: delay enumeration of devices with a _DEP
 pointing to an INT3472 device

The clk and regulator frameworks expect clk/regulator consumer-devices
to have info about the consumed clks/regulators described in the device's
fw_node.

To work around cases where this info is not present in the firmware tables,
which is often the case on x86/ACPI devices, both frameworks allow the
provider-driver to attach info about consumers to the clks/regulators
when registering these.

This causes problems with the probe ordering wrt drivers for consumers
of these clks/regulators. Since the lookups are only registered when the
provider-driver binds, trying to get these clks/regulators before then
results in a -ENOENT error for clks and a dummy regulator for regulators.

One case where we hit this issue is camera sensors such as e.g. the OV8865
sensor found on the Microsoft Surface Go. The sensor uses clks, regulators
and GPIOs provided by a TPS68470 PMIC which is described in an INT3472
ACPI device. There is special platform code handling this and setting
platform_data with the necessary consumer info on the MFD cells
instantiated for the PMIC under: drivers/platform/x86/intel/int3472.

For this to work properly the ov8865 driver must not bind to the I2C-client
for the OV8865 sensor until after the TPS68470 PMIC gpio, regulator and
clk MFD cells have all been fully setup.

The OV8865 on the Microsoft Surface Go is just one example, all X86
devices using the Intel IPU3 camera block found on recent Intel SoCs
have similar issues where there is an INT3472 HID ACPI-device, which
describes the clks and regulators, and the driver for this INT3472 device
must be fully initialized before the sensor driver (any sensor driver)
binds for things to work properly.

On these devices the ACPI nodes describing the sensors all have a _DEP
dependency on the matching INT3472 ACPI device (there is one per sensor).

This allows solving the probe-ordering problem by delaying the enumeration
(instantiation of the I2C-client in the ov8865 example) of ACPI-devices
which have a _DEP dependency on an INT3472 device.

The new acpi_dev_ready_for_enumeration() helper used for this is also
exported because for devices, which have the enumeration_by_parent flag
set, the parent-driver will do its own scan of child ACPI devices and
it will try to enumerate those during its probe(). Code doing this such
as e.g. the i2c-core-acpi.c code must call this new helper to ensure
that it too delays the enumeration until all the _DEP dependencies are
met on devices which have the new honor_deps flag set.

Signed-off-by: Hans de Goede <hdegoede@redhat.com>
Patchset: cameras
---
 drivers/acpi/scan.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index 1329032ed09b..f5ac2e29a35f 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -2194,6 +2194,9 @@ static acpi_status acpi_bus_check_add_2(acpi_handle handle, u32 lvl_not_used,
 
 static void acpi_default_enumeration(struct acpi_device *device)
 {
+	if (!acpi_dev_ready_for_enumeration(device))
+		return;
+
 	/*
 	 * Do not enumerate devices with enumeration_by_parent flag set as
 	 * they will be enumerated by their respective parents.
-- 
2.47.0


From d3333a8802a90aa3a33944ff5f0f75f4d1849b15 Mon Sep 17 00:00:00 2001
From: zouxiaoh <xiaohong.zou@intel.com>
Date: Fri, 25 Jun 2021 08:52:59 +0800
Subject: [PATCH v1.4 035/120] iommu: intel-ipu: use IOMMU passthrough mode for
 Intel IPUs

Intel IPU(Image Processing Unit) has its own (IO)MMU hardware,
The IPU driver allocates its own page table that is not mapped
via the DMA, and thus the Intel IOMMU driver blocks access giving
this error: DMAR: DRHD: handling fault status reg 3 DMAR:
[DMA Read] Request device [00:05.0] PASID ffffffff
fault addr 76406000 [fault reason 06] PTE Read access is not set
As IPU is not an external facing device which is not risky, so use
IOMMU passthrough mode for Intel IPUs.

Change-Id: I6dcccdadac308cf42e20a18e1b593381391e3e6b
Depends-On: Iacd67578e8c6a9b9ac73285f52b4081b72fb68a6
Tracked-On: #JIITL8-411
Signed-off-by: Bingbu Cao <bingbu.cao@intel.com>
Signed-off-by: zouxiaoh <xiaohong.zou@intel.com>
Signed-off-by: Xu Chongyang <chongyang.xu@intel.com>
Patchset: cameras
---
 drivers/iommu/intel/iommu.c | 30 ++++++++++++++++++++++++++++++
 1 file changed, 30 insertions(+)

diff --git a/drivers/iommu/intel/iommu.c b/drivers/iommu/intel/iommu.c
index 85b48ba6e0af..1c2ad04c1abb 100644
--- a/drivers/iommu/intel/iommu.c
+++ b/drivers/iommu/intel/iommu.c
@@ -45,6 +45,13 @@
 		((pdev)->vendor == PCI_VENDOR_ID_INTEL && (pdev)->device == 0x34E4) \
 	)
 
+#define IS_INTEL_IPU(pdev) ((pdev)->vendor == PCI_VENDOR_ID_INTEL &&	\
+			   ((pdev)->device == 0x9a19 ||		\
+			    (pdev)->device == 0x9a39 ||		\
+			    (pdev)->device == 0x4e19 ||		\
+			    (pdev)->device == 0x465d ||		\
+			    (pdev)->device == 0x1919))
+
 #define IOAPIC_RANGE_START	(0xfee00000)
 #define IOAPIC_RANGE_END	(0xfeefffff)
 #define IOVA_START_ADDR		(0x1000)
@@ -223,12 +230,14 @@ int intel_iommu_enabled = 0;
 EXPORT_SYMBOL_GPL(intel_iommu_enabled);
 
 static int dmar_map_ipts = 1;
+static int dmar_map_ipu = 1;
 static int intel_iommu_superpage = 1;
 static int iommu_identity_mapping;
 static int iommu_skip_te_disable;
 static int disable_igfx_iommu;
 
 #define IDENTMAP_AZALIA		4
+#define IDENTMAP_IPU		8
 #define IDENTMAP_IPTS		16
 
 const struct iommu_ops intel_iommu_ops;
@@ -2164,6 +2173,9 @@ static int device_def_domain_type(struct device *dev)
 		if ((iommu_identity_mapping & IDENTMAP_AZALIA) && IS_AZALIA(pdev))
 			return IOMMU_DOMAIN_IDENTITY;
 
+		if ((iommu_identity_mapping & IDENTMAP_IPU) && IS_INTEL_IPU(pdev))
+			return IOMMU_DOMAIN_IDENTITY;
+
 		if ((iommu_identity_mapping & IDENTMAP_IPTS) && IS_IPTS(pdev))
 			return IOMMU_DOMAIN_IDENTITY;
 	}
@@ -2466,6 +2478,9 @@ static int __init init_dmars(void)
 		iommu_set_root_entry(iommu);
 	}
 
+	if (!dmar_map_ipu)
+		iommu_identity_mapping |= IDENTMAP_IPU;
+
 	if (!dmar_map_ipts)
 		iommu_identity_mapping |= IDENTMAP_IPTS;
 
@@ -4714,6 +4729,18 @@ static void quirk_iommu_igfx(struct pci_dev *dev)
 	disable_igfx_iommu = 1;
 }
 
+static void quirk_iommu_ipu(struct pci_dev *dev)
+{
+	if (!IS_INTEL_IPU(dev))
+		return;
+
+	if (risky_device(dev))
+		return;
+
+	pci_info(dev, "Passthrough IOMMU for integrated Intel IPU\n");
+	dmar_map_ipu = 0;
+}
+
 static void quirk_iommu_ipts(struct pci_dev *dev)
 {
 	if (!IS_IPTS(dev))
@@ -4761,6 +4788,9 @@ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x1632, quirk_iommu_igfx);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163A, quirk_iommu_igfx);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x163D, quirk_iommu_igfx);
 
+/* disable IPU dmar support */
+DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, PCI_ANY_ID, quirk_iommu_ipu);
+
 /* disable IPTS dmar support */
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x9D3E, quirk_iommu_ipts);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x34E4, quirk_iommu_ipts);
-- 
2.47.0


From 5cfbfac1f0ada21728afd84e88036746d4e6a630 Mon Sep 17 00:00:00 2001
From: Daniel Scally <djrscally@gmail.com>
Date: Sun, 10 Oct 2021 20:57:02 +0200
Subject: [PATCH v1.4 036/120] platform/x86: int3472: Enable I2c daisy chain

The TPS68470 PMIC has an I2C passthrough mode through which I2C traffic
can be forwarded to a device connected to the PMIC as though it were
connected directly to the system bus. Enable this mode when the chip
is initialised.

Signed-off-by: Daniel Scally <djrscally@gmail.com>
Patchset: cameras
---
 drivers/platform/x86/intel/int3472/tps68470.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/drivers/platform/x86/intel/int3472/tps68470.c b/drivers/platform/x86/intel/int3472/tps68470.c
index 1e107fd49f82..e3e1696e7f0e 100644
--- a/drivers/platform/x86/intel/int3472/tps68470.c
+++ b/drivers/platform/x86/intel/int3472/tps68470.c
@@ -46,6 +46,13 @@ static int tps68470_chip_init(struct device *dev, struct regmap *regmap)
 		return ret;
 	}
 
+	/* Enable I2C daisy chain */
+	ret = regmap_write(regmap, TPS68470_REG_S_I2C_CTL, 0x03);
+	if (ret) {
+		dev_err(dev, "Failed to enable i2c daisy chain\n");
+		return ret;
+	}
+
 	dev_info(dev, "TPS68470 REVID: 0x%02x\n", version);
 
 	return 0;
-- 
2.47.0


From f458936cda96afbf4326122c39ba047e01830524 Mon Sep 17 00:00:00 2001
From: Daniel Scally <dan.scally@ideasonboard.com>
Date: Thu, 2 Mar 2023 12:59:39 +0000
Subject: [PATCH v1.4 037/120] platform/x86: int3472: Remap reset GPIO for
 INT347E

ACPI _HID INT347E represents the OmniVision 7251 camera sensor. The
driver for this sensor expects a single pin named "enable", but on
some Microsoft Surface platforms the sensor is assigned a single
GPIO who's type flag is INT3472_GPIO_TYPE_RESET.

Remap the GPIO pin's function from "reset" to "enable". This is done
outside of the existing remap table since it is a more widespread
discrepancy than that method is designed for. Additionally swap the
polarity of the pin to match the driver's expectation.

Signed-off-by: Daniel Scally <dan.scally@ideasonboard.com>
Patchset: cameras
---
 drivers/platform/x86/intel/int3472/discrete.c | 15 +++++++++++++++
 1 file changed, 15 insertions(+)

diff --git a/drivers/platform/x86/intel/int3472/discrete.c b/drivers/platform/x86/intel/int3472/discrete.c
index 07b302e09340..baad1e50ca81 100644
--- a/drivers/platform/x86/intel/int3472/discrete.c
+++ b/drivers/platform/x86/intel/int3472/discrete.c
@@ -83,12 +83,27 @@ static int skl_int3472_map_gpio_to_sensor(struct int3472_discrete_device *int347
 					  const char *func, u32 polarity)
 {
 	int ret;
+	const struct acpi_device_id ov7251_ids[] = {
+		{ "INT347E" },
+		{ }
+	};
 
 	if (int3472->n_sensor_gpios >= INT3472_MAX_SENSOR_GPIOS) {
 		dev_warn(int3472->dev, "Too many GPIOs mapped\n");
 		return -EINVAL;
 	}
 
+	/*
+	 * In addition to the function remap table we need to bulk remap the
+	 * "reset" GPIO for the OmniVision 7251 sensor, as the driver for that
+	 * expects its only GPIO pin to be called "enable" (and to have the
+	 * opposite polarity).
+	 */
+	if (!strcmp(func, "reset") && !acpi_match_device_ids(int3472->sensor, ov7251_ids)) {
+		func = "enable";
+		polarity ^= GPIO_ACTIVE_LOW;
+	}
+
 	ret = skl_int3472_fill_gpiod_lookup(&int3472->gpios.table[int3472->n_sensor_gpios],
 					    agpio, func, polarity);
 	if (ret)
-- 
2.47.0


From 32f3c43ae61b4900d273c3d23dfb3f641e0f7298 Mon Sep 17 00:00:00 2001
From: Daniel Scally <dan.scally@ideasonboard.com>
Date: Tue, 21 Mar 2023 13:45:26 +0000
Subject: [PATCH v1.4 038/120] media: i2c: Clarify that gain is Analogue gain
 in OV7251

Update the control ID for the gain control in the ov7251 driver to
V4L2_CID_ANALOGUE_GAIN.

Signed-off-by: Daniel Scally <dan.scally@ideasonboard.com>
Patchset: cameras
---
 drivers/media/i2c/ov7251.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/media/i2c/ov7251.c b/drivers/media/i2c/ov7251.c
index 30f61e04ecaf..9c1292ca8552 100644
--- a/drivers/media/i2c/ov7251.c
+++ b/drivers/media/i2c/ov7251.c
@@ -1051,7 +1051,7 @@ static int ov7251_s_ctrl(struct v4l2_ctrl *ctrl)
 	case V4L2_CID_EXPOSURE:
 		ret = ov7251_set_exposure(ov7251, ctrl->val);
 		break;
-	case V4L2_CID_GAIN:
+	case V4L2_CID_ANALOGUE_GAIN:
 		ret = ov7251_set_gain(ov7251, ctrl->val);
 		break;
 	case V4L2_CID_TEST_PATTERN:
@@ -1572,7 +1572,7 @@ static int ov7251_init_ctrls(struct ov7251 *ov7251)
 	ov7251->exposure = v4l2_ctrl_new_std(&ov7251->ctrls, &ov7251_ctrl_ops,
 					     V4L2_CID_EXPOSURE, 1, 32, 1, 32);
 	ov7251->gain = v4l2_ctrl_new_std(&ov7251->ctrls, &ov7251_ctrl_ops,
-					 V4L2_CID_GAIN, 16, 1023, 1, 16);
+					 V4L2_CID_ANALOGUE_GAIN, 16, 1023, 1, 16);
 	v4l2_ctrl_new_std_menu_items(&ov7251->ctrls, &ov7251_ctrl_ops,
 				     V4L2_CID_TEST_PATTERN,
 				     ARRAY_SIZE(ov7251_test_pattern_menu) - 1,
-- 
2.47.0


From bfcbd1befcd126552595a714e9bfff3dcb83e0ff Mon Sep 17 00:00:00 2001
From: Daniel Scally <dan.scally@ideasonboard.com>
Date: Wed, 22 Mar 2023 11:01:42 +0000
Subject: [PATCH v1.4 039/120] media: v4l2-core: Acquire privacy led in
 v4l2_async_register_subdev()

The current call to v4l2_subdev_get_privacy_led() is contained in
v4l2_async_register_subdev_sensor(), but that function isn't used by
all the sensor drivers. Move the acquisition of the privacy led to
v4l2_async_register_subdev() instead.

Signed-off-by: Daniel Scally <dan.scally@ideasonboard.com>
Patchset: cameras
---
 drivers/media/v4l2-core/v4l2-async.c  | 4 ++++
 drivers/media/v4l2-core/v4l2-fwnode.c | 4 ----
 2 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/drivers/media/v4l2-core/v4l2-async.c b/drivers/media/v4l2-core/v4l2-async.c
index ee884a8221fb..4f6bafd900ee 100644
--- a/drivers/media/v4l2-core/v4l2-async.c
+++ b/drivers/media/v4l2-core/v4l2-async.c
@@ -799,6 +799,10 @@ int __v4l2_async_register_subdev(struct v4l2_subdev *sd, struct module *module)
 
 	INIT_LIST_HEAD(&sd->asc_list);
 
+	ret = v4l2_subdev_get_privacy_led(sd);
+	if (ret < 0)
+		return ret;
+
 	/*
 	 * No reference taken. The reference is held by the device (struct
 	 * v4l2_subdev.dev), and async sub-device does not exist independently
diff --git a/drivers/media/v4l2-core/v4l2-fwnode.c b/drivers/media/v4l2-core/v4l2-fwnode.c
index f19c8adf2c61..923ed1b5ab8b 100644
--- a/drivers/media/v4l2-core/v4l2-fwnode.c
+++ b/drivers/media/v4l2-core/v4l2-fwnode.c
@@ -1219,10 +1219,6 @@ int v4l2_async_register_subdev_sensor(struct v4l2_subdev *sd)
 
 	v4l2_async_subdev_nf_init(notifier, sd);
 
-	ret = v4l2_subdev_get_privacy_led(sd);
-	if (ret < 0)
-		goto out_cleanup;
-
 	ret = v4l2_async_nf_parse_fwnode_sensor(sd->dev, notifier);
 	if (ret < 0)
 		goto out_cleanup;
-- 
2.47.0


From a74e1877ba342d91bfd6f35bbc61b4ca615b53ef Mon Sep 17 00:00:00 2001
From: Kate Hsuan <hpa@redhat.com>
Date: Tue, 21 Mar 2023 23:37:16 +0800
Subject: [PATCH v1.4 040/120] platform: x86: int3472: Add MFD cell for
 tps68470 LED

Add MFD cell for tps68470-led.

Reviewed-by: Daniel Scally <dan.scally@ideasonboard.com>
Signed-off-by: Kate Hsuan <hpa@redhat.com>
Reviewed-by: Hans de Goede <hdegoede@redhat.com>
Patchset: cameras
---
 drivers/platform/x86/intel/int3472/tps68470.c | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/drivers/platform/x86/intel/int3472/tps68470.c b/drivers/platform/x86/intel/int3472/tps68470.c
index e3e1696e7f0e..423dc555093f 100644
--- a/drivers/platform/x86/intel/int3472/tps68470.c
+++ b/drivers/platform/x86/intel/int3472/tps68470.c
@@ -17,7 +17,7 @@
 #define DESIGNED_FOR_CHROMEOS		1
 #define DESIGNED_FOR_WINDOWS		2
 
-#define TPS68470_WIN_MFD_CELL_COUNT	3
+#define TPS68470_WIN_MFD_CELL_COUNT	4
 
 static const struct mfd_cell tps68470_cros[] = {
 	{ .name = "tps68470-gpio" },
@@ -200,7 +200,8 @@ static int skl_int3472_tps68470_probe(struct i2c_client *client)
 		cells[1].name = "tps68470-regulator";
 		cells[1].platform_data = (void *)board_data->tps68470_regulator_pdata;
 		cells[1].pdata_size = sizeof(struct tps68470_regulator_platform_data);
-		cells[2].name = "tps68470-gpio";
+		cells[2].name = "tps68470-led";
+		cells[3].name = "tps68470-gpio";
 
 		for (i = 0; i < board_data->n_gpiod_lookups; i++)
 			gpiod_add_lookup_table(board_data->tps68470_gpio_lookup_tables[i]);
-- 
2.47.0


From 546c0b6ca3da7760ef2180ad12826fb3c38dea8c Mon Sep 17 00:00:00 2001
From: Kate Hsuan <hpa@redhat.com>
Date: Tue, 21 Mar 2023 23:37:17 +0800
Subject: [PATCH v1.4 041/120] include: mfd: tps68470: Add masks for LEDA and
 LEDB

Add flags for both LEDA(TPS68470_ILEDCTL_ENA), LEDB
(TPS68470_ILEDCTL_ENB), and current control mask for LEDB
(TPS68470_ILEDCTL_CTRLB)

Reviewed-by: Daniel Scally <dan.scally@ideasonboard.com>
Reviewed-by: Hans de Goede <hdegoede@redhat.com>
Signed-off-by: Kate Hsuan <hpa@redhat.com>
Patchset: cameras
---
 include/linux/mfd/tps68470.h | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/include/linux/mfd/tps68470.h b/include/linux/mfd/tps68470.h
index 7807fa329db0..2d2abb25b944 100644
--- a/include/linux/mfd/tps68470.h
+++ b/include/linux/mfd/tps68470.h
@@ -34,6 +34,7 @@
 #define TPS68470_REG_SGPO		0x22
 #define TPS68470_REG_GPDI		0x26
 #define TPS68470_REG_GPDO		0x27
+#define TPS68470_REG_ILEDCTL		0x28
 #define TPS68470_REG_VCMVAL		0x3C
 #define TPS68470_REG_VAUX1VAL		0x3D
 #define TPS68470_REG_VAUX2VAL		0x3E
@@ -94,4 +95,8 @@
 #define TPS68470_GPIO_MODE_OUT_CMOS	2
 #define TPS68470_GPIO_MODE_OUT_ODRAIN	3
 
+#define TPS68470_ILEDCTL_ENA		BIT(2)
+#define TPS68470_ILEDCTL_ENB		BIT(6)
+#define TPS68470_ILEDCTL_CTRLB		GENMASK(5, 4)
+
 #endif /* __LINUX_MFD_TPS68470_H */
-- 
2.47.0


From 438c5ebc359284b316deb56aa67ae77fd7cec652 Mon Sep 17 00:00:00 2001
From: Kate Hsuan <hpa@redhat.com>
Date: Tue, 21 Mar 2023 23:37:18 +0800
Subject: [PATCH v1.4 042/120] leds: tps68470: Add LED control for tps68470

There are two LED controllers, LEDA indicator LED and LEDB flash LED for
tps68470. LEDA can be enabled by setting TPS68470_ILEDCTL_ENA. Moreover,
tps68470 provides four levels of power status for LEDB. If the
properties called "ti,ledb-current" can be found, the current will be
set according to the property values. These two LEDs can be controlled
through the LED class of sysfs (tps68470-leda and tps68470-ledb).

Signed-off-by: Kate Hsuan <hpa@redhat.com>
Reviewed-by: Hans de Goede <hdegoede@redhat.com>
Patchset: cameras
---
 drivers/leds/Kconfig         |  12 +++
 drivers/leds/Makefile        |   1 +
 drivers/leds/leds-tps68470.c | 185 +++++++++++++++++++++++++++++++++++
 3 files changed, 198 insertions(+)
 create mode 100644 drivers/leds/leds-tps68470.c

diff --git a/drivers/leds/Kconfig b/drivers/leds/Kconfig
index 8d9d8da376e4..d8597897aa83 100644
--- a/drivers/leds/Kconfig
+++ b/drivers/leds/Kconfig
@@ -933,6 +933,18 @@ config LEDS_TPS6105X
 	  It is a single boost converter primarily for white LEDs and
 	  audio amplifiers.
 
+config LEDS_TPS68470
+	tristate "LED support for TI TPS68470"
+	depends on LEDS_CLASS
+	depends on INTEL_SKL_INT3472
+	help
+	  This driver supports TPS68470 PMIC with LED chip.
+	  It provides two LED controllers, with the ability to drive 2
+	  indicator LEDs and 2 flash LEDs.
+
+	  To compile this driver as a module, choose M and it will be
+	  called leds-tps68470
+
 config LEDS_IP30
 	tristate "LED support for SGI Octane machines"
 	depends on LEDS_CLASS
diff --git a/drivers/leds/Makefile b/drivers/leds/Makefile
index 18afbb5a23ee..a1d16c0af82d 100644
--- a/drivers/leds/Makefile
+++ b/drivers/leds/Makefile
@@ -88,6 +88,7 @@ obj-$(CONFIG_LEDS_TCA6507)		+= leds-tca6507.o
 obj-$(CONFIG_LEDS_TI_LMU_COMMON)	+= leds-ti-lmu-common.o
 obj-$(CONFIG_LEDS_TLC591XX)		+= leds-tlc591xx.o
 obj-$(CONFIG_LEDS_TPS6105X)		+= leds-tps6105x.o
+obj-$(CONFIG_LEDS_TPS68470)		+= leds-tps68470.o
 obj-$(CONFIG_LEDS_TURRIS_OMNIA)		+= leds-turris-omnia.o
 obj-$(CONFIG_LEDS_WM831X_STATUS)	+= leds-wm831x-status.o
 obj-$(CONFIG_LEDS_WM8350)		+= leds-wm8350.o
diff --git a/drivers/leds/leds-tps68470.c b/drivers/leds/leds-tps68470.c
new file mode 100644
index 000000000000..35aeb5db89c8
--- /dev/null
+++ b/drivers/leds/leds-tps68470.c
@@ -0,0 +1,185 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * LED driver for TPS68470 PMIC
+ *
+ * Copyright (C) 2023 Red Hat
+ *
+ * Authors:
+ *	Kate Hsuan <hpa@redhat.com>
+ */
+
+#include <linux/leds.h>
+#include <linux/mfd/tps68470.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/property.h>
+#include <linux/regmap.h>
+
+
+#define lcdev_to_led(led_cdev) \
+	container_of(led_cdev, struct tps68470_led, lcdev)
+
+#define led_to_tps68470(led, index) \
+	container_of(led, struct tps68470_device, leds[index])
+
+enum tps68470_led_ids {
+	TPS68470_ILED_A,
+	TPS68470_ILED_B,
+	TPS68470_NUM_LEDS
+};
+
+static const char *tps68470_led_names[] = {
+	[TPS68470_ILED_A] = "tps68470-iled_a",
+	[TPS68470_ILED_B] = "tps68470-iled_b",
+};
+
+struct tps68470_led {
+	unsigned int led_id;
+	struct led_classdev lcdev;
+};
+
+struct tps68470_device {
+	struct device *dev;
+	struct regmap *regmap;
+	struct tps68470_led leds[TPS68470_NUM_LEDS];
+};
+
+enum ctrlb_current {
+	CTRLB_2MA	= 0,
+	CTRLB_4MA	= 1,
+	CTRLB_8MA	= 2,
+	CTRLB_16MA	= 3,
+};
+
+static int tps68470_brightness_set(struct led_classdev *led_cdev, enum led_brightness brightness)
+{
+	struct tps68470_led *led = lcdev_to_led(led_cdev);
+	struct tps68470_device *tps68470 = led_to_tps68470(led, led->led_id);
+	struct regmap *regmap = tps68470->regmap;
+
+	switch (led->led_id) {
+	case TPS68470_ILED_A:
+		return regmap_update_bits(regmap, TPS68470_REG_ILEDCTL, TPS68470_ILEDCTL_ENA,
+					  brightness ? TPS68470_ILEDCTL_ENA : 0);
+	case TPS68470_ILED_B:
+		return regmap_update_bits(regmap, TPS68470_REG_ILEDCTL, TPS68470_ILEDCTL_ENB,
+					  brightness ? TPS68470_ILEDCTL_ENB : 0);
+	}
+	return -EINVAL;
+}
+
+static enum led_brightness tps68470_brightness_get(struct led_classdev *led_cdev)
+{
+	struct tps68470_led *led = lcdev_to_led(led_cdev);
+	struct tps68470_device *tps68470 = led_to_tps68470(led, led->led_id);
+	struct regmap *regmap = tps68470->regmap;
+	int ret = 0;
+	int value = 0;
+
+	ret =  regmap_read(regmap, TPS68470_REG_ILEDCTL, &value);
+	if (ret)
+		return dev_err_probe(led_cdev->dev, -EINVAL, "failed on reading register\n");
+
+	switch (led->led_id) {
+	case TPS68470_ILED_A:
+		value = value & TPS68470_ILEDCTL_ENA;
+		break;
+	case TPS68470_ILED_B:
+		value = value & TPS68470_ILEDCTL_ENB;
+		break;
+	}
+
+	return value ? LED_ON : LED_OFF;
+}
+
+
+static int tps68470_ledb_current_init(struct platform_device *pdev,
+				      struct tps68470_device *tps68470)
+{
+	int ret = 0;
+	unsigned int curr;
+
+	/* configure LEDB current if the properties can be got */
+	if (!device_property_read_u32(&pdev->dev, "ti,ledb-current", &curr)) {
+		if (curr > CTRLB_16MA) {
+			dev_err(&pdev->dev,
+				"Invalid LEDB current value: %d\n",
+				curr);
+			return -EINVAL;
+		}
+		ret = regmap_update_bits(tps68470->regmap, TPS68470_REG_ILEDCTL,
+					 TPS68470_ILEDCTL_CTRLB, curr);
+	}
+	return ret;
+}
+
+static int tps68470_leds_probe(struct platform_device *pdev)
+{
+	int i = 0;
+	int ret = 0;
+	struct tps68470_device *tps68470;
+	struct tps68470_led *led;
+	struct led_classdev *lcdev;
+
+	tps68470 = devm_kzalloc(&pdev->dev, sizeof(struct tps68470_device),
+				GFP_KERNEL);
+	if (!tps68470)
+		return -ENOMEM;
+
+	tps68470->dev = &pdev->dev;
+	tps68470->regmap = dev_get_drvdata(pdev->dev.parent);
+
+	for (i = 0; i < TPS68470_NUM_LEDS; i++) {
+		led = &tps68470->leds[i];
+		lcdev = &led->lcdev;
+
+		led->led_id = i;
+
+		lcdev->name = devm_kasprintf(tps68470->dev, GFP_KERNEL, "%s::%s",
+					     tps68470_led_names[i], LED_FUNCTION_INDICATOR);
+		if (!lcdev->name)
+			return -ENOMEM;
+
+		lcdev->max_brightness = 1;
+		lcdev->brightness = 0;
+		lcdev->brightness_set_blocking = tps68470_brightness_set;
+		lcdev->brightness_get = tps68470_brightness_get;
+		lcdev->dev = &pdev->dev;
+
+		ret = devm_led_classdev_register(tps68470->dev, lcdev);
+		if (ret) {
+			dev_err_probe(tps68470->dev, ret,
+				      "error registering led\n");
+			goto err_exit;
+		}
+
+		if (i == TPS68470_ILED_B) {
+			ret = tps68470_ledb_current_init(pdev, tps68470);
+			if (ret)
+				goto err_exit;
+		}
+	}
+
+err_exit:
+	if (ret) {
+		for (i = 0; i < TPS68470_NUM_LEDS; i++) {
+			if (tps68470->leds[i].lcdev.name)
+				devm_led_classdev_unregister(&pdev->dev,
+							     &tps68470->leds[i].lcdev);
+		}
+	}
+
+	return ret;
+}
+static struct platform_driver tps68470_led_driver = {
+	.driver = {
+		   .name = "tps68470-led",
+	},
+	.probe = tps68470_leds_probe,
+};
+
+module_platform_driver(tps68470_led_driver);
+
+MODULE_ALIAS("platform:tps68470-led");
+MODULE_DESCRIPTION("LED driver for TPS68470 PMIC");
+MODULE_LICENSE("GPL v2");
-- 
2.47.0


From f364e00ffb153373ddd9a3dc7b13d986466654cb Mon Sep 17 00:00:00 2001
From: mojyack <mojyack@gmail.com>
Date: Sat, 3 Feb 2024 12:59:53 +0900
Subject: [PATCH v1.4 043/120] media: staging: ipu3-imgu: Fix multiple calls of
 s_stream on stream stop

Adapt to 009905e "media: v4l2-subdev: Document and enforce .s_stream() requirements"

Patchset: cameras
---
 drivers/staging/media/ipu3/ipu3-v4l2.c | 16 ++++++++--------
 1 file changed, 8 insertions(+), 8 deletions(-)

diff --git a/drivers/staging/media/ipu3/ipu3-v4l2.c b/drivers/staging/media/ipu3/ipu3-v4l2.c
index 3df58eb3e882..81aff2d5d898 100644
--- a/drivers/staging/media/ipu3/ipu3-v4l2.c
+++ b/drivers/staging/media/ipu3/ipu3-v4l2.c
@@ -538,18 +538,18 @@ static void imgu_vb2_stop_streaming(struct vb2_queue *vq)
 
 	WARN_ON(!node->enabled);
 
-	pipe = node->pipe;
-	dev_dbg(dev, "Try to stream off node [%u][%u]", pipe, node->id);
-	imgu_pipe = &imgu->imgu_pipe[pipe];
-	r = v4l2_subdev_call(&imgu_pipe->imgu_sd.subdev, video, s_stream, 0);
-	if (r)
-		dev_err(&imgu->pci_dev->dev,
-			"failed to stop subdev streaming\n");
-
 	mutex_lock(&imgu->streaming_lock);
 	/* Was this the first node with streaming disabled? */
 	if (imgu->streaming && imgu_all_nodes_streaming(imgu, node)) {
 		/* Yes, really stop streaming now */
+		pipe = node->pipe;
+		dev_dbg(dev, "Try to stream off node [%u][%u]", pipe, node->id);
+		imgu_pipe = &imgu->imgu_pipe[pipe];
+		r = v4l2_subdev_call(&imgu_pipe->imgu_sd.subdev, video, s_stream, 0);
+		if (r)
+			dev_err(&imgu->pci_dev->dev,
+				"failed to stop subdev streaming\n");
+
 		dev_dbg(dev, "IMGU streaming is ready to stop");
 		r = imgu_s_stream(imgu, false);
 		if (!r)
-- 
2.47.0


From 88cee610c4b0473e2120e774b4e3a47113e20eef Mon Sep 17 00:00:00 2001
From: mojyack <mojyack@gmail.com>
Date: Tue, 26 Mar 2024 05:55:44 +0900
Subject: [PATCH v1.4 044/120] media: i2c: dw9719: fix probe error on surface
 go 2

On surface go 2, sometimes probing dw9719 fails with "dw9719: probe of i2c-INT347A:00-VCM failed with error -121".
The -121(-EREMOTEIO) is came from drivers/i2c/busses/i2c-designware-common.c:575, and indicates the initialize occurs too early.
So just add some delay.
There is no exact reason for this 10000us, but 100us failed.

Patchset: cameras
---
 drivers/media/i2c/dw9719.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/drivers/media/i2c/dw9719.c b/drivers/media/i2c/dw9719.c
index c626ed845928..0094cfda57ea 100644
--- a/drivers/media/i2c/dw9719.c
+++ b/drivers/media/i2c/dw9719.c
@@ -82,6 +82,9 @@ static int dw9719_power_up(struct dw9719_device *dw9719)
 	if (ret)
 		return ret;
 
+	/* Wait for device to be acknowledged */
+	fsleep(10000);
+
 	/* Jiggle SCL pin to wake up device */
 	cci_write(dw9719->regmap, DW9719_CONTROL, 1, &ret);
 
-- 
2.47.0


From 829d5da00f599c320b24213d7064260888af7682 Mon Sep 17 00:00:00 2001
From: Sachi King <nakato@nakato.io>
Date: Sat, 29 May 2021 17:47:38 +1000
Subject: [PATCH v1.4 045/120] ACPI: Add quirk for Surface Laptop 4 AMD missing
 irq 7 override

This patch is the work of Thomas Gleixner <tglx@linutronix.de> and is
copied from:
https://lore.kernel.org/lkml/87lf8ddjqx.ffs@nanos.tec.linutronix.de/

This patch adds a quirk to the ACPI setup to patch in the the irq 7 pin
setup that is missing in the laptops ACPI table.

This patch was used for validation of the issue, and is not a proper
fix, but is probably a better temporary hack than continuing to probe
the Legacy PIC and run with the PIC in an unknown state.

Patchset: amd-gpio
---
 arch/x86/kernel/acpi/boot.c | 17 +++++++++++++++++
 1 file changed, 17 insertions(+)

diff --git a/arch/x86/kernel/acpi/boot.c b/arch/x86/kernel/acpi/boot.c
index 4efecac49863..88377bb0d137 100644
--- a/arch/x86/kernel/acpi/boot.c
+++ b/arch/x86/kernel/acpi/boot.c
@@ -22,6 +22,7 @@
 #include <linux/efi-bgrt.h>
 #include <linux/serial_core.h>
 #include <linux/pgtable.h>
+#include <linux/dmi.h>
 
 #include <asm/e820/api.h>
 #include <asm/irqdomain.h>
@@ -1132,6 +1133,17 @@ static void __init mp_config_acpi_legacy_irqs(void)
 	}
 }
 
+static const struct dmi_system_id surface_quirk[] __initconst = {
+	{
+		.ident = "Microsoft Surface Laptop 4 (AMD)",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_MATCH(DMI_PRODUCT_SKU, "Surface_Laptop_4_1952:1953")
+		},
+	},
+	{}
+};
+
 /*
  * Parse IOAPIC related entries in MADT
  * returns 0 on success, < 0 on error
@@ -1187,6 +1199,11 @@ static int __init acpi_parse_madt_ioapic_entries(void)
 		acpi_sci_ioapic_setup(acpi_gbl_FADT.sci_interrupt, 0, 0,
 				      acpi_gbl_FADT.sci_interrupt);
 
+	if (dmi_check_system(surface_quirk)) {
+		pr_warn("Surface hack: Override irq 7\n");
+		mp_override_legacy_irq(7, 3, 3, 7);
+	}
+
 	/* Fill in identity legacy mappings where no override */
 	mp_config_acpi_legacy_irqs();
 
-- 
2.47.0


From 4524cd79ede9108cd19645103b7a967cc7692169 Mon Sep 17 00:00:00 2001
From: Maximilian Luz <luzmaximilian@gmail.com>
Date: Thu, 3 Jun 2021 14:04:26 +0200
Subject: [PATCH v1.4 046/120] ACPI: Add AMD 13" Surface Laptop 4 model to irq
 7 override quirk

The 13" version of the Surface Laptop 4 has the same problem as the 15"
version, but uses a different SKU. Add that SKU to the quirk as well.

Patchset: amd-gpio
---
 arch/x86/kernel/acpi/boot.c | 9 ++++++++-
 1 file changed, 8 insertions(+), 1 deletion(-)

diff --git a/arch/x86/kernel/acpi/boot.c b/arch/x86/kernel/acpi/boot.c
index 88377bb0d137..c58f26918b17 100644
--- a/arch/x86/kernel/acpi/boot.c
+++ b/arch/x86/kernel/acpi/boot.c
@@ -1135,12 +1135,19 @@ static void __init mp_config_acpi_legacy_irqs(void)
 
 static const struct dmi_system_id surface_quirk[] __initconst = {
 	{
-		.ident = "Microsoft Surface Laptop 4 (AMD)",
+		.ident = "Microsoft Surface Laptop 4 (AMD 15\")",
 		.matches = {
 			DMI_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
 			DMI_MATCH(DMI_PRODUCT_SKU, "Surface_Laptop_4_1952:1953")
 		},
 	},
+	{
+		.ident = "Microsoft Surface Laptop 4 (AMD 13\")",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "Microsoft Corporation"),
+			DMI_MATCH(DMI_PRODUCT_SKU, "Surface_Laptop_4_1958:1959")
+		},
+	},
 	{}
 };
 
-- 
2.47.0


From 2f7b6610cc9c6a972fa2725369bdf675b03aaf24 Mon Sep 17 00:00:00 2001
From: "Bart Groeneveld | GPX Solutions B.V" <bart@gpxbv.nl>
Date: Mon, 5 Dec 2022 16:08:46 +0100
Subject: [PATCH v1.4 047/120] acpi: allow usage of acpi_tad on HW-reduced
 platforms

The specification [1] allows so-called HW-reduced platforms,
which do not implement everything, especially the wakeup related stuff.

In that case, it is still usable as a RTC. This is helpful for [2]
and [3], which is about a device with no other working RTC,
but it does have an HW-reduced TAD, which can be used as a RTC instead.

[1]: https://uefi.org/specs/ACPI/6.5/09_ACPI_Defined_Devices_and_Device_Specific_Objects.html#time-and-alarm-device
[2]: https://bugzilla.kernel.org/show_bug.cgi?id=212313
[3]: https://github.com/linux-surface/linux-surface/issues/415

Signed-off-by: Bart Groeneveld | GPX Solutions B.V. <bart@gpxbv.nl>
Patchset: rtc
---
 drivers/acpi/acpi_tad.c | 36 ++++++++++++++++++++++++------------
 1 file changed, 24 insertions(+), 12 deletions(-)

diff --git a/drivers/acpi/acpi_tad.c b/drivers/acpi/acpi_tad.c
index b831cb8e53dc..78bd0f926505 100644
--- a/drivers/acpi/acpi_tad.c
+++ b/drivers/acpi/acpi_tad.c
@@ -433,6 +433,14 @@ static ssize_t caps_show(struct device *dev, struct device_attribute *attr,
 
 static DEVICE_ATTR_RO(caps);
 
+static struct attribute *acpi_tad_attrs[] = {
+	&dev_attr_caps.attr,
+	NULL,
+};
+static const struct attribute_group acpi_tad_attr_group = {
+	.attrs	= acpi_tad_attrs,
+};
+
 static ssize_t ac_alarm_store(struct device *dev, struct device_attribute *attr,
 			      const char *buf, size_t count)
 {
@@ -481,15 +489,14 @@ static ssize_t ac_status_show(struct device *dev, struct device_attribute *attr,
 
 static DEVICE_ATTR_RW(ac_status);
 
-static struct attribute *acpi_tad_attrs[] = {
-	&dev_attr_caps.attr,
+static struct attribute *acpi_tad_ac_attrs[] = {
 	&dev_attr_ac_alarm.attr,
 	&dev_attr_ac_policy.attr,
 	&dev_attr_ac_status.attr,
 	NULL,
 };
-static const struct attribute_group acpi_tad_attr_group = {
-	.attrs	= acpi_tad_attrs,
+static const struct attribute_group acpi_tad_ac_attr_group = {
+	.attrs	= acpi_tad_ac_attrs,
 };
 
 static ssize_t dc_alarm_store(struct device *dev, struct device_attribute *attr,
@@ -565,13 +572,18 @@ static void acpi_tad_remove(struct platform_device *pdev)
 
 	pm_runtime_get_sync(dev);
 
+	if (dd->capabilities & ACPI_TAD_AC_WAKE)
+		sysfs_remove_group(&dev->kobj, &acpi_tad_ac_attr_group);
+
 	if (dd->capabilities & ACPI_TAD_DC_WAKE)
 		sysfs_remove_group(&dev->kobj, &acpi_tad_dc_attr_group);
 
 	sysfs_remove_group(&dev->kobj, &acpi_tad_attr_group);
 
-	acpi_tad_disable_timer(dev, ACPI_TAD_AC_TIMER);
-	acpi_tad_clear_status(dev, ACPI_TAD_AC_TIMER);
+	if (dd->capabilities & ACPI_TAD_AC_WAKE) {
+		acpi_tad_disable_timer(dev, ACPI_TAD_AC_TIMER);
+		acpi_tad_clear_status(dev, ACPI_TAD_AC_TIMER);
+	}
 	if (dd->capabilities & ACPI_TAD_DC_WAKE) {
 		acpi_tad_disable_timer(dev, ACPI_TAD_DC_TIMER);
 		acpi_tad_clear_status(dev, ACPI_TAD_DC_TIMER);
@@ -613,12 +625,6 @@ static int acpi_tad_probe(struct platform_device *pdev)
 		goto remove_handler;
 	}
 
-	if (!acpi_has_method(handle, "_PRW")) {
-		dev_info(dev, "Missing _PRW\n");
-		ret = -ENODEV;
-		goto remove_handler;
-	}
-
 	dd = devm_kzalloc(dev, sizeof(*dd), GFP_KERNEL);
 	if (!dd) {
 		ret = -ENOMEM;
@@ -649,6 +655,12 @@ static int acpi_tad_probe(struct platform_device *pdev)
 	if (ret)
 		goto fail;
 
+	if (caps & ACPI_TAD_AC_WAKE) {
+		ret = sysfs_create_group(&dev->kobj, &acpi_tad_ac_attr_group);
+		if (ret)
+			goto fail;
+	}
+
 	if (caps & ACPI_TAD_DC_WAKE) {
 		ret = sysfs_create_group(&dev->kobj, &acpi_tad_dc_attr_group);
 		if (ret)
-- 
2.47.0


From 7d09f5c7cd2d1bfedf262c30cebdf4383da2e087 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:04:14 +0200
Subject: [PATCH v1.4 048/120] [BEGIN] ROG Ally fixes

-- 
2.47.0


From 23aa309859a7922dc06f0eca2d85a066e19ab0db Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:05:23 +0200
Subject: [PATCH v1.4 049/120] add ROG Ally devices to nct6775

---
 drivers/hwmon/nct6775-platform.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/drivers/hwmon/nct6775-platform.c b/drivers/hwmon/nct6775-platform.c
index 096f1daa8f2b..7c4e319ebc47 100644
--- a/drivers/hwmon/nct6775-platform.c
+++ b/drivers/hwmon/nct6775-platform.c
@@ -1360,6 +1360,8 @@ static const char * const asus_msi_boards[] = {
 	"ProArt X670E-CREATOR WIFI",
 	"ProArt Z690-CREATOR WIFI",
 	"ProArt Z790-CREATOR WIFI",
+	"RC71L",
+	"RC72LA",
 	"ROG CROSSHAIR X670E EXTREME",
 	"ROG CROSSHAIR X670E GENE",
 	"ROG CROSSHAIR X670E HERO",
-- 
2.47.0


From f43aa238fcc4e42048d0414cdedf7865a842e375 Mon Sep 17 00:00:00 2001
From: Jonathan LoBue <jlobue10@gmail.com>
Date: Sun, 11 Aug 2024 21:53:25 -0700
Subject: [PATCH v1.4 050/120] ALSA: hda/realtek: tas2781: Fix ROG ALLY X audio

This patch enables the TI TAS2781 amplifier SoC for the ASUS ROG ALLY X.
This is a design change from the original ASUS ROG ALLY, creating the need
for this patch. All other Realtek Codec settings seem to be re-used from
the original ROG ALLY design (on the ROG ALLY X). This patch maintains the
previous settings for the Realtek codec portion, but enables the I2C
binding for the TI TAS2781 amplifier (instead of the Cirrus CS35L41 amp
used on the original ASUS ROG ALLY).

One other requirement must be met for audio to work on the ASUS ROG ALLY X.
A proper firmware file in the correct location with a proper symlink. We
had reached out to TI engineers and confirmed that the firmware found in
the Windows' driver package has a GPL license. Bazzite Github is hosting
this firmware file for now until proper linux-firmware upstreaming can
occur. https://github.com/ublue-os/bazzite

This firmware file should be placed in
/usr/lib/firmware/ti/tas2781/TAS2XXX1EB3.bin with a symlink to it from
/usr/lib/firmware/TAS2XXX1EB3.bin

Co-developed by: Kyle Gospodnetich <me@kylegospodneti.ch>
Co-developed by: Jan Drogehoff <sentrycraft123@gmail.com>

Signed-off-by: Kyle Gospodnetich <me@kylegospodneti.ch>
Signed-off-by: Jan Drogehoff <sentrycraft123@gmail.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
Tested-by: Richard Alvarez <alvarez.richard@gmail.com>
Tested-by: Miles Montierth <cyber_dopamine@intheblackmedia.com>
Signed-off-by: Jonathan LoBue <jlobue10@gmail.com>
---
 sound/pci/hda/patch_realtek.c | 8 ++++++++
 1 file changed, 8 insertions(+)

diff --git a/sound/pci/hda/patch_realtek.c b/sound/pci/hda/patch_realtek.c
index a2737c1ff920..c95a056ed764 100644
--- a/sound/pci/hda/patch_realtek.c
+++ b/sound/pci/hda/patch_realtek.c
@@ -7646,6 +7646,7 @@ enum {
 	ALC285_FIXUP_THINKPAD_X1_GEN7,
 	ALC285_FIXUP_THINKPAD_HEADSET_JACK,
 	ALC294_FIXUP_ASUS_ALLY,
+	ALC294_FIXUP_ASUS_ALLY_X,
 	ALC294_FIXUP_ASUS_ALLY_PINS,
 	ALC294_FIXUP_ASUS_ALLY_VERBS,
 	ALC294_FIXUP_ASUS_ALLY_SPEAKER,
@@ -9117,6 +9118,12 @@ static const struct hda_fixup alc269_fixups[] = {
 		.chained = true,
 		.chain_id = ALC294_FIXUP_ASUS_ALLY_PINS
 	},
+	[ALC294_FIXUP_ASUS_ALLY_X] = {
+		.type = HDA_FIXUP_FUNC,
+		.v.func = tas2781_fixup_i2c,
+		.chained = true,
+		.chain_id = ALC294_FIXUP_ASUS_ALLY_PINS
+	},
 	[ALC294_FIXUP_ASUS_ALLY_PINS] = {
 		.type = HDA_FIXUP_PINS,
 		.v.pins = (const struct hda_pintbl[]) {
@@ -10590,6 +10597,7 @@ static const struct snd_pci_quirk alc269_fixup_tbl[] = {
 	SND_PCI_QUIRK(0x1043, 0x1740, "ASUS UX430UA", ALC295_FIXUP_ASUS_DACS),
 	SND_PCI_QUIRK(0x1043, 0x17d1, "ASUS UX431FL", ALC294_FIXUP_ASUS_DUAL_SPK),
 	SND_PCI_QUIRK(0x1043, 0x17f3, "ROG Ally NR2301L/X", ALC294_FIXUP_ASUS_ALLY),
+	SND_PCI_QUIRK(0x1043, 0x1eb3, "ROG Ally X RC72LA", ALC294_FIXUP_ASUS_ALLY_X),
 	SND_PCI_QUIRK(0x1043, 0x1863, "ASUS UX6404VI/VV", ALC245_FIXUP_CS35L41_SPI_2),
 	SND_PCI_QUIRK(0x1043, 0x1881, "ASUS Zephyrus S/M", ALC294_FIXUP_ASUS_GX502_PINS),
 	SND_PCI_QUIRK(0x1043, 0x18b1, "Asus MJ401TA", ALC256_FIXUP_ASUS_HEADSET_MIC),
-- 
2.47.0


From ec85dfd6f65edb84ae22be5395c55083b45d1805 Mon Sep 17 00:00:00 2001
From: Baojun Xu <baojun.xu@ti.com>
Date: Fri, 18 Oct 2024 15:11:18 +0800
Subject: [PATCH v1.4 051/120] ALSA: hda/tas2781: Add speaker id check for ASUS
 projects

Add speaker id check by gpio in ACPI for ASUS projects.
In other vendors, speaker id was checked by BIOS, and was applied in
last bit of subsys id, so we can load corresponding firmware binary file
for its speaker by subsys id.
But in ASUS project, the firmware binary name will be appended an extra
number to tell the speakers from different vendors. And this single digit
come from gpio level of speaker id in BIOS.

Signed-off-by: Baojun Xu <baojun.xu@ti.com>
---
 include/sound/tas2781.h         |  3 ++
 sound/pci/hda/tas2781_hda_i2c.c | 62 +++++++++++++++++++++++++++++----
 2 files changed, 59 insertions(+), 6 deletions(-)

diff --git a/include/sound/tas2781.h b/include/sound/tas2781.h
index dbda552398b5..d098497bfbfd 100644
--- a/include/sound/tas2781.h
+++ b/include/sound/tas2781.h
@@ -60,6 +60,8 @@
 #define TASDEVICE_CMD_DELAY		0x3
 #define TASDEVICE_CMD_FIELD_W		0x4
 
+#define TAS2781_ASUS_ID			"1043"
+
 enum audio_device {
 	TAS2563,
 	TAS2781,
@@ -91,6 +93,7 @@ struct tasdevice_priv {
 	struct tasdevice_rca rcabin;
 	struct calidata cali_data;
 	struct tasdevice_fw *fmw;
+	struct gpio_desc *speaker_id;
 	struct gpio_desc *reset;
 	struct mutex codec_lock;
 	struct regmap *regmap;
diff --git a/sound/pci/hda/tas2781_hda_i2c.c b/sound/pci/hda/tas2781_hda_i2c.c
index f58f434e7110..b3826944e668 100644
--- a/sound/pci/hda/tas2781_hda_i2c.c
+++ b/sound/pci/hda/tas2781_hda_i2c.c
@@ -110,10 +110,19 @@ static int tas2781_get_i2c_res(struct acpi_resource *ares, void *data)
 	return 1;
 }
 
+static const struct acpi_gpio_params speakerid_gpios = { 0, 0, false };
+
+static const struct acpi_gpio_mapping tas2781_speaker_id_gpios[] = {
+	{ "speakerid-gpios", &speakerid_gpios, 1 },
+	{ }
+};
+
 static int tas2781_read_acpi(struct tasdevice_priv *p, const char *hid)
 {
 	struct acpi_device *adev;
+	struct device *physdev;
 	LIST_HEAD(resources);
+	const char *sub;
 	int ret;
 
 	adev = acpi_dev_get_first_match_dev(hid, NULL, -1);
@@ -122,19 +131,44 @@ static int tas2781_read_acpi(struct tasdevice_priv *p, const char *hid)
 			"Failed to find an ACPI device for %s\n", hid);
 		return -ENODEV;
 	}
-
+	physdev = get_device(acpi_get_first_physical_node(adev));
 	ret = acpi_dev_get_resources(adev, &resources, tas2781_get_i2c_res, p);
-	if (ret < 0)
+	if (ret < 0) {
+		dev_err(p->dev, "Failed to get ACPI resource.\n");
 		goto err;
+	}
+	sub = acpi_get_subsystem_id(ACPI_HANDLE(physdev));
+	if (IS_ERR(sub)) {
+		dev_err(p->dev, "Failed to get SUBSYS ID.\n");
+		goto err;
+	}
+	// Speaker id was needed for ASUS projects.
+	if (strstr(sub, TAS2781_ASUS_ID)) {
+		ret = devm_acpi_dev_add_driver_gpios(p->dev,
+			tas2781_speaker_id_gpios);
+		if (ret) {
+			dev_err(p->dev, "Unable to add GPIO.\n");
+			goto err;
+		}
+		p->speaker_id = devm_gpiod_get(p->dev, "speakerid", GPIOD_IN);
+		if (IS_ERR(p->speaker_id)) {
+			dev_err(p->dev, "Failed to get Speaker id.\n");
+			goto err;
+		}
+	} else {
+		p->speaker_id = NULL;
+	}
 
 	acpi_dev_free_resource_list(&resources);
 	strscpy(p->dev_name, hid, sizeof(p->dev_name));
+	put_device(physdev);
 	acpi_dev_put(adev);
 
 	return 0;
 
 err:
 	dev_err(p->dev, "read acpi error, ret: %d\n", ret);
+	put_device(physdev);
 	acpi_dev_put(adev);
 
 	return ret;
@@ -615,7 +649,7 @@ static void tasdev_fw_ready(const struct firmware *fmw, void *context)
 	struct tasdevice_priv *tas_priv = context;
 	struct tas2781_hda *tas_hda = dev_get_drvdata(tas_priv->dev);
 	struct hda_codec *codec = tas_priv->codec;
-	int i, ret;
+	int i, ret, spk_id;
 
 	pm_runtime_get_sync(tas_priv->dev);
 	mutex_lock(&tas_priv->codec_lock);
@@ -648,8 +682,25 @@ static void tasdev_fw_ready(const struct firmware *fmw, void *context)
 	tasdevice_dsp_remove(tas_priv);
 
 	tas_priv->fw_state = TASDEVICE_DSP_FW_PENDING;
-	scnprintf(tas_priv->coef_binaryname, 64, "TAS2XXX%04X.bin",
-		codec->core.subsystem_id & 0xffff);
+	if (tas_priv->speaker_id != NULL) {
+		// Speaker id need to be checked for ASUS only.
+		spk_id = gpiod_get_value(tas_priv->speaker_id);
+		if (spk_id < 0 || spk_id > 1) {
+			// Speaker id is not valid, use default.
+			dev_dbg(tas_priv->dev, "Wrong spk_id = %d\n", spk_id);
+			spk_id = 0;
+		}
+		scnprintf(tas_priv->coef_binaryname,
+			  sizeof(tas_priv->coef_binaryname),
+			  "TAS2XXX%04X%01d.bin",
+			  lower_16_bits(codec->core.subsystem_id),
+			  spk_id);
+	} else {
+		scnprintf(tas_priv->coef_binaryname,
+			  sizeof(tas_priv->coef_binaryname),
+			  "TAS2XXX%04X.bin",
+			  lower_16_bits(codec->core.subsystem_id));
+	}
 	ret = tasdevice_dsp_parser(tas_priv);
 	if (ret) {
 		dev_err(tas_priv->dev, "dspfw load %s error\n",
@@ -793,7 +844,6 @@ static int tas2781_hda_i2c_probe(struct i2c_client *clt)
 	const char *device_name;
 	int ret;
 
-
 	tas_hda = devm_kzalloc(&clt->dev, sizeof(*tas_hda), GFP_KERNEL);
 	if (!tas_hda)
 		return -ENOMEM;
-- 
2.47.0


From 200907d0341a0943e299885ba5a3798938976c55 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:07:48 +0200
Subject: [PATCH v1.4 052/120] [NOT FOR UPSTREAM] remove ally quirk from
 asus_hid

Unfortunately it bails with ENOMEM preventing the controller which
might cause issues and sends random initialization commands not meant
for the platform (should NOOP though).
---
 drivers/hid/hid-asus.c | 6 ------
 1 file changed, 6 deletions(-)

diff --git a/drivers/hid/hid-asus.c b/drivers/hid/hid-asus.c
index a282388b7aa5..fd58eed1adde 100644
--- a/drivers/hid/hid-asus.c
+++ b/drivers/hid/hid-asus.c
@@ -1245,12 +1245,6 @@ static const struct hid_device_id asus_devices[] = {
 	{ HID_USB_DEVICE(USB_VENDOR_ID_ASUSTEK,
 	    USB_DEVICE_ID_ASUSTEK_ROG_Z13_LIGHTBAR),
 	  QUIRK_USE_KBD_BACKLIGHT | QUIRK_ROG_NKEY_KEYBOARD },
-	{ HID_USB_DEVICE(USB_VENDOR_ID_ASUSTEK,
-	    USB_DEVICE_ID_ASUSTEK_ROG_NKEY_ALLY),
-	  QUIRK_USE_KBD_BACKLIGHT | QUIRK_ROG_NKEY_KEYBOARD },
-	{ HID_USB_DEVICE(USB_VENDOR_ID_ASUSTEK,
-	    USB_DEVICE_ID_ASUSTEK_ROG_NKEY_ALLY_X),
-	  QUIRK_USE_KBD_BACKLIGHT | QUIRK_ROG_NKEY_KEYBOARD },
 	{ HID_USB_DEVICE(USB_VENDOR_ID_ASUSTEK,
 	    USB_DEVICE_ID_ASUSTEK_ROG_CLAYMORE_II_KEYBOARD),
 	  QUIRK_ROG_CLAYMORE_II_KEYBOARD },
-- 
2.47.0


From b689fcaeaef5cde7661aa8c17d7593bf10ec3667 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:11:05 +0200
Subject: [PATCH v1.4 053/120] [NOT FOR UPSTREAM] Add DSDT config for original
 Ally older BIOSes

Older ROG Ally BIOSes do not have a speaker configuration in the DSDT.
Patch is originally by Jonathan LoBue <jlobue10@gmail.com>.
---
 sound/pci/hda/cs35l41_hda_property.c | 38 +++++++++++++++++++++++++++-
 1 file changed, 37 insertions(+), 1 deletion(-)

diff --git a/sound/pci/hda/cs35l41_hda_property.c b/sound/pci/hda/cs35l41_hda_property.c
index 61d2314834e7..0b6ccd930ec8 100644
--- a/sound/pci/hda/cs35l41_hda_property.c
+++ b/sound/pci/hda/cs35l41_hda_property.c
@@ -6,8 +6,10 @@
 //
 // Author: Stefan Binding <sbinding@opensource.cirrus.com>
 
+#include <linux/dmi.h>
 #include <linux/acpi.h>
 #include <linux/gpio/consumer.h>
+#include <linux/kernel.h>
 #include <linux/string.h>
 #include "cs35l41_hda_property.h"
 #include <linux/spi/spi.h>
@@ -30,6 +32,40 @@ struct cs35l41_config {
 	int boost_cap_microfarad; /* Required if boost_type == Internal */
 };
 
+static int asus_rog_2023_ally_fix(struct cs35l41_hda *cs35l41, struct device *physdev, int id,
+				const char *hid)
+{
+	const char *rog_ally_bios_ver = dmi_get_system_info(DMI_BIOS_VERSION);
+	const char *rog_ally_bios_num = rog_ally_bios_ver + 6; // Dropping the RC71L. part before the number
+	int rog_ally_bios_int;
+	kstrtoint(rog_ally_bios_num, 10, &rog_ally_bios_int);
+	if(rog_ally_bios_int >= 330){
+		printk(KERN_INFO "DSD properties exist in the %d BIOS. Not applying DSD override...\n", rog_ally_bios_int);
+		return -ENOENT; //Patch not applicable. Exiting...
+	}
+
+	struct cs35l41_hw_cfg *hw_cfg = &cs35l41->hw_cfg;
+
+	dev_info(cs35l41->dev, "Adding DSD properties for %s\n", cs35l41->acpi_subsystem_id);
+
+	cs35l41->index = id == 0x40 ? 0 : 1;
+	cs35l41->channel_index = 0;
+	cs35l41->reset_gpio = gpiod_get_index(physdev, NULL, 0, GPIOD_OUT_HIGH);
+	cs35l41->speaker_id = cs35l41_get_speaker_id(physdev, 0, 0, 2);
+	hw_cfg->spk_pos = cs35l41->index;
+	hw_cfg->gpio1.func = CS35L41_NOT_USED;
+	hw_cfg->gpio1.valid = true;
+	hw_cfg->gpio2.func = CS35L41_INTERRUPT;
+	hw_cfg->gpio2.valid = true;
+	hw_cfg->bst_type = CS35L41_INT_BOOST;
+	hw_cfg->bst_ind = 1000; /* 1,000nH Inductance value */
+	hw_cfg->bst_ipk = 4500; /* 4,500mA peak current */
+	hw_cfg->bst_cap = 24; /* 24 microFarad cap value */
+	hw_cfg->valid = true;
+
+	return 0;
+}
+
 static const struct cs35l41_config cs35l41_config_table[] = {
 	{ "10280B27", 2, INTERNAL, { CS35L41_LEFT, CS35L41_RIGHT, 0, 0 }, 1, 2, 0, 1000, 4500, 24 },
 	{ "10280B28", 2, INTERNAL, { CS35L41_LEFT, CS35L41_RIGHT, 0, 0 }, 1, 2, 0, 1000, 4500, 24 },
@@ -512,7 +548,7 @@ static const struct cs35l41_prop_model cs35l41_prop_model_table[] = {
 	{ "CSC3551", "104316A3", generic_dsd_config },
 	{ "CSC3551", "104316D3", generic_dsd_config },
 	{ "CSC3551", "104316F3", generic_dsd_config },
-	{ "CSC3551", "104317F3", generic_dsd_config },
+	{ "CSC3551", "104317F3", asus_rog_2023_ally_fix },
 	{ "CSC3551", "10431863", generic_dsd_config },
 	{ "CSC3551", "104318D3", generic_dsd_config },
 	{ "CSC3551", "10431A63", missing_speaker_id_gpio2 },
-- 
2.47.0


From 9d6cf44e0830aba7136fff4b0592c6c2bc74ff37 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:29:58 +0200
Subject: [PATCH v1.4 054/120] [BEGIN] OneXPlayer Handheld patches

-- 
2.47.0


From f22c2f4b4a19f95abe3f7f76ae04ec74e2f3280d Mon Sep 17 00:00:00 2001
From: "Derek J. Clark" <derekjohn.clark@gmail.com>
Date: Thu, 22 Aug 2024 11:35:25 -0700
Subject: [PATCH v1.4 055/120] Add support for multiple new devices.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Add support for the OrangePi NEO-01. It uses different registers for PWM
manual mode, set PWM, and read fan speed than previous devices. Valid PWM
input and duty cycle is 1-244, we scale this from 1-255 to maintain
compatibility with the existing interface.

Add OneXPlayer 2 series, OneXFly, and X1 series models. The 2/X1 series use
new registers for turbo button takeover and read fan speed. X1 has an Intel
variant so change the CPU detection at init to only check for the affected
devices. While at it, adjust formatting of some constants and reorder all
cases alphabetically for consistency. Rename OXP_OLD constants to OXP_MINI
for disambiguation. Update code comments for clarity.

Add support for AYANEO models 2S, AIR 1S, Flip series, GEEK 1S, and KUN.

Signed-off-by: Derek J. Clark <derekjohn.clark@gmail.com>
Tested-by: Kevin Greenberg <kdgreenberg234@protonmail.com>
Tested-by: Joshua Tam <csinaction@pm.me>
Tested-by: Parth Menon <parthasarathymenon@gmail.com>
Tested-by: Philip Müller <philm@manjaro.org>
Reported-by: kernel test robot <lkp@intel.com>
Closes: https://lore.kernel.org/oe-kbuild-all/202408160329.TLNbIwRC-lkp@intel.com/
---
 Documentation/hwmon/oxp-sensors.rst |  54 +++--
 drivers/hwmon/oxp-sensors.c         | 299 +++++++++++++++++++++++-----
 2 files changed, 290 insertions(+), 63 deletions(-)

diff --git a/Documentation/hwmon/oxp-sensors.rst b/Documentation/hwmon/oxp-sensors.rst
index 55b1ef61625e..97e82cbad3ee 100644
--- a/Documentation/hwmon/oxp-sensors.rst
+++ b/Documentation/hwmon/oxp-sensors.rst
@@ -10,41 +10,59 @@ Authors:
 Description:
 ------------
 
-Handheld devices from One Netbook and Aya Neo provide fan readings and fan
-control through their embedded controllers.
+Handheld devices from OneNetbook, AOKZOE, AYANEO, And OrangePi provide fan
+readings and fan control through their embedded controllers.
 
-Currently only supports AMD boards from One X Player, AOK ZOE, and some Aya
-Neo devices. One X Player Intel boards could be supported if we could figure
-out the EC registers and values to write to since the EC layout and model is
-different. Aya Neo devices preceding the AIR may not be supportable as the EC
-model is different and do not appear to have manual control capabilities.
+Currently supports OneXPlayer devices, AOKZOE, AYANEO, and OrangePi
+handheld devices. AYANEO devices preceding the AIR and OneXPlayer devices
+preceding the Mini A07 are not supportable as the EC model is different
+and do not have manual control capabilities.
 
-Some models have a toggle for changing the behaviour of the "Turbo/Silent"
-button of the device. It will change the key event that it triggers with
-a flip of the `tt_toggle` attribute. See below for boards that support this
-function.
+Some OneXPlayer and AOKZOE models have a toggle for changing the behaviour
+of the "Turbo/Silent" button of the device. It will change the key event
+that it triggers with a flip of the `tt_toggle` attribute. See below for
+boards that support this function.
 
 Supported devices
 -----------------
 
 Currently the driver supports the following handhelds:
 
- - AOK ZOE A1
- - AOK ZOE A1 PRO
- - Aya Neo 2
- - Aya Neo AIR
- - Aya Neo AIR Plus (Mendocino)
- - Aya Neo AIR Pro
- - Aya Neo Geek
+ - AOKZOE A1
+ - AOKZOE A1 PRO
+ - AYANEO 2
+ - AYANEO 2S
+ - AYANEO AIR
+ - AYANEO AIR 1S
+ - AYANEO AIR Plus (Mendocino)
+ - AYANEO AIR Pro
+ - AYANEO Flip DS
+ - AYANEO Flip KB
+ - AYANEO Geek
+ - AYANEO Geek 1S
+ - AYANEO KUN
+ - OneXPlayer 2
+ - OneXPlayer 2 Pro
  - OneXPlayer AMD
  - OneXPlayer mini AMD
  - OneXPlayer mini AMD PRO
+ - OneXPlayer OneXFly
+ - OneXPlayer X1 A
+ - OneXPlayer X1 i
+ - OneXPlayer X1 mini
+ - OrangePi NEO-01
 
 "Turbo/Silent" button behaviour toggle is only supported on:
  - AOK ZOE A1
  - AOK ZOE A1 PRO
+ - OneXPlayer 2
+ - OneXPlayer 2 Pro
  - OneXPlayer mini AMD (only with updated alpha BIOS)
  - OneXPlayer mini AMD PRO
+ - OneXPlayer OneXFly
+ - OneXPlayer X1 A
+ - OneXPlayer X1 i
+ - OneXPlayer X1 mini
 
 Sysfs entries
 -------------
diff --git a/drivers/hwmon/oxp-sensors.c b/drivers/hwmon/oxp-sensors.c
index 8d3b0f86cc57..b6d06370469d 100644
--- a/drivers/hwmon/oxp-sensors.c
+++ b/drivers/hwmon/oxp-sensors.c
@@ -1,18 +1,21 @@
 // SPDX-License-Identifier: GPL-2.0+
 /*
- * Platform driver for OneXPlayer, AOK ZOE, and Aya Neo Handhelds that expose
- * fan reading and control via hwmon sysfs.
+ * Platform driver for OneXPlayer, AOKZOE, AYANEO, and OrangePi Handhelds
+ * that expose fan reading and control via hwmon sysfs.
  *
  * Old OXP boards have the same DMI strings and they are told apart by
- * the boot cpu vendor (Intel/AMD). Currently only AMD boards are
- * supported but the code is made to be simple to add other handheld
- * boards in the future.
+ * the boot cpu vendor (Intel/AMD). Of these older models only AMD is
+ * supported.
+ *
  * Fan control is provided via pwm interface in the range [0-255].
  * Old AMD boards use [0-100] as range in the EC, the written value is
  * scaled to accommodate for that. Newer boards like the mini PRO and
- * AOK ZOE are not scaled but have the same EC layout.
+ * AOKZOE are not scaled but have the same EC layout. Newer models
+ * like the 2 and X1 are [0-184] and are scaled to 0-255. OrangePi
+ * are [1-244] and scaled to 0-255.
  *
  * Copyright (C) 2022 Joaquín I. Aramendía <samsagax@gmail.com>
+ * Copyright (C) 2024 Derek J. Clark <derekjohn.clark@gmail.com>
  */
 
 #include <linux/acpi.h>
@@ -43,32 +46,48 @@ enum oxp_board {
 	aok_zoe_a1 = 1,
 	aya_neo_2,
 	aya_neo_air,
+	aya_neo_air_1s,
 	aya_neo_air_plus_mendo,
 	aya_neo_air_pro,
+	aya_neo_flip,
 	aya_neo_geek,
+	aya_neo_kun,
+	orange_pi_neo,
+	oxp_2,
+	oxp_fly,
 	oxp_mini_amd,
 	oxp_mini_amd_a07,
 	oxp_mini_amd_pro,
+	oxp_x1,
 };
 
 static enum oxp_board board;
 
 /* Fan reading and PWM */
-#define OXP_SENSOR_FAN_REG		0x76 /* Fan reading is 2 registers long */
-#define OXP_SENSOR_PWM_ENABLE_REG	0x4A /* PWM enable is 1 register long */
-#define OXP_SENSOR_PWM_REG		0x4B /* PWM reading is 1 register long */
+#define OXP_SENSOR_FAN_REG             0x76 /* Fan reading is 2 registers long */
+#define OXP_2_SENSOR_FAN_REG           0x58 /* Fan reading is 2 registers long */
+#define OXP_SENSOR_PWM_ENABLE_REG      0x4A /* PWM enable is 1 register long */
+#define OXP_SENSOR_PWM_REG             0x4B /* PWM reading is 1 register long */
+#define PWM_MODE_AUTO                  0x00
+#define PWM_MODE_MANUAL                0x01
+
+/* OrangePi fan reading and PWM */
+#define ORANGEPI_SENSOR_FAN_REG        0x78 /* Fan reading is 2 registers long */
+#define ORANGEPI_SENSOR_PWM_ENABLE_REG 0x40 /* PWM enable is 1 register long */
+#define ORANGEPI_SENSOR_PWM_REG        0x38 /* PWM reading is 1 register long */
 
 /* Turbo button takeover function
- * Older boards have different values and EC registers
+ * Different boards have different values and EC registers
  * for the same function
  */
-#define OXP_OLD_TURBO_SWITCH_REG	0x1E
-#define OXP_OLD_TURBO_TAKE_VAL		0x01
-#define OXP_OLD_TURBO_RETURN_VAL	0x00
+#define OXP_TURBO_SWITCH_REG           0xF1 /* Mini Pro, OneXFly, AOKZOE */
+#define OXP_2_TURBO_SWITCH_REG         0xEB /* OXP2 and X1 */
+#define OXP_MINI_TURBO_SWITCH_REG      0x1E /* Mini AO7 */
+
+#define OXP_MINI_TURBO_TAKE_VAL        0x01 /* Mini AO7 */
+#define OXP_TURBO_TAKE_VAL             0x40 /* All other models */
 
-#define OXP_TURBO_SWITCH_REG		0xF1
-#define OXP_TURBO_TAKE_VAL		0x40
-#define OXP_TURBO_RETURN_VAL		0x00
+#define OXP_TURBO_RETURN_VAL           0x00 /* Common return val */
 
 static const struct dmi_system_id dmi_table[] = {
 	{
@@ -88,7 +107,7 @@ static const struct dmi_system_id dmi_table[] = {
 	{
 		.matches = {
 			DMI_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
-			DMI_EXACT_MATCH(DMI_BOARD_NAME, "AYANEO 2"),
+			DMI_MATCH(DMI_BOARD_NAME, "AYANEO 2"),
 		},
 		.driver_data = (void *)aya_neo_2,
 	},
@@ -99,6 +118,13 @@ static const struct dmi_system_id dmi_table[] = {
 		},
 		.driver_data = (void *)aya_neo_air,
 	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "AIR 1S"),
+		},
+		.driver_data = (void *)aya_neo_air_1s,
+	},
 	{
 		.matches = {
 			DMI_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
@@ -116,10 +142,31 @@ static const struct dmi_system_id dmi_table[] = {
 	{
 		.matches = {
 			DMI_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
-			DMI_EXACT_MATCH(DMI_BOARD_NAME, "GEEK"),
+			DMI_MATCH(DMI_BOARD_NAME, "FLIP"),
+		},
+		.driver_data = (void *)aya_neo_flip,
+	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
+			DMI_MATCH(DMI_BOARD_NAME, "GEEK"),
 		},
 		.driver_data = (void *)aya_neo_geek,
 	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "KUN"),
+		},
+		.driver_data = (void *)aya_neo_kun,
+	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "OrangePi"),
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "NEO-01"),
+		},
+		.driver_data = (void *)orange_pi_neo,
+	},
 	{
 		.matches = {
 			DMI_MATCH(DMI_BOARD_VENDOR, "ONE-NETBOOK"),
@@ -127,6 +174,20 @@ static const struct dmi_system_id dmi_table[] = {
 		},
 		.driver_data = (void *)oxp_mini_amd,
 	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "ONE-NETBOOK"),
+			DMI_MATCH(DMI_BOARD_NAME, "ONEXPLAYER 2"),
+		},
+		.driver_data = (void *)oxp_2,
+	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "ONE-NETBOOK"),
+			DMI_EXACT_MATCH(DMI_BOARD_NAME, "ONEXPLAYER F1"),
+		},
+		.driver_data = (void *)oxp_fly,
+	},
 	{
 		.matches = {
 			DMI_MATCH(DMI_BOARD_VENDOR, "ONE-NETBOOK"),
@@ -141,6 +202,13 @@ static const struct dmi_system_id dmi_table[] = {
 		},
 		.driver_data = (void *)oxp_mini_amd_pro,
 	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_VENDOR, "ONE-NETBOOK"),
+			DMI_MATCH(DMI_BOARD_NAME, "ONEXPLAYER X1"),
+		},
+		.driver_data = (void *)oxp_x1,
+	},
 	{},
 };
 
@@ -192,14 +260,20 @@ static int tt_toggle_enable(void)
 
 	switch (board) {
 	case oxp_mini_amd_a07:
-		reg = OXP_OLD_TURBO_SWITCH_REG;
-		val = OXP_OLD_TURBO_TAKE_VAL;
+		reg = OXP_MINI_TURBO_SWITCH_REG;
+		val = OXP_MINI_TURBO_TAKE_VAL;
 		break;
-	case oxp_mini_amd_pro:
 	case aok_zoe_a1:
+	case oxp_fly:
+	case oxp_mini_amd_pro:
 		reg = OXP_TURBO_SWITCH_REG;
 		val = OXP_TURBO_TAKE_VAL;
 		break;
+	case oxp_2:
+	case oxp_x1:
+		reg = OXP_2_TURBO_SWITCH_REG;
+		val = OXP_TURBO_TAKE_VAL;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -213,14 +287,20 @@ static int tt_toggle_disable(void)
 
 	switch (board) {
 	case oxp_mini_amd_a07:
-		reg = OXP_OLD_TURBO_SWITCH_REG;
-		val = OXP_OLD_TURBO_RETURN_VAL;
+		reg = OXP_MINI_TURBO_SWITCH_REG;
+		val = OXP_TURBO_RETURN_VAL;
 		break;
-	case oxp_mini_amd_pro:
 	case aok_zoe_a1:
+	case oxp_fly:
+	case oxp_mini_amd_pro:
 		reg = OXP_TURBO_SWITCH_REG;
 		val = OXP_TURBO_RETURN_VAL;
 		break;
+	case oxp_2:
+	case oxp_x1:
+		reg = OXP_2_TURBO_SWITCH_REG;
+		val = OXP_TURBO_RETURN_VAL;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -233,8 +313,11 @@ static umode_t tt_toggle_is_visible(struct kobject *kobj,
 {
 	switch (board) {
 	case aok_zoe_a1:
+	case oxp_2:
+	case oxp_fly:
 	case oxp_mini_amd_a07:
 	case oxp_mini_amd_pro:
+	case oxp_x1:
 		return attr->mode;
 	default:
 		break;
@@ -273,12 +356,17 @@ static ssize_t tt_toggle_show(struct device *dev,
 
 	switch (board) {
 	case oxp_mini_amd_a07:
-		reg = OXP_OLD_TURBO_SWITCH_REG;
+		reg = OXP_MINI_TURBO_SWITCH_REG;
 		break;
-	case oxp_mini_amd_pro:
 	case aok_zoe_a1:
+	case oxp_fly:
+	case oxp_mini_amd_pro:
 		reg = OXP_TURBO_SWITCH_REG;
 		break;
+	case oxp_2:
+	case oxp_x1:
+		reg = OXP_2_TURBO_SWITCH_REG;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -295,12 +383,53 @@ static DEVICE_ATTR_RW(tt_toggle);
 /* PWM enable/disable functions */
 static int oxp_pwm_enable(void)
 {
-	return write_to_ec(OXP_SENSOR_PWM_ENABLE_REG, 0x01);
+	switch (board) {
+	case orange_pi_neo:
+		return write_to_ec(ORANGEPI_SENSOR_PWM_ENABLE_REG, PWM_MODE_MANUAL);
+	case aok_zoe_a1:
+	case aya_neo_2:
+	case aya_neo_air:
+	case aya_neo_air_plus_mendo:
+	case aya_neo_air_pro:
+	case aya_neo_flip:
+	case aya_neo_geek:
+	case aya_neo_kun:
+	case oxp_2:
+	case oxp_fly:
+	case oxp_mini_amd:
+	case oxp_mini_amd_a07:
+	case oxp_mini_amd_pro:
+	case oxp_x1:
+		return write_to_ec(OXP_SENSOR_PWM_ENABLE_REG, PWM_MODE_MANUAL);
+	default:
+		return -EINVAL;
+	}
 }
 
 static int oxp_pwm_disable(void)
 {
-	return write_to_ec(OXP_SENSOR_PWM_ENABLE_REG, 0x00);
+	switch (board) {
+	case orange_pi_neo:
+		return write_to_ec(ORANGEPI_SENSOR_PWM_ENABLE_REG, PWM_MODE_AUTO);
+	case aok_zoe_a1:
+	case aya_neo_2:
+	case aya_neo_air:
+	case aya_neo_air_1s:
+	case aya_neo_air_plus_mendo:
+	case aya_neo_air_pro:
+	case aya_neo_flip:
+	case aya_neo_geek:
+	case aya_neo_kun:
+	case oxp_2:
+	case oxp_fly:
+	case oxp_mini_amd:
+	case oxp_mini_amd_a07:
+	case oxp_mini_amd_pro:
+	case oxp_x1:
+		return write_to_ec(OXP_SENSOR_PWM_ENABLE_REG, PWM_MODE_AUTO);
+	default:
+		return -EINVAL;
+	}
 }
 
 /* Callbacks for hwmon interface */
@@ -326,7 +455,29 @@ static int oxp_platform_read(struct device *dev, enum hwmon_sensor_types type,
 	case hwmon_fan:
 		switch (attr) {
 		case hwmon_fan_input:
-			return read_from_ec(OXP_SENSOR_FAN_REG, 2, val);
+			switch (board) {
+			case orange_pi_neo:
+				return read_from_ec(ORANGEPI_SENSOR_FAN_REG, 2, val);
+			case oxp_2:
+			case oxp_x1:
+				return read_from_ec(OXP_2_SENSOR_FAN_REG, 2, val);
+			case aok_zoe_a1:
+			case aya_neo_2:
+			case aya_neo_air:
+			case aya_neo_air_1s:
+			case aya_neo_air_plus_mendo:
+			case aya_neo_air_pro:
+			case aya_neo_flip:
+			case aya_neo_geek:
+			case aya_neo_kun:
+			case oxp_fly:
+			case oxp_mini_amd:
+			case oxp_mini_amd_a07:
+			case oxp_mini_amd_pro:
+				return read_from_ec(OXP_SENSOR_FAN_REG, 2, val);
+			default:
+				break;
+			}
 		default:
 			break;
 		}
@@ -334,31 +485,74 @@ static int oxp_platform_read(struct device *dev, enum hwmon_sensor_types type,
 	case hwmon_pwm:
 		switch (attr) {
 		case hwmon_pwm_input:
-			ret = read_from_ec(OXP_SENSOR_PWM_REG, 1, val);
-			if (ret)
-				return ret;
 			switch (board) {
+			case orange_pi_neo:
+				ret = read_from_ec(ORANGEPI_SENSOR_PWM_REG, 1, val);
+				if (ret)
+					return ret;
+				/* scale from range [1-244] */
+				*val = ((*val - 1) * 254 / 243) + 1;
+				break;
+			case oxp_2:
+			case oxp_x1:
+				ret = read_from_ec(OXP_SENSOR_PWM_REG, 1, val);
+				if (ret)
+					return ret;
+				/* scale from range [0-184] */
+				*val = (*val * 255) / 184;
+				break;
 			case aya_neo_2:
 			case aya_neo_air:
+			case aya_neo_air_1s:
 			case aya_neo_air_plus_mendo:
 			case aya_neo_air_pro:
+			case aya_neo_flip:
 			case aya_neo_geek:
+			case aya_neo_kun:
 			case oxp_mini_amd:
 			case oxp_mini_amd_a07:
+				ret = read_from_ec(OXP_SENSOR_PWM_REG, 1, val);
+				if (ret)
+					return ret;
+				/* scale from range [0-100] */
 				*val = (*val * 255) / 100;
 				break;
-			case oxp_mini_amd_pro:
 			case aok_zoe_a1:
+			case oxp_fly:
+			case oxp_mini_amd_pro:
 			default:
+				ret = read_from_ec(OXP_SENSOR_PWM_REG, 1, val);
+				if (ret)
+					return ret;
 				break;
 			}
 			return 0;
 		case hwmon_pwm_enable:
-			return read_from_ec(OXP_SENSOR_PWM_ENABLE_REG, 1, val);
+			switch (board) {
+			case orange_pi_neo:
+				return read_from_ec(ORANGEPI_SENSOR_PWM_ENABLE_REG, 1, val);
+			case aok_zoe_a1:
+			case aya_neo_2:
+			case aya_neo_air:
+			case aya_neo_air_1s:
+			case aya_neo_air_plus_mendo:
+			case aya_neo_air_pro:
+			case aya_neo_flip:
+			case aya_neo_geek:
+			case aya_neo_kun:
+			case oxp_2:
+			case oxp_fly:
+			case oxp_mini_amd:
+			case oxp_mini_amd_a07:
+			case oxp_mini_amd_pro:
+			case oxp_x1:
+				return read_from_ec(OXP_SENSOR_PWM_ENABLE_REG, 1, val);
+			default:
+				break;
+			}
 		default:
 			break;
 		}
-		break;
 	default:
 		break;
 	}
@@ -381,21 +575,35 @@ static int oxp_platform_write(struct device *dev, enum hwmon_sensor_types type,
 			if (val < 0 || val > 255)
 				return -EINVAL;
 			switch (board) {
+			case orange_pi_neo:
+				/* scale to range [1-244] */
+				val = ((val - 1) * 243 / 254) + 1;
+				return write_to_ec(ORANGEPI_SENSOR_PWM_REG, val);
+			case oxp_2:
+			case oxp_x1:
+				/* scale to range [0-184] */
+				val = (val * 184) / 255;
+				return write_to_ec(OXP_SENSOR_PWM_REG, val);
 			case aya_neo_2:
 			case aya_neo_air:
+			case aya_neo_air_1s:
 			case aya_neo_air_plus_mendo:
 			case aya_neo_air_pro:
+			case aya_neo_flip:
 			case aya_neo_geek:
+			case aya_neo_kun:
 			case oxp_mini_amd:
 			case oxp_mini_amd_a07:
+				/* scale to range [0-100] */
 				val = (val * 100) / 255;
-				break;
+				return write_to_ec(OXP_SENSOR_PWM_REG, val);
 			case aok_zoe_a1:
+			case oxp_fly:
 			case oxp_mini_amd_pro:
+				return write_to_ec(OXP_SENSOR_PWM_REG, val);
 			default:
 				break;
 			}
-			return write_to_ec(OXP_SENSOR_PWM_REG, val);
 		default:
 			break;
 		}
@@ -467,19 +675,20 @@ static int __init oxp_platform_init(void)
 {
 	const struct dmi_system_id *dmi_entry;
 
-	/*
-	 * Have to check for AMD processor here because DMI strings are the
-	 * same between Intel and AMD boards, the only way to tell them apart
-	 * is the CPU.
-	 * Intel boards seem to have different EC registers and values to
-	 * read/write.
-	 */
 	dmi_entry = dmi_first_match(dmi_table);
-	if (!dmi_entry || boot_cpu_data.x86_vendor != X86_VENDOR_AMD)
+	if (!dmi_entry)
 		return -ENODEV;
 
 	board = (enum oxp_board)(unsigned long)dmi_entry->driver_data;
 
+	/*
+	 * Have to check for AMD processor here because DMI strings are the same
+	 * between Intel and AMD boards on older OneXPlayer devices, the only way
+	 * to tell them apart is the CPU. Old Intel boards have an unsupported EC.
+	 */
+	if (board == oxp_mini_amd && boot_cpu_data.x86_vendor != X86_VENDOR_AMD)
+		return -ENODEV;
+
 	oxp_platform_device =
 		platform_create_bundle(&oxp_platform_driver,
 				       oxp_platform_probe, NULL, 0, NULL, 0);
-- 
2.47.0


From a3bf0d9d9ceb2c1dde09fd885aa4669f5c7b4f89 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Wed, 2 Oct 2024 22:01:14 +0200
Subject: [PATCH v1.4 056/120] drm/panel-orientation-quirks: add OneXPlayer X1
 (AMD)

---
 drivers/gpu/drm/drm_panel_orientation_quirks.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/drivers/gpu/drm/drm_panel_orientation_quirks.c b/drivers/gpu/drm/drm_panel_orientation_quirks.c
index 0830cae9a4d0..e52e7e73b9d0 100644
--- a/drivers/gpu/drm/drm_panel_orientation_quirks.c
+++ b/drivers/gpu/drm/drm_panel_orientation_quirks.c
@@ -426,6 +426,12 @@ static const struct dmi_system_id orientation_data[] = {
 		  DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "ONE XPLAYER"),
 		},
 		.driver_data = (void *)&lcd1600x2560_leftside_up,
+	}, {	/* OneXPlayer X1 AMD */
+		.matches = {
+		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "ONE-NETBOOK"),
+		  DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "ONEXPLAYER X1 A"),
+		},
+		.driver_data = (void *)&lcd1600x2560_leftside_up,
 	}, {	/* OrangePi Neo */
 		.matches = {
 		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "OrangePi"),
-- 
2.47.0


From 49755fbea1dba38ba677fc7c08c37996247b4cf4 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Wed, 2 Oct 2024 22:17:26 +0200
Subject: [PATCH v1.4 057/120] HID: Add quirk to ignore the touchscreen battery
 on OneXPlayer X1

---
 drivers/hid/hid-ids.h   | 1 +
 drivers/hid/hid-input.c | 2 ++
 2 files changed, 3 insertions(+)

diff --git a/drivers/hid/hid-ids.h b/drivers/hid/hid-ids.h
index 8a991b30e3c6..006c75f80f8a 100644
--- a/drivers/hid/hid-ids.h
+++ b/drivers/hid/hid-ids.h
@@ -994,6 +994,7 @@
 #define USB_VENDOR_ID_NOVATEK		0x0603
 #define USB_DEVICE_ID_NOVATEK_PCT	0x0600
 #define USB_DEVICE_ID_NOVATEK_MOUSE	0x1602
+#define I2C_DEVICE_ID_ONEXPLAYER_X1    0xF001
 
 #define USB_VENDOR_ID_NTI               0x0757
 #define USB_DEVICE_ID_USB_SUN           0x0a00
diff --git a/drivers/hid/hid-input.c b/drivers/hid/hid-input.c
index fda9dce3da99..e52fb045096a 100644
--- a/drivers/hid/hid-input.c
+++ b/drivers/hid/hid-input.c
@@ -390,6 +390,8 @@ static const struct hid_device_id hid_battery_quirks[] = {
 	 * set HID_BATTERY_QUIRK_IGNORE for all Elan I2C-HID devices.
 	 */
 	{ HID_I2C_DEVICE(USB_VENDOR_ID_ELAN, HID_ANY_ID), HID_BATTERY_QUIRK_IGNORE },
+	{ HID_I2C_DEVICE(USB_VENDOR_ID_NOVATEK, I2C_DEVICE_ID_ONEXPLAYER_X1),
+	  HID_BATTERY_QUIRK_IGNORE },
 	{}
 };
 
-- 
2.47.0


From 8e69b7ef3c2409e7a94fabff0da129d9e3863462 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Tue, 22 Oct 2024 18:00:46 +0200
Subject: [PATCH v1.4 058/120] [BEGIN] General Handheld Audio/Display Patches

-- 
2.47.0


From 520a214a020267dd55b472350da8c651ef903bf3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Joaqu=C3=ADn=20Ignacio=20Aramend=C3=ADa?=
 <samsagax@gmail.com>
Date: Wed, 21 Jun 2023 18:22:19 -0300
Subject: [PATCH v1.4 059/120] drm: panel-orientation-quirks: Add quirk for AYA
 NEO 2 model
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Add quirk orientation for AYA NEO 2. The name appears without spaces in
dmi strings. That made it difficult to reuse the 2021 match and the
display is greater in resolution.

Tested by the JELOS team that has been patching their own kernel for a
while now and confirmed by users in the AYA NEO and ChimeraOS discord
servers.

Signed-off-by: Joaquín Ignacio Aramendía <samsagax@gmail.com>
---
 drivers/gpu/drm/drm_panel_orientation_quirks.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/drivers/gpu/drm/drm_panel_orientation_quirks.c b/drivers/gpu/drm/drm_panel_orientation_quirks.c
index e52e7e73b9d0..744416fef085 100644
--- a/drivers/gpu/drm/drm_panel_orientation_quirks.c
+++ b/drivers/gpu/drm/drm_panel_orientation_quirks.c
@@ -184,6 +184,12 @@ static const struct dmi_system_id orientation_data[] = {
 		  DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "T103HAF"),
 		},
 		.driver_data = (void *)&lcd800x1280_rightside_up,
+	}, {	/* AYA NEO AYANEO 2 */
+		.matches = {
+		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "AYANEO"),
+		  DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "AYANEO 2"),
+		},
+		.driver_data = (void *)&lcd1200x1920_rightside_up,
 	}, {	/* AYA NEO 2021 */
 		.matches = {
 		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "AYADEVICE"),
-- 
2.47.0


From 02fabb36aa7e0dff782d2c9f097993aeea20288d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Joaqu=C3=ADn=20Ignacio=20Aramend=C3=ADa?=
 <samsagax@gmail.com>
Date: Wed, 21 Jun 2023 18:40:10 -0300
Subject: [PATCH v1.4 060/120] drm: panel-orientation-quirks: Add quirk for AYA
 NEO Founder edition
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Add quirk orientation for AYA NEO Founder. The name appears with spaces in
dmi strings as other devices of the brand. The panel is the same as the
NEXT and 2021 models. Those could not be reused as the former has VENDOR
name as "AYANEO" without spaces and the latter has "AYADEVICE".

Tested by the JELOS team that has been patching their own kernel for a
while now and confirmed by users in the AYA NEO and ChimeraOS discord
servers.

Signed-off-by: Joaquín Ignacio Aramendía <samsagax@gmail.com>
---
 drivers/gpu/drm/drm_panel_orientation_quirks.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/drivers/gpu/drm/drm_panel_orientation_quirks.c b/drivers/gpu/drm/drm_panel_orientation_quirks.c
index 744416fef085..e863ccd38c82 100644
--- a/drivers/gpu/drm/drm_panel_orientation_quirks.c
+++ b/drivers/gpu/drm/drm_panel_orientation_quirks.c
@@ -202,6 +202,12 @@ static const struct dmi_system_id orientation_data[] = {
 		  DMI_MATCH(DMI_PRODUCT_NAME, "AIR"),
 		},
 		.driver_data = (void *)&lcd1080x1920_leftside_up,
+	}, {	/* AYA NEO Founder */
+		.matches = {
+		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "AYA NEO"),
+		  DMI_MATCH(DMI_PRODUCT_NAME, "AYA NEO Founder"),
+		},
+		.driver_data = (void *)&lcd800x1280_rightside_up,
 	}, {	/* AYA NEO NEXT */
 		.matches = {
 		  DMI_EXACT_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
-- 
2.47.0


From 7b990e27028e104087d5be76ab10e799fce05162 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Joaqu=C3=ADn=20Ignacio=20Aramend=C3=ADa?=
 <samsagax@gmail.com>
Date: Wed, 21 Jun 2023 18:54:44 -0300
Subject: [PATCH v1.4 061/120] drm: panel-orientation-quirks: Add quirk for AYA
 NEO GEEK
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Add quirk orientation for AYA NEO GEEK. One of the more recent devices by
the brand. The name appears without spaces in dmi strings. The board
name is completely different to the previous models making it difficult
to reuse their quirks despite being the same resolution and mounting.

Tested by the JELOS team that has been patching their own kernel for a
while now and confirmed by users in the AYA NEO and ChimeraOS discord
servers.

Signed-off-by: Joaquín Ignacio Aramendía <samsagax@gmail.com>
---
 drivers/gpu/drm/drm_panel_orientation_quirks.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/drivers/gpu/drm/drm_panel_orientation_quirks.c b/drivers/gpu/drm/drm_panel_orientation_quirks.c
index e863ccd38c82..1d61e718dfd6 100644
--- a/drivers/gpu/drm/drm_panel_orientation_quirks.c
+++ b/drivers/gpu/drm/drm_panel_orientation_quirks.c
@@ -208,6 +208,12 @@ static const struct dmi_system_id orientation_data[] = {
 		  DMI_MATCH(DMI_PRODUCT_NAME, "AYA NEO Founder"),
 		},
 		.driver_data = (void *)&lcd800x1280_rightside_up,
+	}, {	/* AYA NEO GEEK */
+		.matches = {
+		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "AYANEO"),
+		  DMI_MATCH(DMI_PRODUCT_NAME, "GEEK"),
+		},
+		.driver_data = (void *)&lcd800x1280_rightside_up,
 	}, {	/* AYA NEO NEXT */
 		.matches = {
 		  DMI_EXACT_MATCH(DMI_BOARD_VENDOR, "AYANEO"),
-- 
2.47.0


From 7d62643655a4d6a5b887d038d33e2c3ade5e87ad Mon Sep 17 00:00:00 2001
From: CVMagic <546352+CVMagic@users.noreply.github.com>
Date: Thu, 25 Apr 2024 09:39:40 -0500
Subject: [PATCH v1.4 062/120] Codec: Add aw87xxx codec with ACPI
 implementation

Full implementation including all commits.

Bouhaa fixed some warnings.

Co-developed-by: bouhaa <boukehaarsma23@gmail.com>
---
 sound/soc/codecs/Kconfig                      |    2 +
 sound/soc/codecs/Makefile                     |    1 +
 sound/soc/codecs/aw87xxx/Kconfig              |    5 +
 sound/soc/codecs/aw87xxx/Makefile             |    4 +
 sound/soc/codecs/aw87xxx/aw87xxx.c            | 1545 +++++
 sound/soc/codecs/aw87xxx/aw87xxx.h            |  126 +
 sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.c    | 1558 +++++
 sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.h    |  191 +
 sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.c  |  515 ++
 sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.h  |   73 +
 sound/soc/codecs/aw87xxx/aw87xxx_device.c     |  977 +++
 sound/soc/codecs/aw87xxx/aw87xxx_device.h     |  149 +
 sound/soc/codecs/aw87xxx/aw87xxx_dsp.c        |  355 ++
 sound/soc/codecs/aw87xxx/aw87xxx_dsp.h        |   65 +
 sound/soc/codecs/aw87xxx/aw87xxx_log.h        |   33 +
 sound/soc/codecs/aw87xxx/aw87xxx_monitor.c    | 1208 ++++
 sound/soc/codecs/aw87xxx/aw87xxx_monitor.h    |   96 +
 sound/soc/codecs/aw87xxx/aw87xxx_pid_18_reg.h | 2315 ++++++++
 sound/soc/codecs/aw87xxx/aw87xxx_pid_39_reg.h |   67 +
 .../codecs/aw87xxx/aw87xxx_pid_59_3x9_reg.h   |   93 +
 .../codecs/aw87xxx/aw87xxx_pid_59_5x9_reg.h   |   94 +
 sound/soc/codecs/aw87xxx/aw87xxx_pid_5a_reg.h | 4124 +++++++++++++
 sound/soc/codecs/aw87xxx/aw87xxx_pid_60_reg.h | 5246 +++++++++++++++++
 sound/soc/codecs/aw87xxx/aw87xxx_pid_76_reg.h | 1205 ++++
 sound/soc/codecs/aw87xxx/aw87xxx_pid_9b_reg.h |   81 +
 25 files changed, 20128 insertions(+)
 create mode 100644 sound/soc/codecs/aw87xxx/Kconfig
 create mode 100644 sound/soc/codecs/aw87xxx/Makefile
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx.c
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.c
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.c
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_device.c
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_device.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_dsp.c
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_dsp.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_log.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_monitor.c
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_monitor.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_18_reg.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_39_reg.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_59_3x9_reg.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_59_5x9_reg.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_5a_reg.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_60_reg.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_76_reg.h
 create mode 100644 sound/soc/codecs/aw87xxx/aw87xxx_pid_9b_reg.h

diff --git a/sound/soc/codecs/Kconfig b/sound/soc/codecs/Kconfig
index b5e6d0a986c8..0626116d59af 100644
--- a/sound/soc/codecs/Kconfig
+++ b/sound/soc/codecs/Kconfig
@@ -2589,4 +2589,6 @@ config SND_SOC_LPASS_TX_MACRO
 	select SND_SOC_LPASS_MACRO_COMMON
 	tristate "Qualcomm TX Macro in LPASS(Low Power Audio SubSystem)"
 
+source "sound/soc/codecs/aw87xxx/Kconfig"
+
 endmenu
diff --git a/sound/soc/codecs/Makefile b/sound/soc/codecs/Makefile
index 622e360f0086..112f28c90cf2 100644
--- a/sound/soc/codecs/Makefile
+++ b/sound/soc/codecs/Makefile
@@ -806,6 +806,7 @@ obj-$(CONFIG_SND_SOC_WSA884X)	+= snd-soc-wsa884x.o
 obj-$(CONFIG_SND_SOC_ZL38060)	+= snd-soc-zl38060.o
 
 # Amp
+obj-$(CONFIG_SND_SOC_AW87XXX)    += aw87xxx/
 obj-$(CONFIG_SND_SOC_MAX9877)	+= snd-soc-max9877.o
 obj-$(CONFIG_SND_SOC_MAX98504)	+= snd-soc-max98504.o
 obj-$(CONFIG_SND_SOC_SIMPLE_AMPLIFIER)	+= snd-soc-simple-amplifier.o
diff --git a/sound/soc/codecs/aw87xxx/Kconfig b/sound/soc/codecs/aw87xxx/Kconfig
new file mode 100644
index 000000000000..bd0f208e2cfe
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/Kconfig
@@ -0,0 +1,5 @@
+config SND_SOC_AW87XXX
+	tristate "SoC Audio for awinic AW87XXX Smart K PA"
+	depends on I2C
+	help
+	  This option enables support for AW87XXX Smart K PA.
diff --git a/sound/soc/codecs/aw87xxx/Makefile b/sound/soc/codecs/aw87xxx/Makefile
new file mode 100644
index 000000000000..d32f319a5b01
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/Makefile
@@ -0,0 +1,4 @@
+#for AWINIC AW87XXX Smart K PA
+snd-soc-aw87xxx-objs := aw87xxx.o aw87xxx_device.o aw87xxx_monitor.o aw87xxx_bin_parse.o aw87xxx_dsp.o aw87xxx_acf_bin.o
+obj-$(CONFIG_SND_SOC_AW87XXX) += snd-soc-aw87xxx.o
+
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx.c b/sound/soc/codecs/aw87xxx/aw87xxx.c
new file mode 100644
index 000000000000..710f9b6109de
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx.c
@@ -0,0 +1,1545 @@
+/*
+ * aw87xxx.c  aw87xxx pa module
+ *
+ * Copyright (c) 2021 AWINIC Technology CO., LTD
+ *
+ * Author: Barry <zhaozhongbo@awinic.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ */
+
+#include <linux/i2c.h>
+#include <sound/pcm.h>
+#include <sound/pcm_params.h>
+#include <linux/gpio.h>
+#include <linux/of_gpio.h>
+#include <linux/gpio/consumer.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/irq.h>
+#include <linux/firmware.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/proc_fs.h>
+#include <linux/uaccess.h>
+#include <linux/io.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/dma-mapping.h>
+#include <linux/gameport.h>
+#include <linux/moduleparam.h>
+#include <linux/mutex.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include <linux/kthread.h>
+#include <linux/vmalloc.h>
+#include <uapi/sound/asound.h>
+#include <sound/control.h>
+#include <sound/soc.h>
+#include "aw87xxx.h"
+#include "aw87xxx_device.h"
+#include "aw87xxx_log.h"
+#include "aw87xxx_monitor.h"
+#include "aw87xxx_acf_bin.h"
+#include "aw87xxx_bin_parse.h"
+#include "aw87xxx_dsp.h"
+
+/*****************************************************************
+* aw87xxx marco
+******************************************************************/
+#define AW87XXX_I2C_NAME	"aw87xxx_pa"
+#define AW87XXX_DRIVER_VERSION	"v2.7.0"
+#define AW87XXX_FW_BIN_NAME	"aw87xxx_acf.bin"
+#define AW87XXX_PROF_MUSIC	"Music"
+/*************************************************************************
+ * aw87xxx variable
+ ************************************************************************/
+static LIST_HEAD(g_aw87xxx_list);
+static DEFINE_MUTEX(g_aw87xxx_mutex_lock);
+unsigned int g_aw87xxx_dev_cnt = 0;
+
+static const char *const aw87xxx_monitor_switch[] = {"Disable", "Enable"};
+static const char *const aw87xxx_spin_switch[] = {"spin_0", "spin_90",
+					 "spin_180", "spin_270"};
+#ifdef AW_KERNEL_VER_OVER_4_19_1
+static struct aw_componet_codec_ops aw_componet_codec_ops = {
+	.add_codec_controls = snd_soc_add_component_controls,
+	.unregister_codec = snd_soc_unregister_component,
+};
+#else
+static struct aw_componet_codec_ops aw_componet_codec_ops = {
+	.add_codec_controls = snd_soc_add_codec_controls,
+	.unregister_codec = snd_soc_unregister_codec,
+};
+#endif
+
+enum smi_bus_type {
+	SMI_I2C,
+	SMI_SPI,
+	SMI_AUTO_DETECT,
+};
+
+struct smi_instance {
+	const char *type;
+	unsigned int flags;
+	int irq_idx;
+};
+
+struct smi_node {
+	enum smi_bus_type bus_type;
+	struct smi_instance instances[];
+};
+
+/************************************************************************
+ *
+ * aw87xxx device update profile
+ *
+ ************************************************************************/
+static int aw87xxx_power_down(struct aw87xxx *aw87xxx, char *profile)
+{
+	int ret = 0;
+	struct aw_prof_desc *prof_desc = NULL;
+	struct aw_prof_info *prof_info = &aw87xxx->acf_info.prof_info;
+	struct aw_data_container *data_container = NULL;
+	struct aw_device *aw_dev = &aw87xxx->aw_dev;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+
+	if (!prof_info->status) {
+		AW_DEV_LOGE(aw87xxx->dev, "profile_cfg not load");
+		return -EINVAL;
+	}
+
+	prof_desc = aw87xxx_acf_get_prof_desc_form_name(aw87xxx->dev, &aw87xxx->acf_info, profile);
+	if (prof_desc == NULL)
+		goto no_bin_pwr_off;
+
+	if (!prof_desc->prof_st)
+		goto no_bin_pwr_off;
+
+
+	data_container = &prof_desc->data_container;
+	AW_DEV_LOGD(aw87xxx->dev, "get profile[%s] data len [%d]",
+			profile, data_container->len);
+
+	if (aw_dev->hwen_status == AW_DEV_HWEN_OFF) {
+		AW_DEV_LOGI(aw87xxx->dev, "profile[%s] has already load ", profile);
+	} else {
+		if (aw_dev->ops.pwr_off_func) {
+			ret = aw_dev->ops.pwr_off_func(aw_dev, data_container);
+			if (ret < 0) {
+				AW_DEV_LOGE(aw87xxx->dev, "load profile[%s] failed ", profile);
+				goto pwr_off_failed;
+			}
+		} else {
+			ret = aw87xxx_dev_default_pwr_off(aw_dev, data_container);
+			if (ret < 0) {
+				AW_DEV_LOGE(aw87xxx->dev, "load profile[%s] failed ", profile);
+				goto pwr_off_failed;
+			}
+		}
+	}
+
+	aw87xxx->current_profile = prof_desc->prof_name;
+	return 0;
+
+pwr_off_failed:
+no_bin_pwr_off:
+	aw87xxx_dev_hw_pwr_ctrl(&aw87xxx->aw_dev, false);
+	aw87xxx->current_profile = aw87xxx->prof_off_name;
+	return ret;
+}
+
+static int aw87xxx_power_on(struct aw87xxx *aw87xxx, char *profile)
+{
+	int ret = -EINVAL;
+	struct aw_prof_desc *prof_desc = NULL;
+	struct aw_prof_info *prof_info = &aw87xxx->acf_info.prof_info;
+	struct aw_data_container *data_container = NULL;
+	struct aw_device *aw_dev = &aw87xxx->aw_dev;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+
+	if (!prof_info->status) {
+		AW_DEV_LOGE(aw87xxx->dev, "profile_cfg not load");
+		return -EINVAL;
+	}
+
+	if (0 == strncmp(profile, aw87xxx->prof_off_name, AW_PROFILE_STR_MAX))
+		return aw87xxx_power_down(aw87xxx, profile);
+
+	prof_desc = aw87xxx_acf_get_prof_desc_form_name(aw87xxx->dev, &aw87xxx->acf_info, profile);
+	if (prof_desc == NULL) {
+		AW_DEV_LOGE(aw87xxx->dev, "not found [%s] parameter", profile);
+		return -EINVAL;
+	}
+
+	if (!prof_desc->prof_st) {
+		AW_DEV_LOGE(aw87xxx->dev, "not found data container");
+		return -EINVAL;
+	}
+
+	data_container = &prof_desc->data_container;
+	AW_DEV_LOGD(aw87xxx->dev, "get profile[%s] data len [%d]",
+			profile, data_container->len);
+
+	if (aw_dev->ops.pwr_on_func) {
+		ret = aw_dev->ops.pwr_on_func(aw_dev, data_container);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev, "load profile[%s] failed ",
+				profile);
+			return aw87xxx_power_down(aw87xxx, aw87xxx->prof_off_name);
+		}
+	} else {
+		ret = aw87xxx_dev_default_pwr_on(aw_dev, data_container);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev, "load profile[%s] failed ",
+				profile);
+			return aw87xxx_power_down(aw87xxx, aw87xxx->prof_off_name);
+		}
+	}
+
+	aw87xxx->current_profile = prof_desc->prof_name;
+	AW_DEV_LOGD(aw87xxx->dev, "load profile[%s] succeed", profile);
+
+	return 0;
+}
+
+
+
+int aw87xxx_update_profile(struct aw87xxx *aw87xxx, char *profile)
+{
+	int ret = -1;
+
+	AW_DEV_LOGD(aw87xxx->dev, "load profile[%s] enter", profile);
+	mutex_lock(&aw87xxx->reg_lock);
+	aw87xxx_monitor_stop(&aw87xxx->monitor);
+	if (0 == strncmp(profile, aw87xxx->prof_off_name, AW_PROFILE_STR_MAX)) {
+		ret = aw87xxx_power_down(aw87xxx, profile);
+	} else {
+		ret = aw87xxx_power_on(aw87xxx, profile);
+		if (!ret)
+			aw87xxx_monitor_start(&aw87xxx->monitor);
+	}
+	mutex_unlock(&aw87xxx->reg_lock);
+
+	return ret;
+}
+
+int aw87xxx_update_profile_esd(struct aw87xxx *aw87xxx, char *profile)
+{
+	int ret = -1;
+
+	if (0 == strncmp(profile, aw87xxx->prof_off_name, AW_PROFILE_STR_MAX))
+		ret = aw87xxx_power_down(aw87xxx, profile);
+	else
+		ret = aw87xxx_power_on(aw87xxx, profile);
+
+	return ret;
+}
+
+char *aw87xxx_show_current_profile(int dev_index)
+{
+	struct list_head *pos = NULL;
+	struct aw87xxx *aw87xxx = NULL;
+
+	list_for_each(pos, &g_aw87xxx_list) {
+		aw87xxx = list_entry(pos, struct aw87xxx, list);
+		if (aw87xxx->dev_index == dev_index) {
+			AW_DEV_LOGI(aw87xxx->dev, "current profile is [%s]",
+				aw87xxx->current_profile);
+			return aw87xxx->current_profile;
+		}
+	}
+
+	AW_LOGE("not found struct aw87xxx, dev_index = [%d]", dev_index);
+	return NULL;
+}
+EXPORT_SYMBOL(aw87xxx_show_current_profile);
+
+int aw87xxx_set_profile(int dev_index, char *profile)
+{
+	struct list_head *pos = NULL;
+	struct aw87xxx *aw87xxx = NULL;
+
+	list_for_each(pos, &g_aw87xxx_list) {
+		aw87xxx = list_entry(pos, struct aw87xxx, list);
+		if (profile && aw87xxx->dev_index == dev_index) {
+			AW_DEV_LOGD(aw87xxx->dev, "set dev_index = %d, profile = %s",
+				dev_index, profile);
+			return aw87xxx_update_profile(aw87xxx, profile);
+		}
+	}
+
+	AW_LOGE("not found struct aw87xxx, dev_index = [%d]", dev_index);
+	return -EINVAL;
+}
+EXPORT_SYMBOL(aw87xxx_set_profile);
+
+int aw87xxx_set_profile_by_id(int dev_index, int profile_id)
+{
+	char *profile = NULL;
+
+	profile = aw87xxx_ctos_get_prof_name(profile_id);
+	if (profile == NULL) {
+		AW_LOGE("aw87xxx, dev_index[%d] profile[%d] not support!",
+					dev_index, profile_id);
+		return -EINVAL;
+	}
+
+	AW_LOGI("aw87xxx, dev_index[%d] set profile[%s] by id[%d]",
+					dev_index, profile, profile_id);
+	return aw87xxx_set_profile(dev_index, profile);
+}
+EXPORT_SYMBOL(aw87xxx_set_profile_by_id);
+
+/****************************************************************************
+ *
+ * aw87xxx Kcontrols
+ *
+ ****************************************************************************/
+static int aw87xxx_profile_switch_info(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_info *uinfo)
+{
+	int count = 0;
+	char *name = NULL;
+	char *profile_name = NULL;
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+
+	if (aw87xxx == NULL) {
+		AW_LOGE("get struct aw87xxx failed");
+		return -EINVAL;
+	}
+
+	uinfo->type = SNDRV_CTL_ELEM_TYPE_ENUMERATED;
+	uinfo->count = 1;
+
+	/*make sure have prof */
+	count = aw87xxx_acf_get_profile_count(aw87xxx->dev, &aw87xxx->acf_info);
+	if (count <= 0) {
+		uinfo->value.enumerated.items = 0;
+		AW_DEV_LOGE(aw87xxx->dev, "get count[%d] failed", count);
+		return 0;
+	}
+
+	uinfo->value.enumerated.items = count;
+	if (uinfo->value.enumerated.item >= count)
+		uinfo->value.enumerated.item = count - 1;
+
+	name = uinfo->value.enumerated.name;
+	count = uinfo->value.enumerated.item;
+	profile_name = aw87xxx_acf_get_prof_name_form_index(aw87xxx->dev,
+		&aw87xxx->acf_info, count);
+	if (profile_name == NULL) {
+		strscpy(uinfo->value.enumerated.name, "NULL",
+			strlen("NULL") + 1);
+		return 0;
+	}
+
+	strscpy(name, profile_name, sizeof(uinfo->value.enumerated.name));
+
+	return 0;
+}
+
+static int aw87xxx_profile_switch_put(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_value *ucontrol)
+{
+	int ret = -1;
+	char *profile_name = NULL;
+	int index = ucontrol->value.integer.value[0];
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+	struct acf_bin_info *acf_info = NULL;
+
+	if (aw87xxx == NULL) {
+		AW_LOGE("get struct aw87xxx failed");
+		return -EINVAL;
+	}
+
+	acf_info = &aw87xxx->acf_info;
+
+	profile_name = aw87xxx_acf_get_prof_name_form_index(aw87xxx->dev, acf_info, index);
+	if (!profile_name) {
+		AW_DEV_LOGE(aw87xxx->dev, "not found profile name,index=[%d]",
+				index);
+		return -EINVAL;
+	}
+
+	AW_DEV_LOGI(aw87xxx->dev, "set profile [%s]", profile_name);
+
+	ret = aw87xxx_update_profile(aw87xxx, profile_name);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "set dev_index[%d] profile failed, profile = %s",
+			aw87xxx->dev_index, profile_name);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int aw87xxx_profile_switch_get(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_value *ucontrol)
+{
+	int index = 0;
+	char *profile;
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+
+	if (aw87xxx == NULL) {
+		AW_LOGE("get struct aw87xxx failed");
+		return -EINVAL;
+	}
+
+	if (!aw87xxx->current_profile) {
+		AW_DEV_LOGE(aw87xxx->dev, "profile not init");
+		return -EINVAL;
+	}
+
+	profile = aw87xxx->current_profile;
+	AW_DEV_LOGI(aw87xxx->dev, "current profile:[%s]",
+		aw87xxx->current_profile);
+
+
+	index = aw87xxx_acf_get_prof_index_form_name(aw87xxx->dev,
+		&aw87xxx->acf_info, aw87xxx->current_profile);
+	if (index < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "get profile index failed");
+		return index;
+	}
+
+	ucontrol->value.integer.value[0] = index;
+
+	return 0;
+}
+
+static int aw87xxx_vmax_get_info(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_info *uinfo)
+{
+	uinfo->type = SNDRV_CTL_ELEM_TYPE_INTEGER;
+	uinfo->count = 1;
+	uinfo->value.integer.min = INT_MIN;
+	uinfo->value.integer.max = AW_VMAX_MAX;
+
+	return 0;
+}
+
+static int aw87xxx_vmax_get(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_value *ucontrol)
+{
+	int ret = -1;
+	int vmax_val = 0;
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+
+	if (aw87xxx == NULL) {
+		AW_LOGE("get struct aw87xxx failed");
+		return -EINVAL;
+	}
+
+	ret = aw87xxx_monitor_no_dsp_get_vmax(&aw87xxx->monitor, &vmax_val);
+	if (ret < 0)
+		return ret;
+
+	ucontrol->value.integer.value[0] = vmax_val;
+	AW_DEV_LOGI(aw87xxx->dev, "get vmax = [0x%x]", vmax_val);
+
+	return 0;
+}
+
+static int aw87xxx_monitor_switch_info(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_info *uinfo)
+{
+	int count;
+
+	uinfo->type = SNDRV_CTL_ELEM_TYPE_ENUMERATED;
+	uinfo->count = 1;
+	count = ARRAY_SIZE(aw87xxx_monitor_switch);
+
+	uinfo->value.enumerated.items = count;
+
+	if (uinfo->value.enumerated.item >= count)
+		uinfo->value.enumerated.item = count - 1;
+
+	strscpy(uinfo->value.enumerated.name,
+		aw87xxx_monitor_switch[uinfo->value.enumerated.item],
+		strlen(aw87xxx_monitor_switch[uinfo->value.enumerated.item]) + 1);
+
+	return 0;
+}
+
+static int aw87xxx_monitor_switch_put(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_value *ucontrol)
+{
+	uint32_t ctrl_value = ucontrol->value.integer.value[0];
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+	struct aw_monitor *aw_monitor = &aw87xxx->monitor;
+	int ret = -1;
+
+	ret = aw87xxx_dev_monitor_switch_set(aw_monitor, ctrl_value);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int aw87xxx_monitor_switch_get(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_value *ucontrol)
+{
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+	struct aw_monitor *aw_monitor = &aw87xxx->monitor;
+
+	ucontrol->value.integer.value[0] = aw_monitor->monitor_hdr.monitor_switch;
+
+	AW_DEV_LOGI(aw87xxx->dev, "monitor switch is %ld", ucontrol->value.integer.value[0]);
+	return 0;
+}
+
+static int aw87xxx_spin_switch_info(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_info *uinfo)
+{
+	int count;
+
+	uinfo->type = SNDRV_CTL_ELEM_TYPE_ENUMERATED;
+	uinfo->count = 1;
+	count = ARRAY_SIZE(aw87xxx_spin_switch);
+
+	uinfo->value.enumerated.items = count;
+
+	if (uinfo->value.enumerated.item >= count)
+		uinfo->value.enumerated.item = count - 1;
+
+	strscpy(uinfo->value.enumerated.name,
+		aw87xxx_spin_switch[uinfo->value.enumerated.item],
+		strlen(aw87xxx_spin_switch[uinfo->value.enumerated.item]) + 1);
+
+	return 0;
+}
+
+static int aw87xxx_spin_switch_put(struct snd_kcontrol *kcontrol,
+			struct snd_ctl_elem_value *ucontrol)
+{
+	uint32_t ctrl_value = 0;
+	int ret = 0;
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+	ctrl_value = ucontrol->value.integer.value[0];
+
+	ret = aw87xxx_dsp_set_spin(ctrl_value);
+	if (ret) {
+		AW_DEV_LOGE(aw87xxx->dev, "write spin failed");
+		return ret;
+	}
+	AW_DEV_LOGD(aw87xxx->dev, "write spin done ctrl_value=%d", ctrl_value);
+	return 0;
+}
+
+static int aw87xxx_spin_switch_get(struct snd_kcontrol *kcontrol,
+	struct snd_ctl_elem_value *ucontrol)
+{
+	struct aw87xxx *aw87xxx = (struct aw87xxx *)kcontrol->private_value;
+
+	ucontrol->value.integer.value[0] = aw87xxx_dsp_get_spin();
+	AW_DEV_LOGD(aw87xxx->dev, "current spin is %ld", ucontrol->value.integer.value[0]);
+
+	return 0;
+}
+
+
+static int aw87xxx_kcontrol_dynamic_create(struct aw87xxx *aw87xxx,
+						void *codec)
+{
+	struct snd_kcontrol_new *aw87xxx_kcontrol = NULL;
+	aw_snd_soc_codec_t *soc_codec = (aw_snd_soc_codec_t *)codec;
+	char *kctl_name[AW87XXX_PRIVATE_KCONTROL_NUM];
+	int kcontrol_num = AW87XXX_PRIVATE_KCONTROL_NUM;
+	int ret = -1;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+	aw87xxx->codec = soc_codec;
+
+	aw87xxx_kcontrol = devm_kzalloc(aw87xxx->dev,
+			sizeof(struct snd_kcontrol_new) * kcontrol_num,
+			GFP_KERNEL);
+	if (aw87xxx_kcontrol == NULL) {
+		AW_DEV_LOGE(aw87xxx->dev, "aw87xxx_kcontrol devm_kzalloc failed");
+		return -ENOMEM;
+	}
+
+	kctl_name[0] = devm_kzalloc(aw87xxx->dev, AW_NAME_BUF_MAX,
+			GFP_KERNEL);
+	if (kctl_name[0] == NULL)
+		return -ENOMEM;
+
+	snprintf(kctl_name[0], AW_NAME_BUF_MAX, "aw87xxx_profile_switch_%d",
+			aw87xxx->dev_index);
+
+	aw87xxx_kcontrol[0].name = kctl_name[0];
+	aw87xxx_kcontrol[0].iface = SNDRV_CTL_ELEM_IFACE_MIXER;
+	aw87xxx_kcontrol[0].info = aw87xxx_profile_switch_info;
+	aw87xxx_kcontrol[0].get = aw87xxx_profile_switch_get;
+	aw87xxx_kcontrol[0].put = aw87xxx_profile_switch_put;
+	aw87xxx_kcontrol[0].private_value = (unsigned long)aw87xxx;
+
+	kctl_name[1] = devm_kzalloc(aw87xxx->codec->dev, AW_NAME_BUF_MAX,
+			GFP_KERNEL);
+	if (kctl_name[1] == NULL)
+		return -ENOMEM;
+
+	snprintf(kctl_name[1], AW_NAME_BUF_MAX, "aw87xxx_vmax_get_%d",
+			aw87xxx->dev_index);
+
+	aw87xxx_kcontrol[1].name = kctl_name[1];
+	aw87xxx_kcontrol[1].iface = SNDRV_CTL_ELEM_IFACE_MIXER;
+	aw87xxx_kcontrol[1].access = SNDRV_CTL_ELEM_ACCESS_READ;
+	aw87xxx_kcontrol[1].info = aw87xxx_vmax_get_info;
+	aw87xxx_kcontrol[1].get = aw87xxx_vmax_get;
+	aw87xxx_kcontrol[1].private_value = (unsigned long)aw87xxx;
+
+	kctl_name[2] = devm_kzalloc(aw87xxx->codec->dev, AW_NAME_BUF_MAX,
+			GFP_KERNEL);
+	if (kctl_name[2] == NULL)
+		return -ENOMEM;
+
+	snprintf(kctl_name[2], AW_NAME_BUF_MAX, "aw87xxx_monitor_switch_%d",
+			aw87xxx->dev_index);
+
+	aw87xxx_kcontrol[2].name = kctl_name[2];
+	aw87xxx_kcontrol[2].iface = SNDRV_CTL_ELEM_IFACE_MIXER;
+	aw87xxx_kcontrol[2].info = aw87xxx_monitor_switch_info;
+	aw87xxx_kcontrol[2].get = aw87xxx_monitor_switch_get;
+	aw87xxx_kcontrol[2].put = aw87xxx_monitor_switch_put;
+	aw87xxx_kcontrol[2].private_value = (unsigned long)aw87xxx;
+
+	ret = aw_componet_codec_ops.add_codec_controls(aw87xxx->codec,
+				aw87xxx_kcontrol, kcontrol_num);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "add codec controls failed, ret = %d",
+			ret);
+		return ret;
+	}
+
+	AW_DEV_LOGI(aw87xxx->dev, "add codec controls[%s,%s,%s]",
+		aw87xxx_kcontrol[0].name,
+		aw87xxx_kcontrol[1].name,
+		aw87xxx_kcontrol[2].name);
+
+	return 0;
+}
+
+static int aw87xxx_public_kcontrol_create(struct aw87xxx *aw87xxx,
+						void *codec)
+{
+	struct snd_kcontrol_new *aw87xxx_kcontrol = NULL;
+	aw_snd_soc_codec_t *soc_codec = (aw_snd_soc_codec_t *)codec;
+	char *kctl_name[AW87XXX_PUBLIC_KCONTROL_NUM];
+	int kcontrol_num = AW87XXX_PUBLIC_KCONTROL_NUM;
+	int ret = -1;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+	aw87xxx->codec = soc_codec;
+
+	aw87xxx_kcontrol = devm_kzalloc(aw87xxx->dev,
+			sizeof(struct snd_kcontrol_new) * kcontrol_num,
+			GFP_KERNEL);
+	if (aw87xxx_kcontrol == NULL) {
+		AW_DEV_LOGE(aw87xxx->dev, "aw87xxx_kcontrol devm_kzalloc failed");
+		return -ENOMEM;
+	}
+
+	kctl_name[0] = devm_kzalloc(aw87xxx->dev, AW_NAME_BUF_MAX,
+			GFP_KERNEL);
+	if (kctl_name[0] == NULL)
+		return -ENOMEM;
+
+	snprintf(kctl_name[0], AW_NAME_BUF_MAX, "aw87xxx_spin_switch");
+
+	aw87xxx_kcontrol[0].name = kctl_name[0];
+	aw87xxx_kcontrol[0].iface = SNDRV_CTL_ELEM_IFACE_MIXER;
+	aw87xxx_kcontrol[0].info = aw87xxx_spin_switch_info;
+	aw87xxx_kcontrol[0].get = aw87xxx_spin_switch_get;
+	aw87xxx_kcontrol[0].put = aw87xxx_spin_switch_put;
+	aw87xxx_kcontrol[0].private_value = (unsigned long)aw87xxx;
+
+	ret = aw_componet_codec_ops.add_codec_controls(aw87xxx->codec,
+				aw87xxx_kcontrol, kcontrol_num);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "add codec controls failed, ret = %d",
+			ret);
+		return ret;
+	}
+
+	AW_DEV_LOGI(aw87xxx->dev, "add public codec controls[%s]",
+		aw87xxx_kcontrol[0].name);
+
+	return 0;
+}
+
+/****************************************************************************
+ *
+ *aw87xxx kcontrol create
+ *
+ ****************************************************************************/
+int aw87xxx_add_codec_controls(void *codec)
+{
+	struct list_head *pos = NULL;
+	struct aw87xxx *aw87xxx = NULL;
+	int ret = -1;
+
+	list_for_each(pos, &g_aw87xxx_list) {
+		aw87xxx = list_entry(pos, struct aw87xxx, list);
+		ret = aw87xxx_kcontrol_dynamic_create(aw87xxx, codec);
+		if (ret < 0)
+			return ret;
+
+		if (aw87xxx->dev_index == 0) {
+			ret = aw87xxx_public_kcontrol_create(aw87xxx, codec);
+			if (ret < 0)
+				return ret;
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(aw87xxx_add_codec_controls);
+
+
+/****************************************************************************
+ *
+ * aw87xxx firmware cfg load
+ *
+ ***************************************************************************/
+static void aw87xxx_fw_cfg_free(struct aw87xxx *aw87xxx)
+{
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+	aw87xxx_acf_profile_free(aw87xxx->dev, &aw87xxx->acf_info);
+	aw87xxx_monitor_cfg_free(&aw87xxx->monitor);
+}
+
+static int aw87xxx_init_default_prof(struct aw87xxx *aw87xxx)
+{
+	char *profile = NULL;
+
+	profile = aw87xxx_acf_get_prof_off_name(aw87xxx->dev, &aw87xxx->acf_info);
+	if (profile == NULL) {
+		AW_DEV_LOGE(aw87xxx->dev, "get profile off name failed");
+		return -EINVAL;
+	}
+
+	snprintf(aw87xxx->prof_off_name, AW_PROFILE_STR_MAX, "%s", profile);
+	aw87xxx->current_profile = profile;
+	AW_DEV_LOGI(aw87xxx->dev, "init profile name [%s]",
+		aw87xxx->current_profile);
+
+	return 0;
+}
+
+static void aw87xxx_fw_load_retry(struct aw87xxx *aw87xxx)
+{
+	struct acf_bin_info *acf_info = &aw87xxx->acf_info;
+	int ram_timer_val = 2000;
+
+	AW_DEV_LOGD(aw87xxx->dev, "failed to read [%s]",
+			aw87xxx->fw_name);
+
+	if (acf_info->load_count < AW_LOAD_FW_RETRIES) {
+		AW_DEV_LOGD(aw87xxx->dev,
+			"restart hrtimer to load firmware");
+		schedule_delayed_work(&aw87xxx->fw_load_work,
+			msecs_to_jiffies(ram_timer_val));
+	} else {
+		acf_info->load_count = 0;
+		AW_DEV_LOGE(aw87xxx->dev,
+			"can not load firmware,please check name or file exists");
+		return;
+	}
+	acf_info->load_count++;
+}
+
+static void aw87xxx_fw_load(const struct firmware *fw, void *context)
+{
+	int ret = -1;
+	struct aw87xxx *aw87xxx = context;
+	struct acf_bin_info *acf_info = &aw87xxx->acf_info;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+
+	if (!fw) {
+		aw87xxx_fw_load_retry(aw87xxx);
+		return;
+	}
+
+	AW_DEV_LOGD(aw87xxx->dev, "loaded %s - size: %ld",
+		aw87xxx->fw_name, (u_long)(fw ? fw->size : 0));
+
+	mutex_lock(&aw87xxx->reg_lock);
+	acf_info->fw_data = vmalloc(fw->size);
+	if (!acf_info->fw_data) {
+		AW_DEV_LOGE(aw87xxx->dev, "fw_data kzalloc memory failed");
+		goto exit_vmalloc_failed;
+	}
+	memset(acf_info->fw_data, 0, fw->size);
+	memcpy(acf_info->fw_data, fw->data, fw->size);
+	acf_info->fw_size = fw->size;
+
+	ret = aw87xxx_acf_parse(aw87xxx->dev, &aw87xxx->acf_info);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "fw_data parse failed");
+		goto exit_acf_parse_failed;
+	}
+
+	ret = aw87xxx_init_default_prof(aw87xxx);
+	if (ret < 0) {
+		aw87xxx_fw_cfg_free(aw87xxx);
+		goto exit_acf_parse_failed;
+	}
+
+	AW_DEV_LOGI(aw87xxx->dev, "acf parse succeed");
+	mutex_unlock(&aw87xxx->reg_lock);
+	release_firmware(fw);
+	// Updating profile to "Music" because the firmware is set to "off" during init
+	aw87xxx_update_profile(aw87xxx, AW87XXX_PROF_MUSIC);
+
+	return;
+
+exit_acf_parse_failed:
+exit_vmalloc_failed:
+	release_firmware(fw);
+	mutex_unlock(&aw87xxx->reg_lock);
+}
+
+static void aw87xxx_fw_load_work_routine(struct work_struct *work)
+{
+	struct aw87xxx *aw87xxx = container_of(work,
+			struct aw87xxx, fw_load_work.work);
+	struct aw_prof_info *prof_info = &aw87xxx->acf_info.prof_info;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+
+	if (prof_info->status == AW_ACF_WAIT) {
+		request_firmware_nowait(THIS_MODULE,
+//				FW_ACTION_HOTPLUG,
+				FW_ACTION_UEVENT,
+				aw87xxx->fw_name,
+				aw87xxx->dev,
+				GFP_KERNEL, aw87xxx,
+				aw87xxx_fw_load);
+	}
+}
+
+static void aw87xxx_fw_load_init(struct aw87xxx *aw87xxx)
+{
+#ifdef AW_CFG_UPDATE_DELAY
+	int cfg_timer_val = AW_CFG_UPDATE_DELAY_TIMER;
+#else
+	int cfg_timer_val = 0;
+#endif
+	AW_DEV_LOGI(aw87xxx->dev, "enter");
+	snprintf(aw87xxx->fw_name, AW87XXX_FW_NAME_MAX, "%s", AW87XXX_FW_BIN_NAME);
+	aw87xxx_acf_init(&aw87xxx->aw_dev, &aw87xxx->acf_info, aw87xxx->dev_index);
+
+	INIT_DELAYED_WORK(&aw87xxx->fw_load_work, aw87xxx_fw_load_work_routine);
+	schedule_delayed_work(&aw87xxx->fw_load_work,
+			msecs_to_jiffies(cfg_timer_val));
+}
+
+/****************************************************************************
+ *
+ *aw87xxx attribute node
+ *
+ ****************************************************************************/
+static ssize_t aw87xxx_attr_get_reg(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	int ret = 0;
+	unsigned int i = 0;
+	unsigned char reg_val = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_device *aw_dev = &aw87xxx->aw_dev;
+
+	mutex_lock(&aw87xxx->reg_lock);
+	for (i = 0; i < aw_dev->reg_max_addr; i++) {
+		if (!(aw_dev->reg_access[i] & AW_DEV_REG_RD_ACCESS))
+			continue;
+		ret = aw87xxx_dev_i2c_read_byte(&aw87xxx->aw_dev, i, &reg_val);
+		if (ret < 0) {
+			len += snprintf(buf + len, PAGE_SIZE - len,
+					"read reg [0x%x] failed\n", i);
+			AW_DEV_LOGE(aw87xxx->dev, "read reg [0x%x] failed", i);
+		} else {
+			len += snprintf(buf + len, PAGE_SIZE - len,
+					"reg:0x%02X=0x%02X\n", i, reg_val);
+			AW_DEV_LOGD(aw87xxx->dev, "reg:0x%02X=0x%02X",
+					i, reg_val);
+		}
+	}
+	mutex_unlock(&aw87xxx->reg_lock);
+
+	return len;
+}
+
+static ssize_t aw87xxx_attr_set_reg(struct device *dev,
+			struct device_attribute *attr, const char *buf,
+			size_t len)
+{
+	unsigned int databuf[2] = { 0 };
+	int ret = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+
+	mutex_lock(&aw87xxx->reg_lock);
+	if (sscanf(buf, "0x%x 0x%x", &databuf[0], &databuf[1]) == 2) {
+		if (databuf[0] >= aw87xxx->aw_dev.reg_max_addr) {
+			AW_DEV_LOGE(aw87xxx->dev, "set reg[0x%x] error,is out of reg_addr_max[0x%x]",
+				databuf[0], aw87xxx->aw_dev.reg_max_addr);
+			mutex_unlock(&aw87xxx->reg_lock);
+			return -EINVAL;
+		}
+
+		ret = aw87xxx_dev_i2c_write_byte(&aw87xxx->aw_dev,
+					databuf[0], databuf[1]);
+		if (ret < 0)
+			AW_DEV_LOGE(aw87xxx->dev, "set [0x%x]=0x%x failed",
+				databuf[0], databuf[1]);
+		else
+			AW_DEV_LOGD(aw87xxx->dev, "set [0x%x]=0x%x succeed",
+				databuf[0], databuf[1]);
+	} else {
+		AW_DEV_LOGE(aw87xxx->dev, "i2c write cmd input error");
+	}
+	mutex_unlock(&aw87xxx->reg_lock);
+
+	return len;
+}
+
+static ssize_t aw87xxx_attr_get_profile(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	unsigned int i = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_prof_info *prof_info = &aw87xxx->acf_info.prof_info;
+
+	if (!prof_info->status) {
+		len += snprintf(buf + len, PAGE_SIZE - len,
+				"profile_cfg not load\n");
+		return len;
+	}
+
+	AW_DEV_LOGI(aw87xxx->dev, "current profile:[%s]", aw87xxx->current_profile);
+
+	for (i = 0; i < prof_info->count; i++) {
+		if (!strncmp(aw87xxx->current_profile, prof_info->prof_name_list[i],
+				AW_PROFILE_STR_MAX))
+			len += snprintf(buf + len, PAGE_SIZE - len,
+				">%s\n", prof_info->prof_name_list[i]);
+		else
+			len += snprintf(buf + len, PAGE_SIZE - len,
+				" %s\n", prof_info->prof_name_list[i]);
+	}
+
+	return len;
+}
+
+static ssize_t aw87xxx_attr_set_profile(struct device *dev,
+			struct device_attribute *attr, const char *buf,
+			size_t len)
+{
+	char profile[AW_PROFILE_STR_MAX] = {0};
+	int ret = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+
+	if (strlen(buf) > AW_PROFILE_STR_MAX) {
+		AW_DEV_LOGE(aw87xxx->dev, "input profile_str_len is out of max[%d]",
+				AW_PROFILE_STR_MAX);
+		return -EINVAL;
+	}
+
+	if (sscanf(buf, "%s", profile) == 1) {
+		AW_DEV_LOGD(aw87xxx->dev, "set profile [%s]", profile);
+		ret = aw87xxx_update_profile(aw87xxx, profile);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev, "set profile[%s] failed",
+				profile);
+			return ret;
+		}
+	}
+
+	return len;
+}
+
+static ssize_t aw87xxx_attr_get_hwen(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	int hwen = aw87xxx->aw_dev.hwen_status;
+
+	if (hwen >= AW_DEV_HWEN_INVALID)
+		len += snprintf(buf + len, PAGE_SIZE - len, "hwen_status: invalid\n");
+	else if (hwen == AW_DEV_HWEN_ON)
+		len += snprintf(buf + len, PAGE_SIZE - len, "hwen_status: on\n");
+	else if (hwen == AW_DEV_HWEN_OFF)
+		len += snprintf(buf + len, PAGE_SIZE - len, "hwen_status: off\n");
+
+	return len;
+}
+
+static ssize_t aw87xxx_attr_set_hwen(struct device *dev,
+				struct device_attribute *attr, const char *buf,
+				size_t len)
+{
+	int ret = -1;
+	unsigned int state;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+
+	ret = kstrtouint(buf, 0, &state);
+	if (ret) {
+		AW_DEV_LOGE(aw87xxx->dev, "fail to channelge str to int");
+		return ret;
+	}
+
+	mutex_lock(&aw87xxx->reg_lock);
+	if (state == AW_DEV_HWEN_OFF)
+		aw87xxx_dev_hw_pwr_ctrl(&aw87xxx->aw_dev, false); /*OFF*/
+	else if (state == AW_DEV_HWEN_ON)
+		aw87xxx_dev_hw_pwr_ctrl(&aw87xxx->aw_dev, true); /*ON*/
+	else
+		AW_DEV_LOGE(aw87xxx->dev, "input [%d] error, hwen_on=[%d],hwen_off=[%d]",
+			state, AW_DEV_HWEN_ON, AW_DEV_HWEN_OFF);
+	mutex_unlock(&aw87xxx->reg_lock);
+	return len;
+}
+
+int aw87xxx_awrw_write(struct aw87xxx *aw87xxx,
+			const char *buf, size_t count)
+{
+	int i = 0, ret = -1;
+	char *data_buf = NULL;
+	int buf_len = 0;
+	int temp_data = 0;
+	int data_str_size = 0;
+	char *reg_data;
+	struct aw_i2c_packet *packet = &aw87xxx->i2c_packet;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+	/* one addr or one data string Composition of Contains two bytes of symbol(0X)*/
+	/* and two byte of hexadecimal data*/
+	data_str_size = 2 + 2 * AWRW_DATA_BYTES;
+
+	/* The buf includes the first address of the register to be written and all data */
+	buf_len = AWRW_ADDR_BYTES + packet->reg_num * AWRW_DATA_BYTES;
+	AW_DEV_LOGI(aw87xxx->dev, "buf_len = %d,reg_num = %d", buf_len, packet->reg_num);
+	data_buf = vmalloc(buf_len);
+	if (data_buf == NULL) {
+		AW_DEV_LOGE(aw87xxx->dev, "alloc memory failed");
+		return -ENOMEM;
+	}
+	memset(data_buf, 0, buf_len);
+
+	data_buf[0] = packet->reg_addr;
+	reg_data = data_buf + 1;
+
+	AW_DEV_LOGD(aw87xxx->dev, "reg_addr: 0x%02x", data_buf[0]);
+
+	/*ag:0x00 0x01 0x01 0x01 0x01 0x00\x0a*/
+	for (i = 0; i < packet->reg_num; i++) {
+		ret = sscanf(buf + AWRW_HDR_LEN + 1 + i * (data_str_size + 1),
+			"0x%x", &temp_data);
+		if (ret != 1) {
+			AW_DEV_LOGE(aw87xxx->dev, "sscanf failed,ret=%d", ret);
+			vfree(data_buf);
+			data_buf = NULL;
+			return ret;
+		}
+		reg_data[i] = temp_data;
+		AW_DEV_LOGD(aw87xxx->dev, "[%d] : 0x%02x", i, reg_data[i]);
+	}
+
+	mutex_lock(&aw87xxx->reg_lock);
+	ret = i2c_master_send(aw87xxx->aw_dev.i2c, data_buf, buf_len);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "write failed");
+		vfree(data_buf);
+		data_buf = NULL;
+		return -EFAULT;
+	}
+	mutex_unlock(&aw87xxx->reg_lock);
+
+	vfree(data_buf);
+	data_buf = NULL;
+
+	AW_DEV_LOGD(aw87xxx->dev, "down");
+	return 0;
+}
+
+static int aw87xxx_awrw_data_check(struct aw87xxx *aw87xxx,
+			int *data, size_t count)
+{
+	struct aw_i2c_packet *packet = &aw87xxx->i2c_packet;
+	int req_data_len = 0;
+	int act_data_len = 0;
+	int data_str_size = 0;
+
+	if ((data[AWRW_HDR_ADDR_BYTES] != AWRW_ADDR_BYTES) ||
+		(data[AWRW_HDR_DATA_BYTES] != AWRW_DATA_BYTES)) {
+		AW_DEV_LOGE(aw87xxx->dev, "addr_bytes [%d] or data_bytes [%d] unsupport",
+			data[AWRW_HDR_ADDR_BYTES], data[AWRW_HDR_DATA_BYTES]);
+		return -EINVAL;
+	}
+
+	/* one data string Composition of Contains two bytes of symbol(0x)*/
+	/* and two byte of hexadecimal data*/
+	data_str_size = 2 + 2 * AWRW_DATA_BYTES;
+	act_data_len = count - AWRW_HDR_LEN - 1;
+
+	/* There is a comma(,) or space between each piece of data */
+	if (data[AWRW_HDR_WR_FLAG] == AWRW_FLAG_WRITE) {
+		/*ag:0x00 0x01 0x01 0x01 0x01 0x00\x0a*/
+		req_data_len = (data_str_size + 1) * packet->reg_num;
+		if (req_data_len > act_data_len) {
+			AW_DEV_LOGE(aw87xxx->dev, "data_len checkfailed,requeset data_len [%d],actaul data_len [%d]",
+				req_data_len, act_data_len);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+/* flag addr_bytes data_bytes reg_num reg_addr*/
+static int aw87xxx_awrw_parse_buf(struct aw87xxx *aw87xxx,
+			const char *buf, size_t count, int *wr_status)
+{
+	int data[AWRW_HDR_MAX] = {0};
+	struct aw_i2c_packet *packet = &aw87xxx->i2c_packet;
+	int ret = -1;
+
+	if (sscanf(buf, "0x%02x 0x%02x 0x%02x 0x%02x 0x%02x",
+		&data[AWRW_HDR_WR_FLAG], &data[AWRW_HDR_ADDR_BYTES],
+		&data[AWRW_HDR_DATA_BYTES], &data[AWRW_HDR_REG_NUM],
+		&data[AWRW_HDR_REG_ADDR]) == 5) {
+
+		packet->reg_addr = data[AWRW_HDR_REG_ADDR];
+		packet->reg_num = data[AWRW_HDR_REG_NUM];
+		*wr_status = data[AWRW_HDR_WR_FLAG];
+		ret = aw87xxx_awrw_data_check(aw87xxx, data, count);
+		if (ret < 0)
+			return ret;
+
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
+static ssize_t aw87xxx_attr_awrw_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_i2c_packet *packet = &aw87xxx->i2c_packet;
+	int wr_status = 0;
+	int ret = -1;
+
+	if (count < AWRW_HDR_LEN) {
+		AW_DEV_LOGE(aw87xxx->dev, "data count too smaller, please check write format");
+		AW_DEV_LOGE(aw87xxx->dev, "string %s,count=%ld",
+			buf, (u_long)count);
+		return -EINVAL;
+	}
+
+	AW_DEV_LOGI(aw87xxx->dev, "string:[%s],count=%ld", buf, (u_long)count);
+	ret = aw87xxx_awrw_parse_buf(aw87xxx, buf, count, &wr_status);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "can not parse string");
+		return ret;
+	}
+
+	if (wr_status == AWRW_FLAG_WRITE) {
+		ret = aw87xxx_awrw_write(aw87xxx, buf, count);
+		if (ret < 0)
+			return ret;
+	} else if (wr_status == AWRW_FLAG_READ) {
+		packet->status = AWRW_I2C_ST_READ;
+		AW_DEV_LOGI(aw87xxx->dev, "read_cmd:reg_addr[0x%02x], reg_num[%d]",
+			packet->reg_addr, packet->reg_num);
+	} else {
+		AW_DEV_LOGE(aw87xxx->dev, "please check str format, unsupport read_write_status: %d",
+			wr_status);
+		return -EINVAL;
+	}
+
+	return count;
+}
+
+static ssize_t aw87xxx_attr_awrw_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_i2c_packet *packet = &aw87xxx->i2c_packet;
+	int data_len = 0;
+	size_t len = 0;
+	int ret = -1, i = 0;
+	char *reg_data = NULL;
+
+	if (packet->status != AWRW_I2C_ST_READ) {
+		AW_DEV_LOGE(aw87xxx->dev, "please write read cmd first");
+		return -EINVAL;
+	}
+
+	data_len = AWRW_DATA_BYTES * packet->reg_num;
+	reg_data = (char *)vmalloc(data_len);
+	if (reg_data == NULL) {
+		AW_DEV_LOGE(aw87xxx->dev, "memory alloc failed");
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	mutex_lock(&aw87xxx->reg_lock);
+	ret = aw87xxx_dev_i2c_read_msg(&aw87xxx->aw_dev, packet->reg_addr,
+				(char *)reg_data, data_len);
+	if (ret < 0) {
+		ret = -EFAULT;
+		mutex_unlock(&aw87xxx->reg_lock);
+		goto exit;
+	}
+	mutex_unlock(&aw87xxx->reg_lock);
+
+	AW_DEV_LOGI(aw87xxx->dev, "reg_addr 0x%02x, reg_num %d",
+		packet->reg_addr, packet->reg_num);
+
+	for (i = 0; i < data_len; i++) {
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"0x%02x,", reg_data[i]);
+		AW_DEV_LOGI(aw87xxx->dev, "0x%02x", reg_data[i]);
+	}
+
+	ret = len;
+
+exit:
+	if (reg_data) {
+		vfree(reg_data);
+		reg_data = NULL;
+	}
+	packet->status = AWRW_I2C_ST_NONE;
+	return ret;
+}
+
+static ssize_t aw87xxx_drv_ver_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+
+	len += snprintf(buf + len, PAGE_SIZE - len,
+		"driver_ver: %s \n", AW87XXX_DRIVER_VERSION);
+
+	return len;
+}
+
+static DEVICE_ATTR(reg, S_IWUSR | S_IRUGO,
+		aw87xxx_attr_get_reg, aw87xxx_attr_set_reg);
+static DEVICE_ATTR(profile, S_IWUSR | S_IRUGO,
+		aw87xxx_attr_get_profile, aw87xxx_attr_set_profile);
+static DEVICE_ATTR(hwen, S_IWUSR | S_IRUGO,
+		aw87xxx_attr_get_hwen, aw87xxx_attr_set_hwen);
+static DEVICE_ATTR(awrw, S_IWUSR | S_IRUGO,
+	aw87xxx_attr_awrw_show, aw87xxx_attr_awrw_store);
+static DEVICE_ATTR(drv_ver, S_IRUGO, aw87xxx_drv_ver_show, NULL);
+
+static struct attribute *aw87xxx_attributes[] = {
+	&dev_attr_reg.attr,
+	&dev_attr_profile.attr,
+	&dev_attr_hwen.attr,
+	&dev_attr_awrw.attr,
+	&dev_attr_drv_ver.attr,
+	NULL
+};
+
+static struct attribute_group aw87xxx_attribute_group = {
+	.attrs = aw87xxx_attributes
+};
+
+/****************************************************************************
+ *
+ *aw87xxx device probe
+ *
+ ****************************************************************************/
+static const struct acpi_gpio_params reset_gpio = { 0, 0, false };
+static const struct acpi_gpio_mapping reset_acpi_gpios[] = {
+  { "reset-gpios", &reset_gpio, 1 },
+  { }
+};
+
+static struct aw87xxx *aw87xxx_malloc_init(struct i2c_client *client)
+{
+	struct aw87xxx *aw87xxx = NULL;
+
+	aw87xxx = devm_kzalloc(&client->dev, sizeof(struct aw87xxx),
+			GFP_KERNEL);
+	if (aw87xxx == NULL) {
+		AW_DEV_LOGE(&client->dev, "failed to devm_kzalloc aw87xxx");
+		return NULL;
+	}
+	memset(aw87xxx, 0, sizeof(struct aw87xxx));
+
+	aw87xxx->dev = &client->dev;
+	aw87xxx->aw_dev.dev = &client->dev;
+	aw87xxx->aw_dev.i2c_bus = client->adapter->nr;
+	aw87xxx->aw_dev.i2c_addr = client->addr;
+	aw87xxx->aw_dev.i2c = client;
+	aw87xxx->aw_dev.hwen_status = false;
+	aw87xxx->aw_dev.reg_access = NULL;
+	aw87xxx->aw_dev.hwen_status = AW_DEV_HWEN_INVALID;
+	aw87xxx->off_bin_status = AW87XXX_NO_OFF_BIN;
+	aw87xxx->codec = NULL;
+	aw87xxx->current_profile = aw87xxx->prof_off_name;
+
+	mutex_init(&aw87xxx->reg_lock);
+
+	AW_DEV_LOGI(&client->dev, "Driver struct alloc and mutex init done, devinfo: i2c_bus=%u, i2c_addr=%x", client->adapter->nr, client->addr);
+	return aw87xxx;
+}
+
+static int aw87xxx_i2c_probe(struct i2c_client *client)
+{
+	struct device_node *dev_node = client->dev.of_node;
+	const struct smi_node *node;
+	struct acpi_device *adev = ACPI_COMPANION(&client->dev);
+	struct aw87xxx *aw87xxx = NULL;
+	struct gpio_desc *gpiod = NULL;
+	struct i2c_board_info board_info = {};
+	char i2c_name[32];
+	int ret = -1;
+	int acpi_dev_count = 0;
+
+	/* aw87xxx Get APCI I2C device count */
+	if(g_aw87xxx_dev_cnt == 0){
+		acpi_dev_count = i2c_acpi_client_count(adev);
+		AW_DEV_LOGI(&client->dev, "I2C_ACPI_CLIENT_COUNT returned [%d]", acpi_dev_count);
+	}
+
+	if (!i2c_check_functionality(client->adapter, I2C_FUNC_I2C)) {
+		AW_DEV_LOGE(&client->dev, "check_functionality failed");
+		ret = -ENODEV;
+		goto exit_check_functionality_failed;
+	}
+
+	/* aw87xxx i2c_dev struct init */
+	aw87xxx = aw87xxx_malloc_init(client);
+	if (aw87xxx == NULL)
+		goto exit_malloc_init_failed;
+
+	i2c_set_clientdata(client, aw87xxx);
+
+	aw87xxx_device_parse_port_id_dt(&aw87xxx->aw_dev);
+	aw87xxx_device_parse_topo_id_dt(&aw87xxx->aw_dev);
+
+	/* aw87xxx Get ACPI GPIO */
+
+	if (g_aw87xxx_dev_cnt == 0){
+		ret = devm_acpi_dev_add_driver_gpios(aw87xxx->dev, reset_acpi_gpios);
+		if(ret){
+			AW_DEV_LOGE(aw87xxx->dev, "Unable to add GPIO mapping table");
+			goto exit_device_init_failed;
+		}
+
+		gpiod = devm_gpiod_get(aw87xxx->dev, "reset", GPIOD_OUT_LOW);
+		if (gpiod == NULL){
+			AW_DEV_LOGE(aw87xxx->dev, "Gpiod returned NULL failing gracefully.");
+			goto exit_device_init_failed;
+		}
+
+		if (IS_ERR(gpiod)){
+			AW_DEV_LOGE(aw87xxx->dev, "Get gpiod failed.");
+			goto exit_device_init_failed;
+		}
+
+		aw87xxx->aw_dev.rst_gpio = desc_to_gpio(gpiod);
+		aw87xxx->aw_dev.hwen_status = AW_DEV_HWEN_OFF;
+		AW_DEV_LOGI(aw87xxx->dev, "reset gpio[%x] parse succeed", aw87xxx->aw_dev.rst_gpio);
+
+		if (gpio_is_valid(aw87xxx->aw_dev.rst_gpio)) {
+			ret = devm_gpio_request_one(aw87xxx->dev, aw87xxx->aw_dev.rst_gpio, GPIOF_OUT_INIT_LOW, "aw87xxx_reset");
+			if ((ret < 0) && (ret != -EBUSY)) {
+					AW_DEV_LOGE(aw87xxx->dev, "reset request failed, returned [%d]", ret);
+					goto exit_device_init_failed;
+			}
+		}else{
+			/*Disabling RESET GPIO*/
+			AW_DEV_LOGI(aw87xxx->dev, "no reset gpio provided, hardware reset unavailable");
+			aw87xxx->aw_dev.rst_gpio = AW_NO_RESET_GPIO;
+			aw87xxx->aw_dev.hwen_status = AW_DEV_HWEN_INVALID;
+		}
+
+	}
+
+	/*hw power on PA*/
+	if(g_aw87xxx_dev_cnt == 0) {
+		aw87xxx_dev_hw_pwr_ctrl(&aw87xxx->aw_dev, true);
+	}
+
+	/* aw87xxx devices private attributes init */
+	ret = aw87xxx_dev_init(&aw87xxx->aw_dev);
+	if (ret < 0)
+		goto exit_device_init_failed;
+
+	/*product register reset */
+	aw87xxx_dev_soft_reset(&aw87xxx->aw_dev);
+
+	/*hw power off */
+	if(g_aw87xxx_dev_cnt == 0) {
+		aw87xxx_dev_hw_pwr_ctrl(&aw87xxx->aw_dev, false);
+	}
+
+	/* create debug attrbute nodes */
+	ret = sysfs_create_group(&aw87xxx->dev->kobj, &aw87xxx_attribute_group);
+	if (ret < 0)
+		AW_DEV_LOGE(aw87xxx->dev, "failed to create sysfs nodes, will not allowed to use");
+
+	/* cfg_load init */
+	aw87xxx_fw_load_init(aw87xxx);
+
+	/*monitor init*/
+	aw87xxx_monitor_init(aw87xxx->dev, &aw87xxx->monitor, dev_node);
+
+	/*add device to total list */
+	mutex_lock(&g_aw87xxx_mutex_lock);
+	g_aw87xxx_dev_cnt++;
+	list_add(&aw87xxx->list, &g_aw87xxx_list);
+	aw87xxx->dev_index = g_aw87xxx_dev_cnt;
+
+	mutex_unlock(&g_aw87xxx_mutex_lock);
+	AW_DEV_LOGI(aw87xxx->dev, "succeed, dev_index=[%d], g_aw87xxx_dev_cnt= [%d]",
+			aw87xxx->dev_index, g_aw87xxx_dev_cnt);
+
+	AW_DEV_LOGI(aw87xxx->dev, "acpi_c=[%d] dev_c=[%d]", acpi_dev_count, g_aw87xxx_dev_cnt);
+
+	/* Attempt to add other I2C AMPs */
+	if ((acpi_dev_count > 1) && (g_aw87xxx_dev_cnt == 1)){
+		/* power on the chip */
+		aw87xxx_dev_hw_pwr_ctrl(&aw87xxx->aw_dev, true);
+
+		node = device_get_match_data(aw87xxx->dev);
+		memset(&board_info, 0, sizeof(board_info));
+		strscpy(board_info.type, client->name, I2C_NAME_SIZE);
+		snprintf(i2c_name, sizeof(i2c_name), "%s.%d", client->name, 1);
+		board_info.dev_name = i2c_name;
+
+		aw87xxx_i2c_probe(i2c_acpi_new_device_by_fwnode(acpi_fwnode_handle(adev), 1, &board_info));
+	}
+
+	return 0;
+
+exit_device_init_failed:
+	AW_DEV_LOGE(aw87xxx->dev, "pa init failed");
+
+	devm_kfree(&client->dev, aw87xxx);
+	aw87xxx = NULL;
+exit_malloc_init_failed:
+exit_check_functionality_failed:
+	return ret;
+}
+
+static void aw87xxx_i2c_remove(struct i2c_client *client)
+{
+	struct aw87xxx *aw87xxx = i2c_get_clientdata(client);
+
+	aw87xxx_monitor_exit(&aw87xxx->monitor);
+
+	/*rm attr node*/
+	sysfs_remove_group(&aw87xxx->dev->kobj, &aw87xxx_attribute_group);
+
+	aw87xxx_fw_cfg_free(aw87xxx);
+
+	mutex_lock(&g_aw87xxx_mutex_lock);
+	g_aw87xxx_dev_cnt--;
+	list_del(&aw87xxx->list);
+	mutex_unlock(&g_aw87xxx_mutex_lock);
+
+	devm_kfree(&client->dev, aw87xxx);
+	aw87xxx = NULL;
+
+//	return 0;
+}
+
+static void aw87xxx_i2c_shutdown(struct i2c_client *client)
+{
+	struct aw87xxx *aw87xxx = i2c_get_clientdata(client);
+
+	AW_DEV_LOGI(&client->dev, "enter");
+
+	/*soft and hw power off*/
+	aw87xxx_update_profile(aw87xxx, aw87xxx->prof_off_name);
+}
+
+static int aw87xxx_runtime_suspend(struct device *dev)
+{
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+
+	AW_DEV_LOGI(aw87xxx->dev, "Suspending...");
+
+	// soft and hw power off
+	aw87xxx_update_profile(aw87xxx, aw87xxx->prof_off_name);
+
+	return 0;
+}
+
+static int aw87xxx_runtime_resume(struct device *dev)
+{
+	struct list_head *pos = NULL;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+
+	// Power on PA
+	if (aw87xxx->dev_index == 1)
+		aw87xxx_dev_hw_pwr_ctrl(&aw87xxx->aw_dev, true);
+
+	// Set profile to Music
+	list_for_each_prev(pos, &g_aw87xxx_list) {
+		aw87xxx = list_entry(pos, struct aw87xxx, list);
+		AW_DEV_LOGI(aw87xxx->dev, "Resuming...");
+
+		mutex_lock(&aw87xxx->reg_lock);
+		aw87xxx_power_on(aw87xxx, AW87XXX_PROF_MUSIC);
+		mutex_unlock(&aw87xxx->reg_lock);
+	}
+
+	return 0;
+}
+
+static SIMPLE_DEV_PM_OPS(aw87xxx_pm_ops, aw87xxx_runtime_suspend, aw87xxx_runtime_resume);
+
+static const struct acpi_device_id aw87xxx_acpi_match[] = {
+        { "AWDZ8830", 0 },
+	{ }
+};
+MODULE_DEVICE_TABLE(acpi, aw87xxx_acpi_match);
+
+// This is not necessary if the acpi match probes correctly. This is needed for userspace `new_device() functionality
+static const struct i2c_device_id aw87xxx_i2c_id[] = {
+	{AW87XXX_I2C_NAME, 0},
+	{},
+};
+
+static struct i2c_driver aw87xxx_i2c_driver = {
+	.driver = {
+		.owner = THIS_MODULE,
+		.name = AW87XXX_I2C_NAME,
+		.acpi_match_table = aw87xxx_acpi_match,
+		.pm = &aw87xxx_pm_ops,
+		},
+	.probe = aw87xxx_i2c_probe,
+	.remove = aw87xxx_i2c_remove,
+	.shutdown = aw87xxx_i2c_shutdown,
+	.id_table = aw87xxx_i2c_id,
+};
+
+module_i2c_driver(aw87xxx_i2c_driver)
+
+MODULE_AUTHOR("<zhaozhongbo@awinic.com>");
+MODULE_DESCRIPTION("awinic aw87xxx pa driver");
+MODULE_LICENSE("GPL v2");
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx.h b/sound/soc/codecs/aw87xxx/aw87xxx.h
new file mode 100644
index 000000000000..4a613e4a8f12
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx.h
@@ -0,0 +1,126 @@
+#ifndef __AW87XXX_H__
+#define __AW87XXX_H__
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <sound/control.h>
+#include <sound/soc.h>
+
+#include "aw87xxx_device.h"
+#include "aw87xxx_monitor.h"
+#include "aw87xxx_acf_bin.h"
+
+#define AW_CFG_UPDATE_DELAY
+#define AW_CFG_UPDATE_DELAY_TIMER	(3000)
+
+#define AW87XXX_NO_OFF_BIN		(0)
+#define AW87XXX_OFF_BIN_OK		(1)
+
+#define AW87XXX_PRIVATE_KCONTROL_NUM	(3)
+#define AW87XXX_PUBLIC_KCONTROL_NUM	(1)
+
+#define AW_I2C_RETRIES			(5)
+#define AW_I2C_RETRY_DELAY		(2)
+#define AW_I2C_READ_MSG_NUM		(2)
+
+#define AW87XXX_FW_NAME_MAX		(64)
+#define AW_NAME_BUF_MAX			(64)
+#define AW_LOAD_FW_RETRIES		(3)
+
+#define AW_DEV_REG_RD_ACCESS		(1 << 0)
+#define AW_DEV_REG_WR_ACCESS		(1 << 1)
+
+#define AWRW_ADDR_BYTES			(1)
+#define AWRW_DATA_BYTES			(1)
+#define AWRW_HDR_LEN			(24)
+
+/***********************************************************
+ *
+ * aw87xxx codec control compatible with kernel 4.19
+ *
+ ***********************************************************/
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 19, 1)
+#define AW_KERNEL_VER_OVER_4_19_1
+#endif
+
+#ifdef AW_KERNEL_VER_OVER_4_19_1
+typedef struct snd_soc_component aw_snd_soc_codec_t;
+#else
+typedef struct snd_soc_codec aw_snd_soc_codec_t;
+#endif
+
+struct aw_componet_codec_ops {
+	int (*add_codec_controls)(aw_snd_soc_codec_t *codec,
+		const struct snd_kcontrol_new *controls, unsigned int num_controls);
+	void (*unregister_codec)(struct device *dev);
+};
+
+
+/********************************************
+ *
+ * aw87xxx devices attributes
+ *
+ *******************************************/
+enum {
+	AWRW_FLAG_WRITE = 0,
+	AWRW_FLAG_READ,
+};
+
+enum {
+	AWRW_I2C_ST_NONE = 0,
+	AWRW_I2C_ST_READ,
+	AWRW_I2C_ST_WRITE,
+};
+
+enum {
+	AWRW_HDR_WR_FLAG = 0,
+	AWRW_HDR_ADDR_BYTES,
+	AWRW_HDR_DATA_BYTES,
+	AWRW_HDR_REG_NUM,
+	AWRW_HDR_REG_ADDR,
+	AWRW_HDR_MAX,
+};
+
+struct aw_i2c_packet {
+	char status;
+	unsigned int reg_num;
+	unsigned int reg_addr;
+	char *reg_data;
+};
+
+
+/********************************************
+ *
+ * aw87xxx device struct
+ *
+ *******************************************/
+struct aw87xxx {
+	char fw_name[AW87XXX_FW_NAME_MAX];
+	int32_t dev_index;
+	char *current_profile;
+	char prof_off_name[AW_PROFILE_STR_MAX];
+	uint32_t off_bin_status;
+	struct device *dev;
+
+	struct mutex reg_lock;
+	struct aw_device aw_dev;
+	struct aw_i2c_packet i2c_packet;
+
+	struct delayed_work fw_load_work;
+	struct acf_bin_info acf_info;
+
+	aw_snd_soc_codec_t *codec;
+
+	struct list_head list;
+
+	struct aw_monitor monitor;
+};
+
+int aw87xxx_update_profile(struct aw87xxx *aw87xxx, char *profile);
+int aw87xxx_update_profile_esd(struct aw87xxx *aw87xxx, char *profile);
+
+char *aw87xxx_show_current_profile(int dev_index);
+int aw87xxx_set_profile(int dev_index, char *profile);
+int aw87xxx_set_profile_by_id(int dev_index, int profile_id);
+int aw87xxx_add_codec_controls(void *codec);
+int aw87xxx_awrw_write(struct aw87xxx *aw87xxx, const char *buf, size_t count);
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.c b/sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.c
new file mode 100644
index 000000000000..00c7aedb7c11
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.c
@@ -0,0 +1,1558 @@
+/*
+ * aw87xxx_acf_bin.c
+ *
+ * Copyright (c) 2021 AWINIC Technology CO., LTD
+ *
+ * Author: Barry <zhaozhongbo@awinic.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/module.h>
+#include <asm/uaccess.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/vmalloc.h>
+#include "aw87xxx.h"
+#include "aw87xxx_acf_bin.h"
+#include "aw87xxx_monitor.h"
+#include "aw87xxx_log.h"
+#include "aw87xxx_bin_parse.h"
+
+/*************************************************************************
+ *
+ *Table corresponding to customized profile ids to profile names
+ *
+ *************************************************************************/
+enum aw_customers_profile_id {
+	AW_CTOS_PROFILE_OFF = 0,
+	AW_CTOS_PROFILE_MUSIC,
+	AW_CTOS_PROFILE_VOICE,
+	AW_CTOS_PROFILE_VOIP,
+	AW_CTOS_PROFILE_RINGTONE,
+	AW_CTOS_PROFILE_RINGTONE_HS,
+	AW_CTOS_PROFILE_LOWPOWER,
+	AW_CTOS_PROFILE_BYPASS,
+	AW_CTOS_PROFILE_MMI,
+	AW_CTOS_PROFILE_FM,
+	AW_CTOS_PROFILE_NOTIFICATION,
+	AW_CTOS_PROFILE_RECEIVER,
+	AW_CTOS_PROFILE_MAX,
+};
+
+static char *g_ctos_profile_name[AW_PROFILE_MAX] = {
+	[AW_CTOS_PROFILE_OFF] = "Off",
+	[AW_CTOS_PROFILE_MUSIC] = "Music",
+	[AW_CTOS_PROFILE_VOICE] = "Voice",
+	[AW_CTOS_PROFILE_VOIP] = "Voip",
+	[AW_CTOS_PROFILE_RINGTONE] = "Ringtone",
+	[AW_CTOS_PROFILE_RINGTONE_HS] = "Ringtone_hs",
+	[AW_CTOS_PROFILE_LOWPOWER] = "Lowpower",
+	[AW_CTOS_PROFILE_BYPASS] = "Bypass",
+	[AW_CTOS_PROFILE_MMI] = "Mmi",
+	[AW_CTOS_PROFILE_FM] = "Fm",
+	[AW_CTOS_PROFILE_NOTIFICATION] = "Notification",
+	[AW_CTOS_PROFILE_RECEIVER] = "Receiver",
+};
+
+
+char *aw87xxx_ctos_get_prof_name(int profile_id)
+{
+	if (profile_id < 0 || profile_id >= AW_CTOS_PROFILE_MAX)
+		return NULL;
+	else
+		return g_ctos_profile_name[profile_id];
+}
+
+
+static char *g_profile_name[] = {"Music", "Voice", "Voip",
+		"Ringtone", "Ringtone_hs", "Lowpower", "Bypass", "Mmi",
+		"Fm", "Notification", "Receiver", "Off"};
+
+static char *g_power_off_name[] = {"Off", "OFF", "off", "oFF", "power_down"};
+
+static char *aw_get_prof_name(int profile)
+{
+	if (profile < 0 || profile >= AW_PROFILE_MAX)
+		return "NULL";
+	else
+		return g_profile_name[profile];
+}
+
+/*************************************************************************
+ *
+ *acf check
+ *
+ *************************************************************************/
+static int aw_crc8_check(const unsigned char *data, unsigned int data_size)
+
+{
+	unsigned char crc_value = 0x00;
+	unsigned char *pdata;
+	int i;
+	unsigned char pdatabuf = 0;
+
+	pdata = (unsigned char *)data;
+
+	while (data_size--) {
+		pdatabuf = *pdata++;
+		for (i = 0; i < 8; i++) {
+			if ((crc_value ^ (pdatabuf)) & 0x01) {
+				crc_value ^= 0x18;
+				crc_value >>= 1;
+				crc_value |= 0x80;
+			} else {
+				crc_value >>= 1;
+			}
+			pdatabuf >>= 1;
+		}
+	}
+
+	return (int)crc_value;
+}
+
+static int aw_check_file_id(struct device *dev,
+		char *fw_data, int32_t file_id)
+{
+	int32_t *acf_file_id = NULL;
+
+	acf_file_id = (int32_t *)fw_data;
+	if (*acf_file_id != file_id) {
+		AW_DEV_LOGE(dev, "file id [%x] check failed", *acf_file_id);
+		return -ENFILE;
+	}
+
+	return 0;
+}
+
+static int aw_check_header_size(struct device *dev,
+			char *fw_data, size_t fw_size)
+{
+	if (fw_size < sizeof(struct aw_acf_hdr)) {
+		AW_DEV_LOGE(dev, "acf size check failed,size less-than aw_acf_hdr");
+		return -ENOEXEC;
+	}
+
+	return 0;
+}
+
+/***************************************************************************
+ * V0.0.0.1 version acf check
+ **************************************************************************/
+static int aw_check_ddt_size_v_0_0_0_1(struct device *dev, char *fw_data)
+{
+	struct aw_acf_hdr *acf_hdr = (struct aw_acf_hdr *)fw_data;
+	struct aw_acf_dde *acf_dde = NULL;
+
+	acf_dde = (struct aw_acf_dde *)(fw_data + acf_hdr->ddt_offset);
+
+	/* check ddt_size in acf_header is aqual to ddt_num multiply by dde_size */
+	if (acf_hdr->ddt_size != acf_hdr->dde_num * sizeof(struct aw_acf_dde)) {
+		AW_DEV_LOGE(dev, "acf ddt size check failed");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int aw_check_data_size_v_0_0_0_1(struct device *dev,
+		char *fw_data, size_t fw_size)
+{
+	int i = 0;
+	size_t data_size = 0;
+	struct aw_acf_hdr *acf_hdr = NULL;
+	struct aw_acf_dde *acf_dde = NULL;
+
+	acf_hdr = (struct aw_acf_hdr *)fw_data;
+	acf_dde = (struct aw_acf_dde *)(fw_data + acf_hdr->ddt_offset);
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		if (acf_dde[i].data_size % 2) {
+			AW_DEV_LOGE(dev, "acf dde[%d].data_size[%d],dev_name[%s],data_type[%d], data_size check failed",
+				i, acf_dde[i].data_size, acf_dde[i].dev_name,
+				acf_dde[i].data_type);
+			return -EINVAL;
+		}
+		data_size += acf_dde[i].data_size;
+	}
+
+	/* Verify that the file size is equal to the header size plus */
+	/* the table size and data size */
+	if (fw_size != data_size + sizeof(struct aw_acf_hdr) + acf_hdr->ddt_size) {
+		AW_DEV_LOGE(dev, "acf size check failed");
+		AW_DEV_LOGE(dev, "fw_size=%ld,hdr_size and ddt size and data size =%ld",
+			(u_long)fw_size, (u_long)(data_size + sizeof(struct aw_acf_hdr) +
+			acf_hdr->ddt_size));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int aw_check_data_crc_v_0_0_0_1(struct device *dev, char *fw_data)
+{
+	int i = 0;
+	size_t crc_val = 0;
+	char *data = NULL;
+	struct aw_acf_hdr *acf_hdr = NULL;
+	struct aw_acf_dde *acf_dde = NULL;
+
+	acf_hdr = (struct aw_acf_hdr *)fw_data;
+	acf_dde = (struct aw_acf_dde *)(fw_data + acf_hdr->ddt_offset);
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		data = fw_data + acf_dde[i].data_offset;
+		crc_val = aw_crc8_check(data, acf_dde[i].data_size);
+		if (crc_val != acf_dde[i].data_crc) {
+			AW_DEV_LOGE(dev, "acf dde_crc check failed");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int aw_check_profile_id_v_0_0_0_1(struct device *dev, char *fw_data)
+{
+	int i = 0;
+	struct aw_acf_hdr *acf_hdr = NULL;
+	struct aw_acf_dde *acf_dde = NULL;
+
+	acf_hdr = (struct aw_acf_hdr *)fw_data;
+	acf_dde = (struct aw_acf_dde *)(fw_data + acf_hdr->ddt_offset);
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		if (acf_dde[i].data_type == AW_MONITOR)
+			continue;
+		if (acf_dde[i].dev_profile > AW_PROFILE_MAX) {
+			AW_DEV_LOGE(dev, "parse profile_id[%d] failed", acf_dde[i].dev_profile);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+static int aw_check_data_v_0_0_0_1(struct device *dev,
+			char *fw_data, size_t size)
+{
+	int ret = -1;
+
+	/* check file type id is awinic acf file */
+	ret = aw_check_file_id(dev, fw_data, AW_ACF_FILE_ID);
+	if (ret < 0)
+		return ret;
+
+	/* check ddt_size in header is equal to all ddt aize */
+	ret = aw_check_ddt_size_v_0_0_0_1(dev, fw_data);
+	if (ret < 0)
+		return ret;
+
+	/* Verify that the file size is equal to the header size plus */
+	/* the table size and data size */
+	ret = aw_check_data_size_v_0_0_0_1(dev, fw_data, size);
+	if (ret < 0)
+		return ret;
+
+	/* check crc in is equal to dde data crc */
+	ret = aw_check_data_crc_v_0_0_0_1(dev, fw_data);
+	if (ret < 0)
+		return ret;
+
+	/* check profile id is in profile_id_max */
+	ret = aw_check_profile_id_v_0_0_0_1(dev, fw_data);
+	if (ret < 0)
+		return ret;
+
+	AW_DEV_LOGI(dev, "acf fimware check succeed");
+
+	return 0;
+}
+
+/***************************************************************************
+ * V1.0.0.0 version acf chack
+ **************************************************************************/
+static int aw_check_ddt_size_v_1_0_0_0(struct device *dev, char *fw_data)
+{
+	struct aw_acf_hdr *acf_hdr = (struct aw_acf_hdr *)fw_data;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde = NULL;
+
+	acf_dde = (struct aw_acf_dde_v_1_0_0_0 *)(fw_data + acf_hdr->ddt_offset);
+
+	/* check ddt_size in acf_header is aqual to ddt_num multiply by dde_size */
+	if (acf_hdr->ddt_size != acf_hdr->dde_num * sizeof(struct aw_acf_dde_v_1_0_0_0)) {
+		AW_DEV_LOGE(dev, "acf ddt size check failed");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int aw_check_data_size_v_1_0_0_0(struct device *dev,
+		char *fw_data, size_t fw_size)
+{
+	int i = 0;
+	size_t data_size = 0;
+	struct aw_acf_hdr *acf_hdr = NULL;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde = NULL;
+
+	acf_hdr = (struct aw_acf_hdr *)fw_data;
+	acf_dde = (struct aw_acf_dde_v_1_0_0_0 *)(fw_data + acf_hdr->ddt_offset);
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		if (acf_dde[i].data_size % 2) {
+			AW_DEV_LOGE(dev, "acf dde[%d].data_size[%d],dev_name[%s],data_type[%d], data_size check failed",
+				i, acf_dde[i].data_size, acf_dde[i].dev_name,
+				acf_dde[i].data_type);
+			return -EINVAL;
+		}
+		data_size += acf_dde[i].data_size;
+	}
+
+	/* Verify that the file size is equal to the header size plus */
+	/* the table size and data size */
+	if (fw_size != data_size + sizeof(struct aw_acf_hdr) + acf_hdr->ddt_size) {
+		AW_DEV_LOGE(dev, "acf size check failed");
+		AW_DEV_LOGE(dev, "fw_size=%ld,hdr_size and ddt size and data size =%ld",
+			(u_long)fw_size, (u_long)(data_size + sizeof(struct aw_acf_hdr) +
+			acf_hdr->ddt_size));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int aw_check_data_crc_v_1_0_0_0(struct device *dev, char *fw_data)
+{
+	int i = 0;
+	size_t crc_val = 0;
+	char *data = NULL;
+	struct aw_acf_hdr *acf_hdr = NULL;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde = NULL;
+
+	acf_hdr = (struct aw_acf_hdr *)fw_data;
+	acf_dde = (struct aw_acf_dde_v_1_0_0_0 *)(fw_data + acf_hdr->ddt_offset);
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		data = fw_data + acf_dde[i].data_offset;
+		crc_val = aw_crc8_check(data, acf_dde[i].data_size);
+		if (crc_val != acf_dde[i].data_crc) {
+			AW_DEV_LOGE(dev, "acf dde_crc check failed");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int aw_check_data_v_1_0_0_0(struct device *dev,
+			char *fw_data, size_t size)
+{
+	int ret = -1;
+
+	/* check file type id is awinic acf file */
+	ret = aw_check_file_id(dev, fw_data, AW_ACF_FILE_ID);
+	if (ret < 0)
+		return ret;
+
+	/* check ddt_size in header is equal to all ddt aize */
+	ret = aw_check_ddt_size_v_1_0_0_0(dev, fw_data);
+	if (ret < 0)
+		return ret;
+
+	/* Verify that the file size is equal to the header size plus */
+	/* the table size and data size */
+	ret = aw_check_data_size_v_1_0_0_0(dev, fw_data, size);
+	if (ret < 0)
+		return ret;
+
+	/* check crc in is equal to dde data crc */
+	ret = aw_check_data_crc_v_1_0_0_0(dev, fw_data);
+	if (ret < 0)
+		return ret;
+
+	AW_DEV_LOGI(dev, "acf fimware check succeed");
+
+	return 0;
+}
+
+/***************************************************************************
+ * acf chack API
+ **************************************************************************/
+static int aw_check_acf_firmware(struct device *dev,
+			char *fw_data, size_t size)
+{
+	int ret = -1;
+	struct aw_acf_hdr *acf_hdr = NULL;
+
+	if (fw_data == NULL) {
+		AW_DEV_LOGE(dev, "fw_data is NULL,fw_data check failed");
+		return -ENODATA;
+	}
+
+	/* check file size is less-than header size */
+	ret = aw_check_header_size(dev, fw_data, size);
+	if (ret < 0)
+		return ret;
+
+	acf_hdr = (struct aw_acf_hdr *)fw_data;
+	AW_DEV_LOGI(dev, "project name: [%s]", acf_hdr->project);
+	AW_DEV_LOGI(dev, "custom name: [%s]", acf_hdr->custom);
+	AW_DEV_LOGI(dev, "version name: [%s]", acf_hdr->version);
+	AW_DEV_LOGI(dev, "author_id: [%d]", acf_hdr->author_id);
+
+	switch (acf_hdr->hdr_version) {
+	case AW_ACF_HDR_VER_0_0_0_1:
+		return aw_check_data_v_0_0_0_1(dev, fw_data, size);
+	case AW_ACF_HDR_VER_1_0_0_0:
+		return aw_check_data_v_1_0_0_0(dev, fw_data, size);
+	default:
+		AW_DEV_LOGE(dev, "unsupported hdr_version [0x%x]",
+			acf_hdr->hdr_version);
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+
+
+/*************************************************************************
+ *
+ *acf parse
+ *
+ *************************************************************************/
+static int aw_parse_raw_reg(struct device *dev, uint8_t *data,
+		uint32_t data_len, struct aw_prof_desc *prof_desc)
+{
+	AW_DEV_LOGD(dev, "data_size:%d enter", data_len);
+
+	prof_desc->data_container.data = data;
+	prof_desc->data_container.len = data_len;
+
+	prof_desc->prof_st = AW_PROFILE_OK;
+
+	return 0;
+}
+
+static int aw_parse_reg_with_hdr(struct device *dev, uint8_t *data,
+			 uint32_t data_len, struct aw_prof_desc *prof_desc)
+{
+	struct aw_bin *aw_bin = NULL;
+	int ret = -1;
+
+	AW_DEV_LOGD(dev, "data_size:%d enter", data_len);
+
+	aw_bin = kzalloc(data_len + sizeof(struct aw_bin), GFP_KERNEL);
+	if (aw_bin == NULL) {
+		AW_DEV_LOGE(dev, "devm_kzalloc aw_bin failed");
+		return -ENOMEM;
+	}
+
+	aw_bin->info.len = data_len;
+	memcpy(aw_bin->info.data, data, data_len);
+
+	ret = aw87xxx_parsing_bin_file(aw_bin);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "parse bin failed");
+		goto parse_bin_failed;
+	}
+
+	if ((aw_bin->all_bin_parse_num != 1) ||
+		(aw_bin->header_info[0].bin_data_type != DATA_TYPE_REGISTER)) {
+		AW_DEV_LOGE(dev, "bin num or type error");
+		goto parse_bin_failed;
+	}
+
+	prof_desc->data_container.data =
+				data + aw_bin->header_info[0].valid_data_addr;
+	prof_desc->data_container.len = aw_bin->header_info[0].valid_data_len;
+	prof_desc->prof_st = AW_PROFILE_OK;
+
+	kfree(aw_bin);
+	aw_bin = NULL;
+
+	return 0;
+
+parse_bin_failed:
+	kfree(aw_bin);
+	aw_bin = NULL;
+	return ret;
+}
+
+static int aw_parse_monitor_config(struct device *dev,
+				char *monitor_data, uint32_t data_len)
+{
+	int ret = -1;
+
+	if (monitor_data == NULL || data_len == 0) {
+		AW_DEV_LOGE(dev, "no data to parse");
+		return -EBFONT;
+	}
+
+	ret = aw87xxx_monitor_bin_parse(dev, monitor_data, data_len);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "monitor_config parse failed");
+		return ret;
+	}
+
+	AW_DEV_LOGI(dev, "monitor_bin parse succeed");
+
+	return 0;
+}
+
+static int aw_check_prof_str_is_off(char *profile_name)
+{
+	int i = 0;
+
+	for (i = 0; i < AW_POWER_OFF_NAME_SUPPORT_COUNT; i++) {
+		if (strnstr(profile_name, g_power_off_name[i],
+				strlen(profile_name) + 1))
+			return 0;
+	}
+
+	return -EINVAL;
+}
+
+/***************************************************************************
+ * V0.0.0.1 version acf paese
+ **************************************************************************/
+static int aw_check_product_name_v_0_0_0_1(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_acf_dde *prof_hdr)
+{
+	int i = 0;
+
+	for (i = 0; i < acf_info->product_cnt; i++) {
+		if (0 == strcmp(acf_info->product_tab[i], prof_hdr->dev_name)) {
+			AW_DEV_LOGD(dev, "bin_dev_name:%s",
+				prof_hdr->dev_name);
+			return 0;
+		}
+	}
+
+	return -ENXIO;
+}
+
+static int aw_check_data_type_is_monitor_v_0_0_0_1(struct device *dev,
+				struct aw_acf_dde *prof_hdr)
+{
+	if (prof_hdr->data_type == AW_MONITOR) {
+		AW_DEV_LOGD(dev, "bin data is monitor");
+		return 0;
+	}
+
+	return -ENXIO;
+}
+
+static int aw_parse_data_by_sec_type_v_0_0_0_1(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_acf_dde *prof_hdr,
+				struct aw_prof_desc *profile_prof_desc)
+{
+	int ret = -1;
+	char *cfg_data = acf_info->fw_data + prof_hdr->data_offset;
+
+	switch (prof_hdr->data_type) {
+	case AW_BIN_TYPE_REG:
+		snprintf(profile_prof_desc->dev_name, sizeof(prof_hdr->dev_name),
+			"%s", prof_hdr->dev_name);
+		profile_prof_desc->prof_name = aw_get_prof_name(prof_hdr->dev_profile);
+		AW_DEV_LOGD(dev, "parse reg type data enter,profile=%s",
+			aw_get_prof_name(prof_hdr->dev_profile));
+		ret =  aw_parse_raw_reg(dev, cfg_data, prof_hdr->data_size,
+					profile_prof_desc);
+		break;
+	case AW_BIN_TYPE_HDR_REG:
+		snprintf(profile_prof_desc->dev_name, sizeof(prof_hdr->dev_name),
+			"%s", prof_hdr->dev_name);
+		profile_prof_desc->prof_name = aw_get_prof_name(prof_hdr->dev_profile);
+		AW_DEV_LOGD(dev, "parse hdr_reg type data enter,profile=%s",
+			aw_get_prof_name(prof_hdr->dev_profile));
+		ret = aw_parse_reg_with_hdr(dev, cfg_data,
+					prof_hdr->data_size,
+					profile_prof_desc);
+		break;
+	}
+
+	return ret;
+}
+
+static int aw_parse_dev_type_v_0_0_0_1(struct device *dev,
+		struct acf_bin_info *acf_info, struct aw_all_prof_info *all_prof_info)
+{
+	int i = 0;
+	int ret = -1;
+	int sec_num = 0;
+	char *cfg_data = NULL;
+	struct aw_prof_desc *prof_desc = NULL;
+	struct aw_acf_dde *acf_dde =
+		(struct aw_acf_dde *)(acf_info->fw_data + acf_info->acf_hdr.ddt_offset);
+
+	AW_DEV_LOGD(dev, "enter");
+
+	for (i = 0; i < acf_info->acf_hdr.dde_num; i++) {
+		if ((acf_info->aw_dev->i2c_bus == acf_dde[i].dev_bus) &&
+			(acf_info->aw_dev->i2c_addr == acf_dde[i].dev_addr) &&
+			(acf_dde[i].type == AW_DDE_DEV_TYPE_ID)) {
+
+			ret = aw_check_product_name_v_0_0_0_1(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			ret = aw_check_data_type_is_monitor_v_0_0_0_1(dev, &acf_dde[i]);
+			if (ret == 0) {
+				cfg_data = acf_info->fw_data + acf_dde[i].data_offset;
+				ret = aw_parse_monitor_config(dev, cfg_data, acf_dde[i].data_size);
+				if (ret < 0)
+					return ret;
+				continue;
+			}
+
+			prof_desc = &all_prof_info->prof_desc[acf_dde[i].dev_profile];
+			ret = aw_parse_data_by_sec_type_v_0_0_0_1(dev, acf_info, &acf_dde[i],
+				prof_desc);
+			if (ret < 0) {
+				AW_DEV_LOGE(dev, "parse dev type data failed");
+				return ret;
+			}
+			sec_num++;
+		}
+	}
+
+	if (sec_num == 0) {
+		AW_DEV_LOGD(dev, "get dev type num is %d, please use default",
+			sec_num);
+		return AW_DEV_TYPE_NONE;
+	}
+
+	return AW_DEV_TYPE_OK;
+}
+
+static int aw_parse_default_type_v_0_0_0_1(struct device *dev,
+	struct acf_bin_info *acf_info, struct aw_all_prof_info *all_prof_info)
+{
+	int i = 0;
+	int ret = -1;
+	int sec_num = 0;
+	char *cfg_data = NULL;
+	struct aw_prof_desc *prof_desc = NULL;
+	struct aw_acf_dde *acf_dde =
+		(struct aw_acf_dde *)(acf_info->fw_data + acf_info->acf_hdr.ddt_offset);
+
+	AW_DEV_LOGD(dev, "enter");
+
+	for (i = 0; i < acf_info->acf_hdr.dde_num; i++) {
+		if ((acf_info->dev_index == acf_dde[i].dev_index) &&
+			(acf_dde[i].type == AW_DDE_DEV_DEFAULT_TYPE_ID)) {
+
+			ret = aw_check_product_name_v_0_0_0_1(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			ret = aw_check_data_type_is_monitor_v_0_0_0_1(dev, &acf_dde[i]);
+			if (ret == 0) {
+				cfg_data = acf_info->fw_data + acf_dde[i].data_offset;
+				ret = aw_parse_monitor_config(dev, cfg_data, acf_dde[i].data_size);
+				if (ret < 0)
+					return ret;
+				continue;
+			}
+
+			prof_desc = &all_prof_info->prof_desc[acf_dde[i].dev_profile];
+			ret = aw_parse_data_by_sec_type_v_0_0_0_1(dev, acf_info, &acf_dde[i],
+				prof_desc);
+			if (ret < 0) {
+				AW_DEV_LOGE(dev, "parse default type data failed");
+				return ret;
+			}
+			sec_num++;
+		}
+	}
+
+	if (sec_num == 0) {
+		AW_DEV_LOGE(dev, "get dev default type failed, get num[%d]",
+			sec_num);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int aw_get_prof_count_v_0_0_0_1(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_all_prof_info *all_prof_info)
+{
+	int i = 0;
+	int prof_count = 0;
+	struct aw_prof_desc *prof_desc = all_prof_info->prof_desc;
+
+	for (i = 0; i < AW_PROFILE_MAX; i++) {
+		if (prof_desc[i].prof_st == AW_PROFILE_OK) {
+			prof_count++;
+		} else if (i == AW_PROFILE_OFF) {
+			prof_count++;
+			AW_DEV_LOGI(dev, "not found profile [Off], set default");
+		}
+	}
+
+	AW_DEV_LOGI(dev, "get profile count=[%d]", prof_count);
+	return prof_count;
+}
+
+static int aw_set_prof_off_info_v_0_0_0_1(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_all_prof_info *all_prof_info,
+				int index)
+{
+	struct aw_prof_desc *prof_desc = all_prof_info->prof_desc;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	if (index >= prof_info->count) {
+		AW_DEV_LOGE(dev, "index[%d] is out of table,profile count[%d]",
+			index, prof_info->count);
+		return -EINVAL;
+	}
+
+	if (prof_desc[AW_PROFILE_OFF].prof_st == AW_PROFILE_OK) {
+		prof_info->prof_desc[index] = prof_desc[AW_PROFILE_OFF];
+		AW_DEV_LOGI(dev, "product=[%s]----profile=[%s]",
+			prof_info->prof_desc[index].dev_name,
+			aw_get_prof_name(AW_PROFILE_OFF));
+	} else {
+		memset(&prof_info->prof_desc[index].data_container, 0,
+			sizeof(struct aw_data_container));
+		prof_info->prof_desc[index].prof_st = AW_PROFILE_WAIT;
+		prof_info->prof_desc[index].prof_name = aw_get_prof_name(AW_PROFILE_OFF);
+		AW_DEV_LOGI(dev, "set default power_off with no data to profile");
+	}
+
+	return 0;
+}
+
+
+static int aw_get_vaild_prof_v_0_0_0_1(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_all_prof_info *all_prof_info)
+{
+	int i = 0;
+	int ret = 0;
+	int index = 0;
+	struct aw_prof_desc *prof_desc = all_prof_info->prof_desc;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	prof_info->count = 0;
+	ret = aw_get_prof_count_v_0_0_0_1(dev, acf_info, all_prof_info);
+	if (ret < 0)
+		return ret;
+	prof_info->count = ret;
+	prof_info->prof_desc = devm_kzalloc(dev,
+			prof_info->count * sizeof(struct aw_prof_desc),
+			GFP_KERNEL);
+	if (prof_info->prof_desc == NULL) {
+		AW_DEV_LOGE(dev, "prof_desc kzalloc failed");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < AW_PROFILE_MAX; i++) {
+		if (i != AW_PROFILE_OFF && prof_desc[i].prof_st == AW_PROFILE_OK) {
+			if (index >= prof_info->count) {
+				AW_DEV_LOGE(dev, "get profile index[%d] overflow count[%d]",
+						index, prof_info->count);
+				return -ENOMEM;
+			}
+			prof_info->prof_desc[index] = prof_desc[i];
+			AW_DEV_LOGI(dev, "product=[%s]----profile=[%s]",
+				prof_info->prof_desc[index].dev_name,
+				aw_get_prof_name(i));
+			index++;
+		}
+	}
+
+	ret = aw_set_prof_off_info_v_0_0_0_1(dev, acf_info, all_prof_info, index);
+	if (ret < 0)
+		return ret;
+
+	AW_DEV_LOGD(dev, "get vaild profile succeed");
+	return 0;
+}
+
+static int aw_set_prof_name_list_v_0_0_0_1(struct device *dev,
+				struct acf_bin_info *acf_info)
+{
+	int i = 0;
+	int count = acf_info->prof_info.count;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	prof_info->prof_name_list = (char (*)[AW_PROFILE_STR_MAX])devm_kzalloc(dev,
+		count * (AW_PROFILE_STR_MAX), GFP_KERNEL);
+	if (prof_info->prof_name_list == NULL) {
+		AW_DEV_LOGE(dev, "prof_name_list devm_kzalloc failed");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < count; ++i) {
+		snprintf(prof_info->prof_name_list[i], AW_PROFILE_STR_MAX, "%s",
+			prof_info->prof_desc[i].prof_name);
+		AW_DEV_LOGI(dev, "index=[%d], profile_name=[%s]",
+				i, prof_info->prof_name_list[i]);
+	}
+
+	return 0;
+}
+
+static int aw_parse_acf_v_0_0_0_1(struct device *dev,
+		struct acf_bin_info *acf_info)
+
+{
+	int ret = 0;
+	struct aw_all_prof_info all_prof_info;
+
+	AW_DEV_LOGD(dev, "enter");
+	acf_info->prof_info.status = AW_ACF_WAIT;
+
+	memset(&all_prof_info, 0, sizeof(struct aw_all_prof_info));
+
+	ret = aw_parse_dev_type_v_0_0_0_1(dev, acf_info, &all_prof_info);
+	if (ret < 0) {
+		return ret;
+	} else if (ret == AW_DEV_TYPE_NONE) {
+		AW_DEV_LOGD(dev, "get dev type num is 0, parse default dev type");
+		ret = aw_parse_default_type_v_0_0_0_1(dev, acf_info, &all_prof_info);
+		if (ret < 0)
+			return ret;
+	}
+
+	ret = aw_get_vaild_prof_v_0_0_0_1(dev, acf_info, &all_prof_info);
+	if (ret < 0) {
+		aw87xxx_acf_profile_free(dev, acf_info);
+		AW_DEV_LOGE(dev,  "hdr_cersion[0x%x] parse failed",
+					acf_info->acf_hdr.hdr_version);
+		return ret;
+	}
+
+	ret = aw_set_prof_name_list_v_0_0_0_1(dev, acf_info);
+	if (ret < 0) {
+		aw87xxx_acf_profile_free(dev, acf_info);
+		AW_DEV_LOGE(dev,  "creat prof_id_and_name_list failed");
+		return ret;
+	}
+
+	acf_info->prof_info.status = AW_ACF_UPDATE;
+	AW_DEV_LOGI(dev, "acf parse success");
+	return 0;
+}
+
+/***************************************************************************
+ * V1.0.0.0 version acf paese
+ **************************************************************************/
+static int aw_check_product_name_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_acf_dde_v_1_0_0_0 *prof_hdr)
+{
+	int i = 0;
+
+	for (i = 0; i < acf_info->product_cnt; i++) {
+		if (0 == strcmp(acf_info->product_tab[i], prof_hdr->dev_name)) {
+			AW_DEV_LOGI(dev, "bin_dev_name:%s", prof_hdr->dev_name);
+			return 0;
+		}
+	}
+
+	return -ENXIO;
+}
+
+static int aw_get_dde_type_info_v_1_0_0_0(struct device *dev,
+					struct acf_bin_info *acf_info)
+{
+	int i;
+	int dev_num = 0;
+	int default_num = 0;
+	struct aw_acf_hdr *acf_hdr = (struct aw_acf_hdr *)acf_info->fw_data;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde =
+		(struct aw_acf_dde_v_1_0_0_0 *)(acf_info->fw_data + acf_hdr->ddt_offset);
+
+	prof_info->prof_type = AW_DEV_NONE_TYPE_ID;
+	for (i = 0; i < acf_hdr->dde_num; i++) {
+		if (acf_dde[i].type == AW_DDE_DEV_TYPE_ID)
+			dev_num++;
+		if (acf_dde[i].type == AW_DDE_DEV_DEFAULT_TYPE_ID)
+			default_num++;
+	}
+
+	if (!(dev_num || default_num)) {
+		AW_DEV_LOGE(dev, "can't find scene");
+		return -EINVAL;
+	}
+
+	if (dev_num != 0)
+		prof_info->prof_type = AW_DDE_DEV_TYPE_ID;
+	else if (default_num != 0)
+		prof_info->prof_type = AW_DDE_DEV_DEFAULT_TYPE_ID;
+
+	return 0;
+}
+
+
+static int aw_parse_get_dev_type_prof_count_v_1_0_0_0(struct device *dev,
+						struct acf_bin_info *acf_info)
+{
+	struct aw_acf_hdr *acf_hdr = (struct aw_acf_hdr *)acf_info->fw_data;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde =
+		(struct aw_acf_dde_v_1_0_0_0 *)(acf_info->fw_data + acf_hdr->ddt_offset);
+	int i = 0;
+	int ret = 0;
+	int found_off_prof_flag = 0;
+	int count = acf_info->prof_info.count;
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		if (((acf_dde[i].data_type == AW_BIN_TYPE_REG) ||
+		(acf_dde[i].data_type == AW_BIN_TYPE_HDR_REG)) &&
+		((acf_info->aw_dev->i2c_bus == acf_dde[i].dev_bus) &&
+		(acf_info->aw_dev->i2c_addr == acf_dde[i].dev_addr)) &&
+		(acf_info->aw_dev->chipid == acf_dde[i].chip_id)) {
+
+			ret = aw_check_product_name_v_1_0_0_0(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			ret = aw_check_prof_str_is_off(acf_dde[i].dev_profile_str);
+			if (ret == 0) {
+				found_off_prof_flag = AW_PROFILE_OK;
+			}
+			count++;
+		}
+	}
+
+	if (count == 0) {
+		AW_DEV_LOGE(dev, "can't find profile");
+		return -EINVAL;
+	}
+
+	if (!found_off_prof_flag) {
+		count++;
+		AW_DEV_LOGD(dev, "set no config power off profile in count");
+	}
+
+	acf_info->prof_info.count = count;
+	AW_DEV_LOGI(dev, "profile dev_type profile count is %d", acf_info->prof_info.count);
+	return 0;
+}
+
+static int aw_parse_get_default_type_prof_count_v_1_0_0_0(struct device *dev,
+						struct acf_bin_info *acf_info)
+{
+	struct aw_acf_hdr *acf_hdr = (struct aw_acf_hdr *)acf_info->fw_data;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde =
+		(struct aw_acf_dde_v_1_0_0_0 *)(acf_info->fw_data + acf_hdr->ddt_offset);
+	int i = 0;
+	int ret = 0;
+	int found_off_prof_flag = 0;
+	int count = acf_info->prof_info.count;
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		if (((acf_dde[i].data_type == AW_BIN_TYPE_REG) ||
+		(acf_dde[i].data_type == AW_BIN_TYPE_HDR_REG)) &&
+		(acf_info->dev_index == acf_dde[i].dev_index) &&
+		(acf_info->aw_dev->chipid == acf_dde[i].chip_id)) {
+
+			ret = aw_check_product_name_v_1_0_0_0(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			ret = aw_check_prof_str_is_off(acf_dde[i].dev_profile_str);
+			if (ret == 0) {
+				found_off_prof_flag = AW_PROFILE_OK;
+			}
+			count++;
+		}
+	}
+
+	if (count == 0) {
+		AW_DEV_LOGE(dev, "can't find profile");
+		return -EINVAL;
+	}
+
+	if (!found_off_prof_flag) {
+		count++;
+		AW_DEV_LOGD(dev, "set no config power off profile in count");
+	}
+
+	acf_info->prof_info.count = count;
+	AW_DEV_LOGI(dev, "profile default_type profile count is %d", acf_info->prof_info.count);
+	return 0;
+}
+
+static int aw_parse_get_profile_count_v_1_0_0_0(struct device *dev,
+						struct acf_bin_info *acf_info)
+{
+	int ret = 0;
+
+	ret = aw_get_dde_type_info_v_1_0_0_0(dev, acf_info);
+	if (ret < 0)
+		return ret;
+
+	if (acf_info->prof_info.prof_type == AW_DDE_DEV_TYPE_ID) {
+		ret = aw_parse_get_dev_type_prof_count_v_1_0_0_0(dev, acf_info);
+		if (ret < 0) {
+			AW_DEV_LOGE(dev, "parse dev_type profile count failed");
+			return ret;
+		}
+	} else if (acf_info->prof_info.prof_type == AW_DDE_DEV_DEFAULT_TYPE_ID) {
+		ret = aw_parse_get_default_type_prof_count_v_1_0_0_0(dev, acf_info);
+		if (ret < 0) {
+			AW_DEV_LOGE(dev, "parse default_type profile count failed");
+			return ret;
+		}
+	} else {
+		AW_DEV_LOGE(dev, "unsupport prof_type[0x%x]",
+			acf_info->prof_info.prof_type);
+		return -EINVAL;
+	}
+
+	AW_DEV_LOGI(dev, "profile count is %d", acf_info->prof_info.count);
+	return 0;
+}
+
+static int aw_parse_dev_type_prof_name_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info)
+{
+	struct aw_acf_hdr *acf_hdr = (struct aw_acf_hdr *)acf_info->fw_data;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde =
+		(struct aw_acf_dde_v_1_0_0_0 *)(acf_info->fw_data + acf_hdr->ddt_offset);
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+	int i, ret, list_index = 0;
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		if (((acf_dde[i].data_type == AW_BIN_TYPE_REG) ||
+		(acf_dde[i].data_type == AW_BIN_TYPE_HDR_REG)) &&
+		(acf_info->aw_dev->i2c_bus == acf_dde[i].dev_bus) &&
+		(acf_info->aw_dev->i2c_addr == acf_dde[i].dev_addr) &&
+		(acf_info->aw_dev->chipid == acf_dde[i].chip_id)) {
+			if (list_index > prof_info->count) {
+				AW_DEV_LOGE(dev, "%s:Alrealdy set list_index [%d], redundant profile [%s]exist\n",
+					__func__, list_index,
+					acf_dde[i].dev_profile_str);
+				return -EINVAL;
+			}
+
+			ret = aw_check_product_name_v_1_0_0_0(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			snprintf(prof_info->prof_name_list[list_index], AW_PROFILE_STR_MAX, "%s",
+				acf_dde[i].dev_profile_str);
+			AW_DEV_LOGI(dev, "profile_name=[%s]",
+					prof_info->prof_name_list[list_index]);
+			list_index++;
+		}
+	}
+
+	return 0;
+}
+
+static int aw_parse_default_type_prof_name_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info)
+{
+	struct aw_acf_hdr *acf_hdr = (struct aw_acf_hdr *)acf_info->fw_data;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde =
+		(struct aw_acf_dde_v_1_0_0_0 *)(acf_info->fw_data + acf_hdr->ddt_offset);
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+	int i, ret, list_index = 0;
+
+	for (i = 0; i < acf_hdr->dde_num; ++i) {
+		if (((acf_dde[i].data_type == AW_BIN_TYPE_REG) ||
+		(acf_dde[i].data_type == AW_BIN_TYPE_HDR_REG)) &&
+		(acf_info->dev_index == acf_dde[i].dev_index) &&
+		(acf_info->aw_dev->chipid == acf_dde[i].chip_id)) {
+			if (list_index > prof_info->count) {
+				AW_DEV_LOGE(dev, "%s:Alrealdy set list_index [%d], redundant profile [%s]exist\n",
+					__func__, list_index,
+					acf_dde[i].dev_profile_str);
+				return -EINVAL;
+			}
+
+			ret = aw_check_product_name_v_1_0_0_0(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			snprintf(prof_info->prof_name_list[list_index], AW_PROFILE_STR_MAX, "%s",
+				acf_dde[i].dev_profile_str);
+			AW_DEV_LOGI(dev, "profile_name=[%s]",
+					prof_info->prof_name_list[list_index]);
+			list_index++;
+		}
+	}
+
+	return 0;
+}
+
+static int aw_parse_prof_name_v_1_0_0_0(struct device *dev,
+						struct acf_bin_info *acf_info)
+{
+	int ret = 0;
+	int count = acf_info->prof_info.count;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	prof_info->prof_name_list = (char (*)[AW_PROFILE_STR_MAX])devm_kzalloc(dev,
+		count * (AW_PROFILE_STR_MAX), GFP_KERNEL);
+	if (prof_info->prof_name_list == NULL) {
+		AW_DEV_LOGE(dev, "prof_name_list devm_kzalloc failed");
+		return -ENOMEM;
+	}
+
+	if (acf_info->prof_info.prof_type == AW_DDE_DEV_TYPE_ID) {
+		ret = aw_parse_dev_type_prof_name_v_1_0_0_0(dev, acf_info);
+		if (ret < 0) {
+			AW_DEV_LOGE(dev, "parse dev_type profile count failed");
+			return ret;
+		}
+	} else if (acf_info->prof_info.prof_type == AW_DDE_DEV_DEFAULT_TYPE_ID) {
+		ret = aw_parse_default_type_prof_name_v_1_0_0_0(dev, acf_info);
+		if (ret < 0) {
+			AW_DEV_LOGE(dev, "parse default_type profile count failed");
+			return ret;
+		}
+	} else {
+		AW_DEV_LOGE(dev, "unsupport prof_type[0x%x]",
+			acf_info->prof_info.prof_type);
+		return -EINVAL;
+	}
+
+	AW_DEV_LOGI(dev, "profile name parse succeed");
+	return 0;
+}
+
+
+static int aw_search_prof_index_from_list_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_prof_desc **prof_desc,
+				struct aw_acf_dde_v_1_0_0_0 *prof_hdr)
+{
+	int i = 0;
+	int count = acf_info->prof_info.count;
+	char (*prof_name_list)[AW_PROFILE_STR_MAX] = acf_info->prof_info.prof_name_list;
+
+	for (i = 0; i < count; i++) {
+		if (!strncmp(prof_name_list[i], prof_hdr->dev_profile_str, AW_PROFILE_STR_MAX)) {
+			*prof_desc = &(acf_info->prof_info.prof_desc[i]);
+			return 0;
+		}
+	}
+
+	if (i == count)
+		AW_DEV_LOGE(dev, "not find prof_id and prof_name in list");
+
+	return -EINVAL;
+}
+
+static int aw_parse_data_by_sec_type_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info,
+				struct aw_acf_dde_v_1_0_0_0 *prof_hdr)
+{
+	int ret = -1;
+	char *cfg_data = acf_info->fw_data + prof_hdr->data_offset;
+	struct aw_prof_desc *prof_desc = NULL;
+
+	ret = aw_search_prof_index_from_list_v_1_0_0_0(dev, acf_info, &prof_desc, prof_hdr);
+	if (ret < 0)
+		return ret;
+
+	switch (prof_hdr->data_type) {
+	case AW_BIN_TYPE_REG:
+		snprintf(prof_desc->dev_name, sizeof(prof_hdr->dev_name),
+			"%s", prof_hdr->dev_name);
+		AW_DEV_LOGI(dev, "parse reg type data enter,product=[%s],prof_id=[%d],prof_name=[%s]",
+			prof_hdr->dev_name, prof_hdr->dev_profile,
+			prof_hdr->dev_profile_str);
+		prof_desc->prof_name = prof_hdr->dev_profile_str;
+		ret =  aw_parse_raw_reg(dev, cfg_data, prof_hdr->data_size,
+					prof_desc);
+		break;
+	case AW_BIN_TYPE_HDR_REG:
+		snprintf(prof_desc->dev_name, sizeof(prof_hdr->dev_name),
+			"%s", prof_hdr->dev_name);
+		AW_DEV_LOGI(dev, "parse hdr_reg type data enter,product=[%s],prof_id=[%d],prof_name=[%s]",
+			prof_hdr->dev_name, prof_hdr->dev_profile,
+			prof_hdr->dev_profile_str);
+		prof_desc->prof_name = prof_hdr->dev_profile_str;
+		ret = aw_parse_reg_with_hdr(dev, cfg_data,
+				prof_hdr->data_size, prof_desc);
+		break;
+	}
+
+	return ret;
+}
+
+static int aw_parse_dev_type_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info)
+{
+	int i = 0;
+	int ret;
+	int parse_prof_count = 0;
+	char *cfg_data = NULL;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde =
+		(struct aw_acf_dde_v_1_0_0_0 *)(acf_info->fw_data + acf_info->acf_hdr.ddt_offset);
+
+	AW_DEV_LOGD(dev, "enter");
+
+	for (i = 0; i < acf_info->acf_hdr.dde_num; i++) {
+		if ((acf_dde[i].type == AW_DDE_DEV_TYPE_ID) &&
+		(acf_info->aw_dev->i2c_bus == acf_dde[i].dev_bus) &&
+		(acf_info->aw_dev->i2c_addr == acf_dde[i].dev_addr) &&
+		(acf_info->aw_dev->chipid == acf_dde[i].chip_id)) {
+			ret = aw_check_product_name_v_1_0_0_0(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			if (acf_dde[i].data_type == AW_MONITOR) {
+				cfg_data = acf_info->fw_data + acf_dde[i].data_offset;
+				AW_DEV_LOGD(dev, "parse monitor type data enter");
+				ret = aw_parse_monitor_config(dev, cfg_data,
+					acf_dde[i].data_size);
+			} else {
+				ret = aw_parse_data_by_sec_type_v_1_0_0_0(dev, acf_info,
+					&acf_dde[i]);
+				if (ret < 0)
+					AW_DEV_LOGE(dev, "parse dev type data failed");
+				else
+					parse_prof_count++;
+			}
+		}
+	}
+
+	if (parse_prof_count == 0) {
+		AW_DEV_LOGE(dev, "get dev type num is %d, parse failed", parse_prof_count);
+		return -EINVAL;
+	}
+
+	return AW_DEV_TYPE_OK;
+}
+
+static int aw_parse_default_type_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info)
+{
+	int i = 0;
+	int ret;
+	int parse_prof_count = 0;
+	char *cfg_data = NULL;
+	struct aw_acf_dde_v_1_0_0_0 *acf_dde =
+		(struct aw_acf_dde_v_1_0_0_0 *)(acf_info->fw_data + acf_info->acf_hdr.ddt_offset);
+
+	AW_DEV_LOGD(dev, "enter");
+
+	for (i = 0; i < acf_info->acf_hdr.dde_num; i++) {
+		if ((acf_dde[i].type == AW_DDE_DEV_DEFAULT_TYPE_ID) &&
+		(acf_info->dev_index == acf_dde[i].dev_index) &&
+		(acf_info->aw_dev->chipid == acf_dde[i].chip_id)) {
+			ret = aw_check_product_name_v_1_0_0_0(dev, acf_info, &acf_dde[i]);
+			if (ret < 0)
+				continue;
+
+			if (acf_dde[i].data_type == AW_MONITOR) {
+				cfg_data = acf_info->fw_data + acf_dde[i].data_offset;
+				AW_DEV_LOGD(dev, "parse monitor type data enter");
+				ret = aw_parse_monitor_config(dev, cfg_data,
+					acf_dde[i].data_size);
+			} else {
+				ret = aw_parse_data_by_sec_type_v_1_0_0_0(dev, acf_info,
+					&acf_dde[i]);
+				if (ret < 0)
+					AW_DEV_LOGE(dev, "parse default type data failed");
+				else
+					parse_prof_count++;
+			}
+		}
+	}
+
+	if (parse_prof_count == 0) {
+		AW_DEV_LOGE(dev, "get default type num is %d,parse failed", parse_prof_count);
+		return -EINVAL;
+	}
+
+	return AW_DEV_TYPE_OK;
+}
+
+static int aw_parse_by_hdr_v_1_0_0_0(struct device *dev,
+				struct acf_bin_info *acf_info)
+{
+	int ret;
+
+	if (acf_info->prof_info.prof_type == AW_DDE_DEV_TYPE_ID) {
+		ret = aw_parse_dev_type_v_1_0_0_0(dev, acf_info);
+		if (ret < 0)
+			return ret;
+	} else if (acf_info->prof_info.prof_type == AW_DDE_DEV_DEFAULT_TYPE_ID) {
+		ret = aw_parse_default_type_v_1_0_0_0(dev, acf_info);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int aw_set_prof_off_info_v_1_0_0_0(struct device *dev,
+						struct acf_bin_info *acf_info)
+{
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+	int i = 0;
+	int ret = 0;
+
+	for (i = 0; i < prof_info->count; ++i) {
+		if (!(prof_info->prof_desc[i].prof_st)) {
+			snprintf(prof_info->prof_name_list[i], AW_PROFILE_STR_MAX, "%s",
+					g_power_off_name[0]);
+			prof_info->prof_desc[i].prof_name = prof_info->prof_name_list[i];
+			prof_info->prof_desc[i].prof_st = AW_PROFILE_WAIT;
+			memset(&prof_info->prof_desc[i].data_container, 0,
+					sizeof(struct aw_data_container));
+			return 0;
+		}
+
+		ret = aw_check_prof_str_is_off(prof_info->prof_name_list[i]);
+		if (ret == 0) {
+			AW_DEV_LOGD(dev, "found profile off,data_len=[%d]",
+				prof_info->prof_desc[i].data_container.len);
+			return 0;
+		}
+	}
+
+	AW_DEV_LOGE(dev, "index[%d] is out of table,profile count[%d]",
+		i, prof_info->count);
+	return -EINVAL;
+}
+
+static int aw_parse_acf_v_1_0_0_0(struct device *dev,
+		struct acf_bin_info *acf_info)
+
+{
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+	int ret;
+
+	ret = aw_parse_get_profile_count_v_1_0_0_0(dev, acf_info);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "get profile count failed");
+		return ret;
+	}
+
+	ret = aw_parse_prof_name_v_1_0_0_0(dev, acf_info);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "get profile count failed");
+		return ret;
+	}
+
+	acf_info->prof_info.prof_desc = devm_kzalloc(dev,
+		prof_info->count * sizeof(struct aw_prof_desc), GFP_KERNEL);
+	if (acf_info->prof_info.prof_desc == NULL) {
+		AW_DEV_LOGE(dev, "prof_desc devm_kzalloc failed");
+		return -ENOMEM;
+	}
+
+	ret = aw_parse_by_hdr_v_1_0_0_0(dev, acf_info);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "parse data failed");
+		return ret;
+	}
+
+	ret = aw_set_prof_off_info_v_1_0_0_0(dev, acf_info);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "set profile off info failed");
+		return ret;
+	}
+
+	prof_info->status = AW_ACF_UPDATE;
+	AW_DEV_LOGI(dev, "acf paese succeed");
+	return 0;
+}
+
+
+/*************************************************************************
+ *
+ *acf parse API
+ *
+ *************************************************************************/
+void aw87xxx_acf_profile_free(struct device *dev, struct acf_bin_info *acf_info)
+{
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	prof_info->count = 0;
+	prof_info->status = AW_ACF_WAIT;
+	memset(&acf_info->acf_hdr, 0, sizeof(struct aw_acf_hdr));
+
+	if (prof_info->prof_desc) {
+		devm_kfree(dev, prof_info->prof_desc);
+		prof_info->prof_desc = NULL;
+	}
+
+	if (prof_info->prof_name_list) {
+		devm_kfree(dev, prof_info->prof_name_list);
+		prof_info->prof_name_list = NULL;
+	}
+
+	if (acf_info->fw_data) {
+		vfree(acf_info->fw_data);
+		acf_info->fw_data = NULL;
+	}
+}
+
+int aw87xxx_acf_parse(struct device *dev, struct acf_bin_info *acf_info)
+{
+	int ret = 0;
+
+	AW_DEV_LOGD(dev, "enter");
+	acf_info->prof_info.status = AW_ACF_WAIT;
+	ret = aw_check_acf_firmware(dev, acf_info->fw_data,
+					acf_info->fw_size);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "load firmware check failed");
+		return -EINVAL;
+	}
+
+	memcpy(&acf_info->acf_hdr, acf_info->fw_data,
+		sizeof(struct aw_acf_hdr));
+
+	switch (acf_info->acf_hdr.hdr_version) {
+	case AW_ACF_HDR_VER_0_0_0_1:
+		return aw_parse_acf_v_0_0_0_1(dev, acf_info);
+	case AW_ACF_HDR_VER_1_0_0_0:
+		return aw_parse_acf_v_1_0_0_0(dev, acf_info);
+	default:
+		AW_DEV_LOGE(dev, "unsupported hdr_version [0x%x]",
+			acf_info->acf_hdr.hdr_version);
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+struct aw_prof_desc *aw87xxx_acf_get_prof_desc_form_name(struct device *dev,
+			struct acf_bin_info *acf_info, char *profile_name)
+{
+	int i = 0;
+	struct aw_prof_desc *prof_desc = NULL;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	AW_DEV_LOGD(dev, "enter");
+
+	if (!acf_info->prof_info.status) {
+		AW_DEV_LOGE(dev, "profile_cfg not load");
+		return NULL;
+	}
+
+	for (i = 0; i < prof_info->count; i++) {
+		if (!strncmp(profile_name, prof_info->prof_desc[i].prof_name,
+				AW_PROFILE_STR_MAX)) {
+			prof_desc = &prof_info->prof_desc[i];
+			break;
+		}
+	}
+
+	if (i == prof_info->count) {
+		AW_DEV_LOGE(dev, "profile not found");
+		return NULL;
+	}
+
+	AW_DEV_LOGI(dev, "get prof desc down");
+	return prof_desc;
+}
+
+int aw87xxx_acf_get_prof_index_form_name(struct device *dev,
+			struct acf_bin_info *acf_info, char *profile_name)
+{
+	int i = 0;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	if (!acf_info->prof_info.status) {
+		AW_DEV_LOGE(dev, "profile_cfg not load");
+		return -EINVAL;
+	}
+
+	for (i = 0; i < prof_info->count; i++) {
+		if (!strncmp(profile_name, prof_info->prof_name_list[i],
+				AW_PROFILE_STR_MAX)) {
+			return i;
+		}
+	}
+
+	AW_DEV_LOGE(dev, "profile_index not found");
+	return -EINVAL;
+}
+
+char *aw87xxx_acf_get_prof_name_form_index(struct device *dev,
+			struct acf_bin_info *acf_info, int index)
+{
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	if (!acf_info->prof_info.status) {
+		AW_DEV_LOGE(dev, "profile_cfg not load");
+		return NULL;
+	}
+
+	if (index >= prof_info->count  || index < 0) {
+		AW_DEV_LOGE(dev, "profile_index out of table");
+		return NULL;
+	}
+
+	return prof_info->prof_desc[index].prof_name;
+}
+
+
+int aw87xxx_acf_get_profile_count(struct device *dev,
+			struct acf_bin_info *acf_info)
+{
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	if (!acf_info->prof_info.status) {
+		AW_DEV_LOGE(dev, "profile_cfg not load");
+		return -EINVAL;
+	}
+
+	if (prof_info->count > 0) {
+		return prof_info->count;
+	}
+
+	return -EINVAL;
+}
+
+char *aw87xxx_acf_get_prof_off_name(struct device *dev,
+			struct acf_bin_info *acf_info)
+{
+	int i = 0;
+	int ret = 0;
+	struct aw_prof_info *prof_info = &acf_info->prof_info;
+
+	if (!acf_info->prof_info.status) {
+		AW_DEV_LOGE(dev, "profile_cfg not load");
+		return NULL;
+	}
+
+	for (i = 0; i < prof_info->count; i++) {
+		ret  = aw_check_prof_str_is_off(prof_info->prof_name_list[i]);
+		if (ret == 0)
+			return prof_info->prof_name_list[i];
+	}
+
+	return NULL;
+}
+
+void aw87xxx_acf_init(struct aw_device *aw_dev, struct acf_bin_info *acf_info, int index)
+{
+
+	acf_info->load_count = 0;
+	acf_info->prof_info.status = AW_ACF_WAIT;
+	acf_info->dev_index = index;
+	acf_info->aw_dev = aw_dev;
+	acf_info->product_cnt = aw_dev->product_cnt;
+	acf_info->product_tab = aw_dev->product_tab;
+	acf_info->prof_info.prof_desc = NULL;
+	acf_info->fw_data = NULL;
+	acf_info->fw_size = 0;
+}
+
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.h b/sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.h
new file mode 100644
index 000000000000..ebe0c77f5674
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_acf_bin.h
@@ -0,0 +1,191 @@
+#ifndef __AW87XXX_ACF_BIN_H__
+#define __AW87XXX_ACF_BIN_H__
+
+#include "aw87xxx_device.h"
+
+#define AW_PROJECT_NAME_MAX		(24)
+#define AW_CUSTOMER_NAME_MAX		(16)
+#define AW_CFG_VERSION_MAX		(4)
+#define AW_TBL_VERSION_MAX		(4)
+#define AW_DDE_DEVICE_TYPE		(0)
+#define AW_DDE_SKT_TYPE			(1)
+#define AW_DDE_DEFAULT_TYPE		(2)
+
+#define AW_REG_ADDR_BYTE		(1)
+#define AW_REG_DATA_BYTE		(1)
+
+#define AW_ACF_FILE_ID			(0xa15f908)
+#define AW_PROFILE_STR_MAX 		(32)
+#define AW_POWER_OFF_NAME_SUPPORT_COUNT	(5)
+
+enum aw_cfg_hdr_version {
+	AW_ACF_HDR_VER_0_0_0_1 = 0x00000001,
+	AW_ACF_HDR_VER_1_0_0_0 = 0x01000000,
+};
+
+enum aw_acf_dde_type_id {
+	AW_DEV_NONE_TYPE_ID = 0xFFFFFFFF,
+	AW_DDE_DEV_TYPE_ID = 0x00000000,
+	AW_DDE_SKT_TYPE_ID = 0x00000001,
+	AW_DDE_DEV_DEFAULT_TYPE_ID = 0x00000002,
+	AW_DDE_TYPE_MAX,
+};
+
+enum aw_raw_data_type_id {
+	AW_BIN_TYPE_REG = 0x00000000,
+	AW_BIN_TYPE_DSP,
+	AW_BIN_TYPE_DSP_CFG,
+	AW_BIN_TYPE_DSP_FW,
+	AW_BIN_TYPE_HDR_REG,
+	AW_BIN_TYPE_HDR_DSP_CFG,
+	AW_BIN_TYPE_HDR_DSP_FW,
+	AW_BIN_TYPE_MUTLBIN,
+	AW_SKT_UI_PROJECT,
+	AW_DSP_CFG,
+	AW_MONITOR,
+	AW_BIN_TYPE_MAX,
+};
+
+enum {
+	AW_DEV_TYPE_OK = 0,
+	AW_DEV_TYPE_NONE = 1,
+};
+
+enum aw_profile_status {
+	AW_PROFILE_WAIT = 0,
+	AW_PROFILE_OK,
+};
+
+enum aw_acf_load_status {
+	AW_ACF_WAIT = 0,
+	AW_ACF_UPDATE,
+};
+
+enum aw_bin_dev_profile_id {
+	AW_PROFILE_MUSIC = 0x0000,
+	AW_PROFILE_VOICE,
+	AW_PROFILE_VOIP,
+	AW_PROFILE_RINGTONE,
+	AW_PROFILE_RINGTONE_HS,
+	AW_PROFILE_LOWPOWER,
+	AW_PROFILE_BYPASS,
+	AW_PROFILE_MMI,
+	AW_PROFILE_FM,
+	AW_PROFILE_NOTIFICATION,
+	AW_PROFILE_RECEIVER,
+	AW_PROFILE_OFF,
+	AW_PROFILE_MAX,
+};
+
+struct aw_acf_hdr {
+	int32_t a_id;				/* acf file ID 0xa15f908 */
+	char project[AW_PROJECT_NAME_MAX];	/* project name */
+	char custom[AW_CUSTOMER_NAME_MAX];	/* custom name :huawei xiaomi vivo oppo */
+	uint8_t version[AW_CFG_VERSION_MAX];	/* author update version */
+	int32_t author_id;			/* author id */
+	int32_t ddt_size;			/* sub section table entry size */
+	int32_t dde_num;			/* sub section table entry num */
+	int32_t ddt_offset;			/* sub section table offset in file */
+	int32_t hdr_version;			/* sub section table version */
+	int32_t reserve[3];			/* Reserved Bits */
+};
+
+struct aw_acf_dde {
+	int32_t type;				/* dde type id */
+	char dev_name[AW_CUSTOMER_NAME_MAX];	/* customer dev name */
+	int16_t dev_index;			/* dev id */
+	int16_t dev_bus;			/* dev bus id */
+	int16_t dev_addr;			/* dev addr id */
+	int16_t dev_profile;			/* dev profile id */
+	int32_t data_type;			/* data type id */
+	int32_t data_size;			/* dde data size in block */
+	int32_t data_offset;			/* dde data offset in block */
+	int32_t data_crc;			/* dde data crc checkout */
+	int32_t reserve[5];			/* Reserved Bits */
+};
+
+struct aw_acf_dde_v_1_0_0_0 {
+	uint32_t type;				/* DDE type id */
+	char dev_name[AW_CUSTOMER_NAME_MAX];	/* customer dev name */
+	uint16_t dev_index;			/* dev id */
+	uint16_t dev_bus;			/* dev bus id */
+	uint16_t dev_addr;			/* dev addr id */
+	uint16_t dev_profile;			/* dev profile id*/
+	uint32_t data_type;			/* data type id */
+	uint32_t data_size;			/* dde data size in block */
+	uint32_t data_offset;			/* dde data offset in block */
+	uint32_t data_crc;			/* dde data crc checkout */
+	char dev_profile_str[AW_PROFILE_STR_MAX];	/* dde custom profile name */
+	uint32_t chip_id;			/* dde custom product chip id */
+	uint32_t reserve[4];
+};
+
+struct aw_data_with_header {
+	uint32_t check_sum;
+	uint32_t header_ver;
+	uint32_t bin_data_type;
+	uint32_t bin_data_ver;
+	uint32_t bin_data_size;
+	uint32_t ui_ver;
+	char product[8];
+	uint32_t addr_byte_len;
+	uint32_t data_byte_len;
+	uint32_t device_addr;
+	uint32_t reserve[4];
+};
+
+struct aw_data_container {
+	uint32_t len;
+	uint8_t *data;
+};
+
+struct aw_prof_desc {
+	uint32_t prof_st;
+	char *prof_name;
+	char dev_name[AW_CUSTOMER_NAME_MAX];
+	struct aw_data_container data_container;
+};
+
+struct aw_all_prof_info {
+	struct aw_prof_desc prof_desc[AW_PROFILE_MAX];
+};
+
+struct aw_prof_info {
+	int count;
+	int status;
+	int prof_type;
+	char (*prof_name_list)[AW_PROFILE_STR_MAX];
+	struct aw_prof_desc *prof_desc;
+};
+
+struct acf_bin_info {
+	int load_count;
+	int fw_size;
+	int16_t dev_index;
+	char *fw_data;
+	int product_cnt;
+	const char **product_tab;
+	struct aw_device *aw_dev;
+
+	struct aw_acf_hdr acf_hdr;
+	struct aw_prof_info prof_info;
+};
+
+char *aw87xxx_ctos_get_prof_name(int profile_id);
+void aw87xxx_acf_profile_free(struct device *dev,
+		struct acf_bin_info *acf_info);
+int aw87xxx_acf_parse(struct device *dev, struct acf_bin_info *acf_info);
+struct aw_prof_desc *aw87xxx_acf_get_prof_desc_form_name(struct device *dev,
+			struct acf_bin_info *acf_info, char *profile_name);
+int aw87xxx_acf_get_prof_index_form_name(struct device *dev,
+			struct acf_bin_info *acf_info, char *profile_name);
+char *aw87xxx_acf_get_prof_name_form_index(struct device *dev,
+			struct acf_bin_info *acf_info, int index);
+int aw87xxx_acf_get_profile_count(struct device *dev,
+			struct acf_bin_info *acf_info);
+char *aw87xxx_acf_get_prof_off_name(struct device *dev,
+			struct acf_bin_info *acf_info);
+void aw87xxx_acf_init(struct aw_device *aw_dev, struct acf_bin_info *acf_info, int index);
+
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.c b/sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.c
new file mode 100644
index 000000000000..7eab9efde147
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.c
@@ -0,0 +1,515 @@
+/*
+* aw87xxx_bin_parse.c
+*
+* Copyright (c) 2020 AWINIC Technology CO., LTD
+*
+* This program is free software; you can redistribute it and/or modify it
+* under the terms of the GNU General Public License as published by the
+* Free Software Foundation; either version 2 of the License, or (at your
+* option) any later version.
+*/
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/i2c.h>
+#include <linux/of_gpio.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/firmware.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <linux/input.h>
+#include <linux/interrupt.h>
+#include <linux/debugfs.h>
+#include <linux/miscdevice.h>
+#include <asm/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/hrtimer.h>
+#include <linux/mutex.h>
+#include <linux/cdev.h>
+#include <linux/list.h>
+#include <linux/string.h>
+#include "aw87xxx_bin_parse.h"
+
+#define AWINIC_CODE_VERSION "V0.0.7-V1.0.4"	/* "code version"-"excel version" */
+
+#define DEBUG_LOG_LEVEL
+#ifdef DEBUG_LOG_LEVEL
+#define DBG(fmt, arg...)   do {\
+printk("AWINIC_BIN %s,line= %d,"fmt, __func__, __LINE__, ##arg);\
+} while (0)
+#define DBG_ERR(fmt, arg...)   do {\
+printk("AWINIC_BIN_ERR %s,line= %d,"fmt, __func__, __LINE__, ##arg);\
+} while (0)
+#else
+#define DBG(fmt, arg...) do {} while (0)
+#define DBG_ERR(fmt, arg...) do {} while (0)
+#endif
+
+#define printing_data_code
+
+typedef unsigned short int aw_uint16;
+typedef unsigned long int aw_uint32;
+
+#define BigLittleSwap16(A)	((((aw_uint16)(A) & 0xff00) >> 8) | \
+				 (((aw_uint16)(A) & 0x00ff) << 8))
+
+#define BigLittleSwap32(A)	((((aw_uint32)(A) & 0xff000000) >> 24) | \
+				(((aw_uint32)(A) & 0x00ff0000) >> 8) | \
+				(((aw_uint32)(A) & 0x0000ff00) << 8) | \
+				(((aw_uint32)(A) & 0x000000ff) << 24))
+
+
+static int aw_parse_bin_header_1_0_0(struct aw_bin *bin);
+
+/**
+*
+* Interface function
+*
+* return value:
+*       value = 0 :success;
+*       value = -1 :check bin header version
+*       value = -2 :check bin data type
+*       value = -3 :check sum or check bin data len error
+*       value = -4 :check data version
+*       value = -5 :check register num
+*       value = -6 :check dsp reg num
+*       value = -7 :check soc app num
+*       value = -8 :bin is NULL point
+*
+**/
+
+/********************************************************
+*
+* check sum data
+*
+********************************************************/
+static int aw_check_sum(struct aw_bin *bin, int bin_num)
+{
+	unsigned int i = 0;
+	unsigned int sum_data = 0;
+	unsigned int check_sum = 0;
+	unsigned char *p_check_sum = NULL;
+
+	DBG("enter\n");
+
+	p_check_sum =
+	    &(bin->info.data[(bin->header_info[bin_num].valid_data_addr -
+			      bin->header_info[bin_num].header_len)]);
+	DBG("aw_bin_parse p_check_sum = %p\n", p_check_sum);
+	check_sum = GET_32_DATA(*(p_check_sum + 3),
+				*(p_check_sum + 2),
+				*(p_check_sum + 1), *(p_check_sum));
+
+	for (i = 4;
+	     i <
+	     bin->header_info[bin_num].bin_data_len +
+	     bin->header_info[bin_num].header_len; i++) {
+		sum_data += *(p_check_sum + i);
+	}
+	DBG("aw_bin_parse bin_num=%d, check_sum = 0x%x, sum_data = 0x%x\n",
+		bin_num, check_sum, sum_data);
+	if (sum_data != check_sum) {
+		p_check_sum = NULL;
+		DBG_ERR("aw_bin_parse check sum or check bin data len error\n");
+		DBG_ERR("aw_bin_parse bin_num=%d, check_sum = 0x%x, sum_data = 0x%x\n", bin_num, check_sum, sum_data);
+		return -3;
+	}
+	p_check_sum = NULL;
+
+	return 0;
+}
+
+static int aw_check_data_version(struct aw_bin *bin, int bin_num)
+{
+	int i = 0;
+	DBG("enter\n");
+
+	for (i = DATA_VERSION_V1; i < DATA_VERSION_MAX; i++) {
+		if (bin->header_info[bin_num].bin_data_ver == i) {
+			return 0;
+		}
+	}
+	DBG_ERR("aw_bin_parse Unrecognized this bin data version\n");
+	return -4;
+}
+
+static int aw_check_register_num_v1(struct aw_bin *bin, int bin_num)
+{
+	unsigned int check_register_num = 0;
+	unsigned int parse_register_num = 0;
+	unsigned char *p_check_sum = NULL;
+
+	DBG("enter\n");
+
+	p_check_sum =
+	    &(bin->info.data[(bin->header_info[bin_num].valid_data_addr)]);
+	DBG("aw_bin_parse p_check_sum = %p\n", p_check_sum);
+	parse_register_num = GET_32_DATA(*(p_check_sum + 3),
+					 *(p_check_sum + 2),
+					 *(p_check_sum + 1), *(p_check_sum));
+	check_register_num = (bin->header_info[bin_num].bin_data_len - 4) /
+	    (bin->header_info[bin_num].reg_byte_len +
+	     bin->header_info[bin_num].data_byte_len);
+	DBG
+	    ("aw_bin_parse bin_num=%d, parse_register_num = 0x%x, check_register_num = 0x%x\n",
+	     bin_num, parse_register_num, check_register_num);
+	if (parse_register_num != check_register_num) {
+		p_check_sum = NULL;
+		DBG_ERR("aw_bin_parse register num is error\n");
+		DBG_ERR("aw_bin_parse bin_num=%d, parse_register_num = 0x%x, check_register_num = 0x%x\n", bin_num, parse_register_num, check_register_num);
+		return -5;
+	}
+	bin->header_info[bin_num].reg_num = parse_register_num;
+	bin->header_info[bin_num].valid_data_len =
+	    bin->header_info[bin_num].bin_data_len - 4;
+	p_check_sum = NULL;
+	bin->header_info[bin_num].valid_data_addr =
+	    bin->header_info[bin_num].valid_data_addr + 4;
+	return 0;
+}
+
+static int aw_check_dsp_reg_num_v1(struct aw_bin *bin, int bin_num)
+{
+	unsigned int check_dsp_reg_num = 0;
+	unsigned int parse_dsp_reg_num = 0;
+	unsigned char *p_check_sum = NULL;
+
+	DBG("enter\n");
+
+	p_check_sum =
+	    &(bin->info.data[(bin->header_info[bin_num].valid_data_addr)]);
+	DBG("aw_bin_parse p_check_sum = %p\n", p_check_sum);
+	parse_dsp_reg_num = GET_32_DATA(*(p_check_sum + 7),
+					*(p_check_sum + 6),
+					*(p_check_sum + 5), *(p_check_sum + 4));
+	bin->header_info[bin_num].reg_data_byte_len =
+	    GET_32_DATA(*(p_check_sum + 11), *(p_check_sum + 10),
+			*(p_check_sum + 9), *(p_check_sum + 8));
+	check_dsp_reg_num =
+	    (bin->header_info[bin_num].bin_data_len -
+	     12) / bin->header_info[bin_num].reg_data_byte_len;
+	DBG
+	    ("aw_bin_parse bin_num=%d, parse_dsp_reg_num = 0x%x, check_dsp_reg_num = 0x%x\n",
+	     bin_num, parse_dsp_reg_num, check_dsp_reg_num);
+	if (parse_dsp_reg_num != check_dsp_reg_num) {
+		p_check_sum = NULL;
+		DBG_ERR("aw_bin_parse dsp reg num is error\n");
+		DBG_ERR("aw_bin_parse bin_num=%d, parse_dsp_reg_num = 0x%x, check_dsp_reg_num = 0x%x\n", bin_num, parse_dsp_reg_num, check_dsp_reg_num);
+		return -6;
+	}
+	bin->header_info[bin_num].download_addr =
+	    GET_32_DATA(*(p_check_sum + 3), *(p_check_sum + 2),
+			*(p_check_sum + 1), *(p_check_sum));
+	bin->header_info[bin_num].reg_num = parse_dsp_reg_num;
+	bin->header_info[bin_num].valid_data_len =
+	    bin->header_info[bin_num].bin_data_len - 12;
+	p_check_sum = NULL;
+	bin->header_info[bin_num].valid_data_addr =
+	    bin->header_info[bin_num].valid_data_addr + 12;
+	return 0;
+}
+
+static int aw_check_soc_app_num_v1(struct aw_bin *bin, int bin_num)
+{
+	unsigned int check_soc_app_num = 0;
+	unsigned int parse_soc_app_num = 0;
+	unsigned char *p_check_sum = NULL;
+
+	DBG("enter\n");
+
+	p_check_sum =
+	    &(bin->info.data[(bin->header_info[bin_num].valid_data_addr)]);
+	DBG("aw_bin_parse p_check_sum = %p\n", p_check_sum);
+	bin->header_info[bin_num].app_version = GET_32_DATA(*(p_check_sum + 3),
+							    *(p_check_sum + 2),
+							    *(p_check_sum + 1),
+							    *(p_check_sum));
+	parse_soc_app_num = GET_32_DATA(*(p_check_sum + 11),
+					*(p_check_sum + 10),
+					*(p_check_sum + 9), *(p_check_sum + 8));
+	check_soc_app_num = bin->header_info[bin_num].bin_data_len - 12;
+	DBG
+	    ("aw_bin_parse bin_num=%d, parse_soc_app_num = 0x%x, check_soc_app_num = 0x%x\n",
+	     bin_num, parse_soc_app_num, check_soc_app_num);
+	if (parse_soc_app_num != check_soc_app_num) {
+		p_check_sum = NULL;
+		DBG_ERR("aw_bin_parse soc app num is error\n");
+		DBG_ERR("aw_bin_parse bin_num=%d, parse_soc_app_num = 0x%x, check_soc_app_num = 0x%x\n", bin_num, parse_soc_app_num, check_soc_app_num);
+		return -7;
+	}
+	bin->header_info[bin_num].reg_num = parse_soc_app_num;
+	bin->header_info[bin_num].download_addr =
+	    GET_32_DATA(*(p_check_sum + 7), *(p_check_sum + 6),
+			*(p_check_sum + 5), *(p_check_sum + 4));
+	bin->header_info[bin_num].valid_data_len =
+	    bin->header_info[bin_num].bin_data_len - 12;
+	p_check_sum = NULL;
+	bin->header_info[bin_num].valid_data_addr =
+	    bin->header_info[bin_num].valid_data_addr + 12;
+	return 0;
+}
+
+/************************
+***
+***bin header 1_0_0
+***
+************************/
+static void aw_get_single_bin_header_1_0_0(struct aw_bin *bin)
+{
+	int i;
+	DBG("enter %s\n", __func__);
+	bin->header_info[bin->all_bin_parse_num].header_len = 60;
+	bin->header_info[bin->all_bin_parse_num].check_sum =
+	    GET_32_DATA(*(bin->p_addr + 3), *(bin->p_addr + 2),
+			*(bin->p_addr + 1), *(bin->p_addr));
+	bin->header_info[bin->all_bin_parse_num].header_ver =
+	    GET_32_DATA(*(bin->p_addr + 7), *(bin->p_addr + 6),
+			*(bin->p_addr + 5), *(bin->p_addr + 4));
+	bin->header_info[bin->all_bin_parse_num].bin_data_type =
+	    GET_32_DATA(*(bin->p_addr + 11), *(bin->p_addr + 10),
+			*(bin->p_addr + 9), *(bin->p_addr + 8));
+	bin->header_info[bin->all_bin_parse_num].bin_data_ver =
+	    GET_32_DATA(*(bin->p_addr + 15), *(bin->p_addr + 14),
+			*(bin->p_addr + 13), *(bin->p_addr + 12));
+	bin->header_info[bin->all_bin_parse_num].bin_data_len =
+	    GET_32_DATA(*(bin->p_addr + 19), *(bin->p_addr + 18),
+			*(bin->p_addr + 17), *(bin->p_addr + 16));
+	bin->header_info[bin->all_bin_parse_num].ui_ver =
+	    GET_32_DATA(*(bin->p_addr + 23), *(bin->p_addr + 22),
+			*(bin->p_addr + 21), *(bin->p_addr + 20));
+	bin->header_info[bin->all_bin_parse_num].reg_byte_len =
+	    GET_32_DATA(*(bin->p_addr + 35), *(bin->p_addr + 34),
+			*(bin->p_addr + 33), *(bin->p_addr + 32));
+	bin->header_info[bin->all_bin_parse_num].data_byte_len =
+	    GET_32_DATA(*(bin->p_addr + 39), *(bin->p_addr + 38),
+			*(bin->p_addr + 37), *(bin->p_addr + 36));
+	bin->header_info[bin->all_bin_parse_num].device_addr =
+	    GET_32_DATA(*(bin->p_addr + 43), *(bin->p_addr + 42),
+			*(bin->p_addr + 41), *(bin->p_addr + 40));
+	for (i = 0; i < 8; i++) {
+		bin->header_info[bin->all_bin_parse_num].chip_type[i] =
+		    *(bin->p_addr + 24 + i);
+	}
+	bin->header_info[bin->all_bin_parse_num].reg_num = 0x00000000;
+	bin->header_info[bin->all_bin_parse_num].reg_data_byte_len = 0x00000000;
+	bin->header_info[bin->all_bin_parse_num].download_addr = 0x00000000;
+	bin->header_info[bin->all_bin_parse_num].app_version = 0x00000000;
+	bin->header_info[bin->all_bin_parse_num].valid_data_len = 0x00000000;
+	bin->all_bin_parse_num += 1;
+}
+
+static int aw_parse_each_of_multi_bins_1_0_0(unsigned int bin_num, int bin_serial_num,
+				      struct aw_bin *bin)
+{
+	int ret = 0;
+	unsigned int bin_start_addr = 0;
+	unsigned int valid_data_len = 0;
+	DBG("aw_bin_parse enter multi bin branch -- %s\n", __func__);
+	if (!bin_serial_num) {
+		bin_start_addr = GET_32_DATA(*(bin->p_addr + 67),
+					     *(bin->p_addr + 66),
+					     *(bin->p_addr + 65),
+					     *(bin->p_addr + 64));
+		bin->p_addr += (60 + bin_start_addr);
+		bin->header_info[bin->all_bin_parse_num].valid_data_addr =
+		    bin->header_info[bin->all_bin_parse_num -
+				     1].valid_data_addr + 4 + 8 * bin_num + 60;
+	} else {
+		valid_data_len =
+		    bin->header_info[bin->all_bin_parse_num - 1].bin_data_len;
+		bin->p_addr += (60 + valid_data_len);
+		bin->header_info[bin->all_bin_parse_num].valid_data_addr =
+		    bin->header_info[bin->all_bin_parse_num -
+				     1].valid_data_addr +
+		    bin->header_info[bin->all_bin_parse_num - 1].bin_data_len +
+		    60;
+	}
+
+	ret = aw_parse_bin_header_1_0_0(bin);
+	return ret;
+}
+
+/* Get the number of bins in multi bins, and set a for loop, loop processing each bin data */
+static int aw_get_multi_bin_header_1_0_0(struct aw_bin *bin)
+{
+	int i = 0;
+	int ret = 0;
+	unsigned int bin_num = 0;
+	DBG("aw_bin_parse enter multi bin branch -- %s\n", __func__);
+	bin_num = GET_32_DATA(*(bin->p_addr + 63),
+			      *(bin->p_addr + 62),
+			      *(bin->p_addr + 61), *(bin->p_addr + 60));
+	if (bin->multi_bin_parse_num == 1) {
+		bin->header_info[bin->all_bin_parse_num].valid_data_addr = 60;
+	}
+	aw_get_single_bin_header_1_0_0(bin);
+
+	for (i = 0; i < bin_num; i++) {
+		DBG("aw_bin_parse enter multi bin for is %d\n", i);
+		ret = aw_parse_each_of_multi_bins_1_0_0(bin_num, i, bin);
+		if (ret < 0) {
+			return ret;
+		}
+	}
+	return 0;
+}
+
+/********************************************************
+*
+* If the bin framework header version is 1.0.0,
+  determine the data type of bin, and then perform different processing
+  according to the data type
+  If it is a single bin data type, write the data directly into the structure array
+  If it is a multi-bin data type, first obtain the number of bins,
+  and then recursively call the bin frame header processing function
+  according to the bin number to process the frame header information of each bin separately
+*
+********************************************************/
+static int aw_parse_bin_header_1_0_0(struct aw_bin *bin)
+{
+	int ret = 0;
+	unsigned int bin_data_type;
+	DBG("enter %s\n", __func__);
+	bin_data_type = GET_32_DATA(*(bin->p_addr + 11),
+				    *(bin->p_addr + 10),
+				    *(bin->p_addr + 9), *(bin->p_addr + 8));
+	DBG("aw_bin_parse bin_data_type 0x%x\n", bin_data_type);
+	switch (bin_data_type) {
+	case DATA_TYPE_REGISTER:
+	case DATA_TYPE_DSP_REG:
+	case DATA_TYPE_SOC_APP:
+		/* Divided into two processing methods,
+		   one is single bin processing,
+		   and the other is single bin processing in multi bin */
+		DBG("aw_bin_parse enter single bin branch\n");
+		bin->single_bin_parse_num += 1;
+		DBG("%s bin->single_bin_parse_num is %d\n", __func__,
+			bin->single_bin_parse_num);
+		if (!bin->multi_bin_parse_num) {
+			bin->header_info[bin->
+					 all_bin_parse_num].valid_data_addr =
+			    60;
+		}
+		aw_get_single_bin_header_1_0_0(bin);
+		break;
+	case DATA_TYPE_MULTI_BINS:
+		/* Get the number of times to enter multi bins */
+		DBG("aw_bin_parse enter multi bin branch\n");
+		bin->multi_bin_parse_num += 1;
+		DBG("%s bin->multi_bin_parse_num is %d\n", __func__,
+			bin->multi_bin_parse_num);
+		ret = aw_get_multi_bin_header_1_0_0(bin);
+		if (ret < 0) {
+			return ret;
+		}
+		break;
+	default:
+		DBG_ERR("aw_bin_parse Unrecognized this bin data type\n");
+		return -2;
+	}
+	return 0;
+}
+
+/* get the bin's header version */
+static int aw_check_bin_header_version(struct aw_bin *bin)
+{
+	int ret = 0;
+	unsigned int header_version = 0;
+
+	header_version = GET_32_DATA(*(bin->p_addr + 7),
+				     *(bin->p_addr + 6),
+				     *(bin->p_addr + 5), *(bin->p_addr + 4));
+
+	DBG("aw_bin_parse header_version 0x%x\n", header_version);
+
+	/* Write data to the corresponding structure array
+	   according to different formats of the bin frame header version */
+	switch (header_version) {
+	case HEADER_VERSION_1_0_0:
+		ret = aw_parse_bin_header_1_0_0(bin);
+		return ret;
+	default:
+		DBG_ERR("aw_bin_parse Unrecognized this bin header version \n");
+		return -1;
+	}
+}
+
+int aw87xxx_parsing_bin_file(struct aw_bin *bin)
+{
+	int i = 0;
+	int ret = 0;
+
+	DBG("aw_bin_parse code version:%s\n", AWINIC_CODE_VERSION);
+	if (!bin) {
+		DBG_ERR("aw_bin_parse bin is NULL\n");
+		return -8;
+	}
+	bin->p_addr = bin->info.data;
+	bin->all_bin_parse_num = 0;
+	bin->multi_bin_parse_num = 0;
+	bin->single_bin_parse_num = 0;
+
+	/* filling bins header info */
+	ret = aw_check_bin_header_version(bin);
+	if (ret < 0) {
+		DBG_ERR("aw_bin_parse check bin header version error\n");
+		return ret;
+	}
+	bin->p_addr = NULL;
+
+	/* check bin header info */
+	for (i = 0; i < bin->all_bin_parse_num; i++) {
+		/* check sum */
+		ret = aw_check_sum(bin, i);
+		if (ret < 0) {
+			DBG_ERR("aw_bin_parse check sum data error\n");
+			return ret;
+		}
+		/* check bin data version */
+		ret = aw_check_data_version(bin, i);
+		if (ret < 0) {
+			DBG_ERR("aw_bin_parse check data version error\n");
+			return ret;
+		}
+		/* check valid data */
+		if (bin->header_info[i].bin_data_ver == DATA_VERSION_V1) {
+			/* check register num */
+			if (bin->header_info[i].bin_data_type ==
+			    DATA_TYPE_REGISTER) {
+				ret = aw_check_register_num_v1(bin, i);
+				if (ret < 0) {
+					DBG_ERR
+					    ("aw_bin_parse check register num error\n");
+					return ret;
+				}
+				/* check dsp reg num */
+			} else if (bin->header_info[i].bin_data_type ==
+				   DATA_TYPE_DSP_REG) {
+				ret = aw_check_dsp_reg_num_v1(bin, i);
+				if (ret < 0) {
+					DBG_ERR
+					    ("aw_bin_parse check dsp reg num error\n");
+					return ret;
+				}
+				/* check soc app num */
+			} else if (bin->header_info[i].bin_data_type ==
+				   DATA_TYPE_SOC_APP) {
+				ret = aw_check_soc_app_num_v1(bin, i);
+				if (ret < 0) {
+					DBG_ERR
+					    ("aw_bin_parse check soc app num error\n");
+					return ret;
+				}
+			} else {
+				bin->header_info[i].valid_data_len =
+				    bin->header_info[i].bin_data_len;
+			}
+		}
+	}
+	DBG("aw_bin_parse parsing success\n");
+
+	return 0;
+}
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.h b/sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.h
new file mode 100644
index 000000000000..a99c2409e613
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_bin_parse.h
@@ -0,0 +1,73 @@
+#ifndef __AW87XXX_BIN_PARSE_H__
+#define __AW87XXX_BIN_PARSE_H__
+
+#define NULL    ((void *)0)
+#define GET_32_DATA(w, x, y, z) ((unsigned int)(((w) << 24) | ((x) << 16) | ((y) << 8) | (z)))
+#define BIN_NUM_MAX   100
+#define HEADER_LEN    60
+/*********************************************************
+ *
+ * header information
+ *
+ ********************************************************/
+enum bin_header_version_enum {
+	HEADER_VERSION_1_0_0 = 0x01000000,
+};
+
+enum data_type_enum {
+	DATA_TYPE_REGISTER = 0x00000000,
+	DATA_TYPE_DSP_REG = 0x00000010,
+	DATA_TYPE_DSP_CFG = 0x00000011,
+	DATA_TYPE_SOC_REG = 0x00000020,
+	DATA_TYPE_SOC_APP = 0x00000021,
+	DATA_TYPE_MULTI_BINS = 0x00002000,
+	DATA_TYPE_MONITOR_ANALOG = 0x00020000,
+};
+
+enum data_version_enum {
+	DATA_VERSION_V1 = 0X00000001,	/*default little edian */
+	DATA_VERSION_MAX,
+};
+
+struct bin_header_info {
+	unsigned int header_len; /* Frame header length */
+	unsigned int check_sum; /* Frame header information-Checksum */
+	unsigned int header_ver; /* Frame header information-Frame header version */
+	unsigned int bin_data_type; /* Frame header information-Data type */
+	unsigned int bin_data_ver; /* Frame header information-Data version */
+	unsigned int bin_data_len; /* Frame header information-Data length */
+	unsigned int ui_ver; /* Frame header information-ui version */
+	unsigned char chip_type[8]; /* Frame header information-chip type */
+	unsigned int reg_byte_len; /* Frame header information-reg byte len */
+	unsigned int data_byte_len; /* Frame header information-data byte len */
+	unsigned int device_addr; /* Frame header information-device addr */
+	unsigned int valid_data_len; /* Length of valid data obtained after parsing */
+	unsigned int valid_data_addr; /* The offset address of the valid data obtained after parsing relative to info */
+
+	unsigned int reg_num; /* The number of registers obtained after parsing */
+	unsigned int reg_data_byte_len; /* The byte length of the register obtained after parsing */
+	unsigned int download_addr; /* The starting address or download address obtained after parsing */
+	unsigned int app_version; /* The software version number obtained after parsing */
+};
+
+/************************************************************
+*
+* function define
+*
+************************************************************/
+struct bin_container {
+	unsigned int len; /* The size of the bin file obtained from the firmware */
+	unsigned char data[]; /* Store the bin file obtained from the firmware */
+};
+
+struct aw_bin {
+	unsigned char *p_addr; /* Offset pointer (backward offset pointer to obtain frame header information and important information) */
+	unsigned int all_bin_parse_num; /* The number of all bin files */
+	unsigned int multi_bin_parse_num; /* The number of single bin files */
+	unsigned int single_bin_parse_num; /* The number of multiple bin files */
+	struct bin_header_info header_info[BIN_NUM_MAX]; /* Frame header information and other important data obtained after parsing */
+	struct bin_container info; /* Obtained bin file data that needs to be parsed */
+};
+
+extern int aw87xxx_parsing_bin_file(struct aw_bin *bin);
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_device.c b/sound/soc/codecs/aw87xxx/aw87xxx_device.c
new file mode 100644
index 000000000000..a4c9ad7d96dc
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_device.c
@@ -0,0 +1,977 @@
+/*
+ * aw87xxx_device.c  aw87xxx pa module
+ *
+ * Copyright (c) 2021 AWINIC Technology CO., LTD
+ *
+ * Author: Barry <zhaozhongbo@awinic.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ */
+
+#include <linux/i2c.h>
+#include <linux/gpio.h>
+#include <linux/of_gpio.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/irq.h>
+#include <linux/io.h>
+#include <linux/init.h>
+#include <linux/timer.h>
+#include "aw87xxx.h"
+#include "aw87xxx_device.h"
+#include "aw87xxx_log.h"
+#include "aw87xxx_pid_9b_reg.h"
+#include "aw87xxx_pid_18_reg.h"
+#include "aw87xxx_pid_39_reg.h"
+#include "aw87xxx_pid_59_3x9_reg.h"
+#include "aw87xxx_pid_59_5x9_reg.h"
+#include "aw87xxx_pid_5a_reg.h"
+#include "aw87xxx_pid_76_reg.h"
+#include "aw87xxx_pid_60_reg.h"
+
+/*************************************************************************
+ * aw87xxx variable
+ ************************************************************************/
+const char *g_aw_pid_9b_product[] = {
+	"aw87319",
+};
+const char *g_aw_pid_18_product[] = {
+	"aw87418",
+};
+
+const char *g_aw_pid_39_product[] = {
+	"aw87329",
+	"aw87339",
+	"aw87349",
+};
+
+const char *g_aw_pid_59_3x9_product[] = {
+	"aw87359",
+	"aw87389",
+};
+
+const char *g_aw_pid_59_5x9_product[] = {
+	"aw87509",
+	"aw87519",
+	"aw87529",
+	"aw87539",
+};
+
+const char *g_aw_pid_5a_product[] = {
+	"aw87549",
+	"aw87559",
+	"aw87569",
+	"aw87579",
+	"aw81509",
+};
+
+const char *g_aw_pid_76_product[] = {
+	"aw87390",
+	"aw87320",
+	"aw87401",
+	"aw87360",
+};
+
+const char *g_aw_pid_60_product[] = {
+	"aw87560",
+	"aw87561",
+	"aw87562",
+	"aw87501",
+	"aw87550",
+};
+
+static int aw87xxx_dev_get_chipid(struct aw_device *aw_dev);
+
+/***************************************************************************
+ *
+ * reading and writing of I2C bus
+ *
+ ***************************************************************************/
+int aw87xxx_dev_i2c_write_byte(struct aw_device *aw_dev,
+			uint8_t reg_addr, uint8_t reg_data)
+{
+	int ret = -1;
+	unsigned char cnt = 0;
+
+	while (cnt < AW_I2C_RETRIES) {
+		ret = i2c_smbus_write_byte_data(aw_dev->i2c, reg_addr, reg_data);
+		if (ret < 0)
+			AW_DEV_LOGE(aw_dev->dev, "i2c_write cnt=%d error=%d i2c_bus=%u i2c_addr=%X chipid=%X",
+				cnt, ret, aw_dev->i2c_bus, aw_dev->i2c_addr, aw_dev->chipid);
+		else
+			break;
+
+		cnt++;
+		msleep(AW_I2C_RETRY_DELAY);
+	}
+
+	return ret;
+}
+
+int aw87xxx_dev_i2c_read_byte(struct aw_device *aw_dev,
+			uint8_t reg_addr, uint8_t *reg_data)
+{
+	int ret = -1;
+	unsigned char cnt = 0;
+
+	while (cnt < AW_I2C_RETRIES) {
+		ret = i2c_smbus_read_byte_data(aw_dev->i2c, reg_addr);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw_dev->dev, "i2c_read cnt=%d error=%d i2c_bus=%u i2c_addr=%X chipid=%X",
+				cnt, ret, aw_dev->i2c_bus, aw_dev->i2c_addr, aw_dev->chipid);
+		} else {
+			*reg_data = ret;
+			break;
+		}
+		cnt++;
+		msleep(AW_I2C_RETRY_DELAY);
+	}
+
+	return ret;
+}
+
+int aw87xxx_dev_i2c_read_msg(struct aw_device *aw_dev,
+	uint8_t reg_addr, uint8_t *data_buf, uint32_t data_len)
+{
+	int ret = -1;
+
+	struct i2c_msg msg[] = {
+	[0] = {
+		.addr = aw_dev->i2c_addr,
+		.flags = 0,
+		.len = sizeof(uint8_t),
+		.buf = &reg_addr,
+		},
+	[1] = {
+		.addr = aw_dev->i2c_addr,
+		.flags = I2C_M_RD,
+		.len = data_len,
+		.buf = data_buf,
+		},
+	};
+
+	ret = i2c_transfer(aw_dev->i2c->adapter, msg, ARRAY_SIZE(msg));
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "transfer failed");
+		return ret;
+	} else if (ret != AW_I2C_READ_MSG_NUM) {
+		AW_DEV_LOGE(aw_dev->dev, "transfer failed(size error)");
+		return -ENXIO;
+	}
+
+	return 0;
+}
+
+int aw87xxx_dev_i2c_write_bits(struct aw_device *aw_dev,
+	uint8_t reg_addr, uint8_t mask, uint8_t reg_data)
+{
+	int ret = -1;
+	unsigned char reg_val = 0;
+
+	ret = aw87xxx_dev_i2c_read_byte(aw_dev, reg_addr, &reg_val);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "i2c read error, ret=%d", ret);
+		return ret;
+	}
+	reg_val &= mask;
+	reg_val |= (reg_data & (~mask));
+	ret = aw87xxx_dev_i2c_write_byte(aw_dev, reg_addr, reg_val);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "i2c write error, ret=%d", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+/************************************************************************
+ *
+ * aw87xxx device update profile data to registers
+ *
+ ************************************************************************/
+static int aw87xxx_dev_reg_update(struct aw_device *aw_dev,
+			struct aw_data_container *profile_data)
+{
+	int i = 0;
+	int ret = -1;
+
+	if (profile_data == NULL)
+		return -EINVAL;
+
+	if (aw_dev->hwen_status == AW_DEV_HWEN_OFF) {
+		AW_DEV_LOGE(aw_dev->dev, "dev is pwr_off,can not update reg");
+		return -EINVAL;
+	}
+
+	for (i = 0; i < profile_data->len; i = i + 2) {
+		AW_DEV_LOGI(aw_dev->dev, "reg=0x%02x, val = 0x%02x",
+			profile_data->data[i], profile_data->data[i + 1]);
+
+		ret = aw87xxx_dev_i2c_write_byte(aw_dev, profile_data->data[i],
+				profile_data->data[i + 1]);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static void aw87xxx_dev_reg_mute_bits_set(struct aw_device *aw_dev,
+				uint8_t *reg_val, bool enable)
+{
+	if (enable) {
+		*reg_val &= aw_dev->mute_desc.mask;
+		*reg_val |= aw_dev->mute_desc.enable;
+	} else {
+		*reg_val &= aw_dev->mute_desc.mask;
+		*reg_val |= aw_dev->mute_desc.disable;
+	}
+}
+
+static int aw87xxx_dev_reg_update_mute(struct aw_device *aw_dev,
+			struct aw_data_container *profile_data)
+{
+	int i = 0;
+	int ret = -1;
+	uint8_t reg_val = 0;
+
+	if (profile_data == NULL)
+		return -EINVAL;
+
+	if (aw_dev->hwen_status == AW_DEV_HWEN_OFF) {
+		AW_DEV_LOGE(aw_dev->dev, "hwen is off,can not update reg");
+		return -EINVAL;
+	}
+
+	if (aw_dev->mute_desc.mask == AW_DEV_REG_INVALID_MASK) {
+		AW_DEV_LOGE(aw_dev->dev, "mute ctrl mask invalid");
+		return -EINVAL;
+	}
+
+	for (i = 0; i < profile_data->len; i = i + 2) {
+		AW_DEV_LOGI(aw_dev->dev, "reg=0x%02x, val = 0x%02x",
+			profile_data->data[i], profile_data->data[i + 1]);
+
+		reg_val = profile_data->data[i + 1];
+		if (profile_data->data[i] == aw_dev->mute_desc.addr) {
+			aw87xxx_dev_reg_mute_bits_set(aw_dev, &reg_val, true);
+			AW_DEV_LOGD(aw_dev->dev, "change mute_mask, val = 0x%02x",
+				reg_val);
+		}
+
+		ret = aw87xxx_dev_i2c_write_byte(aw_dev, profile_data->data[i], reg_val);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+/************************************************************************
+ *
+ * aw87xxx device hadware and soft contols
+ *
+ ************************************************************************/
+static bool aw87xxx_dev_gpio_is_valid(struct aw_device *aw_dev)
+{
+	if (gpio_is_valid(aw_dev->rst_gpio))
+		return true;
+	else
+		return false;
+}
+
+void aw87xxx_dev_hw_pwr_ctrl(struct aw_device *aw_dev, bool enable)
+{
+	if (aw_dev->hwen_status == AW_DEV_HWEN_INVALID) {
+		AW_DEV_LOGD(aw_dev->dev, "product not have reset-pin,hardware pwd control invalid");
+		return;
+	}
+	if (enable) {
+		if (aw87xxx_dev_gpio_is_valid(aw_dev)) {
+			gpio_set_value_cansleep(aw_dev->rst_gpio, AW_GPIO_LOW_LEVEL);
+			mdelay(2);
+			gpio_set_value_cansleep(aw_dev->rst_gpio, AW_GPIO_HIGHT_LEVEL);
+			mdelay(2);
+			aw_dev->hwen_status = AW_DEV_HWEN_ON;
+			AW_DEV_LOGI(aw_dev->dev, "hw power on");
+		} else {
+			AW_DEV_LOGI(aw_dev->dev, "hw already power on");
+		}
+	} else {
+		if (aw87xxx_dev_gpio_is_valid(aw_dev)) {
+			gpio_set_value_cansleep(aw_dev->rst_gpio, AW_GPIO_LOW_LEVEL);
+			mdelay(2);
+			aw_dev->hwen_status = AW_DEV_HWEN_OFF;
+			AW_DEV_LOGI(aw_dev->dev, "hw power off");
+		} else {
+			AW_DEV_LOGI(aw_dev->dev, "hw already power off");
+		}
+	}
+}
+
+static int aw87xxx_dev_mute_ctrl(struct aw_device *aw_dev, bool enable)
+{
+	int ret = 0;
+
+	if (enable) {
+		ret = aw87xxx_dev_i2c_write_bits(aw_dev, aw_dev->mute_desc.addr,
+				aw_dev->mute_desc.mask, aw_dev->mute_desc.enable);
+		if (ret < 0)
+			return ret;
+		AW_DEV_LOGI(aw_dev->dev, "set mute down");
+	} else {
+		ret = aw87xxx_dev_i2c_write_bits(aw_dev, aw_dev->mute_desc.addr,
+				aw_dev->mute_desc.mask, aw_dev->mute_desc.disable);
+		if (ret < 0)
+			return ret;
+		AW_DEV_LOGI(aw_dev->dev, "close mute down");
+	}
+
+	return 0;
+}
+
+void aw87xxx_dev_soft_reset(struct aw_device *aw_dev)
+{
+	int i = 0;
+	int ret = -1;
+	struct aw_soft_rst_desc *soft_rst = &aw_dev->soft_rst_desc;
+
+	AW_DEV_LOGD(aw_dev->dev, "enter");
+
+	if (aw_dev->hwen_status == AW_DEV_HWEN_OFF) {
+		AW_DEV_LOGE(aw_dev->dev, "hw is off,can not softrst");
+		return;
+	}
+
+	if (aw_dev->soft_rst_enable == AW_DEV_SOFT_RST_DISENABLE) {
+		AW_DEV_LOGD(aw_dev->dev, "softrst is disenable");
+		return;
+	}
+
+	if (soft_rst->access == NULL || soft_rst->len == 0) {
+		AW_DEV_LOGE(aw_dev->dev, "softrst_info not init");
+		return;
+	}
+
+	if (soft_rst->len % 2) {
+		AW_DEV_LOGE(aw_dev->dev, "softrst data_len[%d] is odd number,data not available",
+			aw_dev->soft_rst_desc.len);
+		return;
+	}
+
+	for (i = 0; i < soft_rst->len; i += 2) {
+		AW_DEV_LOGD(aw_dev->dev, "softrst_reg=0x%02x, val = 0x%02x",
+			soft_rst->access[i], soft_rst->access[i + 1]);
+
+		ret = aw87xxx_dev_i2c_write_byte(aw_dev, soft_rst->access[i],
+				soft_rst->access[i + 1]);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw_dev->dev, "write failed,ret = %d,cnt=%d",
+				ret, i);
+			return;
+		}
+	}
+	AW_DEV_LOGD(aw_dev->dev, "down");
+}
+
+
+int aw87xxx_dev_default_pwr_off(struct aw_device *aw_dev,
+		struct aw_data_container *profile_data)
+{
+	int ret = 0;
+
+	AW_DEV_LOGD(aw_dev->dev, "enter");
+	if (aw_dev->hwen_status == AW_DEV_HWEN_OFF) {
+		AW_DEV_LOGE(aw_dev->dev, "hwen is already off");
+		return 0;
+	}
+
+	if (aw_dev->soft_off_enable && profile_data) {
+		ret = aw87xxx_dev_reg_update(aw_dev, profile_data);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw_dev->dev, "update profile[Off] fw config failed");
+			goto reg_off_update_failed;
+		}
+	}
+
+	aw87xxx_dev_hw_pwr_ctrl(aw_dev, false);
+	AW_DEV_LOGD(aw_dev->dev, "down");
+	return 0;
+
+reg_off_update_failed:
+	aw87xxx_dev_hw_pwr_ctrl(aw_dev, false);
+	return ret;
+}
+
+
+/************************************************************************
+ *
+ * aw87xxx device power on process function
+ *
+ ************************************************************************/
+
+int aw87xxx_dev_default_pwr_on(struct aw_device *aw_dev,
+			struct aw_data_container *profile_data)
+{
+	int ret = 0;
+
+	/*hw power on*/
+	aw87xxx_dev_hw_pwr_ctrl(aw_dev, true);
+
+	ret = aw87xxx_dev_reg_update(aw_dev, profile_data);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+/****************************************************************************
+ *
+ * aw87xxx chip esd status check
+ *
+ ****************************************************************************/
+int aw87xxx_dev_esd_reg_status_check(struct aw_device *aw_dev)
+{
+	int ret;
+	unsigned char reg_val = 0;
+	struct aw_esd_check_desc *esd_desc = &aw_dev->esd_desc;
+
+	AW_DEV_LOGD(aw_dev->dev, "enter");
+
+	if (!esd_desc->first_update_reg_addr) {
+		AW_DEV_LOGE(aw_dev->dev, "esd check info if not init,please check");
+		return -EINVAL;
+	}
+
+	ret = aw87xxx_dev_i2c_read_byte(aw_dev, esd_desc->first_update_reg_addr,
+			&reg_val);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "read reg 0x%02x failed",
+			esd_desc->first_update_reg_addr);
+		return ret;
+	}
+
+	AW_DEV_LOGD(aw_dev->dev, "0x%02x:default val=0x%02x real val=0x%02x",
+		esd_desc->first_update_reg_addr,
+		esd_desc->first_update_reg_val, reg_val);
+
+	if (reg_val == esd_desc->first_update_reg_val) {
+		AW_DEV_LOGE(aw_dev->dev, "reg status check failed");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+int aw87xxx_dev_check_reg_is_rec_mode(struct aw_device *aw_dev)
+{
+	int ret;
+	unsigned char reg_val = 0;
+	struct aw_rec_mode_desc *rec_desc = &aw_dev->rec_desc;
+
+	if (!rec_desc->addr) {
+		AW_DEV_LOGE(aw_dev->dev, "rec check info if not init,please check");
+		return -EINVAL;
+	}
+
+	ret = aw87xxx_dev_i2c_read_byte(aw_dev, rec_desc->addr, &reg_val);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "read reg 0x%02x failed",
+			rec_desc->addr);
+		return ret;
+	}
+
+	if (rec_desc->enable) {
+		if (reg_val & ~(rec_desc->mask)) {
+			AW_DEV_LOGI(aw_dev->dev, "reg status is receiver mode");
+			aw_dev->is_rec_mode = AW_IS_REC_MODE;
+		} else {
+			aw_dev->is_rec_mode = AW_NOT_REC_MODE;
+		}
+	} else {
+		if (!(reg_val & ~(rec_desc->mask))) {
+			AW_DEV_LOGI(aw_dev->dev, "reg status is receiver mode");
+			aw_dev->is_rec_mode = AW_IS_REC_MODE;
+		} else {
+			aw_dev->is_rec_mode = AW_NOT_REC_MODE;
+		}
+	}
+	return 0;
+}
+
+
+/****************************************************************************
+ *
+ * aw87xxx product attributes init info
+ *
+ ****************************************************************************/
+
+/********************** aw87xxx_pid_9A attributes ***************************/
+
+static int aw_dev_pid_9b_reg_update(struct aw_device *aw_dev,
+			struct aw_data_container *profile_data)
+{
+	int i = 0;
+	int ret = -1;
+	uint8_t reg_val = 0;
+
+	if (profile_data == NULL)
+		return -EINVAL;
+
+	if (aw_dev->hwen_status == AW_DEV_HWEN_OFF) {
+		AW_DEV_LOGE(aw_dev->dev, "dev is pwr_off,can not update reg");
+		return -EINVAL;
+	}
+
+	if (profile_data->len != AW_PID_9B_BIN_REG_CFG_COUNT) {
+		AW_DEV_LOGE(aw_dev->dev, "reg_config count of bin is error,can not update reg");
+		return -EINVAL;
+	}
+	ret = aw87xxx_dev_i2c_write_byte(aw_dev, AW87XXX_PID_9B_ENCRYPTION_REG,
+		AW87XXX_PID_9B_ENCRYPTION_BOOST_OUTPUT_SET);
+	if (ret < 0)
+		return ret;
+
+	for (i = 1; i < AW_PID_9B_BIN_REG_CFG_COUNT; i++) {
+		AW_DEV_LOGI(aw_dev->dev, "reg=0x%02x, val = 0x%02x",
+			i, profile_data->data[i]);
+		reg_val = profile_data->data[i];
+		if (i == AW87XXX_PID_9B_SYSCTRL_REG) {
+			aw87xxx_dev_reg_mute_bits_set(aw_dev, &reg_val, true);
+			AW_DEV_LOGD(aw_dev->dev, "change mute_mask, val = 0x%02x",
+				reg_val);
+		}
+
+		ret = aw87xxx_dev_i2c_write_byte(aw_dev, i, reg_val);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int aw_dev_pid_9b_pwr_on(struct aw_device *aw_dev, struct aw_data_container *data)
+{
+	int ret = 0;
+
+	/*hw power on*/
+	aw87xxx_dev_hw_pwr_ctrl(aw_dev, true);
+
+	/* open the mute */
+	ret = aw87xxx_dev_mute_ctrl(aw_dev, true);
+	if (ret < 0)
+		return ret;
+
+	/* Update scene parameters in mute mode */
+	ret = aw_dev_pid_9b_reg_update(aw_dev, data);
+	if (ret < 0)
+		return ret;
+
+	/* close the mute */
+	ret = aw87xxx_dev_mute_ctrl(aw_dev, false);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+static void aw_dev_pid_9b_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_9B_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_9b_reg_access;
+
+	aw_dev->mute_desc.addr = AW87XXX_PID_9B_SYSCTRL_REG;
+	aw_dev->mute_desc.mask = AW87XXX_PID_9B_REG_EN_SW_MASK;
+	aw_dev->mute_desc.enable = AW87XXX_PID_9B_REG_EN_SW_DISABLE_VALUE;
+	aw_dev->mute_desc.disable = AW87XXX_PID_9B_REG_EN_SW_ENABLE_VALUE;
+	aw_dev->ops.pwr_on_func = aw_dev_pid_9b_pwr_on;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_9b_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_9b_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* Whether to allow register operation to power off */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_DISENABLE;
+
+	aw_dev->product_tab = g_aw_pid_9b_product;
+	aw_dev->product_cnt = AW87XXX_PID_9B_PRODUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_9B_SYSCTRL_REG;
+	aw_dev->rec_desc.disable = AW87XXX_PID_9B_SPK_MODE_ENABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_9B_SPK_MODE_DISABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_9B_SPK_MODE_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_9B_SYSCTRL_REG;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_9B_SYSCTRL_DEFAULT;
+}
+
+static int aw_dev_pid_9a_init(struct aw_device *aw_dev)
+{
+	int ret = 0;
+
+	ret = aw87xxx_dev_i2c_write_byte(aw_dev, AW87XXX_PID_9B_ENCRYPTION_REG,
+		AW87XXX_PID_9B_ENCRYPTION_BOOST_OUTPUT_SET);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "write 0x64=0x2C error");
+		return -EINVAL;
+	}
+
+	ret = aw87xxx_dev_get_chipid(aw_dev);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "read chipid is failed,ret=%d", ret);
+		return ret;
+	}
+
+	if (aw_dev->chipid == AW_DEV_CHIPID_9B) {
+		AW_DEV_LOGI(aw_dev->dev, "product is pid_9B class");
+		aw_dev_pid_9b_init(aw_dev);
+	} else {
+		AW_DEV_LOGE(aw_dev->dev, "product is not pid_9B class，not support");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/********************** aw87xxx_pid_9b attributes end ***********************/
+
+/********************** aw87xxx_pid_18 attributes ***************************/
+static int aw_dev_pid_18_pwr_on(struct aw_device *aw_dev, struct aw_data_container *data)
+{
+	int ret = 0;
+
+	/*hw power on*/
+	aw87xxx_dev_hw_pwr_ctrl(aw_dev, true);
+
+	/* open the mute */
+	ret = aw87xxx_dev_mute_ctrl(aw_dev, true);
+	if (ret < 0)
+		return ret;
+
+	/* Update scene parameters in mute mode */
+	ret = aw87xxx_dev_reg_update_mute(aw_dev, data);
+	if (ret < 0)
+		return ret;
+
+	/* close the mute */
+	ret = aw87xxx_dev_mute_ctrl(aw_dev, false);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+static void aw_dev_chipid_18_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_18_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_18_reg_access;
+
+	aw_dev->mute_desc.addr = AW87XXX_PID_18_SYSCTRL_REG;
+	aw_dev->mute_desc.mask = AW87XXX_PID_18_REG_EN_SW_MASK;
+	aw_dev->mute_desc.enable = AW87XXX_PID_18_REG_EN_SW_DISABLE_VALUE;
+	aw_dev->mute_desc.disable = AW87XXX_PID_18_REG_EN_SW_ENABLE_VALUE;
+	aw_dev->ops.pwr_on_func = aw_dev_pid_18_pwr_on;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_18_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_18_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* Whether to allow register operation to power off */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_ENABLE;
+
+	aw_dev->product_tab = g_aw_pid_18_product;
+	aw_dev->product_cnt = AW87XXX_PID_18_PRODUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_18_SYSCTRL_REG;
+	aw_dev->rec_desc.disable = AW87XXX_PID_18_REG_REC_MODE_DISABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_18_REG_REC_MODE_ENABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_18_REG_REC_MODE_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_18_CLASSD_REG;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_18_CLASSD_DEFAULT;
+}
+/********************** aw87xxx_pid_18 attributes end ***********************/
+
+/********************** aw87xxx_pid_39 attributes ***************************/
+static void aw_dev_chipid_39_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_39_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_39_reg_access;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_39_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_39_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* Whether to allow register operation to power off */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_ENABLE;
+
+	aw_dev->product_tab = g_aw_pid_39_product;
+	aw_dev->product_cnt = AW87XXX_PID_39_PRODUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_39_REG_MODECTRL;
+	aw_dev->rec_desc.disable = AW87XXX_PID_39_REC_MODE_DISABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_39_REC_MODE_ENABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_39_REC_MODE_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_39_REG_MODECTRL;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_39_MODECTRL_DEFAULT;
+}
+/********************* aw87xxx_pid_39 attributes end *************************/
+
+
+/********************* aw87xxx_pid_59_5x9 attributes *************************/
+static void aw_dev_chipid_59_5x9_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_59_5X9_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_59_5x9_reg_access;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_59_5x9_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_59_5x9_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* Whether to allow register operation to power off */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_ENABLE;
+
+	aw_dev->product_tab = g_aw_pid_59_5x9_product;
+	aw_dev->product_cnt = AW87XXX_PID_59_5X9_PRODUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_59_5X9_REG_SYSCTRL;
+	aw_dev->rec_desc.disable = AW87XXX_PID_59_5X9_REC_MODE_DISABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_59_5X9_REC_MODE_ENABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_59_5X9_REC_MODE_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_59_5X9_REG_ENCR;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_59_5X9_ENCRY_DEFAULT;
+}
+/******************* aw87xxx_pid_59_5x9 attributes end ***********************/
+
+/********************* aw87xxx_pid_59_3x9 attributes *************************/
+static void aw_dev_chipid_59_3x9_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_59_3X9_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_59_3x9_reg_access;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_59_3x9_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_59_3x9_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* Whether to allow register operation to power off */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_ENABLE;
+
+	aw_dev->product_tab = g_aw_pid_59_3x9_product;
+	aw_dev->product_cnt = AW87XXX_PID_59_3X9_PRODUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_59_3X9_REG_MDCRTL;
+	aw_dev->rec_desc.disable = AW87XXX_PID_59_3X9_SPK_MODE_ENABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_59_3X9_SPK_MODE_DISABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_59_3X9_SPK_MODE_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_59_3X9_REG_ENCR;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_59_3X9_ENCR_DEFAULT;
+}
+/******************* aw87xxx_pid_59_3x9 attributes end ***********************/
+
+/********************** aw87xxx_pid_5a attributes ****************************/
+static void aw_dev_chipid_5a_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_5A_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_5a_reg_access;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_5a_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_5a_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* Whether to allow register operation to power off */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_ENABLE;
+
+	aw_dev->product_tab = g_aw_pid_5a_product;
+	aw_dev->product_cnt = AW87XXX_PID_5A_PRODUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_5A_REG_SYSCTRL_REG;
+	aw_dev->rec_desc.disable = AW87XXX_PID_5A_REG_RCV_MODE_DISABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_5A_REG_RCV_MODE_ENABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_5A_REG_RCV_MODE_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_5A_REG_DFT3R_REG;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_5A_DFT3R_DEFAULT;
+}
+/********************** aw87xxx_pid_5a attributes end ************************/
+
+/********************** aw87xxx_pid_76 attributes ****************************/
+static void aw_dev_chipid_76_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_76_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_76_reg_access;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_76_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_76_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* software power off control info */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_ENABLE;
+
+	aw_dev->product_tab = g_aw_pid_76_product;
+	aw_dev->product_cnt = AW87XXX_PID_76_PROFUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_76_MDCTRL_REG;
+	aw_dev->rec_desc.disable = AW87XXX_PID_76_EN_SPK_ENABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_76_EN_SPK_DISABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_76_EN_SPK_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_76_DFT_ADP1_REG;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_76_DFT_ADP1_CHECK;
+}
+/********************** aw87xxx_pid_76 attributes end ************************/
+
+/********************** aw87xxx_pid_60 attributes ****************************/
+static void aw_dev_chipid_60_init(struct aw_device *aw_dev)
+{
+	/* Product register permission info */
+	aw_dev->reg_max_addr = AW87XXX_PID_60_REG_MAX;
+	aw_dev->reg_access = aw87xxx_pid_60_reg_access;
+
+	/* software reset control info */
+	aw_dev->soft_rst_desc.len = sizeof(aw87xxx_pid_60_softrst_access);
+	aw_dev->soft_rst_desc.access = aw87xxx_pid_60_softrst_access;
+	aw_dev->soft_rst_enable = AW_DEV_SOFT_RST_ENABLE;
+
+	/* software power off control info */
+	aw_dev->soft_off_enable = AW_DEV_SOFT_OFF_ENABLE;
+
+	aw_dev->product_tab = g_aw_pid_60_product;
+	aw_dev->product_cnt = AW87XXX_PID_60_PROFUCT_MAX;
+
+	aw_dev->rec_desc.addr = AW87XXX_PID_60_SYSCTRL_REG;
+	aw_dev->rec_desc.disable = AW87XXX_PID_60_RCV_MODE_DISABLE;
+	aw_dev->rec_desc.enable = AW87XXX_PID_60_RCV_MODE_ENABLE;
+	aw_dev->rec_desc.mask = AW87XXX_PID_60_RCV_MODE_MASK;
+
+	/* esd reg info */
+	aw_dev->esd_desc.first_update_reg_addr = AW87XXX_PID_60_NG3_REG;
+	aw_dev->esd_desc.first_update_reg_val = AW87XXX_PID_60_ESD_REG_VAL;
+}
+/********************** aw87xxx_pid_60 attributes end ************************/
+
+static int aw_dev_chip_init(struct aw_device *aw_dev)
+{
+	int ret  = 0;
+
+	/*get info by chipid*/
+	switch (aw_dev->chipid) {
+	case AW_DEV_CHIPID_9A:
+		ret = aw_dev_pid_9a_init(aw_dev);
+		if (ret < 0)
+			AW_DEV_LOGE(aw_dev->dev, "product is pid_9B init failed");
+		break;
+	case AW_DEV_CHIPID_9B:
+		aw_dev_pid_9b_init(aw_dev);
+		AW_DEV_LOGI(aw_dev->dev, "product is pid_9B class");
+		break;
+	case AW_DEV_CHIPID_18:
+		aw_dev_chipid_18_init(aw_dev);
+		AW_DEV_LOGI(aw_dev->dev, "product is pid_18 class");
+		break;
+	case AW_DEV_CHIPID_39:
+		aw_dev_chipid_39_init(aw_dev);
+		AW_DEV_LOGI(aw_dev->dev, "product is pid_39 class");
+		break;
+	case AW_DEV_CHIPID_59:
+		if (aw87xxx_dev_gpio_is_valid(aw_dev)) {
+			aw_dev_chipid_59_5x9_init(aw_dev);
+			AW_DEV_LOGI(aw_dev->dev, "product is pid_59_5x9 class");
+		} else {
+			aw_dev_chipid_59_3x9_init(aw_dev);
+			AW_DEV_LOGI(aw_dev->dev, "product is pid_59_3x9 class");
+		}
+		break;
+	case AW_DEV_CHIPID_5A:
+		aw_dev_chipid_5a_init(aw_dev);
+		AW_DEV_LOGI(aw_dev->dev, "product is pid_5A class");
+		break;
+	case AW_DEV_CHIPID_76:
+		aw_dev_chipid_76_init(aw_dev);
+		AW_DEV_LOGI(aw_dev->dev, "product is pid_76 class");
+		break;
+	case AW_DEV_CHIPID_60:
+		aw_dev_chipid_60_init(aw_dev);
+		AW_DEV_LOGI(aw_dev->dev, "product is pid_60 class");
+		break;
+	default:
+		AW_DEV_LOGE(aw_dev->dev, "unsupported device revision [0x%x]",
+			aw_dev->chipid);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int aw87xxx_dev_get_chipid(struct aw_device *aw_dev)
+{
+	int ret = -1;
+	unsigned int cnt = 0;
+	unsigned char reg_val = 0;
+
+	for (cnt = 0; cnt < AW_READ_CHIPID_RETRIES; cnt++) {
+		ret = aw87xxx_dev_i2c_read_byte(aw_dev, AW_DEV_REG_CHIPID, &reg_val);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw_dev->dev, "[%d] read chip is failed, ret=%d",
+				cnt, ret);
+			continue;
+		}
+		break;
+	}
+
+
+	if (cnt == AW_READ_CHIPID_RETRIES) {
+		AW_DEV_LOGE(aw_dev->dev, "read chip is failed,cnt=%d", cnt);
+		return -EINVAL;
+	}
+
+	AW_DEV_LOGI(aw_dev->dev, "read chipid[0x%x] succeed", reg_val);
+	aw_dev->chipid = reg_val;
+
+	return 0;
+}
+
+int aw87xxx_dev_init(struct aw_device *aw_dev)
+{
+	int ret = -1;
+
+	ret = aw87xxx_dev_get_chipid(aw_dev);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw_dev->dev, "read chipid is failed,ret=%d", ret);
+		return ret;
+	}
+
+	ret = aw_dev_chip_init(aw_dev);
+
+	return ret;
+}
+
+
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_device.h b/sound/soc/codecs/aw87xxx/aw87xxx_device.h
new file mode 100644
index 000000000000..7c85f80a958e
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_device.h
@@ -0,0 +1,149 @@
+#ifndef __AW87XXX_DEVICE_H__
+#define __AW87XXX_DEVICE_H__
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <sound/control.h>
+#include <sound/soc.h>
+#include "aw87xxx_acf_bin.h"
+
+#define AW87XXX_PID_9B_PRODUCT_MAX	(1)
+#define AW87XXX_PID_18_PRODUCT_MAX	(1)
+#define AW87XXX_PID_39_PRODUCT_MAX	(3)
+#define AW87XXX_PID_59_3X9_PRODUCT_MAX	(2)
+#define AW87XXX_PID_59_5X9_PRODUCT_MAX	(4)
+#define AW87XXX_PID_5A_PRODUCT_MAX	(5)
+#define AW87XXX_PID_76_PROFUCT_MAX	(4)
+#define AW87XXX_PID_60_PROFUCT_MAX	(5)
+#define AW_PRODUCT_NAME_LEN		(8)
+
+#define AW_GPIO_HIGHT_LEVEL		(1)
+#define AW_GPIO_LOW_LEVEL		(0)
+
+#define AW_I2C_RETRIES			(5)
+#define AW_I2C_RETRY_DELAY		(2)
+#define AW_I2C_READ_MSG_NUM		(2)
+
+#define AW_READ_CHIPID_RETRIES		(5)
+#define AW_READ_CHIPID_RETRY_DELAY	(2)
+#define AW_DEV_REG_CHIPID		(0x00)
+
+#define AW_DEV_REG_INVALID_MASK		(0xff)
+
+#define AW_NO_RESET_GPIO		(-1)
+
+#define AW_PID_9B_BIN_REG_CFG_COUNT	(10)
+
+/********************************************
+ *
+ * aw87xxx devices attributes
+ *
+ *******************************************/
+struct aw_device;
+
+struct aw_device_ops {
+	int (*pwr_on_func)(struct aw_device *aw_dev, struct aw_data_container *data);
+	int (*pwr_off_func)(struct aw_device *aw_dev, struct aw_data_container *data);
+};
+
+enum aw_dev_chipid {
+	AW_DEV_CHIPID_18 = 0x18,
+	AW_DEV_CHIPID_39 = 0x39,
+	AW_DEV_CHIPID_59 = 0x59,
+	AW_DEV_CHIPID_69 = 0x69,
+	AW_DEV_CHIPID_5A = 0x5A,
+	AW_DEV_CHIPID_9A = 0x9A,
+	AW_DEV_CHIPID_9B = 0x9B,
+	AW_DEV_CHIPID_76 = 0x76,
+	AW_DEV_CHIPID_60 = 0x60,
+};
+
+enum aw_dev_hw_status {
+	AW_DEV_HWEN_OFF = 0,
+	AW_DEV_HWEN_ON,
+	AW_DEV_HWEN_INVALID,
+	AW_DEV_HWEN_STATUS_MAX,
+};
+
+enum aw_dev_soft_off_enable {
+	AW_DEV_SOFT_OFF_DISENABLE = 0,
+	AW_DEV_SOFT_OFF_ENABLE = 1,
+};
+
+enum aw_dev_soft_rst_enable {
+	AW_DEV_SOFT_RST_DISENABLE = 0,
+	AW_DEV_SOFT_RST_ENABLE = 1,
+};
+
+enum aw_reg_receiver_mode {
+	AW_NOT_REC_MODE = 0,
+	AW_IS_REC_MODE = 1,
+};
+
+struct aw_mute_desc {
+	uint8_t addr;
+	uint8_t enable;
+	uint8_t disable;
+	uint16_t mask;
+};
+
+struct aw_soft_rst_desc {
+	int len;
+	unsigned char *access;
+};
+
+struct aw_esd_check_desc {
+	uint8_t first_update_reg_addr;
+	uint8_t first_update_reg_val;
+};
+
+struct aw_rec_mode_desc {
+	uint8_t addr;
+	uint8_t enable;
+	uint8_t disable;
+	uint8_t mask;
+};
+
+struct aw_device {
+	uint8_t i2c_addr;
+	uint8_t chipid;
+	uint8_t soft_rst_enable;
+	uint8_t soft_off_enable;
+	uint8_t is_rec_mode;
+	int hwen_status;
+	int i2c_bus;
+	int rst_gpio;
+	int reg_max_addr;
+	int product_cnt;
+	const char **product_tab;
+	const unsigned char *reg_access;
+
+	struct device *dev;
+	struct i2c_client *i2c;
+	struct aw_mute_desc mute_desc;
+	struct aw_soft_rst_desc soft_rst_desc;
+	struct aw_esd_check_desc esd_desc;
+	struct aw_rec_mode_desc rec_desc;
+
+	struct aw_device_ops ops;
+};
+
+
+int aw87xxx_dev_i2c_write_byte(struct aw_device *aw_dev,
+			uint8_t reg_addr, uint8_t reg_data);
+int aw87xxx_dev_i2c_read_byte(struct aw_device *aw_dev,
+			uint8_t reg_addr, uint8_t *reg_data);
+int aw87xxx_dev_i2c_read_msg(struct aw_device *aw_dev,
+	uint8_t reg_addr, uint8_t *data_buf, uint32_t data_len);
+int aw87xxx_dev_i2c_write_bits(struct aw_device *aw_dev,
+	uint8_t reg_addr, uint8_t mask, uint8_t reg_data);
+void aw87xxx_dev_soft_reset(struct aw_device *aw_dev);
+void aw87xxx_dev_hw_pwr_ctrl(struct aw_device *aw_dev, bool enable);
+int aw87xxx_dev_default_pwr_on(struct aw_device *aw_dev,
+			struct aw_data_container *profile_data);
+int aw87xxx_dev_default_pwr_off(struct aw_device *aw_dev,
+			struct aw_data_container *profile_data);
+int aw87xxx_dev_esd_reg_status_check(struct aw_device *aw_dev);
+int aw87xxx_dev_check_reg_is_rec_mode(struct aw_device *aw_dev);
+int aw87xxx_dev_init(struct aw_device *aw_dev);
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_dsp.c b/sound/soc/codecs/aw87xxx/aw87xxx_dsp.c
new file mode 100644
index 000000000000..93b02e30122d
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_dsp.c
@@ -0,0 +1,355 @@
+/*
+ * aw87xxx_dsp.c
+ *
+ * Copyright (c) 2021 AWINIC Technology CO., LTD
+ *
+ * Author: Barry <zhaozhongbo@awinic.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/uaccess.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/of.h>
+#include <linux/slab.h>
+#include <linux/hrtimer.h>
+#include <linux/proc_fs.h>
+#include <linux/init.h>
+#include "aw87xxx_log.h"
+#include "aw87xxx_dsp.h"
+
+static DEFINE_MUTEX(g_dsp_lock);
+static unsigned int g_spin_value = 0;
+
+static int g_rx_topo_id = AW_RX_DEFAULT_TOPO_ID;
+static int g_rx_port_id = AW_RX_DEFAULT_PORT_ID;
+
+#ifdef AW_MTK_OPEN_DSP_PLATFORM
+extern int mtk_spk_send_ipi_buf_to_dsp(void *data_buffer,
+				uint32_t data_size);
+extern int mtk_spk_recv_ipi_buf_from_dsp(int8_t *buffer,
+				int16_t size, uint32_t *buf_len);
+/*
+static int mtk_spk_send_ipi_buf_to_dsp(void *data_buffer,
+				uint32_t data_size)
+{
+	AW_LOGI("enter");
+	return 0;
+}
+
+static int mtk_spk_recv_ipi_buf_from_dsp(int8_t *buffer,
+				int16_t size, uint32_t *buf_len)
+{
+	AW_LOGI("enter");
+	return 0;
+}
+*/
+#elif defined AW_QCOM_OPEN_DSP_PLATFORM
+extern int afe_get_topology(int port_id);
+extern int aw_send_afe_cal_apr(uint32_t param_id,
+	void *buf, int cmd_size, bool write);
+/*
+static int afe_get_topology(int port_id)
+{
+	return -EPERM;
+}
+
+static int aw_send_afe_cal_apr(uint32_t param_id,
+	void *buf, int cmd_size, bool write)
+{
+	AW_LOGI("enter, no define AWINIC_ADSP_ENABLE", __func__);
+	return 0;
+}
+*/
+#endif
+
+#ifdef AW_QCOM_OPEN_DSP_PLATFORM
+extern void aw_set_port_id(int rx_port_id);
+#else
+static void aw_set_port_id(int rx_port_id)
+{
+	return;
+}
+#endif
+
+uint8_t aw87xxx_dsp_isEnable(void)
+{
+#if (defined AW_QCOM_OPEN_DSP_PLATFORM) || (defined AW_MTK_OPEN_DSP_PLATFORM)
+	return true;
+#else
+	return false;
+#endif
+}
+
+/*****************mtk dsp communication function start**********************/
+#ifdef AW_MTK_OPEN_DSP_PLATFORM
+static int aw_mtk_write_data_to_dsp(int32_t param_id,
+			void *data, int size)
+{
+	int32_t *dsp_data = NULL;
+	mtk_dsp_hdr_t *hdr = NULL;
+	int ret;
+
+	dsp_data = kzalloc(sizeof(mtk_dsp_hdr_t) + size, GFP_KERNEL);
+	if (!dsp_data) {
+		AW_LOGE("kzalloc dsp_msg error");
+		return -ENOMEM;
+	}
+
+	hdr = (mtk_dsp_hdr_t *)dsp_data;
+	hdr->type = DSP_MSG_TYPE_DATA;
+	hdr->opcode_id = param_id;
+	hdr->version = AW_DSP_MSG_HDR_VER;
+
+	memcpy(((char *)dsp_data) + sizeof(mtk_dsp_hdr_t),
+		data, size);
+
+	ret = mtk_spk_send_ipi_buf_to_dsp(dsp_data,
+				sizeof(mtk_dsp_hdr_t) + size);
+	if (ret < 0) {
+		AW_LOGE("write data failed");
+		kfree(dsp_data);
+		dsp_data = NULL;
+		return ret;
+	}
+
+	kfree(dsp_data);
+	dsp_data = NULL;
+	return 0;
+}
+
+static int aw_mtk_read_data_from_dsp(int32_t param_id, void *data,
+					int data_size)
+{
+	int ret;
+	mtk_dsp_hdr_t hdr;
+
+	mutex_lock(&g_dsp_lock);
+	hdr.type = DSP_MSG_TYPE_CMD;
+	hdr.opcode_id = param_id;
+	hdr.version = AW_DSP_MSG_HDR_VER;
+
+	ret = mtk_spk_send_ipi_buf_to_dsp(&hdr, sizeof(mtk_dsp_hdr_t));
+	if (ret < 0)
+		goto failed;
+
+	ret = mtk_spk_recv_ipi_buf_from_dsp(data, data_size, &data_size);
+	if (ret < 0)
+		goto failed;
+
+	mutex_unlock(&g_dsp_lock);
+	return 0;
+
+failed:
+	mutex_unlock(&g_dsp_lock);
+	return ret;
+}
+
+#endif
+/********************mtk dsp communication function end***********************/
+
+/******************qcom dsp communication function start**********************/
+#ifdef AW_QCOM_OPEN_DSP_PLATFORM
+static void aw_check_dsp_ready(void)
+{
+	int ret;
+
+	ret = afe_get_topology(g_rx_port_id);
+	AW_LOGD("topo_id 0x%x", ret);
+
+	if (ret != g_rx_topo_id)
+		AW_LOGE("topo id 0x%x", ret);
+
+}
+
+static int aw_qcom_write_data_to_dsp(int32_t param_id,
+				void *data, int data_size)
+{
+	int ret = 0;
+
+	AW_LOGI("enter");
+	mutex_lock(&g_dsp_lock);
+	aw_check_dsp_ready();
+	ret = aw_send_afe_cal_apr(param_id, data,
+		data_size, true);
+	mutex_unlock(&g_dsp_lock);
+	return ret;
+}
+
+static int aw_qcom_read_data_from_dsp(int32_t param_id,
+				void *data, int data_size)
+{
+	int ret = 0;
+
+	AW_LOGI("enter");
+	mutex_lock(&g_dsp_lock);
+	aw_check_dsp_ready();
+	ret = aw_send_afe_cal_apr(param_id, data,
+			data_size, false);
+	mutex_unlock(&g_dsp_lock);
+	return ret;
+}
+
+#endif
+/*****************qcom dsp communication function end*********************/
+
+/*****************read/write msg communication function*********************/
+static int aw_write_data_to_dsp(int32_t param_id, void *data, int data_size)
+{
+#if defined AW_QCOM_OPEN_DSP_PLATFORM
+	return aw_qcom_write_data_to_dsp(param_id, data, data_size);
+#elif defined AW_MTK_OPEN_DSP_PLATFORM
+	return aw_mtk_write_data_to_dsp(param_id, data, data_size);
+#else
+	return -EINVAL;
+#endif
+}
+
+static int aw_read_data_from_dsp(int32_t param_id, void *data, int data_size)
+{
+#if defined AW_QCOM_OPEN_DSP_PLATFORM
+	return aw_qcom_read_data_from_dsp(param_id, data, data_size);
+#elif defined AW_MTK_OPEN_DSP_PLATFORM
+	return aw_mtk_read_data_from_dsp(param_id, data, data_size);
+#else
+	return -EINVAL;
+#endif
+}
+
+/***************read/write msg communication function end*******************/
+
+int aw87xxx_dsp_get_rx_module_enable(int *enable)
+{
+	if (!enable) {
+		AW_LOGE("enable is NULL");
+		return -EINVAL;
+	}
+
+	return aw_read_data_from_dsp(AWDSP_RX_SET_ENABLE,
+			(void *)enable, sizeof(uint32_t));
+}
+
+int aw87xxx_dsp_set_rx_module_enable(int enable)
+{
+	switch (enable) {
+	case AW_RX_MODULE_DISENABLE:
+		AW_LOGD("set enable=%d", enable);
+		break;
+	case AW_RX_MODULE_ENABLE:
+		AW_LOGD("set enable=%d", enable);
+		break;
+	default:
+		AW_LOGE("unsupport enable=%d", enable);
+		return -EINVAL;
+	}
+
+	return aw_write_data_to_dsp(AWDSP_RX_SET_ENABLE,
+			&enable, sizeof(uint32_t));
+}
+
+
+int aw87xxx_dsp_get_vmax(uint32_t *vmax, int dev_index)
+{
+	int32_t param_id = 0;
+
+	switch (dev_index % AW_DSP_CHANNEL_MAX) {
+	case AW_DSP_CHANNEL_0:
+		param_id = AWDSP_RX_VMAX_0;
+		break;
+	case AW_DSP_CHANNEL_1:
+		param_id = AWDSP_RX_VMAX_1;
+		break;
+	default:
+		AW_LOGE("algo only support double PA channel:%d unsupport",
+			dev_index);
+		return -EINVAL;
+	}
+
+	return aw_read_data_from_dsp(param_id,
+			(void *)vmax, sizeof(uint32_t));
+}
+
+int aw87xxx_dsp_set_vmax(uint32_t vmax, int dev_index)
+{
+	int32_t param_id = 0;
+
+	switch (dev_index % AW_DSP_CHANNEL_MAX) {
+	case AW_DSP_CHANNEL_0:
+		param_id = AWDSP_RX_VMAX_0;
+		break;
+	case AW_DSP_CHANNEL_1:
+		param_id = AWDSP_RX_VMAX_1;
+		break;
+	default:
+		AW_LOGE("algo only support double PA channel:%d unsupport",
+			dev_index);
+		return -EINVAL;
+	}
+
+	return aw_write_data_to_dsp(param_id, &vmax, sizeof(uint32_t));
+}
+
+int aw87xxx_dsp_set_spin(uint32_t ctrl_value)
+{
+	int ret = 0;
+
+	if (ctrl_value >= AW_SPIN_MAX) {
+		AW_LOGE("spin [%d] unsupported ", ctrl_value);
+		return -EINVAL;
+	}
+	ret = aw_write_data_to_dsp(AW_MSG_ID_SPIN, &ctrl_value,
+		sizeof(uint32_t));
+	if (ret) {
+		AW_LOGE("spin [%d] set failed ", ctrl_value);
+		return ret;
+	}
+
+	g_spin_value = ctrl_value;
+	return 0;
+}
+
+int aw87xxx_dsp_get_spin(void)
+{
+	return g_spin_value;
+}
+
+int aw87xxx_spin_set_record_val(void)
+{
+	AW_LOGD("record write spin enter");
+
+	return aw87xxx_dsp_set_spin(g_spin_value);
+}
+EXPORT_SYMBOL(aw87xxx_spin_set_record_val);
+
+void aw87xxx_device_parse_topo_id_dt(struct aw_device *aw_dev)
+{
+	int ret;
+
+	ret = of_property_read_u32(aw_dev->dev->of_node, "aw-rx-topo-id", &g_rx_topo_id);
+	if (ret < 0) {
+		g_rx_topo_id = AW_RX_DEFAULT_TOPO_ID;
+		AW_DEV_LOGI(aw_dev->dev, "read aw-rx-topo-id failed,use default");
+	}
+
+	AW_DEV_LOGI(aw_dev->dev, "rx-topo-id: 0x%x",  g_rx_topo_id);
+}
+
+void aw87xxx_device_parse_port_id_dt(struct aw_device *aw_dev)
+{
+	int ret;
+
+	ret = of_property_read_u32(aw_dev->dev->of_node, "aw-rx-port-id", &g_rx_port_id);
+	if (ret < 0) {
+		g_rx_port_id = AW_RX_DEFAULT_PORT_ID;
+		AW_DEV_LOGI(aw_dev->dev, "read aw-rx-port-id failed,use default");
+	}
+
+	aw_set_port_id(g_rx_port_id);
+	AW_DEV_LOGI(aw_dev->dev, "rx-port-id: 0x%x", g_rx_port_id);
+}
+
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_dsp.h b/sound/soc/codecs/aw87xxx/aw87xxx_dsp.h
new file mode 100644
index 000000000000..7acc4dc0dfd9
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_dsp.h
@@ -0,0 +1,65 @@
+#ifndef __AW87XXX_DSP_H__
+#define __AW87XXX_DSP_H__
+
+#include "aw87xxx_device.h"
+
+/*#define AW_MTK_OPEN_DSP_PLATFORM*/
+/*#define AW_QCOM_OPEN_DSP_PLATFORM*/
+
+/*Note: The pord_ID is configured according to different platforms*/
+#define AW_DSP_SLEEP_TIME	(10)
+
+#define AW_DSP_MSG_HDR_VER (1)
+
+#define AW_RX_DEFAULT_TOPO_ID		(0x1000FF01)
+#define AW_RX_DEFAULT_PORT_ID		(0x4000)
+
+#define AWDSP_RX_SET_ENABLE		(0x10013D11)
+#define AWDSP_RX_PARAMS			(0x10013D12)
+#define AWDSP_RX_VMAX_0			(0X10013D17)
+#define AWDSP_RX_VMAX_1			(0X10013D18)
+#define AW_MSG_ID_SPIN 			(0x10013D2E)
+
+enum {
+	AW_SPIN_0 = 0,
+	AW_SPIN_90,
+	AW_SPIN_180,
+	AW_SPIN_270,
+	AW_SPIN_MAX,
+};
+
+typedef struct mtk_dsp_msg_header {
+	int32_t type;
+	int32_t opcode_id;
+	int32_t version;
+	int32_t reserver[3];
+} mtk_dsp_hdr_t;
+
+enum aw_rx_module_enable {
+	AW_RX_MODULE_DISENABLE = 0,
+	AW_RX_MODULE_ENABLE,
+};
+
+enum aw_dsp_msg_type {
+	DSP_MSG_TYPE_DATA = 0,
+	DSP_MSG_TYPE_CMD = 1,
+};
+
+enum aw_dsp_channel {
+	AW_DSP_CHANNEL_0 = 0,
+	AW_DSP_CHANNEL_1,
+	AW_DSP_CHANNEL_MAX,
+};
+
+uint8_t aw87xxx_dsp_isEnable(void);
+int aw87xxx_dsp_get_rx_module_enable(int *enable);
+int aw87xxx_dsp_set_rx_module_enable(int enable);
+int aw87xxx_dsp_get_vmax(uint32_t *vmax, int channel);
+int aw87xxx_dsp_set_vmax(uint32_t vmax, int channel);
+int aw87xxx_dsp_set_spin(uint32_t ctrl_value);
+int aw87xxx_dsp_get_spin(void);
+int aw87xxx_spin_set_record_val(void);
+void aw87xxx_device_parse_port_id_dt(struct aw_device *aw_dev);
+void aw87xxx_device_parse_topo_id_dt(struct aw_device *aw_dev);
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_log.h b/sound/soc/codecs/aw87xxx/aw87xxx_log.h
new file mode 100644
index 000000000000..b3bde38a23c6
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_log.h
@@ -0,0 +1,33 @@
+#ifndef __AW87XXX_LOG_H__
+#define __AW87XXX_LOG_H__
+
+#include <linux/kernel.h>
+
+
+/********************************************
+ *
+ * print information control
+ *
+ *******************************************/
+#define AW_LOGI(fmt, ...)\
+	pr_info("[Awinic] %s:" fmt "\n", __func__, ##__VA_ARGS__)
+
+#define AW_LOGD(fmt, ...)\
+	pr_debug("[Awinic] %s:" fmt "\n", __func__, ##__VA_ARGS__)
+
+#define AW_LOGE(fmt, ...)\
+	pr_err("[Awinic] %s:" fmt "\n", __func__, ##__VA_ARGS__)
+
+
+#define AW_DEV_LOGI(dev, fmt, ...)\
+	pr_info("[Awinic] [%s]%s: " fmt "\n", dev_name(dev), __func__, ##__VA_ARGS__)
+
+#define AW_DEV_LOGD(dev, fmt, ...)\
+	pr_debug("[Awinic] [%s]%s: " fmt "\n", dev_name(dev), __func__, ##__VA_ARGS__)
+
+#define AW_DEV_LOGE(dev, fmt, ...)\
+	pr_err("[Awinic] [%s]%s: " fmt "\n", dev_name(dev), __func__, ##__VA_ARGS__)
+
+
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_monitor.c b/sound/soc/codecs/aw87xxx/aw87xxx_monitor.c
new file mode 100644
index 000000000000..f580506b2786
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_monitor.c
@@ -0,0 +1,1208 @@
+/*
+ * aw87xxx_monitor.c
+ *
+ * Copyright (c) 2021 AWINIC Technology CO., LTD
+ *
+ * Author: Barry <zhaozhongbo@awinic.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+#include <linux/module.h>
+#include <linux/uaccess.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/power_supply.h>
+#include <linux/of.h>
+#include <linux/power_supply.h>
+#include <linux/hrtimer.h>
+#include <linux/i2c.h>
+#include <linux/gpio.h>
+#include <linux/of_gpio.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/firmware.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/io.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/dma-mapping.h>
+#include <linux/gameport.h>
+#include <linux/moduleparam.h>
+#include <linux/mutex.h>
+#include <linux/workqueue.h>
+#include "aw87xxx.h"
+#include "aw87xxx_log.h"
+#include "aw87xxx_monitor.h"
+#include "aw87xxx_dsp.h"
+#include "aw87xxx_bin_parse.h"
+#include "aw87xxx_device.h"
+
+#define AW_MONITOT_BIN_PARSE_VERSION	"V0.1.0"
+
+#define AW_GET_32_DATA(w, x, y, z) \
+	((uint32_t)((((uint8_t)w) << 24) | (((uint8_t)x) << 16) | \
+	(((uint8_t)y) << 8) | ((uint8_t)z)))
+
+/****************************************************************************
+ *
+ * aw87xxx monitor bin check
+ *
+ ****************************************************************************/
+static int aw_monitor_check_header_v_1_0_0(struct device *dev,
+				char *data, uint32_t data_len)
+{
+	int i = 0;
+	struct aw_bin_header *header = (struct aw_bin_header *)data;
+
+	if (header->bin_data_type != DATA_TYPE_MONITOR_ANALOG) {
+		AW_DEV_LOGE(dev, "monitor data_type check error!");
+		return -EINVAL;
+	}
+
+	if (header->bin_data_size != AW_MONITOR_HDR_DATA_SIZE) {
+		AW_DEV_LOGE(dev, "monitor data_size error!");
+		return -EINVAL;
+	}
+
+	if (header->data_byte_len != AW_MONITOR_HDR_DATA_BYTE_LEN) {
+		AW_DEV_LOGE(dev, "monitor data_byte_len error!");
+		return -EINVAL;
+	}
+
+	for (i = 0; i < AW_MONITOR_DATA_VER_MAX; i++) {
+		if (header->bin_data_ver == i) {
+			AW_LOGD("monitor bin_data_ver[0x%x]", i);
+			break;
+		}
+	}
+	if (i == AW_MONITOR_DATA_VER_MAX)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int aw_monitor_check_data_v1_size(struct device *dev,
+				char *data, int32_t data_len)
+{
+	int32_t bin_header_len  = sizeof(struct aw_bin_header);
+	int32_t monitor_header_len = sizeof(struct aw_monitor_header);
+	int32_t monitor_data_len = sizeof(struct vmax_step_config);
+	int32_t len = 0;
+	struct aw_monitor_header *monitor_header = NULL;
+
+	AW_DEV_LOGD(dev, "enter");
+
+	if (data_len < bin_header_len + monitor_header_len) {
+		AW_DEV_LOGE(dev, "bin len is less than aw_bin_header and monitoor_header,check failed");
+		return -EINVAL;
+	}
+
+	monitor_header = (struct aw_monitor_header *)(data + bin_header_len);
+	len = data_len - bin_header_len - monitor_header_len;
+	if (len < monitor_header->step_count * monitor_data_len) {
+		AW_DEV_LOGE(dev, "bin data len is not enough,check failed");
+		return -EINVAL;
+	}
+
+	AW_DEV_LOGD(dev, "succeed");
+
+	return 0;
+}
+
+static int aw_monitor_check_data_size(struct device *dev,
+			char *data, int32_t data_len)
+{
+	int ret = -1;
+	struct aw_bin_header *header = (struct aw_bin_header *)data;
+
+	switch (header->bin_data_ver) {
+	case AW_MONITOR_DATA_VER:
+		ret = aw_monitor_check_data_v1_size(dev, data, data_len);
+		if (ret < 0)
+			return ret;
+		break;
+	default:
+		AW_DEV_LOGE(dev, "bin data_ver[0x%x] non support",
+			header->bin_data_ver);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+
+static int aw_monitor_check_bin_header(struct device *dev,
+				char *data, int32_t data_len)
+{
+	int ret = -1;
+	struct aw_bin_header *header = NULL;
+
+	if (data_len < sizeof(struct aw_bin_header)) {
+		AW_DEV_LOGE(dev, "bin len is less than aw_bin_header,check failed");
+		return -EINVAL;
+	}
+	header = (struct aw_bin_header *)data;
+
+	switch (header->header_ver) {
+	case HEADER_VERSION_1_0_0:
+		ret = aw_monitor_check_header_v_1_0_0(dev, data, data_len);
+		if (ret < 0) {
+			AW_DEV_LOGE(dev, "monitor bin haeder info check error!");
+			return ret;
+		}
+		break;
+	default:
+		AW_DEV_LOGE(dev, "bin version[0x%x] non support",
+			header->header_ver);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int aw_monitor_bin_check_sum(struct device *dev,
+			char *data, int32_t data_len)
+{
+	int i, data_sum = 0;
+	uint32_t *check_sum = (uint32_t *)data;
+
+	for (i = 4; i < data_len; i++)
+		data_sum += data[i];
+
+	if (*check_sum != data_sum) {
+		AW_DEV_LOGE(dev, "check_sum[%d] is not equal to data_sum[%d]",
+				*check_sum, data_sum);
+		return -ENOMEM;
+	}
+
+	AW_DEV_LOGD(dev, "succeed");
+
+	return 0;
+}
+
+static int aw_monitor_bin_check(struct device *dev,
+				char *monitor_data, uint32_t data_len)
+{
+	int ret = -1;
+
+	if (monitor_data == NULL || data_len == 0) {
+		AW_DEV_LOGE(dev, "none data to parse");
+		return -EINVAL;
+	}
+
+	ret = aw_monitor_bin_check_sum(dev, monitor_data, data_len);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "bin data check sum failed");
+		return ret;
+	}
+
+	ret = aw_monitor_check_bin_header(dev, monitor_data, data_len);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "bin data len check failed");
+		return ret;
+	}
+
+	ret = aw_monitor_check_data_size(dev, monitor_data, data_len);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "bin header info check failed");
+		return ret;
+	}
+
+	return 0;
+}
+
+/*****************************************************************************
+ *
+ * aw87xxx monitor header bin parse
+ *
+ *****************************************************************************/
+static void aw_monitor_write_to_table_v1(struct device *dev,
+			struct vmax_step_config *vmax_step,
+			char *vmax_data, uint32_t step_count)
+{
+	int i = 0;
+	int index = 0;
+	int vmax_step_size = (int)sizeof(struct vmax_step_config);
+
+	for (i = 0; i < step_count; i++) {
+		index = vmax_step_size * i;
+		vmax_step[i].vbat_min =
+			AW_GET_32_DATA(vmax_data[index + 3],
+					vmax_data[index + 2],
+					vmax_data[index + 1],
+					vmax_data[index + 0]);
+		vmax_step[i].vbat_max =
+			AW_GET_32_DATA(vmax_data[index + 7],
+					vmax_data[index + 6],
+					vmax_data[index + 5],
+					vmax_data[index + 4]);
+		vmax_step[i].vmax_vol =
+			AW_GET_32_DATA(vmax_data[index + 11],
+					vmax_data[index + 10],
+					vmax_data[index + 9],
+					vmax_data[index + 8]);
+	}
+
+	for (i = 0; i < step_count; i++)
+		AW_DEV_LOGI(dev, "vbat_min:%d, vbat_max%d, vmax_vol:0x%x",
+			vmax_step[i].vbat_min,
+			vmax_step[i].vbat_max,
+			vmax_step[i].vmax_vol);
+}
+
+static int aw_monitor_parse_vol_data_v1(struct device *dev,
+			struct aw_monitor *monitor, char *monitor_data)
+{
+	uint32_t step_count = 0;
+	char *vmax_data = NULL;
+	struct vmax_step_config *vmax_step = NULL;
+
+	AW_DEV_LOGD(dev, "enter");
+
+	step_count = monitor->monitor_hdr.step_count;
+	if (step_count) {
+		vmax_step = devm_kzalloc(dev, sizeof(struct vmax_step_config) * step_count,
+					GFP_KERNEL);
+		if (vmax_step == NULL) {
+			AW_DEV_LOGE(dev, "vmax_cfg vmalloc failed");
+			return -ENOMEM;
+		}
+		memset(vmax_step, 0,
+			sizeof(struct vmax_step_config) * step_count);
+	}
+
+	vmax_data = monitor_data + sizeof(struct aw_bin_header) +
+		sizeof(struct aw_monitor_header);
+	aw_monitor_write_to_table_v1(dev, vmax_step, vmax_data, step_count);
+	monitor->vmax_cfg = vmax_step;
+
+	AW_DEV_LOGI(dev, "vmax_data parse succeed");
+
+	return 0;
+}
+
+static int aw_monitor_parse_data_v1(struct device *dev,
+			struct aw_monitor *monitor, char *monitor_data)
+{
+	int ret = -1;
+	int header_len = 0;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	header_len = sizeof(struct aw_bin_header);
+	memcpy(monitor_hdr, monitor_data + header_len,
+		sizeof(struct aw_monitor_header));
+
+	AW_DEV_LOGI(dev, "monitor_switch:%d, monitor_time:%d (ms), monitor_count:%d, step_count:%d",
+		monitor_hdr->monitor_switch, monitor_hdr->monitor_time,
+		monitor_hdr->monitor_count, monitor_hdr->step_count);
+
+	ret = aw_monitor_parse_vol_data_v1(dev, monitor, monitor_data);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "vmax_data parse failed");
+		return ret;
+	}
+
+	monitor->bin_status = AW_MONITOR_CFG_OK;
+
+	return 0;
+}
+
+
+static int aw_monitor_parse_v_1_0_0(struct device *dev,
+			struct aw_monitor *monitor, char *monitor_data)
+{
+	int ret = -1;
+	struct aw_bin_header *header = (struct aw_bin_header *)monitor_data;
+
+	switch (header->bin_data_ver) {
+	case AW_MONITOR_DATA_VER:
+		ret = aw_monitor_parse_data_v1(dev, monitor, monitor_data);
+		if (ret < 0)
+			return ret;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+void aw87xxx_monitor_cfg_free(struct aw_monitor *monitor)
+{
+	struct aw87xxx *aw87xxx =
+		container_of(monitor, struct aw87xxx, monitor);
+
+	monitor->bin_status = AW_MONITOR_CFG_WAIT;
+	memset(&monitor->monitor_hdr, 0,
+		sizeof(struct aw_monitor_header));
+	if (monitor->vmax_cfg) {
+		devm_kfree(aw87xxx->dev, monitor->vmax_cfg);
+		monitor->vmax_cfg = NULL;
+	}
+}
+
+int aw87xxx_monitor_bin_parse(struct device *dev,
+				char *monitor_data, uint32_t data_len)
+{
+	int ret = -1;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = NULL;
+	struct aw_bin_header *bin_header = NULL;
+
+	if (aw87xxx == NULL) {
+		AW_DEV_LOGE(dev, "get struct aw87xxx failed");
+		return -EINVAL;
+	}
+
+	monitor = &aw87xxx->monitor;
+	monitor->bin_status = AW_MONITOR_CFG_WAIT;
+
+	AW_DEV_LOGI(dev, "monitor bin parse version: %s",
+			AW_MONITOT_BIN_PARSE_VERSION);
+
+	ret = aw_monitor_bin_check(dev, monitor_data, data_len);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "monitor bin check failed");
+		return ret;
+	}
+
+	bin_header = (struct aw_bin_header *)monitor_data;
+	switch (bin_header->bin_data_ver) {
+	case DATA_VERSION_V1:
+		ret = aw_monitor_parse_v_1_0_0(dev, monitor,
+				monitor_data);
+		if (ret < 0) {
+			aw87xxx_monitor_cfg_free(monitor);
+			return ret;
+		}
+		break;
+	default:
+		AW_DEV_LOGE(dev, "Unrecognized this bin data version[0x%x]",
+			bin_header->bin_data_ver);
+	}
+
+	return 0;
+}
+
+/***************************************************************************
+ *
+ * aw87xxx monitor get adjustment vmax of power
+ *
+ ***************************************************************************/
+static int aw_monitor_get_battery_capacity(struct device *dev,
+				struct aw_monitor *monitor, int *vbat_capacity)
+{
+	char name[] = "battery";
+	int ret = -1;
+	union power_supply_propval prop = { 0 };
+	struct power_supply *psy = NULL;
+
+	psy = power_supply_get_by_name(name);
+	if (psy == NULL) {
+		AW_DEV_LOGE(dev, "no struct power supply name:%s", name);
+		return -EINVAL;
+	}
+
+	ret = power_supply_get_property(psy, POWER_SUPPLY_PROP_CAPACITY, &prop);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "get vbat capacity failed");
+		return -EINVAL;
+	}
+	*vbat_capacity = prop.intval;
+	AW_DEV_LOGI(dev, "The percentage is %d",
+		*vbat_capacity);
+
+	return 0;
+}
+
+static int aw_search_vmax_from_table(struct device *dev,
+				struct aw_monitor *monitor,
+				const int vbat_vol, int *vmax_vol)
+{
+	int i = 0;
+	int vmax_set = 0;
+	uint32_t vmax_flag = 0;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+	struct vmax_step_config *vmax_cfg = monitor->vmax_cfg;
+
+	if (monitor->bin_status == AW_MONITOR_CFG_WAIT) {
+		AW_DEV_LOGE(dev, "vmax_cfg not loaded or parse failed");
+		return -ENODATA;
+	}
+
+	for (i = 0; i < monitor_hdr->step_count; i++) {
+		if (vbat_vol == AW_VBAT_MAX) {
+			vmax_set = AW_VMAX_MAX;
+			vmax_flag = 1;
+			AW_DEV_LOGD(dev, "vbat=%d, setting vmax=0x%x",
+				vbat_vol, vmax_set);
+			break;
+		}
+
+		if (vbat_vol >= vmax_cfg[i].vbat_min &&
+			vbat_vol < vmax_cfg[i].vbat_max) {
+			vmax_set = vmax_cfg[i].vmax_vol;
+			vmax_flag = 1;
+			AW_DEV_LOGD(dev, "read setting vmax=0x%x, step[%d]: vbat_min=%d,vbat_max=%d",
+				vmax_set, i,
+				vmax_cfg[i].vbat_min,
+				vmax_cfg[i].vbat_max);
+			break;
+		}
+	}
+
+	if (!vmax_flag) {
+		AW_DEV_LOGE(dev, "vmax_cfg not found");
+		return -ENODATA;
+	}
+
+	*vmax_vol = vmax_set;
+	return 0;
+}
+
+
+/***************************************************************************
+ *
+ *monitor_esd_func
+ *
+ ***************************************************************************/
+static int aw_chip_status_recover(struct aw87xxx *aw87xxx)
+{
+	int ret = -1;
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	char *profile = aw87xxx->current_profile;
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+
+	ret = aw87xxx_update_profile_esd(aw87xxx, profile);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "load profile[%s] failed ",
+			profile);
+		return ret;
+	}
+
+	AW_DEV_LOGI(aw87xxx->dev, "current prof[%s], dev_index[%d] ",
+			profile, aw87xxx->dev_index);
+
+	monitor->pre_vmax = AW_VMAX_INIT_VAL;
+	monitor->first_entry = AW_FIRST_ENTRY;
+	monitor->timer_cnt = 0;
+	monitor->vbat_sum = 0;
+
+	return 0;
+}
+
+static int aw_monitor_chip_esd_check_work(struct aw87xxx *aw87xxx)
+{
+	int ret = 0;
+	int i = 0;
+
+	for (i = 0; i < REG_STATUS_CHECK_MAX; i++) {
+		AW_DEV_LOGD(aw87xxx->dev, "reg_status_check[%d]", i);
+
+		ret = aw87xxx_dev_esd_reg_status_check(&aw87xxx->aw_dev);
+		if (ret < 0) {
+			aw_chip_status_recover(aw87xxx);
+		} else {
+			AW_DEV_LOGD(aw87xxx->dev, "chip status check succeed");
+			break;
+		}
+		msleep(AW_ESD_CHECK_DELAY);
+	}
+
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "chip status recover failed,chip off");
+		aw87xxx_update_profile_esd(aw87xxx, aw87xxx->prof_off_name);
+		return ret;
+	}
+
+	return 0;
+}
+
+
+/***************************************************************************
+ *
+ * aw87xxx monitor work with dsp
+ *
+ ***************************************************************************/
+static int aw_monitor_update_vmax_to_dsp(struct device *dev,
+				struct aw_monitor *monitor, int vmax_set)
+{
+	int ret = -1;
+	uint32_t enable = 0;
+
+	if (monitor->pre_vmax != vmax_set) {
+		ret = aw87xxx_dsp_get_rx_module_enable(&enable);
+		if (!enable || ret < 0) {
+			AW_DEV_LOGE(dev, "get rx failed or rx disable, ret=%d, enable=%d",
+				ret, enable);
+			return -EPERM;
+		}
+
+		ret = aw87xxx_dsp_set_vmax(vmax_set, monitor->dev_index);
+		if (ret) {
+			AW_DEV_LOGE(dev, "set dsp msg fail, ret=%d", ret);
+			return ret;
+		}
+
+		AW_DEV_LOGI(dev, "set dsp vmax=0x%x sucess", vmax_set);
+		monitor->pre_vmax = vmax_set;
+	} else {
+		AW_DEV_LOGI(dev, "vmax=0x%x no change", vmax_set);
+	}
+
+	return 0;
+}
+
+static void aw_monitor_with_dsp_vmax_work(struct device *dev,
+					struct aw_monitor *monitor)
+{
+	int ret = -1;
+	int vmax_set = 0;
+	int vbat_capacity = 0;
+	int ave_capacity = 0;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	AW_DEV_LOGD(dev, "enter with dsp monitor");
+
+	ret = aw_monitor_get_battery_capacity(dev, monitor, &vbat_capacity);
+	if (ret < 0)
+		return;
+
+	if (monitor->timer_cnt < monitor_hdr->monitor_count) {
+		monitor->timer_cnt++;
+		monitor->vbat_sum += vbat_capacity;
+			AW_DEV_LOGI(dev, "timer_cnt = %d",
+			monitor->timer_cnt);
+	}
+	if ((monitor->timer_cnt >= monitor_hdr->monitor_count) ||
+	    (monitor->first_entry == AW_FIRST_ENTRY)) {
+		if (monitor->first_entry == AW_FIRST_ENTRY)
+			monitor->first_entry = AW_NOT_FIRST_ENTRY;
+		ave_capacity = monitor->vbat_sum / monitor->timer_cnt;
+
+		if (monitor->custom_capacity)
+			ave_capacity = monitor->custom_capacity;
+
+		AW_DEV_LOGI(dev, "get average capacity = %d", ave_capacity);
+
+		ret = aw_search_vmax_from_table(dev, monitor,
+				ave_capacity, &vmax_set);
+		if (ret < 0)
+			AW_DEV_LOGE(dev, "not find vmax_vol");
+		else
+			aw_monitor_update_vmax_to_dsp(dev, monitor, vmax_set);
+
+		monitor->timer_cnt = 0;
+		monitor->vbat_sum = 0;
+	}
+}
+
+static void aw_monitor_work_func(struct work_struct *work)
+{
+	int ret = 0;
+	struct aw87xxx *aw87xxx = container_of(work,
+				struct aw87xxx, monitor.with_dsp_work.work);
+	struct device *dev = aw87xxx->dev;
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	AW_DEV_LOGD(dev, "enter");
+
+	if (monitor->esd_enable) {
+		ret = aw_monitor_chip_esd_check_work(aw87xxx);
+		if (ret < 0)
+			return;
+	}
+
+	if (monitor_hdr->monitor_switch && !(aw87xxx->aw_dev.is_rec_mode) &&
+		monitor->open_dsp_en && monitor->bin_status == AW_ACF_UPDATE) {
+		AW_DEV_LOGD(dev, "start low power protection");
+		aw_monitor_with_dsp_vmax_work(dev, monitor);
+	}
+
+	if (monitor->esd_enable || (monitor_hdr->monitor_switch &&
+		!(aw87xxx->aw_dev.is_rec_mode) && monitor->open_dsp_en &&
+		monitor->bin_status == AW_ACF_UPDATE)) {
+		schedule_delayed_work(&monitor->with_dsp_work,
+			msecs_to_jiffies(monitor_hdr->monitor_time));
+	}
+}
+
+void aw87xxx_monitor_stop(struct aw_monitor *monitor)
+{
+	struct aw87xxx *aw87xxx =
+		container_of(monitor, struct aw87xxx, monitor);
+
+	AW_DEV_LOGD(aw87xxx->dev, "enter");
+	cancel_delayed_work_sync(&monitor->with_dsp_work);
+}
+
+void aw87xxx_monitor_start(struct aw_monitor *monitor)
+{
+	struct aw87xxx *aw87xxx =
+		container_of(monitor, struct aw87xxx, monitor);
+	int ret = 0;
+
+	ret = aw87xxx_dev_check_reg_is_rec_mode(&aw87xxx->aw_dev);
+	if (ret < 0) {
+		AW_DEV_LOGE(aw87xxx->dev, "get reg current mode failed");
+		return;
+	}
+
+	if (monitor->esd_enable || (monitor->monitor_hdr.monitor_switch &&
+			!(aw87xxx->aw_dev.is_rec_mode) && monitor->open_dsp_en
+			&& monitor->bin_status == AW_ACF_UPDATE)) {
+
+		AW_DEV_LOGD(aw87xxx->dev, "enter");
+		monitor->pre_vmax = AW_VMAX_INIT_VAL;
+		monitor->first_entry = AW_FIRST_ENTRY;
+		monitor->timer_cnt = 0;
+		monitor->vbat_sum = 0;
+
+		schedule_delayed_work(&monitor->with_dsp_work,
+				msecs_to_jiffies(monitor->monitor_hdr.monitor_time));
+	}
+}
+/***************************************************************************
+ *
+ * aw87xxx no dsp monitor func
+ *
+ ***************************************************************************/
+int aw87xxx_monitor_no_dsp_get_vmax(struct aw_monitor *monitor, int32_t *vmax)
+{
+	int vbat_capacity = 0;
+	int ret = -1;
+	int vmax_vol = 0;
+	struct aw87xxx *aw87xxx =
+		container_of(monitor, struct aw87xxx, monitor);
+	struct device *dev = aw87xxx->dev;
+
+	ret = aw_monitor_get_battery_capacity(dev, monitor, &vbat_capacity);
+	if (ret < 0)
+		return ret;
+
+	if (monitor->custom_capacity)
+		vbat_capacity = monitor->custom_capacity;
+	AW_DEV_LOGI(dev, "get_battery_capacity is[%d]", vbat_capacity);
+
+	ret = aw_search_vmax_from_table(dev, monitor,
+				vbat_capacity, &vmax_vol);
+	if (ret < 0) {
+		AW_DEV_LOGE(dev, "not find vmax_vol");
+		return ret;
+	}
+
+	*vmax = vmax_vol;
+	return 0;
+}
+
+
+/***************************************************************************
+ *
+ * aw87xxx monitor sysfs nodes
+ *
+ ***************************************************************************/
+static ssize_t aw_attr_get_esd_enable(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+
+	if (monitor->esd_enable) {
+		AW_DEV_LOGI(aw87xxx->dev, "esd-enable=true");
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"esd-enable=true\n");
+	} else {
+		AW_DEV_LOGI(aw87xxx->dev, "esd-enable=false");
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"esd-enable=false\n");
+	}
+
+	return len;
+}
+
+static ssize_t aw_attr_set_esd_enable(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t len)
+{
+	char esd_enable[AW_ESD_ENABLE_STRLEN] = {0};
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+
+	if (strlen(buf) > AW_ESD_ENABLE_STRLEN) {
+		AW_DEV_LOGE(aw87xxx->dev, "input esd_enable_str_len is out of max[%d]",
+				AW_ESD_ENABLE_STRLEN);
+		return -EINVAL;
+	}
+
+	if (sscanf(buf, "%s", esd_enable) == 1) {
+		AW_DEV_LOGD(aw87xxx->dev, "input esd-enable=[%s]", esd_enable);
+		if (!strcmp(esd_enable, "true"))
+			monitor->esd_enable = AW_ESD_ENABLE;
+		else
+			monitor->esd_enable = AW_ESD_DISABLE;
+		AW_DEV_LOGI(dev, "set esd-enable=[%s]",
+				monitor->esd_enable ? "true" : "false");
+	} else {
+		AW_DEV_LOGE(aw87xxx->dev, "input esd-enable error");
+		return -EINVAL;
+	}
+
+	return len;
+}
+
+static ssize_t aw_attr_get_vbat(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	int ret = -1;
+	int vbat_capacity = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+
+	if (monitor->custom_capacity == 0) {
+		ret = aw_monitor_get_battery_capacity(dev, monitor,
+					&vbat_capacity);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev, "get battery_capacity failed");
+			return ret;
+		}
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"vbat capacity=%d\n", vbat_capacity);
+	} else {
+		len += snprintf(buf + len, PAGE_SIZE - len,
+				"vbat capacity=%d\n",
+				monitor->custom_capacity);
+	}
+
+	return len;
+}
+
+static ssize_t aw_attr_set_vbat(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t len)
+{
+	int ret = -1;
+	int capacity = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+
+	ret = kstrtouint(buf, 0, &capacity);
+	if (ret < 0)
+		return ret;
+	AW_DEV_LOGI(aw87xxx->dev, "set capacity = %d", capacity);
+	if (capacity >= AW_VBAT_CAPACITY_MIN &&
+			capacity <= AW_VBAT_CAPACITY_MAX){
+		monitor->custom_capacity = capacity;
+	} else {
+		AW_DEV_LOGE(aw87xxx->dev, "vbat_set=invalid,please input value [%d-%d]",
+			AW_VBAT_CAPACITY_MIN, AW_VBAT_CAPACITY_MAX);
+		return -EINVAL;
+	}
+
+	return len;
+}
+
+static ssize_t aw_attr_get_vmax(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	int ret = -1;
+	int vbat_capacity = 0;
+	int vmax_get = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+
+	if (monitor->open_dsp_en) {
+		ret = aw87xxx_dsp_get_vmax(&vmax_get, aw87xxx->dev_index);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev,
+				"get dsp vmax fail, ret=%d", ret);
+			return ret;
+		}
+		len += snprintf(buf + len, PAGE_SIZE - len,
+				"get_vmax=%d\n", vmax_get);
+	} else {
+		ret = aw_monitor_get_battery_capacity(dev, monitor,
+						&vbat_capacity);
+		if (ret < 0)
+			return ret;
+		AW_DEV_LOGI(aw87xxx->dev, "get_battery_capacity is [%d]",
+			vbat_capacity);
+
+		if (monitor->custom_capacity) {
+			vbat_capacity = monitor->custom_capacity;
+			AW_DEV_LOGI(aw87xxx->dev, "get custom_capacity is [%d]",
+				vbat_capacity);
+		}
+
+		ret = aw_search_vmax_from_table(aw87xxx->dev, monitor,
+					vbat_capacity, &vmax_get);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev, "not find vmax_vol");
+			len += snprintf(buf + len, PAGE_SIZE - len,
+				"not_find_vmax_vol\n");
+			return len;
+		}
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"0x%x\n", vmax_get);
+		AW_DEV_LOGI(aw87xxx->dev, "0x%x", vmax_get);
+	}
+
+	return len;
+}
+
+static ssize_t aw_attr_set_vmax(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	uint32_t vmax_set = 0;
+	int ret = -1;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+
+	ret = kstrtouint(buf, 0, &vmax_set);
+	if (ret < 0)
+		return ret;
+
+	AW_DEV_LOGI(aw87xxx->dev, "vmax_set=0x%x", vmax_set);
+
+	if (monitor->open_dsp_en) {
+		ret = aw87xxx_dsp_set_vmax(vmax_set, aw87xxx->dev_index);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev, "send dsp_msg error, ret = %d",
+				ret);
+			return ret;
+		}
+		msleep(2);
+	} else {
+		AW_DEV_LOGE(aw87xxx->dev, "no_dsp system,vmax_set invalid");
+		return -EINVAL;
+	}
+
+	return count;
+}
+
+static ssize_t aw_attr_get_monitor_switch(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	len += snprintf(buf + len, PAGE_SIZE - len,
+			"aw87xxx monitor switch: %u\n",
+			monitor_hdr->monitor_switch);
+	return len;
+}
+
+
+int aw87xxx_dev_monitor_switch_set(struct aw_monitor *monitor, uint32_t enable)
+{
+	struct aw87xxx *aw87xxx =
+			container_of(monitor, struct aw87xxx, monitor);
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	AW_DEV_LOGI(aw87xxx->dev, "monitor switch set =%d", enable);
+
+	if (!monitor->bin_status) {
+		AW_DEV_LOGE(aw87xxx->dev, "bin parse faile or not loaded,set invalid");
+		return -EINVAL;
+	}
+
+	if (monitor_hdr->monitor_switch == enable)
+		return 0;
+
+	if (enable > 0) {
+		monitor_hdr->monitor_switch = 1;
+		if (monitor->open_dsp_en) {
+			monitor->pre_vmax = AW_VMAX_INIT_VAL;
+			monitor->first_entry = AW_FIRST_ENTRY;
+			monitor->timer_cnt = 0;
+			monitor->vbat_sum = 0;
+		}
+	} else {
+		monitor_hdr->monitor_switch = 0;
+	}
+
+	return 0;
+}
+
+static ssize_t aw_attr_set_monitor_switch(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	uint32_t enable = 0;
+	int ret = -1;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+
+	ret = kstrtouint(buf, 0, &enable);
+	if (ret < 0)
+		return ret;
+
+	ret = aw87xxx_dev_monitor_switch_set(monitor, enable);
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t aw_attr_get_monitor_time(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	len += snprintf(buf + len, PAGE_SIZE - len,
+			"aw_monitor_timer = %u(ms)\n",
+			monitor_hdr->monitor_time);
+	return len;
+}
+
+static ssize_t aw_attr_set_monitor_time(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	unsigned int timer_val = 0;
+	int ret = -1;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	ret = kstrtouint(buf, 0, &timer_val);
+	if (ret < 0)
+		return ret;
+
+	AW_DEV_LOGI(aw87xxx->dev, "input monitor timer=%d(ms)", timer_val);
+
+	if (!monitor->bin_status) {
+		AW_DEV_LOGE(aw87xxx->dev, "bin parse faile or not loaded,set invalid");
+		return -EINVAL;
+	}
+
+	if (timer_val != monitor_hdr->monitor_time)
+		monitor_hdr->monitor_time = timer_val;
+	else
+		AW_DEV_LOGI(aw87xxx->dev, "no_change monitor_time");
+
+	return count;
+}
+
+static ssize_t aw_attr_get_monitor_count(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	ssize_t len = 0;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	len += snprintf(buf + len, PAGE_SIZE - len,
+			"aw_monitor_count = %u\n",
+			monitor_hdr->monitor_count);
+	return len;
+}
+
+static ssize_t aw_attr_set_monitor_count(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	unsigned int monitor_count = 0;
+	int ret = -1;
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	struct aw_monitor_header *monitor_hdr = &monitor->monitor_hdr;
+
+	ret = kstrtouint(buf, 0, &monitor_count);
+	if (ret < 0)
+		return ret;
+	AW_DEV_LOGI(aw87xxx->dev, "input monitor count=%d", monitor_count);
+
+	if (!monitor->bin_status) {
+		AW_DEV_LOGE(aw87xxx->dev, "bin parse faile or not loaded,set invalid");
+		return -EINVAL;
+	}
+
+	if (monitor_count != monitor_hdr->monitor_count)
+		monitor_hdr->monitor_count = monitor_count;
+	else
+		AW_DEV_LOGI(aw87xxx->dev, "no_change monitor_count");
+
+	return count;
+}
+
+
+static ssize_t aw_attr_get_rx(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	ssize_t len = 0;
+	int ret = -1;
+	uint32_t enable = 0;
+
+	if (monitor->open_dsp_en) {
+		ret = aw87xxx_dsp_get_rx_module_enable(&enable);
+		if (ret) {
+			AW_DEV_LOGE(aw87xxx->dev, "dsp_msg error, ret=%d", ret);
+			return ret;
+		}
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"aw87xxx rx: %u\n", enable);
+	} else {
+		len += snprintf(buf + len, PAGE_SIZE - len,
+				"command is invalid\n");
+	}
+
+	return len;
+}
+
+static ssize_t aw_attr_set_rx(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct aw87xxx *aw87xxx = dev_get_drvdata(dev);
+	struct aw_monitor *monitor = &aw87xxx->monitor;
+	int ret = -1;
+	uint32_t enable;
+
+	ret = kstrtouint(buf, 0, &enable);
+	if (ret < 0)
+		return ret;
+
+	if (monitor->open_dsp_en) {
+		AW_DEV_LOGI(aw87xxx->dev, "set rx enable=%d", enable);
+
+		ret = aw87xxx_dsp_set_rx_module_enable(enable);
+		if (ret < 0) {
+			AW_DEV_LOGE(aw87xxx->dev, "dsp_msg error, ret=%d",
+				ret);
+			return ret;
+		}
+	} else {
+		AW_DEV_LOGE(aw87xxx->dev, "command is invalid");
+		return -EINVAL;
+	}
+
+	return count;
+}
+
+
+static DEVICE_ATTR(esd_enable, S_IWUSR | S_IRUGO,
+	aw_attr_get_esd_enable, aw_attr_set_esd_enable);
+static DEVICE_ATTR(vbat, S_IWUSR | S_IRUGO,
+	aw_attr_get_vbat, aw_attr_set_vbat);
+static DEVICE_ATTR(vmax, S_IWUSR | S_IRUGO,
+	aw_attr_get_vmax, aw_attr_set_vmax);
+
+static DEVICE_ATTR(monitor_switch, S_IWUSR | S_IRUGO,
+	aw_attr_get_monitor_switch, aw_attr_set_monitor_switch);
+static DEVICE_ATTR(monitor_time, S_IWUSR | S_IRUGO,
+	aw_attr_get_monitor_time, aw_attr_set_monitor_time);
+static DEVICE_ATTR(monitor_count, S_IWUSR | S_IRUGO,
+	aw_attr_get_monitor_count, aw_attr_set_monitor_count);
+static DEVICE_ATTR(rx, S_IWUSR | S_IRUGO,
+	aw_attr_get_rx, aw_attr_set_rx);
+
+static struct attribute *aw_monitor_vol_adjust[] = {
+	&dev_attr_esd_enable.attr,
+	&dev_attr_vbat.attr,
+	&dev_attr_vmax.attr,
+	NULL
+};
+
+static struct attribute_group aw_monitor_vol_adjust_group = {
+	.attrs = aw_monitor_vol_adjust,
+};
+
+static struct attribute *aw_monitor_control[] = {
+	&dev_attr_monitor_switch.attr,
+	&dev_attr_monitor_time.attr,
+	&dev_attr_monitor_count.attr,
+	&dev_attr_rx.attr,
+	NULL
+};
+
+static struct attribute_group aw_monitor_control_group = {
+	.attrs = aw_monitor_control,
+};
+
+/***************************************************************************
+ *
+ * aw87xxx monitor init
+ *
+ ***************************************************************************/
+static void aw_monitor_dtsi_parse(struct device *dev,
+				struct aw_monitor *monitor,
+				struct device_node *dev_node)
+{
+	int ret = -1;
+	const char *esd_enable;
+
+	ret = of_property_read_string(dev_node, "esd-enable", &esd_enable);
+	if (ret < 0) {
+		AW_DEV_LOGI(dev, "esd_enable parse failed, user default[disable]");
+		monitor->esd_enable = AW_ESD_DISABLE;
+	} else {
+		if (!strcmp(esd_enable, "true"))
+			monitor->esd_enable = AW_ESD_ENABLE;
+		else
+			monitor->esd_enable = AW_ESD_DISABLE;
+
+		AW_DEV_LOGI(dev, "parse esd-enable=[%s]",
+				monitor->esd_enable ? "true" : "false");
+	}
+}
+
+void aw87xxx_monitor_init(struct device *dev, struct aw_monitor *monitor,
+				struct device_node *dev_node)
+{
+	int ret = -1;
+	struct aw87xxx *aw87xxx =
+		container_of(monitor, struct aw87xxx, monitor);
+
+	monitor->dev_index = aw87xxx->dev_index;
+	monitor->monitor_hdr.monitor_time = AW_DEFAULT_MONITOR_TIME;
+
+	aw_monitor_dtsi_parse(dev, monitor, dev_node);
+
+	/* get platform open dsp type */
+	monitor->open_dsp_en = aw87xxx_dsp_isEnable();
+
+	ret = sysfs_create_group(&dev->kobj, &aw_monitor_vol_adjust_group);
+	if (ret < 0)
+		AW_DEV_LOGE(dev, "failed to create monitor vol_adjust sysfs nodes");
+
+	INIT_DELAYED_WORK(&monitor->with_dsp_work, aw_monitor_work_func);
+
+	if (monitor->open_dsp_en) {
+		ret = sysfs_create_group(&dev->kobj, &aw_monitor_control_group);
+		if (ret < 0)
+			AW_DEV_LOGE(dev, "failed to create monitor dsp control sysfs nodes");
+	}
+
+	if (!ret)
+		AW_DEV_LOGI(dev, "monitor init succeed");
+}
+
+void aw87xxx_monitor_exit(struct aw_monitor *monitor)
+{
+	struct aw87xxx *aw87xxx =
+		container_of(monitor, struct aw87xxx, monitor);
+	/*rm attr node*/
+	sysfs_remove_group(&aw87xxx->dev->kobj,
+			&aw_monitor_vol_adjust_group);
+
+	aw87xxx_monitor_stop(monitor);
+
+	if (monitor->open_dsp_en) {
+		sysfs_remove_group(&aw87xxx->dev->kobj,
+				&aw_monitor_control_group);
+	}
+}
+
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_monitor.h b/sound/soc/codecs/aw87xxx/aw87xxx_monitor.h
new file mode 100644
index 000000000000..daf9f2bfa09f
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_monitor.h
@@ -0,0 +1,96 @@
+#ifndef __AW87XXX_MONITOR_H__
+#define __AW87XXX_MONITOR_H__
+
+#define AW_WAIT_DSP_OPEN_TIME			(3000)
+#define AW_VBAT_CAPACITY_MIN			(0)
+#define AW_VBAT_CAPACITY_MAX			(100)
+#define AW_VMAX_INIT_VAL			(0xFFFFFFFF)
+#define AW_VBAT_MAX				(100)
+#define AW_VMAX_MAX				(0)
+#define AW_DEFAULT_MONITOR_TIME			(3000)
+#define AW_WAIT_TIME				(3000)
+#define REG_STATUS_CHECK_MAX			(10)
+#define AW_ESD_CHECK_DELAY			(1)
+
+#define AW_ESD_ENABLE				(true)
+#define AW_ESD_DISABLE				(false)
+#define AW_ESD_ENABLE_STRLEN			(16)
+
+enum aw_monitor_init {
+	AW_MONITOR_CFG_WAIT = 0,
+	AW_MONITOR_CFG_OK = 1,
+};
+
+enum aw_monitor_hdr_info {
+	AW_MONITOR_HDR_DATA_SIZE = 0x00000004,
+	AW_MONITOR_HDR_DATA_BYTE_LEN = 0x00000004,
+};
+
+enum aw_monitor_data_ver {
+	AW_MONITOR_DATA_VER = 0x00000001,
+	AW_MONITOR_DATA_VER_MAX,
+};
+
+enum aw_monitor_first_enter {
+	AW_FIRST_ENTRY = 0,
+	AW_NOT_FIRST_ENTRY = 1,
+};
+
+struct aw_bin_header {
+	uint32_t check_sum;
+	uint32_t header_ver;
+	uint32_t bin_data_type;
+	uint32_t bin_data_ver;
+	uint32_t bin_data_size;
+	uint32_t ui_ver;
+	char product[8];
+	uint32_t addr_byte_len;
+	uint32_t data_byte_len;
+	uint32_t device_addr;
+	uint32_t reserve[4];
+};
+
+struct aw_monitor_header {
+	uint32_t monitor_switch;
+	uint32_t monitor_time;
+	uint32_t monitor_count;
+	uint32_t step_count;
+	uint32_t reserve[4];
+};
+
+struct vmax_step_config {
+	uint32_t vbat_min;
+	uint32_t vbat_max;
+	int vmax_vol;
+};
+
+struct aw_monitor {
+	bool open_dsp_en;
+	bool esd_enable;
+	int32_t dev_index;
+	uint8_t first_entry;
+	uint8_t timer_cnt;
+	uint32_t vbat_sum;
+	int32_t custom_capacity;
+	uint32_t pre_vmax;
+
+	int bin_status;
+	struct aw_monitor_header monitor_hdr;
+	struct vmax_step_config *vmax_cfg;
+
+	struct delayed_work with_dsp_work;
+};
+
+void aw87xxx_monitor_cfg_free(struct aw_monitor *monitor);
+int aw87xxx_monitor_bin_parse(struct device *dev,
+			char *monitor_data, uint32_t data_len);
+void aw87xxx_monitor_stop(struct aw_monitor *monitor);
+void aw87xxx_monitor_start(struct aw_monitor *monitor);
+int aw87xxx_monitor_no_dsp_get_vmax(struct aw_monitor *monitor,
+					int32_t *vmax);
+void aw87xxx_monitor_init(struct device *dev, struct aw_monitor *monitor,
+				struct device_node *dev_node);
+void aw87xxx_monitor_exit(struct aw_monitor *monitor);
+int aw87xxx_dev_monitor_switch_set(struct aw_monitor *monitor, uint32_t enable);
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_18_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_18_reg.h
new file mode 100644
index 000000000000..74d6548db91e
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_18_reg.h
@@ -0,0 +1,2315 @@
+#ifndef __AW87XXX_PID_18_REG_H__
+#define __AW87XXX_PID_18_REG_H__
+
+/* registers list */
+#define AW87XXX_PID_18_CHIPID_REG		(0x00)
+#define AW87XXX_PID_18_SYSST_REG		(0x01)
+#define AW87XXX_PID_18_SYSINT_REG		(0x02)
+#define AW87XXX_PID_18_SYSCTRL_REG		(0x03)
+#define AW87XXX_PID_18_CPOC_REG			(0x04)
+#define AW87XXX_PID_18_CLASSD_REG		(0x05)
+#define AW87XXX_PID_18_MADPVTH_REG		(0x06)
+#define AW87XXX_PID_18_A3PARAM_REG		(0x07)
+#define AW87XXX_PID_18_A3A2PO_REG		(0x08)
+#define AW87XXX_PID_18_A2PARAM_REG		(0x09)
+#define AW87XXX_PID_18_A1PARAM_REG		(0x0A)
+#define AW87XXX_PID_18_POPCLK_REG		(0x0B)
+#define AW87XXX_PID_18_GTDRCPSS_REG		(0x0C)
+#define AW87XXX_PID_18_MULTI_REG		(0x0D)
+#define AW87XXX_PID_18_DFT1_REG			(0x61)
+#define AW87XXX_PID_18_DFT2_REG			(0x62)
+#define AW87XXX_PID_18_DFT3_REG			(0x63)
+#define AW87XXX_PID_18_DFT4_REG			(0x64)
+#define AW87XXX_PID_18_DFT5_REG			(0x65)
+#define AW87XXX_PID_18_DFT6_REG			(0x66)
+
+#define AW87XXX_PID_18_CLASSD_DEFAULT		(0x10)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_18_softrst_access[2] = {0x00, 0xaa};
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_18_REG_MAX			(0x67)
+
+#define REG_NONE_ACCESS					(0)
+#define REG_RD_ACCESS					(1 << 0)
+#define REG_WR_ACCESS					(1 << 1)
+
+const unsigned char aw87xxx_pid_18_reg_access[AW87XXX_PID_18_REG_MAX] = {
+	[AW87XXX_PID_18_CHIPID_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_18_SYSST_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_18_SYSINT_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_18_SYSCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_CPOC_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_CLASSD_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_MADPVTH_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_A3PARAM_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_A3A2PO_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_A2PARAM_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_A1PARAM_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_POPCLK_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_GTDRCPSS_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_MULTI_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_DFT1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_DFT2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_DFT3_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_DFT4_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_DFT5_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_18_DFT6_REG]	= (REG_RD_ACCESS),
+};
+
+/* detail information of registers begin */
+/* CHIPID (0x00) detail */
+/* IDCODE bit 7:0 (CHIPID 0x00) */
+#define AW87XXX_PID_18_IDCODE_START_BIT	(0)
+#define AW87XXX_PID_18_IDCODE_BITS_LEN	(8)
+#define AW87XXX_PID_18_IDCODE_MASK		\
+	(~(((1<<AW87XXX_PID_18_IDCODE_BITS_LEN)-1) << AW87XXX_PID_18_IDCODE_START_BIT))
+
+#define AW87XXX_PID_18_IDCODE_DEFAULT_VALUE	(0x18)
+#define AW87XXX_PID_18_IDCODE_DEFAULT	\
+	(AW87XXX_PID_18_IDCODE_DEFAULT_VALUE << AW87XXX_PID_18_IDCODE_START_BIT)
+
+/* default value of CHIPID (0x00) */
+/* #define AW87XXX_PID_18_CHIPID_DEFAULT		(0x18) */
+
+/* SYSST (0x01) detail */
+/* UVLOS bit 7 (SYSST 0x01) */
+#define AW87XXX_PID_18_UVLOS_START_BIT	(7)
+#define AW87XXX_PID_18_UVLOS_BITS_LEN	(1)
+#define AW87XXX_PID_18_UVLOS_MASK		\
+	(~(((1<<AW87XXX_PID_18_UVLOS_BITS_LEN)-1) << AW87XXX_PID_18_UVLOS_START_BIT))
+
+#define AW87XXX_PID_18_UVLOS_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_18_UVLOS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_18_UVLOS_NORMAL_OPERATION << AW87XXX_PID_18_UVLOS_START_BIT)
+
+#define AW87XXX_PID_18_UVLOS_VBAT_UNDER_VOLTAGE	(1)
+#define AW87XXX_PID_18_UVLOS_VBAT_UNDER_VOLTAGE_VALUE	\
+	(AW87XXX_PID_18_UVLOS_VBAT_UNDER_VOLTAGE << AW87XXX_PID_18_UVLOS_START_BIT)
+
+#define AW87XXX_PID_18_UVLOS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_UVLOS_DEFAULT	\
+	(AW87XXX_PID_18_UVLOS_DEFAULT_VALUE << AW87XXX_PID_18_UVLOS_START_BIT)
+
+/* OTNS bit 6 (SYSST 0x01) */
+#define AW87XXX_PID_18_OTNS_START_BIT	(6)
+#define AW87XXX_PID_18_OTNS_BITS_LEN	(1)
+#define AW87XXX_PID_18_OTNS_MASK		\
+	(~(((1<<AW87XXX_PID_18_OTNS_BITS_LEN)-1) << AW87XXX_PID_18_OTNS_START_BIT))
+
+#define AW87XXX_PID_18_OTNS_PA_OVER_TEMPRETURE_PROTECTION_DETECTED	(0)
+#define AW87XXX_PID_18_OTNS_PA_OVER_TEMPRETURE_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_18_OTNS_PA_OVER_TEMPRETURE_PROTECTION_DETECTED << AW87XXX_PID_18_OTNS_START_BIT)
+
+#define AW87XXX_PID_18_OTNS_NORMAL_OPERATION	(1)
+#define AW87XXX_PID_18_OTNS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_18_OTNS_NORMAL_OPERATION << AW87XXX_PID_18_OTNS_START_BIT)
+
+#define AW87XXX_PID_18_OTNS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_OTNS_DEFAULT		\
+	(AW87XXX_PID_18_OTNS_DEFAULT_VALUE << AW87XXX_PID_18_OTNS_START_BIT)
+
+/* OC_FLAGS bit 5 (SYSST 0x01) */
+#define AW87XXX_PID_18_OC_FLAGS_START_BIT	(5)
+#define AW87XXX_PID_18_OC_FLAGS_BITS_LEN	(1)
+#define AW87XXX_PID_18_OC_FLAGS_MASK	\
+	(~(((1<<AW87XXX_PID_18_OC_FLAGS_BITS_LEN)-1) << AW87XXX_PID_18_OC_FLAGS_START_BIT))
+
+#define AW87XXX_PID_18_OC_FLAGS_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_18_OC_FLAGS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_18_OC_FLAGS_NORMAL_OPERATION << AW87XXX_PID_18_OC_FLAGS_START_BIT)
+
+#define AW87XXX_PID_18_OC_FLAGS_PA_OVER_CURRENT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_18_OC_FLAGS_PA_OVER_CURRENT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_18_OC_FLAGS_PA_OVER_CURRENT_PROTECTION_DETECTED << AW87XXX_PID_18_OC_FLAGS_START_BIT)
+
+#define AW87XXX_PID_18_OC_FLAGS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_OC_FLAGS_DEFAULT	\
+	(AW87XXX_PID_18_OC_FLAGS_DEFAULT_VALUE << AW87XXX_PID_18_OC_FLAGS_START_BIT)
+
+/* VOUTDECTS bit 4 (SYSST 0x01) */
+#define AW87XXX_PID_18_VOUTDECTS_START_BIT	(4)
+#define AW87XXX_PID_18_VOUTDECTS_BITS_LEN	(1)
+#define AW87XXX_PID_18_VOUTDECTS_MASK	\
+	(~(((1<<AW87XXX_PID_18_VOUTDECTS_BITS_LEN)-1) << AW87XXX_PID_18_VOUTDECTS_START_BIT))
+
+#define AW87XXX_PID_18_VOUTDECTS_PVDDBELOWVDD	(0)
+#define AW87XXX_PID_18_VOUTDECTS_PVDDBELOWVDD_VALUE	\
+	(AW87XXX_PID_18_VOUTDECTS_PVDDBELOWVDD << AW87XXX_PID_18_VOUTDECTS_START_BIT)
+
+#define AW87XXX_PID_18_VOUTDECTS_PVDDABOVEVDD	(1)
+#define AW87XXX_PID_18_VOUTDECTS_PVDDABOVEVDD_VALUE	\
+	(AW87XXX_PID_18_VOUTDECTS_PVDDABOVEVDD << AW87XXX_PID_18_VOUTDECTS_START_BIT)
+
+#define AW87XXX_PID_18_VOUTDECTS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_VOUTDECTS_DEFAULT	\
+	(AW87XXX_PID_18_VOUTDECTS_DEFAULT_VALUE << AW87XXX_PID_18_VOUTDECTS_START_BIT)
+
+/* STARTOKS bit 3 (SYSST 0x01) */
+#define AW87XXX_PID_18_STARTOKS_START_BIT	(3)
+#define AW87XXX_PID_18_STARTOKS_BITS_LEN	(1)
+#define AW87XXX_PID_18_STARTOKS_MASK	\
+	(~(((1<<AW87XXX_PID_18_STARTOKS_BITS_LEN)-1) << AW87XXX_PID_18_STARTOKS_START_BIT))
+
+#define AW87XXX_PID_18_STARTOKS_CP_START_FAIL_DECTECTED	(0)
+#define AW87XXX_PID_18_STARTOKS_CP_START_FAIL_DECTECTED_VALUE	\
+	(AW87XXX_PID_18_STARTOKS_CP_START_FAIL_DECTECTED << AW87XXX_PID_18_STARTOKS_START_BIT)
+
+#define AW87XXX_PID_18_STARTOKS_NORMAL_OPERATION	(1)
+#define AW87XXX_PID_18_STARTOKS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_18_STARTOKS_NORMAL_OPERATION << AW87XXX_PID_18_STARTOKS_START_BIT)
+
+#define AW87XXX_PID_18_STARTOKS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_STARTOKS_DEFAULT	\
+	(AW87XXX_PID_18_STARTOKS_DEFAULT_VALUE << AW87XXX_PID_18_STARTOKS_START_BIT)
+
+/* VBGOKN1S bit 2 (SYSST 0x01) */
+#define AW87XXX_PID_18_VBGOKN1S_START_BIT	(2)
+#define AW87XXX_PID_18_VBGOKN1S_BITS_LEN	(1)
+#define AW87XXX_PID_18_VBGOKN1S_MASK	\
+	(~(((1<<AW87XXX_PID_18_VBGOKN1S_BITS_LEN)-1) << AW87XXX_PID_18_VBGOKN1S_START_BIT))
+
+#define AW87XXX_PID_18_VBGOKN1S_NORMAL_WORKS	(0)
+#define AW87XXX_PID_18_VBGOKN1S_NORMAL_WORKS_VALUE	\
+	(AW87XXX_PID_18_VBGOKN1S_NORMAL_WORKS << AW87XXX_PID_18_VBGOKN1S_START_BIT)
+
+#define AW87XXX_PID_18_VBGOKN1S_ABNORMAL_WORKS	(1)
+#define AW87XXX_PID_18_VBGOKN1S_ABNORMAL_WORKS_VALUE	\
+	(AW87XXX_PID_18_VBGOKN1S_ABNORMAL_WORKS << AW87XXX_PID_18_VBGOKN1S_START_BIT)
+
+#define AW87XXX_PID_18_VBGOKN1S_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_VBGOKN1S_DEFAULT	\
+	(AW87XXX_PID_18_VBGOKN1S_DEFAULT_VALUE << AW87XXX_PID_18_VBGOKN1S_START_BIT)
+
+/* OVPS bit 1 (SYSST 0x01) */
+#define AW87XXX_PID_18_OVPS_START_BIT	(1)
+#define AW87XXX_PID_18_OVPS_BITS_LEN	(1)
+#define AW87XXX_PID_18_OVPS_MASK		\
+	(~(((1<<AW87XXX_PID_18_OVPS_BITS_LEN)-1) << AW87XXX_PID_18_OVPS_START_BIT))
+
+#define AW87XXX_PID_18_OVPS_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_18_OVPS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_18_OVPS_NORMAL_OPERATION << AW87XXX_PID_18_OVPS_START_BIT)
+
+#define AW87XXX_PID_18_OVPS_CP_OVP_DETECTED	(1)
+#define AW87XXX_PID_18_OVPS_CP_OVP_DETECTED_VALUE	\
+	(AW87XXX_PID_18_OVPS_CP_OVP_DETECTED << AW87XXX_PID_18_OVPS_START_BIT)
+
+#define AW87XXX_PID_18_OVPS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_OVPS_DEFAULT		\
+	(AW87XXX_PID_18_OVPS_DEFAULT_VALUE << AW87XXX_PID_18_OVPS_START_BIT)
+
+/* CP_2PS bit 0 (SYSST 0x01) */
+#define AW87XXX_PID_18_CP_2PS_START_BIT	(0)
+#define AW87XXX_PID_18_CP_2PS_BITS_LEN	(1)
+#define AW87XXX_PID_18_CP_2PS_MASK		\
+	(~(((1<<AW87XXX_PID_18_CP_2PS_BITS_LEN)-1) << AW87XXX_PID_18_CP_2PS_START_BIT))
+
+#define AW87XXX_PID_18_CP_2PS_WEAK_SIGNAL	(0)
+#define AW87XXX_PID_18_CP_2PS_WEAK_SIGNAL_VALUE	\
+	(AW87XXX_PID_18_CP_2PS_WEAK_SIGNAL << AW87XXX_PID_18_CP_2PS_START_BIT)
+
+#define AW87XXX_PID_18_CP_2PS_STRONG_SIGNAL	(1)
+#define AW87XXX_PID_18_CP_2PS_STRONG_SIGNAL_VALUE	\
+	(AW87XXX_PID_18_CP_2PS_STRONG_SIGNAL << AW87XXX_PID_18_CP_2PS_START_BIT)
+
+#define AW87XXX_PID_18_CP_2PS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_CP_2PS_DEFAULT	\
+	(AW87XXX_PID_18_CP_2PS_DEFAULT_VALUE << AW87XXX_PID_18_CP_2PS_START_BIT)
+
+/* default value of SYSST (0x01) */
+/* #define AW87XXX_PID_18_SYSST_DEFAULT		(0x00) */
+
+/* SYSINT (0x02) detail */
+/* UVLOI bit 7 (SYSINT 0x02) */
+#define AW87XXX_PID_18_UVLOI_START_BIT	(7)
+#define AW87XXX_PID_18_UVLOI_BITS_LEN	(1)
+#define AW87XXX_PID_18_UVLOI_MASK		\
+	(~(((1<<AW87XXX_PID_18_UVLOI_BITS_LEN)-1) << AW87XXX_PID_18_UVLOI_START_BIT))
+
+#define AW87XXX_PID_18_UVLOI_SIGNAL_STATUS_DO_NOT_CHANGE	(0)
+#define AW87XXX_PID_18_UVLOI_SIGNAL_STATUS_DO_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_18_UVLOI_SIGNAL_STATUS_DO_NOT_CHANGE << AW87XXX_PID_18_UVLOI_START_BIT)
+
+#define AW87XXX_PID_18_UVLOI_UNDER_VOLTAGE_LOCK_OUT_DETECTED	(1)
+#define AW87XXX_PID_18_UVLOI_UNDER_VOLTAGE_LOCK_OUT_DETECTED_VALUE	\
+	(AW87XXX_PID_18_UVLOI_UNDER_VOLTAGE_LOCK_OUT_DETECTED << AW87XXX_PID_18_UVLOI_START_BIT)
+
+#define AW87XXX_PID_18_UVLOI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_UVLOI_DEFAULT	\
+	(AW87XXX_PID_18_UVLOI_DEFAULT_VALUE << AW87XXX_PID_18_UVLOI_START_BIT)
+
+/* OTNI bit 6 (SYSINT 0x02) */
+#define AW87XXX_PID_18_OTNI_START_BIT	(6)
+#define AW87XXX_PID_18_OTNI_BITS_LEN	(1)
+#define AW87XXX_PID_18_OTNI_MASK		\
+	(~(((1<<AW87XXX_PID_18_OTNI_BITS_LEN)-1) << AW87XXX_PID_18_OTNI_START_BIT))
+
+#define AW87XXX_PID_18_OTNI_SIGNAL_STATUS_DO_NOT_CHANGE	(0)
+#define AW87XXX_PID_18_OTNI_SIGNAL_STATUS_DO_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_18_OTNI_SIGNAL_STATUS_DO_NOT_CHANGE << AW87XXX_PID_18_OTNI_START_BIT)
+
+#define AW87XXX_PID_18_OTNI_OVER_TEMPRETURE_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_18_OTNI_OVER_TEMPRETURE_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_18_OTNI_OVER_TEMPRETURE_PROTECTION_DETECTED << AW87XXX_PID_18_OTNI_START_BIT)
+
+#define AW87XXX_PID_18_OTNI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_OTNI_DEFAULT		\
+	(AW87XXX_PID_18_OTNI_DEFAULT_VALUE << AW87XXX_PID_18_OTNI_START_BIT)
+
+/* OC_FLAGI bit 5 (SYSINT 0x02) */
+#define AW87XXX_PID_18_OC_FLAGI_START_BIT	(5)
+#define AW87XXX_PID_18_OC_FLAGI_BITS_LEN	(1)
+#define AW87XXX_PID_18_OC_FLAGI_MASK	\
+	(~(((1<<AW87XXX_PID_18_OC_FLAGI_BITS_LEN)-1) << AW87XXX_PID_18_OC_FLAGI_START_BIT))
+
+#define AW87XXX_PID_18_OC_FLAGI_SIGNAL_STATUS_DO_NOT_CHANGE	(0)
+#define AW87XXX_PID_18_OC_FLAGI_SIGNAL_STATUS_DO_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_18_OC_FLAGI_SIGNAL_STATUS_DO_NOT_CHANGE << AW87XXX_PID_18_OC_FLAGI_START_BIT)
+
+#define AW87XXX_PID_18_OC_FLAGI_PA_OVER_CURRENT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_18_OC_FLAGI_PA_OVER_CURRENT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_18_OC_FLAGI_PA_OVER_CURRENT_PROTECTION_DETECTED << AW87XXX_PID_18_OC_FLAGI_START_BIT)
+
+#define AW87XXX_PID_18_OC_FLAGI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_OC_FLAGI_DEFAULT	\
+	(AW87XXX_PID_18_OC_FLAGI_DEFAULT_VALUE << AW87XXX_PID_18_OC_FLAGI_START_BIT)
+
+/* VOUTDECTI bit 4 (SYSINT 0x02) */
+#define AW87XXX_PID_18_VOUTDECTI_START_BIT	(4)
+#define AW87XXX_PID_18_VOUTDECTI_BITS_LEN	(1)
+#define AW87XXX_PID_18_VOUTDECTI_MASK	\
+	(~(((1<<AW87XXX_PID_18_VOUTDECTI_BITS_LEN)-1) << AW87XXX_PID_18_VOUTDECTI_START_BIT))
+
+#define AW87XXX_PID_18_VOUTDECTI_PVDDBELOWVDD	(0)
+#define AW87XXX_PID_18_VOUTDECTI_PVDDBELOWVDD_VALUE	\
+	(AW87XXX_PID_18_VOUTDECTI_PVDDBELOWVDD << AW87XXX_PID_18_VOUTDECTI_START_BIT)
+
+#define AW87XXX_PID_18_VOUTDECTI_PVDDABOVEVDD	(1)
+#define AW87XXX_PID_18_VOUTDECTI_PVDDABOVEVDD_VALUE	\
+	(AW87XXX_PID_18_VOUTDECTI_PVDDABOVEVDD << AW87XXX_PID_18_VOUTDECTI_START_BIT)
+
+#define AW87XXX_PID_18_VOUTDECTI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_VOUTDECTI_DEFAULT	\
+	(AW87XXX_PID_18_VOUTDECTI_DEFAULT_VALUE << AW87XXX_PID_18_VOUTDECTI_START_BIT)
+
+/* STARTOKI bit 3 (SYSINT 0x02) */
+#define AW87XXX_PID_18_STARTOKI_START_BIT	(3)
+#define AW87XXX_PID_18_STARTOKI_BITS_LEN	(1)
+#define AW87XXX_PID_18_STARTOKI_MASK	\
+	(~(((1<<AW87XXX_PID_18_STARTOKI_BITS_LEN)-1) << AW87XXX_PID_18_STARTOKI_START_BIT))
+
+#define AW87XXX_PID_18_STARTOKI_SIGNAL_STATUS_DO_NOT_CHANGE	(0)
+#define AW87XXX_PID_18_STARTOKI_SIGNAL_STATUS_DO_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_18_STARTOKI_SIGNAL_STATUS_DO_NOT_CHANGE << AW87XXX_PID_18_STARTOKI_START_BIT)
+
+#define AW87XXX_PID_18_STARTOKI_CHARGEPUMB_START_UP_OK_DECTECTED	(1)
+#define AW87XXX_PID_18_STARTOKI_CHARGEPUMB_START_UP_OK_DECTECTED_VALUE	\
+	(AW87XXX_PID_18_STARTOKI_CHARGEPUMB_START_UP_OK_DECTECTED << AW87XXX_PID_18_STARTOKI_START_BIT)
+
+#define AW87XXX_PID_18_STARTOKI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_STARTOKI_DEFAULT	\
+	(AW87XXX_PID_18_STARTOKI_DEFAULT_VALUE << AW87XXX_PID_18_STARTOKI_START_BIT)
+
+/* VBGOKN1I bit 2 (SYSINT 0x02) */
+#define AW87XXX_PID_18_VBGOKN1I_START_BIT	(2)
+#define AW87XXX_PID_18_VBGOKN1I_BITS_LEN	(1)
+#define AW87XXX_PID_18_VBGOKN1I_MASK	\
+	(~(((1<<AW87XXX_PID_18_VBGOKN1I_BITS_LEN)-1) << AW87XXX_PID_18_VBGOKN1I_START_BIT))
+
+#define AW87XXX_PID_18_VBGOKN1I_NORMAL_WORKS	(0)
+#define AW87XXX_PID_18_VBGOKN1I_NORMAL_WORKS_VALUE	\
+	(AW87XXX_PID_18_VBGOKN1I_NORMAL_WORKS << AW87XXX_PID_18_VBGOKN1I_START_BIT)
+
+#define AW87XXX_PID_18_VBGOKN1I_ABNORMAL_WORKS	(1)
+#define AW87XXX_PID_18_VBGOKN1I_ABNORMAL_WORKS_VALUE	\
+	(AW87XXX_PID_18_VBGOKN1I_ABNORMAL_WORKS << AW87XXX_PID_18_VBGOKN1I_START_BIT)
+
+#define AW87XXX_PID_18_VBGOKN1I_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_VBGOKN1I_DEFAULT	\
+	(AW87XXX_PID_18_VBGOKN1I_DEFAULT_VALUE << AW87XXX_PID_18_VBGOKN1I_START_BIT)
+
+/* OVPI bit 1 (SYSINT 0x02) */
+#define AW87XXX_PID_18_OVPI_START_BIT	(1)
+#define AW87XXX_PID_18_OVPI_BITS_LEN	(1)
+#define AW87XXX_PID_18_OVPI_MASK		\
+	(~(((1<<AW87XXX_PID_18_OVPI_BITS_LEN)-1) << AW87XXX_PID_18_OVPI_START_BIT))
+
+#define AW87XXX_PID_18_OVPI_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_18_OVPI_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_18_OVPI_NORMAL_OPERATION << AW87XXX_PID_18_OVPI_START_BIT)
+
+#define AW87XXX_PID_18_OVPI_CP_OVP_DETECTED	(1)
+#define AW87XXX_PID_18_OVPI_CP_OVP_DETECTED_VALUE	\
+	(AW87XXX_PID_18_OVPI_CP_OVP_DETECTED << AW87XXX_PID_18_OVPI_START_BIT)
+
+#define AW87XXX_PID_18_OVPI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_OVPI_DEFAULT		\
+	(AW87XXX_PID_18_OVPI_DEFAULT_VALUE << AW87XXX_PID_18_OVPI_START_BIT)
+
+/* CP_2PI bit 0 (SYSINT 0x02) */
+#define AW87XXX_PID_18_CP_2PI_START_BIT	(0)
+#define AW87XXX_PID_18_CP_2PI_BITS_LEN	(1)
+#define AW87XXX_PID_18_CP_2PI_MASK		\
+	(~(((1<<AW87XXX_PID_18_CP_2PI_BITS_LEN)-1) << AW87XXX_PID_18_CP_2PI_START_BIT))
+
+#define AW87XXX_PID_18_CP_2PI_WEAK_SIGNAL	(0)
+#define AW87XXX_PID_18_CP_2PI_WEAK_SIGNAL_VALUE	\
+	(AW87XXX_PID_18_CP_2PI_WEAK_SIGNAL << AW87XXX_PID_18_CP_2PI_START_BIT)
+
+#define AW87XXX_PID_18_CP_2PI_STRONG_SIGNAL	(1)
+#define AW87XXX_PID_18_CP_2PI_STRONG_SIGNAL_VALUE	\
+	(AW87XXX_PID_18_CP_2PI_STRONG_SIGNAL << AW87XXX_PID_18_CP_2PI_START_BIT)
+
+#define AW87XXX_PID_18_CP_2PI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_CP_2PI_DEFAULT	\
+	(AW87XXX_PID_18_CP_2PI_DEFAULT_VALUE << AW87XXX_PID_18_CP_2PI_START_BIT)
+
+/* default value of SYSINT (0x02) */
+/* #define AW87XXX_PID_18_SYSINT_DEFAULT		(0x00) */
+
+/* SYSCTRL (0x03) detail */
+/* EN_SS bit 7 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_EN_SS_START_BIT	(7)
+#define AW87XXX_PID_18_EN_SS_BITS_LEN	(1)
+#define AW87XXX_PID_18_EN_SS_MASK		\
+	(~(((1<<AW87XXX_PID_18_EN_SS_BITS_LEN)-1) << AW87XXX_PID_18_EN_SS_START_BIT))
+
+#define AW87XXX_PID_18_EN_SS_DISABLE_REG_FSS11001P6MHZ	(0)
+#define AW87XXX_PID_18_EN_SS_DISABLE_REG_FSS11001P6MHZ_VALUE	\
+	(AW87XXX_PID_18_EN_SS_DISABLE_REG_FSS11001P6MHZ << AW87XXX_PID_18_EN_SS_START_BIT)
+
+#define AW87XXX_PID_18_EN_SS_ENABLE		(1)
+#define AW87XXX_PID_18_EN_SS_ENABLE_VALUE	\
+	(AW87XXX_PID_18_EN_SS_ENABLE << AW87XXX_PID_18_EN_SS_START_BIT)
+
+#define AW87XXX_PID_18_EN_SS_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_EN_SS_DEFAULT	\
+	(AW87XXX_PID_18_EN_SS_DEFAULT_VALUE << AW87XXX_PID_18_EN_SS_START_BIT)
+
+/* REG_EN_SW bit 6 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_REG_EN_SW_START_BIT	(6)
+#define AW87XXX_PID_18_REG_EN_SW_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_SW_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_SW_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_SW_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_SW_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_SW_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_SW_DISABLE << AW87XXX_PID_18_REG_EN_SW_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_SW_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_SW_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_SW_ENABLE << AW87XXX_PID_18_REG_EN_SW_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_SW_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_EN_SW_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_SW_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_SW_START_BIT)
+
+/* REG_EN_PA bit 5 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_REG_EN_PA_START_BIT	(5)
+#define AW87XXX_PID_18_REG_EN_PA_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_PA_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_PA_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_PA_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_PA_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_PA_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_PA_DISABLE << AW87XXX_PID_18_REG_EN_PA_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_PA_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_PA_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_PA_ENABLE << AW87XXX_PID_18_REG_EN_PA_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_PA_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_EN_PA_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_PA_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_PA_START_BIT)
+
+/* REG_EN_ADAP bit 4 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_REG_EN_ADAP_START_BIT	(4)
+#define AW87XXX_PID_18_REG_EN_ADAP_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_ADAP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_ADAP_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_ADAP_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_ADAP_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_ADAP_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_ADAP_DISABLE << AW87XXX_PID_18_REG_EN_ADAP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_ADAP_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_ADAP_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_ADAP_ENABLE << AW87XXX_PID_18_REG_EN_ADAP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_ADAP_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EN_ADAP_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_ADAP_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_ADAP_START_BIT)
+
+/* REG_EN_MPD bit 3 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_REG_EN_MPD_START_BIT	(3)
+#define AW87XXX_PID_18_REG_EN_MPD_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_MPD_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_MPD_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_MPD_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_MPD_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_MPD_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_MPD_DISABLE << AW87XXX_PID_18_REG_EN_MPD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_MPD_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_MPD_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_MPD_ENABLE << AW87XXX_PID_18_REG_EN_MPD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_MPD_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EN_MPD_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_MPD_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_MPD_START_BIT)
+
+/* REG_EN_CP bit 2 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_REG_EN_CP_START_BIT	(2)
+#define AW87XXX_PID_18_REG_EN_CP_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_CP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_CP_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_CP_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_CP_DISABLE_PVDDVBAT_DIRECT_TROUGH_MODE	(0)
+#define AW87XXX_PID_18_REG_EN_CP_DISABLE_PVDDVBAT_DIRECT_TROUGH_MODE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_CP_DISABLE_PVDDVBAT_DIRECT_TROUGH_MODE << AW87XXX_PID_18_REG_EN_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_CP_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_CP_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_CP_ENABLE << AW87XXX_PID_18_REG_EN_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_CP_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_EN_CP_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_CP_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_CP_START_BIT)
+
+/* REG_REC_MODE bit 1 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_REG_REC_MODE_START_BIT	(1)
+#define AW87XXX_PID_18_REG_REC_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_REC_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_REC_MODE_BITS_LEN)-1) << AW87XXX_PID_18_REG_REC_MODE_START_BIT))
+
+#define AW87XXX_PID_18_REG_REC_MODE_DISABLE	(0)
+#define AW87XXX_PID_18_REG_REC_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_REC_MODE_DISABLE << AW87XXX_PID_18_REG_REC_MODE_START_BIT)
+
+#define AW87XXX_PID_18_REG_REC_MODE_ENABLE	(1)
+#define AW87XXX_PID_18_REG_REC_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_REC_MODE_ENABLE << AW87XXX_PID_18_REG_REC_MODE_START_BIT)
+
+#define AW87XXX_PID_18_REG_REC_MODE_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_REC_MODE_DEFAULT	\
+	(AW87XXX_PID_18_REG_REC_MODE_DEFAULT_VALUE << AW87XXX_PID_18_REG_REC_MODE_START_BIT)
+
+/* REG_FORCE_2X bit 0 (SYSCTRL 0x03) */
+#define AW87XXX_PID_18_REG_FORCE_2X_START_BIT	(0)
+#define AW87XXX_PID_18_REG_FORCE_2X_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_FORCE_2X_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_FORCE_2X_BITS_LEN)-1) << AW87XXX_PID_18_REG_FORCE_2X_START_BIT))
+
+#define AW87XXX_PID_18_REG_FORCE_2X_DISABLE_CPS_WORKING_STATUS_DEPENDS_ON_THE_SYSTEM	(0)
+#define AW87XXX_PID_18_REG_FORCE_2X_DISABLE_CPS_WORKING_STATUS_DEPENDS_ON_THE_SYSTEM_VALUE	\
+	(AW87XXX_PID_18_REG_FORCE_2X_DISABLE_CPS_WORKING_STATUS_DEPENDS_ON_THE_SYSTEM << AW87XXX_PID_18_REG_FORCE_2X_START_BIT)
+
+#define AW87XXX_PID_18_REG_FORCE_2X_ENABLE_FORCE_THE_CP_WORKS_IN_X2_MODE	(1)
+#define AW87XXX_PID_18_REG_FORCE_2X_ENABLE_FORCE_THE_CP_WORKS_IN_X2_MODE_VALUE	\
+	(AW87XXX_PID_18_REG_FORCE_2X_ENABLE_FORCE_THE_CP_WORKS_IN_X2_MODE << AW87XXX_PID_18_REG_FORCE_2X_START_BIT)
+
+#define AW87XXX_PID_18_REG_FORCE_2X_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_FORCE_2X_DEFAULT	\
+	(AW87XXX_PID_18_REG_FORCE_2X_DEFAULT_VALUE << AW87XXX_PID_18_REG_FORCE_2X_START_BIT)
+
+/* default value of SYSCTRL (0x03) */
+/* #define AW87XXX_PID_18_SYSCTRL_DEFAULT		(0xE5) */
+
+/* CPOC (0x04) detail */
+/* REG_CP_OVP bit 5:2 (CPOC 0x04) */
+#define AW87XXX_PID_18_REG_CP_OVP_START_BIT	(2)
+#define AW87XXX_PID_18_REG_CP_OVP_BITS_LEN	(4)
+#define AW87XXX_PID_18_REG_CP_OVP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CP_OVP_BITS_LEN)-1) << AW87XXX_PID_18_REG_CP_OVP_START_BIT))
+
+#define AW87XXX_PID_18_REG_CP_OVP_8P5V	(8)
+#define AW87XXX_PID_18_REG_CP_OVP_8P5V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_8P5V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_8P25V	(7)
+#define AW87XXX_PID_18_REG_CP_OVP_8P25V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_8P25V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_8V	(6)
+#define AW87XXX_PID_18_REG_CP_OVP_8V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_8V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_7P75V	(5)
+#define AW87XXX_PID_18_REG_CP_OVP_7P75V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_7P75V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_7P5V	(4)
+#define AW87XXX_PID_18_REG_CP_OVP_7P5V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_7P5V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_7P25V	(3)
+#define AW87XXX_PID_18_REG_CP_OVP_7P25V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_7P25V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_7V	(2)
+#define AW87XXX_PID_18_REG_CP_OVP_7V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_7V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_6P75V	(1)
+#define AW87XXX_PID_18_REG_CP_OVP_6P75V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_6P75V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_6P5V	(0)
+#define AW87XXX_PID_18_REG_CP_OVP_6P5V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_6P5V << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_DEFAULT_VALUE	(0x6)
+#define AW87XXX_PID_18_REG_CP_OVP_DEFAULT	\
+	(AW87XXX_PID_18_REG_CP_OVP_DEFAULT_VALUE << AW87XXX_PID_18_REG_CP_OVP_START_BIT)
+
+/* REG_OC_DELAY bit 1:0 (CPOC 0x04) */
+#define AW87XXX_PID_18_REG_OC_DELAY_START_BIT	(0)
+#define AW87XXX_PID_18_REG_OC_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_OC_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_OC_DELAY_BITS_LEN)-1) << AW87XXX_PID_18_REG_OC_DELAY_START_BIT))
+
+#define AW87XXX_PID_18_REG_OC_DELAY_60NS	(0)
+#define AW87XXX_PID_18_REG_OC_DELAY_60NS_VALUE	\
+	(AW87XXX_PID_18_REG_OC_DELAY_60NS << AW87XXX_PID_18_REG_OC_DELAY_START_BIT)
+
+#define AW87XXX_PID_18_REG_OC_DELAY_80NS	(1)
+#define AW87XXX_PID_18_REG_OC_DELAY_80NS_VALUE	\
+	(AW87XXX_PID_18_REG_OC_DELAY_80NS << AW87XXX_PID_18_REG_OC_DELAY_START_BIT)
+
+#define AW87XXX_PID_18_REG_OC_DELAY_90NS	(2)
+#define AW87XXX_PID_18_REG_OC_DELAY_90NS_VALUE	\
+	(AW87XXX_PID_18_REG_OC_DELAY_90NS << AW87XXX_PID_18_REG_OC_DELAY_START_BIT)
+
+#define AW87XXX_PID_18_REG_OC_DELAY_110NS	(3)
+#define AW87XXX_PID_18_REG_OC_DELAY_110NS_VALUE	\
+	(AW87XXX_PID_18_REG_OC_DELAY_110NS << AW87XXX_PID_18_REG_OC_DELAY_START_BIT)
+
+#define AW87XXX_PID_18_REG_OC_DELAY_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_OC_DELAY_DEFAULT	\
+	(AW87XXX_PID_18_REG_OC_DELAY_DEFAULT_VALUE << AW87XXX_PID_18_REG_OC_DELAY_START_BIT)
+
+/* default value of CPOC (0x04) */
+/* #define AW87XXX_PID_18_CPOC_DEFAULT		(0x18) */
+
+/* CLASSD (0x05) detail */
+/* REG_BK1 bit 7 (CLASSD 0x05) */
+#define AW87XXX_PID_18_REG_BK1_START_BIT	(7)
+#define AW87XXX_PID_18_REG_BK1_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_BK1_MASK		\
+	(~(((1<<AW87XXX_PID_18_REG_BK1_BITS_LEN)-1) << AW87XXX_PID_18_REG_BK1_START_BIT))
+
+#define AW87XXX_PID_18_REG_BK1_22MV		(0)
+#define AW87XXX_PID_18_REG_BK1_22MV_VALUE	\
+	(AW87XXX_PID_18_REG_BK1_22MV << AW87XXX_PID_18_REG_BK1_START_BIT)
+
+#define AW87XXX_PID_18_REG_BK1_15MV		(1)
+#define AW87XXX_PID_18_REG_BK1_15MV_VALUE	\
+	(AW87XXX_PID_18_REG_BK1_15MV << AW87XXX_PID_18_REG_BK1_START_BIT)
+
+#define AW87XXX_PID_18_REG_BK1_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_BK1_DEFAULT	\
+	(AW87XXX_PID_18_REG_BK1_DEFAULT_VALUE << AW87XXX_PID_18_REG_BK1_START_BIT)
+
+/* REG_BK2 bit 6 (CLASSD 0x05) */
+#define AW87XXX_PID_18_REG_BK2_START_BIT	(6)
+#define AW87XXX_PID_18_REG_BK2_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_BK2_MASK		\
+	(~(((1<<AW87XXX_PID_18_REG_BK2_BITS_LEN)-1) << AW87XXX_PID_18_REG_BK2_START_BIT))
+
+#define AW87XXX_PID_18_REG_BK2_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_BK2_DEFAULT	\
+	(AW87XXX_PID_18_REG_BK2_DEFAULT_VALUE << AW87XXX_PID_18_REG_BK2_START_BIT)
+
+/* REG_BK3 bit 5 (CLASSD 0x05) */
+#define AW87XXX_PID_18_REG_BK3_START_BIT	(5)
+#define AW87XXX_PID_18_REG_BK3_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_BK3_MASK		\
+	(~(((1<<AW87XXX_PID_18_REG_BK3_BITS_LEN)-1) << AW87XXX_PID_18_REG_BK3_START_BIT))
+
+#define AW87XXX_PID_18_REG_BK3_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_BK3_DEFAULT	\
+	(AW87XXX_PID_18_REG_BK3_DEFAULT_VALUE << AW87XXX_PID_18_REG_BK3_START_BIT)
+
+/* REG_D_GAIN bit 4:0 (CLASSD 0x05) */
+#define AW87XXX_PID_18_REG_D_GAIN_START_BIT	(0)
+#define AW87XXX_PID_18_REG_D_GAIN_BITS_LEN	(5)
+#define AW87XXX_PID_18_REG_D_GAIN_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_D_GAIN_BITS_LEN)-1) << AW87XXX_PID_18_REG_D_GAIN_START_BIT))
+
+#define AW87XXX_PID_18_REG_D_GAIN_0DB	(0)
+#define AW87XXX_PID_18_REG_D_GAIN_0DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_0DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_1P5DB	(1)
+#define AW87XXX_PID_18_REG_D_GAIN_1P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_1P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_3DB	(2)
+#define AW87XXX_PID_18_REG_D_GAIN_3DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_3DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_4P5DB	(3)
+#define AW87XXX_PID_18_REG_D_GAIN_4P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_4P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_6DB	(4)
+#define AW87XXX_PID_18_REG_D_GAIN_6DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_6DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_7P5DB	(5)
+#define AW87XXX_PID_18_REG_D_GAIN_7P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_7P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_9DB	(6)
+#define AW87XXX_PID_18_REG_D_GAIN_9DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_9DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_10P5DB	(7)
+#define AW87XXX_PID_18_REG_D_GAIN_10P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_10P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_12DB	(8)
+#define AW87XXX_PID_18_REG_D_GAIN_12DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_12DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_13P5DB	(9)
+#define AW87XXX_PID_18_REG_D_GAIN_13P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_13P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_15DB	(10)
+#define AW87XXX_PID_18_REG_D_GAIN_15DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_15DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_16P5DB	(11)
+#define AW87XXX_PID_18_REG_D_GAIN_16P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_16P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_18DB	(12)
+#define AW87XXX_PID_18_REG_D_GAIN_18DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_18DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_19P5DB	(13)
+#define AW87XXX_PID_18_REG_D_GAIN_19P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_19P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_21DB	(14)
+#define AW87XXX_PID_18_REG_D_GAIN_21DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_21DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_22P5DB	(15)
+#define AW87XXX_PID_18_REG_D_GAIN_22P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_22P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_24DB	(16)
+#define AW87XXX_PID_18_REG_D_GAIN_24DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_24DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_25P5DB	(17)
+#define AW87XXX_PID_18_REG_D_GAIN_25P5DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_25P5DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_27DB	(18)
+#define AW87XXX_PID_18_REG_D_GAIN_27DB_VALUE	\
+	(AW87XXX_PID_18_REG_D_GAIN_27DB << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+#define AW87XXX_PID_18_REG_D_GAIN_DEFAULT_VALUE	(0x10)
+#define AW87XXX_PID_18_REG_D_GAIN_DEFAULT	\
+	(AW87XXX_PID_18_REG_D_GAIN_DEFAULT_VALUE << AW87XXX_PID_18_REG_D_GAIN_START_BIT)
+
+/* default value of CLASSD (0x05) */
+/* #define AW87XXX_PID_18_CLASSD_DEFAULT		(0x10) */
+
+/* MADPVTH (0x06) detail */
+/* REG_ADAP_VTH bit 3:2 (MADPVTH 0x06) */
+#define AW87XXX_PID_18_REG_ADAP_VTH_START_BIT	(2)
+#define AW87XXX_PID_18_REG_ADAP_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_ADAP_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_ADAP_VTH_BITS_LEN)-1) << AW87XXX_PID_18_REG_ADAP_VTH_START_BIT))
+
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P1W0P05W	(0)
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P1W0P05W_VALUE	\
+	(AW87XXX_PID_18_REG_ADAP_VTH_0P1W0P05W << AW87XXX_PID_18_REG_ADAP_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P2W0P15W	(1)
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P2W0P15W_VALUE	\
+	(AW87XXX_PID_18_REG_ADAP_VTH_0P2W0P15W << AW87XXX_PID_18_REG_ADAP_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P3W0P25W	(2)
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P3W0P25W_VALUE	\
+	(AW87XXX_PID_18_REG_ADAP_VTH_0P3W0P25W << AW87XXX_PID_18_REG_ADAP_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P4W0P35W	(3)
+#define AW87XXX_PID_18_REG_ADAP_VTH_0P4W0P35W_VALUE	\
+	(AW87XXX_PID_18_REG_ADAP_VTH_0P4W0P35W << AW87XXX_PID_18_REG_ADAP_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_ADAP_VTH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_ADAP_VTH_DEFAULT	\
+	(AW87XXX_PID_18_REG_ADAP_VTH_DEFAULT_VALUE << AW87XXX_PID_18_REG_ADAP_VTH_START_BIT)
+
+/* REG_MPD_VTH bit 1:0 (MADPVTH 0x06) */
+#define AW87XXX_PID_18_REG_MPD_VTH_START_BIT	(0)
+#define AW87XXX_PID_18_REG_MPD_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_MPD_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_MPD_VTH_BITS_LEN)-1) << AW87XXX_PID_18_REG_MPD_VTH_START_BIT))
+
+#define AW87XXX_PID_18_REG_MPD_VTH_8P1MW3P6MW	(0)
+#define AW87XXX_PID_18_REG_MPD_VTH_8P1MW3P6MW_VALUE	\
+	(AW87XXX_PID_18_REG_MPD_VTH_8P1MW3P6MW << AW87XXX_PID_18_REG_MPD_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_MPD_VTH_11MW5P6MW	(1)
+#define AW87XXX_PID_18_REG_MPD_VTH_11MW5P6MW_VALUE	\
+	(AW87XXX_PID_18_REG_MPD_VTH_11MW5P6MW << AW87XXX_PID_18_REG_MPD_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_MPD_VTH_14P4MW8P1MW	(2)
+#define AW87XXX_PID_18_REG_MPD_VTH_14P4MW8P1MW_VALUE	\
+	(AW87XXX_PID_18_REG_MPD_VTH_14P4MW8P1MW << AW87XXX_PID_18_REG_MPD_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_MPD_VTH_18P2MW11W	(3)
+#define AW87XXX_PID_18_REG_MPD_VTH_18P2MW11W_VALUE	\
+	(AW87XXX_PID_18_REG_MPD_VTH_18P2MW11W << AW87XXX_PID_18_REG_MPD_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_MPD_VTH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_MPD_VTH_DEFAULT	\
+	(AW87XXX_PID_18_REG_MPD_VTH_DEFAULT_VALUE << AW87XXX_PID_18_REG_MPD_VTH_START_BIT)
+
+/* default value of MADPVTH (0x06) */
+/* #define AW87XXX_PID_18_MADPVTH_DEFAULT		(0x05) */
+
+/* A3PARAM (0x07) detail */
+/* REG_AGC3_RT bit 7:5 (A3PARAM 0x07) */
+#define AW87XXX_PID_18_REG_AGC3_RT_START_BIT	(5)
+#define AW87XXX_PID_18_REG_AGC3_RT_BITS_LEN	(3)
+#define AW87XXX_PID_18_REG_AGC3_RT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC3_RT_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC3_RT_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC3_RT_69P12MS	(0)
+#define AW87XXX_PID_18_REG_AGC3_RT_69P12MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_RT_69P12MS << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_RT_138P24MS	(1)
+#define AW87XXX_PID_18_REG_AGC3_RT_138P24MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_RT_138P24MS << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_RT_276P48MS	(2)
+#define AW87XXX_PID_18_REG_AGC3_RT_276P48MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_RT_276P48MS << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_RT_552P96MS	(3)
+#define AW87XXX_PID_18_REG_AGC3_RT_552P96MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_RT_552P96MS << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_RT_1107MS	(4)
+#define AW87XXX_PID_18_REG_AGC3_RT_1107MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_RT_1107MS << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_RT_2160MS	(5)
+#define AW87XXX_PID_18_REG_AGC3_RT_2160MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_RT_2160MS << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_RT_4320MS	(6)
+#define AW87XXX_PID_18_REG_AGC3_RT_4320MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_RT_4320MS << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_RT_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_18_REG_AGC3_RT_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC3_RT_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC3_RT_START_BIT)
+
+/* REG_AGC3_AT bit 4:2 (A3PARAM 0x07) */
+#define AW87XXX_PID_18_REG_AGC3_AT_START_BIT	(2)
+#define AW87XXX_PID_18_REG_AGC3_AT_BITS_LEN	(3)
+#define AW87XXX_PID_18_REG_AGC3_AT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC3_AT_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC3_AT_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC3_AT_5P76MS_0P32MSSTEP	(0)
+#define AW87XXX_PID_18_REG_AGC3_AT_5P76MS_0P32MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_5P76MS_0P32MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_11P52MS_0P64MSSTEP	(1)
+#define AW87XXX_PID_18_REG_AGC3_AT_11P52MS_0P64MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_11P52MS_0P64MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_23P04MS_1P28MSSTEP	(2)
+#define AW87XXX_PID_18_REG_AGC3_AT_23P04MS_1P28MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_23P04MS_1P28MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_92P16MS_5P12MSSTEP	(3)
+#define AW87XXX_PID_18_REG_AGC3_AT_92P16MS_5P12MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_92P16MS_5P12MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_368P64MS_20P48MSSTEP	(4)
+#define AW87XXX_PID_18_REG_AGC3_AT_368P64MS_20P48MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_368P64MS_20P48MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_738MS_41MSSTEP	(5)
+#define AW87XXX_PID_18_REG_AGC3_AT_738MS_41MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_738MS_41MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_1476MS_82MSSTEP	(6)
+#define AW87XXX_PID_18_REG_AGC3_AT_1476MS_82MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_1476MS_82MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_2952MS_164MSSTEP	(7)
+#define AW87XXX_PID_18_REG_AGC3_AT_2952MS_164MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_AT_2952MS_164MSSTEP << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_AT_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_18_REG_AGC3_AT_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC3_AT_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC3_AT_START_BIT)
+
+/* REG_AGC3_1ST_AT bit 1:0 (A3PARAM 0x07) */
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_START_BIT	(0)
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC3_1ST_AT_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC3_1ST_AT_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_5P12MS	(0)
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_5P12MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_1ST_AT_5P12MS << AW87XXX_PID_18_REG_AGC3_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_10P24MS	(1)
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_10P24MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_1ST_AT_10P24MS << AW87XXX_PID_18_REG_AGC3_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_20P48MS	(2)
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_20P48MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_1ST_AT_20P48MS << AW87XXX_PID_18_REG_AGC3_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_41MS	(3)
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_41MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_1ST_AT_41MS << AW87XXX_PID_18_REG_AGC3_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_18_REG_AGC3_1ST_AT_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC3_1ST_AT_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC3_1ST_AT_START_BIT)
+
+/* default value of A3PARAM (0x07) */
+/* #define AW87XXX_PID_18_A3PARAM_DEFAULT		(0x52) */
+
+/* A3A2PO (0x08) detail */
+/* REG_AGC3_PO bit 7:4 (A3A2PO 0x08) */
+#define AW87XXX_PID_18_REG_AGC3_PO_START_BIT	(4)
+#define AW87XXX_PID_18_REG_AGC3_PO_BITS_LEN	(4)
+#define AW87XXX_PID_18_REG_AGC3_PO_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC3_PO_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC3_PO_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P2W8_OHM_0P27W6_OHM_0P05W32_OHM	(0)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P2W8_OHM_0P27W6_OHM_0P05W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P2W8_OHM_0P27W6_OHM_0P05W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P3W8_OHM_0P4W6_OHM_0P075W32_OHM	(1)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P3W8_OHM_0P4W6_OHM_0P075W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P3W8_OHM_0P4W6_OHM_0P075W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P4W8_OHM_0P53W6_OHM_0P1W32_OHM	(2)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P4W8_OHM_0P53W6_OHM_0P1W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P4W8_OHM_0P53W6_OHM_0P1W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P5W8_OHM_0P67W6_OHM_0P125W32_OHM	(3)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P5W8_OHM_0P67W6_OHM_0P125W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P5W8_OHM_0P67W6_OHM_0P125W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P6W8_OHM_0P8W6_OHM_0P15W32_OHM	(4)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P6W8_OHM_0P8W6_OHM_0P15W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P6W8_OHM_0P8W6_OHM_0P15W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P7W8_OHM_0P93W6_OHM_0P175W32_OHM	(5)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P7W8_OHM_0P93W6_OHM_0P175W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P7W8_OHM_0P93W6_OHM_0P175W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P8W8_OHM_1P06W6_OHM_0P2W32_OHM	(6)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P8W8_OHM_1P06W6_OHM_0P2W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P8W8_OHM_1P06W6_OHM_0P2W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_0P9W8_OHM_1P2W6_OHM_0P225W32_OHM	(7)
+#define AW87XXX_PID_18_REG_AGC3_PO_0P9W8_OHM_1P2W6_OHM_0P225W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_0P9W8_OHM_1P2W6_OHM_0P225W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_1P0W8_OHM_1P33W6_OHM_0P25W32_OHM	(8)
+#define AW87XXX_PID_18_REG_AGC3_PO_1P0W8_OHM_1P33W6_OHM_0P25W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_1P0W8_OHM_1P33W6_OHM_0P25W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_1P1W8_OHM_1P46W6_OHM_0P275W32_OHM	(9)
+#define AW87XXX_PID_18_REG_AGC3_PO_1P1W8_OHM_1P46W6_OHM_0P275W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_1P1W8_OHM_1P46W6_OHM_0P275W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_1P2W8_OHM_1P6W6_OHM_0P30W32_OHM	(10)
+#define AW87XXX_PID_18_REG_AGC3_PO_1P2W8_OHM_1P6W6_OHM_0P30W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_1P2W8_OHM_1P6W6_OHM_0P30W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_1P3W8_OHM_1P73W6_OHM_0P325W32_OHM	(11)
+#define AW87XXX_PID_18_REG_AGC3_PO_1P3W8_OHM_1P73W6_OHM_0P325W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_1P3W8_OHM_1P73W6_OHM_0P325W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_1P4W8_OHM_1P86W6_OHM_0P35W32_OHM	(12)
+#define AW87XXX_PID_18_REG_AGC3_PO_1P4W8_OHM_1P86W6_OHM_0P35W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_1P4W8_OHM_1P86W6_OHM_0P35W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_1P5W8_OHM_2W6_OHM_0P375W32_OHM	(13)
+#define AW87XXX_PID_18_REG_AGC3_PO_1P5W8_OHM_2W6_OHM_0P375W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_1P5W8_OHM_2W6_OHM_0P375W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_1P6W8_OHM_2P13W6_OHM_0P4W32_OHM	(14)
+#define AW87XXX_PID_18_REG_AGC3_PO_1P6W8_OHM_2P13W6_OHM_0P4W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_1P6W8_OHM_2P13W6_OHM_0P4W32_OHM << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_AGC3_OFF	(15)
+#define AW87XXX_PID_18_REG_AGC3_PO_AGC3_OFF_VALUE	\
+	(AW87XXX_PID_18_REG_AGC3_PO_AGC3_OFF << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC3_PO_DEFAULT_VALUE	(0xA)
+#define AW87XXX_PID_18_REG_AGC3_PO_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC3_PO_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC3_PO_START_BIT)
+
+/* REG_AGC2_PO bit 3:0 (A3A2PO 0x08) */
+#define AW87XXX_PID_18_REG_AGC2_PO_START_BIT	(0)
+#define AW87XXX_PID_18_REG_AGC2_PO_BITS_LEN	(4)
+#define AW87XXX_PID_18_REG_AGC2_PO_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC2_PO_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC2_PO_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC2_PO_0P4W8_OHM_0P53W6_OHM_0P1W32_OHM	(0)
+#define AW87XXX_PID_18_REG_AGC2_PO_0P4W8_OHM_0P53W6_OHM_0P1W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_0P4W8_OHM_0P53W6_OHM_0P1W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_0P6W8_OHM_0P8W6_OHM_0P15W32_OHM	(1)
+#define AW87XXX_PID_18_REG_AGC2_PO_0P6W8_OHM_0P8W6_OHM_0P15W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_0P6W8_OHM_0P8W6_OHM_0P15W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_0P8W8_OHM_1P06W6_OHM_0P2W32_OHM	(2)
+#define AW87XXX_PID_18_REG_AGC2_PO_0P8W8_OHM_1P06W6_OHM_0P2W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_0P8W8_OHM_1P06W6_OHM_0P2W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_1P0W8_OHM_1P33W6_OHM_0P25W32_OHM	(3)
+#define AW87XXX_PID_18_REG_AGC2_PO_1P0W8_OHM_1P33W6_OHM_0P25W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_1P0W8_OHM_1P33W6_OHM_0P25W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_1P2W8_OHM_1P6W6_OHM_0P3W32_OHM	(4)
+#define AW87XXX_PID_18_REG_AGC2_PO_1P2W8_OHM_1P6W6_OHM_0P3W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_1P2W8_OHM_1P6W6_OHM_0P3W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_1P4W8_OHM_1P86W6_OHM_0P35W32_OHM	(5)
+#define AW87XXX_PID_18_REG_AGC2_PO_1P4W8_OHM_1P86W6_OHM_0P35W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_1P4W8_OHM_1P86W6_OHM_0P35W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_1P6W8_OHM_2P13W6_OHM_0P4W32_OHM	(6)
+#define AW87XXX_PID_18_REG_AGC2_PO_1P6W8_OHM_2P13W6_OHM_0P4W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_1P6W8_OHM_2P13W6_OHM_0P4W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_1P8W8_OHM_2P4W6_OHM_0P45W32_OHM	(7)
+#define AW87XXX_PID_18_REG_AGC2_PO_1P8W8_OHM_2P4W6_OHM_0P45W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_1P8W8_OHM_2P4W6_OHM_0P45W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_2P0W8_OHM_2P66W6_OHM_0P5W32_OHM	(8)
+#define AW87XXX_PID_18_REG_AGC2_PO_2P0W8_OHM_2P66W6_OHM_0P5W32_OHM_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_2P0W8_OHM_2P66W6_OHM_0P5W32_OHM << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_AGC2_OFF	(9)
+#define AW87XXX_PID_18_REG_AGC2_PO_AGC2_OFF_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_PO_AGC2_OFF << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_PO_DEFAULT_VALUE	(0x6)
+#define AW87XXX_PID_18_REG_AGC2_PO_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC2_PO_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC2_PO_START_BIT)
+
+/* default value of A3A2PO (0x08) */
+/* #define AW87XXX_PID_18_A3A2PO_DEFAULT		(0xA6) */
+
+/* A2PARAM (0x09) detail */
+/* REG_TEDGE bit 5 (A2PARAM 0x09) */
+#define AW87XXX_PID_18_REG_TEDGE_START_BIT	(5)
+#define AW87XXX_PID_18_REG_TEDGE_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_TEDGE_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_TEDGE_BITS_LEN)-1) << AW87XXX_PID_18_REG_TEDGE_START_BIT))
+
+#define AW87XXX_PID_18_REG_TEDGE_4NS	(0)
+#define AW87XXX_PID_18_REG_TEDGE_4NS_VALUE	\
+	(AW87XXX_PID_18_REG_TEDGE_4NS << AW87XXX_PID_18_REG_TEDGE_START_BIT)
+
+#define AW87XXX_PID_18_REG_TEDGE_12NS	(1)
+#define AW87XXX_PID_18_REG_TEDGE_12NS_VALUE	\
+	(AW87XXX_PID_18_REG_TEDGE_12NS << AW87XXX_PID_18_REG_TEDGE_START_BIT)
+
+#define AW87XXX_PID_18_REG_TEDGE_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_TEDGE_DEFAULT	\
+	(AW87XXX_PID_18_REG_TEDGE_DEFAULT_VALUE << AW87XXX_PID_18_REG_TEDGE_START_BIT)
+
+/* REG_AGC2_AT bit 4:2 (A2PARAM 0x09) */
+#define AW87XXX_PID_18_REG_AGC2_AT_START_BIT	(2)
+#define AW87XXX_PID_18_REG_AGC2_AT_BITS_LEN	(3)
+#define AW87XXX_PID_18_REG_AGC2_AT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC2_AT_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC2_AT_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC2_AT_1P44MS_0P08MSSTEP	(0)
+#define AW87XXX_PID_18_REG_AGC2_AT_1P44MS_0P08MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_1P44MS_0P08MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_2P88MS_0P16MSSTEP	(1)
+#define AW87XXX_PID_18_REG_AGC2_AT_2P88MS_0P16MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_2P88MS_0P16MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_5P76MS_0P32MSSTEP	(2)
+#define AW87XXX_PID_18_REG_AGC2_AT_5P76MS_0P32MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_5P76MS_0P32MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_23P04MS_1P28MSSTEP	(3)
+#define AW87XXX_PID_18_REG_AGC2_AT_23P04MS_1P28MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_23P04MS_1P28MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_92P16MS_5P12MSSTEP	(4)
+#define AW87XXX_PID_18_REG_AGC2_AT_92P16MS_5P12MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_92P16MS_5P12MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_368P64MS_20P48MSSTEP	(5)
+#define AW87XXX_PID_18_REG_AGC2_AT_368P64MS_20P48MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_368P64MS_20P48MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_737P28MS_41MSSTEP	(6)
+#define AW87XXX_PID_18_REG_AGC2_AT_737P28MS_41MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_737P28MS_41MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_1474P56MS_82MSSTEP	(7)
+#define AW87XXX_PID_18_REG_AGC2_AT_1474P56MS_82MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_AT_1474P56MS_82MSSTEP << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_AT_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_18_REG_AGC2_AT_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC2_AT_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC2_AT_START_BIT)
+
+/* REG_AGC2_1ST_AT bit 1:0 (A2PARAM 0x09) */
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_START_BIT	(0)
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC2_1ST_AT_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC2_1ST_AT_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_0P08MS	(0)
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_0P08MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_1ST_AT_0P08MS << AW87XXX_PID_18_REG_AGC2_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_0P32MS	(1)
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_0P32MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_1ST_AT_0P32MS << AW87XXX_PID_18_REG_AGC2_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_1P28MS	(2)
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_1P28MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_1ST_AT_1P28MS << AW87XXX_PID_18_REG_AGC2_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_5P12MS	(3)
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_5P12MS_VALUE	\
+	(AW87XXX_PID_18_REG_AGC2_1ST_AT_5P12MS << AW87XXX_PID_18_REG_AGC2_1ST_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_AGC2_1ST_AT_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC2_1ST_AT_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC2_1ST_AT_START_BIT)
+
+/* default value of A2PARAM (0x09) */
+/* #define AW87XXX_PID_18_A2PARAM_DEFAULT		(0x08) */
+
+/* A1PARAM (0x0A) detail */
+/* REG_AGC1_PO bit 6:3 (A1PARAM 0x0A) */
+#define AW87XXX_PID_18_REG_AGC1_PO_START_BIT	(3)
+#define AW87XXX_PID_18_REG_AGC1_PO_BITS_LEN	(4)
+#define AW87XXX_PID_18_REG_AGC1_PO_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC1_PO_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC1_PO_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC1_PO_5V	(0)
+#define AW87XXX_PID_18_REG_AGC1_PO_5V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_5V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_5P2V	(1)
+#define AW87XXX_PID_18_REG_AGC1_PO_5P2V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_5P2V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_5P4V	(2)
+#define AW87XXX_PID_18_REG_AGC1_PO_5P4V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_5P4V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_5P6V	(3)
+#define AW87XXX_PID_18_REG_AGC1_PO_5P6V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_5P6V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_5P8V	(4)
+#define AW87XXX_PID_18_REG_AGC1_PO_5P8V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_5P8V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_6P0V	(5)
+#define AW87XXX_PID_18_REG_AGC1_PO_6P0V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_6P0V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_6P2V	(6)
+#define AW87XXX_PID_18_REG_AGC1_PO_6P2V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_6P2V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_6P4V	(7)
+#define AW87XXX_PID_18_REG_AGC1_PO_6P4V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_6P4V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_6P6V	(8)
+#define AW87XXX_PID_18_REG_AGC1_PO_6P6V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_6P6V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_6P8V	(9)
+#define AW87XXX_PID_18_REG_AGC1_PO_6P8V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_6P8V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_7V	(10)
+#define AW87XXX_PID_18_REG_AGC1_PO_7V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_7V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_7P2V	(11)
+#define AW87XXX_PID_18_REG_AGC1_PO_7P2V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_7P2V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_7P4V	(12)
+#define AW87XXX_PID_18_REG_AGC1_PO_7P4V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_7P4V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_7P6V	(13)
+#define AW87XXX_PID_18_REG_AGC1_PO_7P6V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_7P6V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_7P8V	(14)
+#define AW87XXX_PID_18_REG_AGC1_PO_7P8V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_7P8V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_8V	(15)
+#define AW87XXX_PID_18_REG_AGC1_PO_8V_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_PO_8V << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_PO_DEFAULT_VALUE	(0x9)
+#define AW87XXX_PID_18_REG_AGC1_PO_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC1_PO_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC1_PO_START_BIT)
+
+/* REG_AGC1_AT bit 2:1 (A1PARAM 0x0A) */
+#define AW87XXX_PID_18_REG_AGC1_AT_START_BIT	(1)
+#define AW87XXX_PID_18_REG_AGC1_AT_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_AGC1_AT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC1_AT_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC1_AT_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC1_AT_0P48MS_0P02MSSTEP	(0)
+#define AW87XXX_PID_18_REG_AGC1_AT_0P48MS_0P02MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_AT_0P48MS_0P02MSSTEP << AW87XXX_PID_18_REG_AGC1_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_AT_0P96MS_0P04MSSTEP	(1)
+#define AW87XXX_PID_18_REG_AGC1_AT_0P96MS_0P04MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_AT_0P96MS_0P04MSSTEP << AW87XXX_PID_18_REG_AGC1_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_AT_1P92MS_0P08MSSTEP	(2)
+#define AW87XXX_PID_18_REG_AGC1_AT_1P92MS_0P08MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_AT_1P92MS_0P08MSSTEP << AW87XXX_PID_18_REG_AGC1_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_AT_3P84MS_0P16MSSTEP	(3)
+#define AW87XXX_PID_18_REG_AGC1_AT_3P84MS_0P16MSSTEP_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_AT_3P84MS_0P16MSSTEP << AW87XXX_PID_18_REG_AGC1_AT_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_AT_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_AGC1_AT_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC1_AT_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC1_AT_START_BIT)
+
+/* REG_PD_AGC1 bit 0 (A1PARAM 0x0A) */
+#define AW87XXX_PID_18_REG_PD_AGC1_START_BIT	(0)
+#define AW87XXX_PID_18_REG_PD_AGC1_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_PD_AGC1_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_PD_AGC1_BITS_LEN)-1) << AW87XXX_PID_18_REG_PD_AGC1_START_BIT))
+
+#define AW87XXX_PID_18_REG_PD_AGC1_ENABLE_FASTEST_LEVEL_AGC	(0)
+#define AW87XXX_PID_18_REG_PD_AGC1_ENABLE_FASTEST_LEVEL_AGC_VALUE	\
+	(AW87XXX_PID_18_REG_PD_AGC1_ENABLE_FASTEST_LEVEL_AGC << AW87XXX_PID_18_REG_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_AGC1_DISABLE_FASTEST_LEVEL_AGC	(1)
+#define AW87XXX_PID_18_REG_PD_AGC1_DISABLE_FASTEST_LEVEL_AGC_VALUE	\
+	(AW87XXX_PID_18_REG_PD_AGC1_DISABLE_FASTEST_LEVEL_AGC << AW87XXX_PID_18_REG_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_AGC1_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_PD_AGC1_DEFAULT	\
+	(AW87XXX_PID_18_REG_PD_AGC1_DEFAULT_VALUE << AW87XXX_PID_18_REG_PD_AGC1_START_BIT)
+
+/* default value of A1PARAM (0x0A) */
+/* #define AW87XXX_PID_18_A1PARAM_DEFAULT		(0x4A) */
+
+/* POPCLK (0x0B) detail */
+/* REG_DCLK_L bit 7 (POPCLK 0x0B) */
+#define AW87XXX_PID_18_REG_DCLK_L_START_BIT	(7)
+#define AW87XXX_PID_18_REG_DCLK_L_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_DCLK_L_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_DCLK_L_BITS_LEN)-1) << AW87XXX_PID_18_REG_DCLK_L_START_BIT))
+
+#define AW87XXX_PID_18_REG_DCLK_L_30NS	(0)
+#define AW87XXX_PID_18_REG_DCLK_L_30NS_VALUE	\
+	(AW87XXX_PID_18_REG_DCLK_L_30NS << AW87XXX_PID_18_REG_DCLK_L_START_BIT)
+
+#define AW87XXX_PID_18_REG_DCLK_L_45NS	(1)
+#define AW87XXX_PID_18_REG_DCLK_L_45NS_VALUE	\
+	(AW87XXX_PID_18_REG_DCLK_L_45NS << AW87XXX_PID_18_REG_DCLK_L_START_BIT)
+
+#define AW87XXX_PID_18_REG_DCLK_L_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_DCLK_L_DEFAULT	\
+	(AW87XXX_PID_18_REG_DCLK_L_DEFAULT_VALUE << AW87XXX_PID_18_REG_DCLK_L_START_BIT)
+
+/* REG_CLK_MAPD bit 6:5 (POPCLK 0x0B) */
+#define AW87XXX_PID_18_REG_CLK_MAPD_START_BIT	(5)
+#define AW87XXX_PID_18_REG_CLK_MAPD_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_CLK_MAPD_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CLK_MAPD_BITS_LEN)-1) << AW87XXX_PID_18_REG_CLK_MAPD_START_BIT))
+
+#define AW87XXX_PID_18_REG_CLK_MAPD_40MS	(0)
+#define AW87XXX_PID_18_REG_CLK_MAPD_40MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_MAPD_40MS << AW87XXX_PID_18_REG_CLK_MAPD_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_MAPD_80MS	(1)
+#define AW87XXX_PID_18_REG_CLK_MAPD_80MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_MAPD_80MS << AW87XXX_PID_18_REG_CLK_MAPD_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_MAPD_160MS	(2)
+#define AW87XXX_PID_18_REG_CLK_MAPD_160MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_MAPD_160MS << AW87XXX_PID_18_REG_CLK_MAPD_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_MAPD_320MS	(3)
+#define AW87XXX_PID_18_REG_CLK_MAPD_320MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_MAPD_320MS << AW87XXX_PID_18_REG_CLK_MAPD_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_MAPD_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_CLK_MAPD_DEFAULT	\
+	(AW87XXX_PID_18_REG_CLK_MAPD_DEFAULT_VALUE << AW87XXX_PID_18_REG_CLK_MAPD_START_BIT)
+
+/* REG_CLK_POP bit 4:3 (POPCLK 0x0B) */
+#define AW87XXX_PID_18_REG_CLK_POP_START_BIT	(3)
+#define AW87XXX_PID_18_REG_CLK_POP_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_CLK_POP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CLK_POP_BITS_LEN)-1) << AW87XXX_PID_18_REG_CLK_POP_START_BIT))
+
+#define AW87XXX_PID_18_REG_CLK_POP_40MS	(0)
+#define AW87XXX_PID_18_REG_CLK_POP_40MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_POP_40MS << AW87XXX_PID_18_REG_CLK_POP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_POP_10MS	(1)
+#define AW87XXX_PID_18_REG_CLK_POP_10MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_POP_10MS << AW87XXX_PID_18_REG_CLK_POP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_POP_5MS	(2)
+#define AW87XXX_PID_18_REG_CLK_POP_5MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_POP_5MS << AW87XXX_PID_18_REG_CLK_POP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_POP_2P5MS	(3)
+#define AW87XXX_PID_18_REG_CLK_POP_2P5MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_POP_2P5MS << AW87XXX_PID_18_REG_CLK_POP_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_POP_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_CLK_POP_DEFAULT	\
+	(AW87XXX_PID_18_REG_CLK_POP_DEFAULT_VALUE << AW87XXX_PID_18_REG_CLK_POP_START_BIT)
+
+/* REG_CLK_OC bit 2:1 (POPCLK 0x0B) */
+#define AW87XXX_PID_18_REG_CLK_OC_START_BIT	(1)
+#define AW87XXX_PID_18_REG_CLK_OC_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_CLK_OC_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CLK_OC_BITS_LEN)-1) << AW87XXX_PID_18_REG_CLK_OC_START_BIT))
+
+#define AW87XXX_PID_18_REG_CLK_OC_160MS	(0)
+#define AW87XXX_PID_18_REG_CLK_OC_160MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_OC_160MS << AW87XXX_PID_18_REG_CLK_OC_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_OC_640MS	(1)
+#define AW87XXX_PID_18_REG_CLK_OC_640MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_OC_640MS << AW87XXX_PID_18_REG_CLK_OC_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_OC_1280MS	(2)
+#define AW87XXX_PID_18_REG_CLK_OC_1280MS_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_OC_1280MS << AW87XXX_PID_18_REG_CLK_OC_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_OC_SHUTDOWN_OUTPUT	(3)
+#define AW87XXX_PID_18_REG_CLK_OC_SHUTDOWN_OUTPUT_VALUE	\
+	(AW87XXX_PID_18_REG_CLK_OC_SHUTDOWN_OUTPUT << AW87XXX_PID_18_REG_CLK_OC_START_BIT)
+
+#define AW87XXX_PID_18_REG_CLK_OC_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_CLK_OC_DEFAULT	\
+	(AW87XXX_PID_18_REG_CLK_OC_DEFAULT_VALUE << AW87XXX_PID_18_REG_CLK_OC_START_BIT)
+
+/* REG_AGC1_VTH bit 0 (POPCLK 0x0B) */
+#define AW87XXX_PID_18_REG_AGC1_VTH_START_BIT	(0)
+#define AW87XXX_PID_18_REG_AGC1_VTH_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_AGC1_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_AGC1_VTH_BITS_LEN)-1) << AW87XXX_PID_18_REG_AGC1_VTH_START_BIT))
+
+#define AW87XXX_PID_18_REG_AGC1_VTH_AGC1_VTH_SELECT_ONLY_FROM_RAMP_GEN	(0)
+#define AW87XXX_PID_18_REG_AGC1_VTH_AGC1_VTH_SELECT_ONLY_FROM_RAMP_GEN_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_VTH_AGC1_VTH_SELECT_ONLY_FROM_RAMP_GEN << AW87XXX_PID_18_REG_AGC1_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_VTH_AGC1_VTH_ADAPTIVELY_SELECT_FROM_RAMP_GEN_AND_THGEN	(1)
+#define AW87XXX_PID_18_REG_AGC1_VTH_AGC1_VTH_ADAPTIVELY_SELECT_FROM_RAMP_GEN_AND_THGEN_VALUE	\
+	(AW87XXX_PID_18_REG_AGC1_VTH_AGC1_VTH_ADAPTIVELY_SELECT_FROM_RAMP_GEN_AND_THGEN << AW87XXX_PID_18_REG_AGC1_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_AGC1_VTH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_AGC1_VTH_DEFAULT	\
+	(AW87XXX_PID_18_REG_AGC1_VTH_DEFAULT_VALUE << AW87XXX_PID_18_REG_AGC1_VTH_START_BIT)
+
+/* default value of POPCLK (0x0B) */
+/* #define AW87XXX_PID_18_POPCLK_DEFAULT		(0x21) */
+
+/* GTDRCPSS (0x0C) detail */
+/* REG_TDEAD bit 5 (GTDRCPSS 0x0C) */
+#define AW87XXX_PID_18_REG_TDEAD_START_BIT	(5)
+#define AW87XXX_PID_18_REG_TDEAD_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_TDEAD_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_TDEAD_BITS_LEN)-1) << AW87XXX_PID_18_REG_TDEAD_START_BIT))
+
+#define AW87XXX_PID_18_REG_TDEAD_17NS	(0)
+#define AW87XXX_PID_18_REG_TDEAD_17NS_VALUE	\
+	(AW87XXX_PID_18_REG_TDEAD_17NS << AW87XXX_PID_18_REG_TDEAD_START_BIT)
+
+#define AW87XXX_PID_18_REG_TDEAD_25NS	(1)
+#define AW87XXX_PID_18_REG_TDEAD_25NS_VALUE	\
+	(AW87XXX_PID_18_REG_TDEAD_25NS << AW87XXX_PID_18_REG_TDEAD_START_BIT)
+
+#define AW87XXX_PID_18_REG_TDEAD_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_TDEAD_DEFAULT	\
+	(AW87XXX_PID_18_REG_TDEAD_DEFAULT_VALUE << AW87XXX_PID_18_REG_TDEAD_START_BIT)
+
+/* REG_CZ_35MV bit 4 (GTDRCPSS 0x0C) */
+#define AW87XXX_PID_18_REG_CZ_35MV_START_BIT	(4)
+#define AW87XXX_PID_18_REG_CZ_35MV_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_CZ_35MV_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CZ_35MV_BITS_LEN)-1) << AW87XXX_PID_18_REG_CZ_35MV_START_BIT))
+
+#define AW87XXX_PID_18_REG_CZ_35MV_25MV	(0)
+#define AW87XXX_PID_18_REG_CZ_35MV_25MV_VALUE	\
+	(AW87XXX_PID_18_REG_CZ_35MV_25MV << AW87XXX_PID_18_REG_CZ_35MV_START_BIT)
+
+#define AW87XXX_PID_18_REG_CZ_35MV_35MV	(1)
+#define AW87XXX_PID_18_REG_CZ_35MV_35MV_VALUE	\
+	(AW87XXX_PID_18_REG_CZ_35MV_35MV << AW87XXX_PID_18_REG_CZ_35MV_START_BIT)
+
+#define AW87XXX_PID_18_REG_CZ_35MV_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_CZ_35MV_DEFAULT	\
+	(AW87XXX_PID_18_REG_CZ_35MV_DEFAULT_VALUE << AW87XXX_PID_18_REG_CZ_35MV_START_BIT)
+
+/* BIT_CTRL bit 3 (GTDRCPSS 0x0C) */
+#define AW87XXX_PID_18_BIT_CTRL_START_BIT	(3)
+#define AW87XXX_PID_18_BIT_CTRL_BITS_LEN	(1)
+#define AW87XXX_PID_18_BIT_CTRL_MASK	\
+	(~(((1<<AW87XXX_PID_18_BIT_CTRL_BITS_LEN)-1) << AW87XXX_PID_18_BIT_CTRL_START_BIT))
+
+#define AW87XXX_PID_18_BIT_CTRL_32_STEP_SPREAD_SPECTRUM	(0)
+#define AW87XXX_PID_18_BIT_CTRL_32_STEP_SPREAD_SPECTRUM_VALUE	\
+	(AW87XXX_PID_18_BIT_CTRL_32_STEP_SPREAD_SPECTRUM << AW87XXX_PID_18_BIT_CTRL_START_BIT)
+
+#define AW87XXX_PID_18_BIT_CTRL_14_STEP_SPREAD_SPECTRUM	(1)
+#define AW87XXX_PID_18_BIT_CTRL_14_STEP_SPREAD_SPECTRUM_VALUE	\
+	(AW87XXX_PID_18_BIT_CTRL_14_STEP_SPREAD_SPECTRUM << AW87XXX_PID_18_BIT_CTRL_START_BIT)
+
+#define AW87XXX_PID_18_BIT_CTRL_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_BIT_CTRL_DEFAULT	\
+	(AW87XXX_PID_18_BIT_CTRL_DEFAULT_VALUE << AW87XXX_PID_18_BIT_CTRL_START_BIT)
+
+/* SS_EXCH bit 2 (GTDRCPSS 0x0C) */
+#define AW87XXX_PID_18_SS_EXCH_START_BIT	(2)
+#define AW87XXX_PID_18_SS_EXCH_BITS_LEN	(1)
+#define AW87XXX_PID_18_SS_EXCH_MASK		\
+	(~(((1<<AW87XXX_PID_18_SS_EXCH_BITS_LEN)-1) << AW87XXX_PID_18_SS_EXCH_START_BIT))
+
+#define AW87XXX_PID_18_SS_EXCH_12_RANGE	(0)
+#define AW87XXX_PID_18_SS_EXCH_12_RANGE_VALUE	\
+	(AW87XXX_PID_18_SS_EXCH_12_RANGE << AW87XXX_PID_18_SS_EXCH_START_BIT)
+
+#define AW87XXX_PID_18_SS_EXCH_6_RANGE	(1)
+#define AW87XXX_PID_18_SS_EXCH_6_RANGE_VALUE	\
+	(AW87XXX_PID_18_SS_EXCH_6_RANGE << AW87XXX_PID_18_SS_EXCH_START_BIT)
+
+#define AW87XXX_PID_18_SS_EXCH_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_SS_EXCH_DEFAULT	\
+	(AW87XXX_PID_18_SS_EXCH_DEFAULT_VALUE << AW87XXX_PID_18_SS_EXCH_START_BIT)
+
+/* REG_ISTART bit 1 (GTDRCPSS 0x0C) */
+#define AW87XXX_PID_18_REG_ISTART_START_BIT	(1)
+#define AW87XXX_PID_18_REG_ISTART_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_ISTART_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_ISTART_BITS_LEN)-1) << AW87XXX_PID_18_REG_ISTART_START_BIT))
+
+#define AW87XXX_PID_18_REG_ISTART_IBIAS_WI_46P8NA	(0)
+#define AW87XXX_PID_18_REG_ISTART_IBIAS_WI_46P8NA_VALUE	\
+	(AW87XXX_PID_18_REG_ISTART_IBIAS_WI_46P8NA << AW87XXX_PID_18_REG_ISTART_START_BIT)
+
+#define AW87XXX_PID_18_REG_ISTART_IBIAS_WI_62P5NA	(1)
+#define AW87XXX_PID_18_REG_ISTART_IBIAS_WI_62P5NA_VALUE	\
+	(AW87XXX_PID_18_REG_ISTART_IBIAS_WI_62P5NA << AW87XXX_PID_18_REG_ISTART_START_BIT)
+
+#define AW87XXX_PID_18_REG_ISTART_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_ISTART_DEFAULT	\
+	(AW87XXX_PID_18_REG_ISTART_DEFAULT_VALUE << AW87XXX_PID_18_REG_ISTART_START_BIT)
+
+/* REG_PD_OVPICTRL bit 0 (GTDRCPSS 0x0C) */
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_START_BIT	(0)
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_PD_OVPICTRL_BITS_LEN)-1) << AW87XXX_PID_18_REG_PD_OVPICTRL_START_BIT))
+
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_DISABLE	(0)
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_OVPICTRL_DISABLE << AW87XXX_PID_18_REG_PD_OVPICTRL_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_ENABLE	(1)
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_OVPICTRL_ENABLE << AW87XXX_PID_18_REG_PD_OVPICTRL_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_PD_OVPICTRL_DEFAULT	\
+	(AW87XXX_PID_18_REG_PD_OVPICTRL_DEFAULT_VALUE << AW87XXX_PID_18_REG_PD_OVPICTRL_START_BIT)
+
+/* default value of GTDRCPSS (0x0C) */
+/* #define AW87XXX_PID_18_GTDRCPSS_DEFAULT		(0x08) */
+
+/* MULTI (0x0D) detail */
+/* REG_CP_FREQ bit 7:6 (MULTI 0x0D) */
+#define AW87XXX_PID_18_REG_CP_FREQ_START_BIT	(6)
+#define AW87XXX_PID_18_REG_CP_FREQ_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_CP_FREQ_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CP_FREQ_BITS_LEN)-1) << AW87XXX_PID_18_REG_CP_FREQ_START_BIT))
+
+#define AW87XXX_PID_18_REG_CP_FREQ_1P8MHZ	(0)
+#define AW87XXX_PID_18_REG_CP_FREQ_1P8MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_CP_FREQ_1P8MHZ << AW87XXX_PID_18_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_FREQ_1P6MHZ	(1)
+#define AW87XXX_PID_18_REG_CP_FREQ_1P6MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_CP_FREQ_1P6MHZ << AW87XXX_PID_18_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_FREQ_1P4MHZ	(2)
+#define AW87XXX_PID_18_REG_CP_FREQ_1P4MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_CP_FREQ_1P4MHZ << AW87XXX_PID_18_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_FREQ_2P1MHZ	(3)
+#define AW87XXX_PID_18_REG_CP_FREQ_2P1MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_CP_FREQ_2P1MHZ << AW87XXX_PID_18_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_FREQ_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_CP_FREQ_DEFAULT	\
+	(AW87XXX_PID_18_REG_CP_FREQ_DEFAULT_VALUE << AW87XXX_PID_18_REG_CP_FREQ_START_BIT)
+
+/* REG_EN_OT150 bit 5 (MULTI 0x0D) */
+#define AW87XXX_PID_18_REG_EN_OT150_START_BIT	(5)
+#define AW87XXX_PID_18_REG_EN_OT150_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_OT150_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_OT150_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_OT150_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_OT150_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_OT150_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_OT150_DISABLE << AW87XXX_PID_18_REG_EN_OT150_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_OT150_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_OT150_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_OT150_ENABLE << AW87XXX_PID_18_REG_EN_OT150_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_OT150_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_EN_OT150_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_OT150_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_OT150_START_BIT)
+
+/* REG_EN_TEST bit 4 (MULTI 0x0D) */
+#define AW87XXX_PID_18_REG_EN_TEST_START_BIT	(4)
+#define AW87XXX_PID_18_REG_EN_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_TEST_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_TEST_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_TEST_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_TEST_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_TEST_DISABLE << AW87XXX_PID_18_REG_EN_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_TEST_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_TEST_ENABLE << AW87XXX_PID_18_REG_EN_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EN_TEST_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_TEST_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_TEST_START_BIT)
+
+/* REG_EN_CLASSD bit 3 (MULTI 0x0D) */
+#define AW87XXX_PID_18_REG_EN_CLASSD_START_BIT	(3)
+#define AW87XXX_PID_18_REG_EN_CLASSD_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_CLASSD_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_CLASSD_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_CLASSD_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_CLASSD_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_CLASSD_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_CLASSD_DISABLE << AW87XXX_PID_18_REG_EN_CLASSD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_CLASSD_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_CLASSD_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_CLASSD_ENABLE << AW87XXX_PID_18_REG_EN_CLASSD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_CLASSD_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_EN_CLASSD_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_CLASSD_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_CLASSD_START_BIT)
+
+/* REG_EN_DEFAULT bit 2 (MULTI 0x0D) */
+#define AW87XXX_PID_18_REG_EN_DEFAULT_START_BIT	(2)
+#define AW87XXX_PID_18_REG_EN_DEFAULT_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_DEFAULT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_DEFAULT_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_DEFAULT_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_DEFAULT_SELF_DEFINE_THE_SETTINGS	(0)
+#define AW87XXX_PID_18_REG_EN_DEFAULT_SELF_DEFINE_THE_SETTINGS_VALUE	\
+	(AW87XXX_PID_18_REG_EN_DEFAULT_SELF_DEFINE_THE_SETTINGS << AW87XXX_PID_18_REG_EN_DEFAULT_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_DEFAULT_USE_THE_DEFAULT_SETTING_IN_THE_SYSCTRL_BLOCK	(1)
+#define AW87XXX_PID_18_REG_EN_DEFAULT_USE_THE_DEFAULT_SETTING_IN_THE_SYSCTRL_BLOCK_VALUE	\
+	(AW87XXX_PID_18_REG_EN_DEFAULT_USE_THE_DEFAULT_SETTING_IN_THE_SYSCTRL_BLOCK << AW87XXX_PID_18_REG_EN_DEFAULT_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_DEFAULT_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EN_DEFAULT_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_DEFAULT_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_DEFAULT_START_BIT)
+
+/* REG_EN_ESD bit 1 (MULTI 0x0D) */
+#define AW87XXX_PID_18_REG_EN_ESD_START_BIT	(1)
+#define AW87XXX_PID_18_REG_EN_ESD_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_ESD_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_ESD_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_ESD_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_ESD_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_ESD_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_ESD_DISABLE << AW87XXX_PID_18_REG_EN_ESD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_ESD_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_ESD_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_ESD_ENABLE << AW87XXX_PID_18_REG_EN_ESD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_ESD_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EN_ESD_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_ESD_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_ESD_START_BIT)
+
+/* REG_EN_MT bit 0 (MULTI 0x0D) */
+#define AW87XXX_PID_18_REG_EN_MT_START_BIT	(0)
+#define AW87XXX_PID_18_REG_EN_MT_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_MT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_MT_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_MT_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_MT_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_MT_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_MT_DISABLE << AW87XXX_PID_18_REG_EN_MT_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_MT_ENBLAE	(1)
+#define AW87XXX_PID_18_REG_EN_MT_ENBLAE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_MT_ENBLAE << AW87XXX_PID_18_REG_EN_MT_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_MT_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EN_MT_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_MT_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_MT_START_BIT)
+
+/* default value of MULTI (0x0D) */
+/* #define AW87XXX_PID_18_MULTI_DEFAULT		(0x68) */
+
+/* DFT1 (0x61) detail */
+/* REG_SET_R2 bit 7 (DFT1 0x61) */
+#define AW87XXX_PID_18_REG_SET_R2_START_BIT	(7)
+#define AW87XXX_PID_18_REG_SET_R2_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_SET_R2_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_SET_R2_BITS_LEN)-1) << AW87XXX_PID_18_REG_SET_R2_START_BIT))
+
+#define AW87XXX_PID_18_REG_SET_R2_NOT_LIMIT_THE_HIGH_LEVEL_VTH	(0)
+#define AW87XXX_PID_18_REG_SET_R2_NOT_LIMIT_THE_HIGH_LEVEL_VTH_VALUE	\
+	(AW87XXX_PID_18_REG_SET_R2_NOT_LIMIT_THE_HIGH_LEVEL_VTH << AW87XXX_PID_18_REG_SET_R2_START_BIT)
+
+#define AW87XXX_PID_18_REG_SET_R2_LIMIT_THE_HIGH_LEVEL_VTH	(1)
+#define AW87XXX_PID_18_REG_SET_R2_LIMIT_THE_HIGH_LEVEL_VTH_VALUE	\
+	(AW87XXX_PID_18_REG_SET_R2_LIMIT_THE_HIGH_LEVEL_VTH << AW87XXX_PID_18_REG_SET_R2_START_BIT)
+
+#define AW87XXX_PID_18_REG_SET_R2_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_SET_R2_DEFAULT	\
+	(AW87XXX_PID_18_REG_SET_R2_DEFAULT_VALUE << AW87XXX_PID_18_REG_SET_R2_START_BIT)
+
+/* REG_CP_ISOFT bit 6:5 (DFT1 0x61) */
+#define AW87XXX_PID_18_REG_CP_ISOFT_START_BIT	(5)
+#define AW87XXX_PID_18_REG_CP_ISOFT_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_CP_ISOFT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CP_ISOFT_BITS_LEN)-1) << AW87XXX_PID_18_REG_CP_ISOFT_START_BIT))
+
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P2A	(0)
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P2A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_ISOFT_0P2A << AW87XXX_PID_18_REG_CP_ISOFT_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P3A	(1)
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P3A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_ISOFT_0P3A << AW87XXX_PID_18_REG_CP_ISOFT_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P4A	(2)
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P4A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_ISOFT_0P4A << AW87XXX_PID_18_REG_CP_ISOFT_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P5A	(3)
+#define AW87XXX_PID_18_REG_CP_ISOFT_0P5A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_ISOFT_0P5A << AW87XXX_PID_18_REG_CP_ISOFT_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_ISOFT_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_CP_ISOFT_DEFAULT	\
+	(AW87XXX_PID_18_REG_CP_ISOFT_DEFAULT_VALUE << AW87XXX_PID_18_REG_CP_ISOFT_START_BIT)
+
+/* REG_CP_IPEAK bit 4:2 (DFT1 0x61) */
+#define AW87XXX_PID_18_REG_CP_IPEAK_START_BIT	(2)
+#define AW87XXX_PID_18_REG_CP_IPEAK_BITS_LEN	(3)
+#define AW87XXX_PID_18_REG_CP_IPEAK_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CP_IPEAK_BITS_LEN)-1) << AW87XXX_PID_18_REG_CP_IPEAK_START_BIT))
+
+#define AW87XXX_PID_18_REG_CP_IPEAK_2A	(0)
+#define AW87XXX_PID_18_REG_CP_IPEAK_2A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_IPEAK_2A << AW87XXX_PID_18_REG_CP_IPEAK_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_IPEAK_2P5A	(1)
+#define AW87XXX_PID_18_REG_CP_IPEAK_2P5A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_IPEAK_2P5A << AW87XXX_PID_18_REG_CP_IPEAK_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_IPEAK_3A	(2)
+#define AW87XXX_PID_18_REG_CP_IPEAK_3A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_IPEAK_3A << AW87XXX_PID_18_REG_CP_IPEAK_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_IPEAK_3P5A	(3)
+#define AW87XXX_PID_18_REG_CP_IPEAK_3P5A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_IPEAK_3P5A << AW87XXX_PID_18_REG_CP_IPEAK_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_IPEAK_4A	(4)
+#define AW87XXX_PID_18_REG_CP_IPEAK_4A_VALUE	\
+	(AW87XXX_PID_18_REG_CP_IPEAK_4A << AW87XXX_PID_18_REG_CP_IPEAK_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_IPEAK_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_CP_IPEAK_DEFAULT	\
+	(AW87XXX_PID_18_REG_CP_IPEAK_DEFAULT_VALUE << AW87XXX_PID_18_REG_CP_IPEAK_START_BIT)
+
+/* REG_SET_OCDT bit 1:0 (DFT1 0x61) */
+#define AW87XXX_PID_18_REG_SET_OCDT_START_BIT	(0)
+#define AW87XXX_PID_18_REG_SET_OCDT_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_SET_OCDT_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_SET_OCDT_BITS_LEN)-1) << AW87XXX_PID_18_REG_SET_OCDT_START_BIT))
+
+#define AW87XXX_PID_18_REG_SET_OCDT_4P1A	(0)
+#define AW87XXX_PID_18_REG_SET_OCDT_4P1A_VALUE	\
+	(AW87XXX_PID_18_REG_SET_OCDT_4P1A << AW87XXX_PID_18_REG_SET_OCDT_START_BIT)
+
+#define AW87XXX_PID_18_REG_SET_OCDT_4P5A	(1)
+#define AW87XXX_PID_18_REG_SET_OCDT_4P5A_VALUE	\
+	(AW87XXX_PID_18_REG_SET_OCDT_4P5A << AW87XXX_PID_18_REG_SET_OCDT_START_BIT)
+
+#define AW87XXX_PID_18_REG_SET_OCDT_4P9A	(2)
+#define AW87XXX_PID_18_REG_SET_OCDT_4P9A_VALUE	\
+	(AW87XXX_PID_18_REG_SET_OCDT_4P9A << AW87XXX_PID_18_REG_SET_OCDT_START_BIT)
+
+#define AW87XXX_PID_18_REG_SET_OCDT_5P3A	(3)
+#define AW87XXX_PID_18_REG_SET_OCDT_5P3A_VALUE	\
+	(AW87XXX_PID_18_REG_SET_OCDT_5P3A << AW87XXX_PID_18_REG_SET_OCDT_START_BIT)
+
+#define AW87XXX_PID_18_REG_SET_OCDT_DEFAULT_VALUE	(0X2)
+#define AW87XXX_PID_18_REG_SET_OCDT_DEFAULT	\
+	(AW87XXX_PID_18_REG_SET_OCDT_DEFAULT_VALUE << AW87XXX_PID_18_REG_SET_OCDT_START_BIT)
+
+/* default value of DFT1 (0x61) */
+/* #define AW87XXX_PID_18_DFT1_DEFAULT		(0xA6) */
+
+/* DFT2 (0x62) detail */
+/* REG_CP_TEST bit 7 (DFT2 0x62) */
+#define AW87XXX_PID_18_REG_CP_TEST_START_BIT	(7)
+#define AW87XXX_PID_18_REG_CP_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_CP_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CP_TEST_BITS_LEN)-1) << AW87XXX_PID_18_REG_CP_TEST_START_BIT))
+
+#define AW87XXX_PID_18_REG_CP_TEST_DISABLE	(0)
+#define AW87XXX_PID_18_REG_CP_TEST_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_CP_TEST_DISABLE << AW87XXX_PID_18_REG_CP_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_TEST_ENABLE	(1)
+#define AW87XXX_PID_18_REG_CP_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_CP_TEST_ENABLE << AW87XXX_PID_18_REG_CP_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_CP_TEST_DEFAULT	\
+	(AW87XXX_PID_18_REG_CP_TEST_DEFAULT_VALUE << AW87XXX_PID_18_REG_CP_TEST_START_BIT)
+
+/* REG_VFAGC bit 6:4 (DFT2 0x62) */
+#define AW87XXX_PID_18_REG_VFAGC_START_BIT	(4)
+#define AW87XXX_PID_18_REG_VFAGC_BITS_LEN	(3)
+#define AW87XXX_PID_18_REG_VFAGC_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_VFAGC_BITS_LEN)-1) << AW87XXX_PID_18_REG_VFAGC_START_BIT))
+
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P775VDDVREF_FAGC_VHYS0P7VDD	(0)
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P775VDDVREF_FAGC_VHYS0P7VDD_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P775VDDVREF_FAGC_VHYS0P7VDD << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P8VDDVREF_FAGC_VHYS0P725VDD	(1)
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P8VDDVREF_FAGC_VHYS0P725VDD_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P8VDDVREF_FAGC_VHYS0P725VDD << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P825VDDVREF_FAGC_VHYS0P75VDD	(2)
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P825VDDVREF_FAGC_VHYS0P75VDD_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P825VDDVREF_FAGC_VHYS0P75VDD << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P85VDDVREF_FAGC_VHYS0P775VDD	(3)
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P85VDDVREF_FAGC_VHYS0P775VDD_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P85VDDVREF_FAGC_VHYS0P775VDD << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P875VDDVREF_FAGC_VHYS0P8VDD	(4)
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P875VDDVREF_FAGC_VHYS0P8VDD_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P875VDDVREF_FAGC_VHYS0P8VDD << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P9VDDVREF_FAGC_VHYS0P825VDD	(5)
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P9VDDVREF_FAGC_VHYS0P825VDD_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P9VDDVREF_FAGC_VHYS0P825VDD << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P925VDDVREF_FAGC_VHYS0P85VDD	(6)
+#define AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P925VDDVREF_FAGC_VHYS0P85VDD_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_VREF_FAGC0P925VDDVREF_FAGC_VHYS0P85VDD << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_001	(7)
+#define AW87XXX_PID_18_REG_VFAGC_001_VALUE	\
+	(AW87XXX_PID_18_REG_VFAGC_001 << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_VFAGC_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_VFAGC_DEFAULT	\
+	(AW87XXX_PID_18_REG_VFAGC_DEFAULT_VALUE << AW87XXX_PID_18_REG_VFAGC_START_BIT)
+
+/* REG_CP_OVP_TEST bit 3:2 (DFT2 0x62) */
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_START_BIT	(2)
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_CP_OVP_TEST_BITS_LEN)-1) << AW87XXX_PID_18_REG_CP_OVP_TEST_START_BIT))
+
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_8P7V	(0)
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_8P7V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_TEST_8P7V << AW87XXX_PID_18_REG_CP_OVP_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_9P0V	(1)
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_9P0V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_TEST_9P0V << AW87XXX_PID_18_REG_CP_OVP_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_9P20V	(2)
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_9P20V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_TEST_9P20V << AW87XXX_PID_18_REG_CP_OVP_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_9P5V	(3)
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_9P5V_VALUE	\
+	(AW87XXX_PID_18_REG_CP_OVP_TEST_9P5V << AW87XXX_PID_18_REG_CP_OVP_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_CP_OVP_TEST_DEFAULT	\
+	(AW87XXX_PID_18_REG_CP_OVP_TEST_DEFAULT_VALUE << AW87XXX_PID_18_REG_CP_OVP_TEST_START_BIT)
+
+/* REG_PAVG bit 1:0 (DFT2 0x62) */
+#define AW87XXX_PID_18_REG_PAVG_START_BIT	(0)
+#define AW87XXX_PID_18_REG_PAVG_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_PAVG_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_PAVG_BITS_LEN)-1) << AW87XXX_PID_18_REG_PAVG_START_BIT))
+
+#define AW87XXX_PID_18_REG_PAVG_PO0P94	(0)
+#define AW87XXX_PID_18_REG_PAVG_PO0P94_VALUE	\
+	(AW87XXX_PID_18_REG_PAVG_PO0P94 << AW87XXX_PID_18_REG_PAVG_START_BIT)
+
+#define AW87XXX_PID_18_REG_PAVG_PO1		(1)
+#define AW87XXX_PID_18_REG_PAVG_PO1_VALUE	\
+	(AW87XXX_PID_18_REG_PAVG_PO1 << AW87XXX_PID_18_REG_PAVG_START_BIT)
+
+#define AW87XXX_PID_18_REG_PAVG_PO1P06	(2)
+#define AW87XXX_PID_18_REG_PAVG_PO1P06_VALUE	\
+	(AW87XXX_PID_18_REG_PAVG_PO1P06 << AW87XXX_PID_18_REG_PAVG_START_BIT)
+
+#define AW87XXX_PID_18_REG_PAVG_TURN_TO_10	(3)
+#define AW87XXX_PID_18_REG_PAVG_TURN_TO_10_VALUE	\
+	(AW87XXX_PID_18_REG_PAVG_TURN_TO_10 << AW87XXX_PID_18_REG_PAVG_START_BIT)
+
+#define AW87XXX_PID_18_REG_PAVG_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_PAVG_DEFAULT	\
+	(AW87XXX_PID_18_REG_PAVG_DEFAULT_VALUE << AW87XXX_PID_18_REG_PAVG_START_BIT)
+
+/* default value of DFT2 (0x62) */
+/* #define AW87XXX_PID_18_DFT2_DEFAULT		(0x11) */
+
+/* DFT3 (0x63) detail */
+/* REG_TDEAD_CP bit 7 (DFT3 0x63) */
+#define AW87XXX_PID_18_REG_TDEAD_CP_START_BIT	(7)
+#define AW87XXX_PID_18_REG_TDEAD_CP_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_TDEAD_CP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_TDEAD_CP_BITS_LEN)-1) << AW87XXX_PID_18_REG_TDEAD_CP_START_BIT))
+
+#define AW87XXX_PID_18_REG_TDEAD_CP_DEFAULT_SETTIG	(0)
+#define AW87XXX_PID_18_REG_TDEAD_CP_DEFAULT_SETTIG_VALUE	\
+	(AW87XXX_PID_18_REG_TDEAD_CP_DEFAULT_SETTIG << AW87XXX_PID_18_REG_TDEAD_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_TDEAD_CP_ENLARGE_THE_DEAD_TIME	(1)
+#define AW87XXX_PID_18_REG_TDEAD_CP_ENLARGE_THE_DEAD_TIME_VALUE	\
+	(AW87XXX_PID_18_REG_TDEAD_CP_ENLARGE_THE_DEAD_TIME << AW87XXX_PID_18_REG_TDEAD_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_TDEAD_CP_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_TDEAD_CP_DEFAULT	\
+	(AW87XXX_PID_18_REG_TDEAD_CP_DEFAULT_VALUE << AW87XXX_PID_18_REG_TDEAD_CP_START_BIT)
+
+/* REG_EN_EXPVDD bit 6 (DFT3 0x63) */
+#define AW87XXX_PID_18_REG_EN_EXPVDD_START_BIT	(6)
+#define AW87XXX_PID_18_REG_EN_EXPVDD_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_EXPVDD_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_EXPVDD_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_EXPVDD_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_EXPVDD_DISABLE	(0)
+#define AW87XXX_PID_18_REG_EN_EXPVDD_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_EXPVDD_DISABLE << AW87XXX_PID_18_REG_EN_EXPVDD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_EXPVDD_ENABLE	(1)
+#define AW87XXX_PID_18_REG_EN_EXPVDD_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_EN_EXPVDD_ENABLE << AW87XXX_PID_18_REG_EN_EXPVDD_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_EXPVDD_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EN_EXPVDD_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_EXPVDD_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_EXPVDD_START_BIT)
+
+/* REG_TM_MADP bit 5 (DFT3 0x63) */
+#define AW87XXX_PID_18_REG_TM_MADP_START_BIT	(5)
+#define AW87XXX_PID_18_REG_TM_MADP_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_TM_MADP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_TM_MADP_BITS_LEN)-1) << AW87XXX_PID_18_REG_TM_MADP_START_BIT))
+
+#define AW87XXX_PID_18_REG_TM_MADP_DISABLE	(0)
+#define AW87XXX_PID_18_REG_TM_MADP_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_TM_MADP_DISABLE << AW87XXX_PID_18_REG_TM_MADP_START_BIT)
+
+#define AW87XXX_PID_18_REG_TM_MADP_ENABLE	(1)
+#define AW87XXX_PID_18_REG_TM_MADP_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_TM_MADP_ENABLE << AW87XXX_PID_18_REG_TM_MADP_START_BIT)
+
+#define AW87XXX_PID_18_REG_TM_MADP_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_TM_MADP_DEFAULT	\
+	(AW87XXX_PID_18_REG_TM_MADP_DEFAULT_VALUE << AW87XXX_PID_18_REG_TM_MADP_START_BIT)
+
+/* REG_PD_UVLO bit 4 (DFT3 0x63) */
+#define AW87XXX_PID_18_REG_PD_UVLO_START_BIT	(4)
+#define AW87XXX_PID_18_REG_PD_UVLO_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_PD_UVLO_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_PD_UVLO_BITS_LEN)-1) << AW87XXX_PID_18_REG_PD_UVLO_START_BIT))
+
+#define AW87XXX_PID_18_REG_PD_UVLO_ENABLE	(0)
+#define AW87XXX_PID_18_REG_PD_UVLO_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_UVLO_ENABLE << AW87XXX_PID_18_REG_PD_UVLO_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_UVLO_DISABLE	(1)
+#define AW87XXX_PID_18_REG_PD_UVLO_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_UVLO_DISABLE << AW87XXX_PID_18_REG_PD_UVLO_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_UVLO_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_PD_UVLO_DEFAULT	\
+	(AW87XXX_PID_18_REG_PD_UVLO_DEFAULT_VALUE << AW87XXX_PID_18_REG_PD_UVLO_START_BIT)
+
+/* REG_UVLO_VTH bit 3:2 (DFT3 0x63) */
+#define AW87XXX_PID_18_REG_UVLO_VTH_START_BIT	(2)
+#define AW87XXX_PID_18_REG_UVLO_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_UVLO_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_UVLO_VTH_BITS_LEN)-1) << AW87XXX_PID_18_REG_UVLO_VTH_START_BIT))
+
+#define AW87XXX_PID_18_REG_UVLO_VTH_2P6V2P5V	(0)
+#define AW87XXX_PID_18_REG_UVLO_VTH_2P6V2P5V_VALUE	\
+	(AW87XXX_PID_18_REG_UVLO_VTH_2P6V2P5V << AW87XXX_PID_18_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_UVLO_VTH_2P7V2P6V	(1)
+#define AW87XXX_PID_18_REG_UVLO_VTH_2P7V2P6V_VALUE	\
+	(AW87XXX_PID_18_REG_UVLO_VTH_2P7V2P6V << AW87XXX_PID_18_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_UVLO_VTH_2P5V2P4V	(2)
+#define AW87XXX_PID_18_REG_UVLO_VTH_2P5V2P4V_VALUE	\
+	(AW87XXX_PID_18_REG_UVLO_VTH_2P5V2P4V << AW87XXX_PID_18_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_UVLO_VTH_TURN_TO_00	(3)
+#define AW87XXX_PID_18_REG_UVLO_VTH_TURN_TO_00_VALUE	\
+	(AW87XXX_PID_18_REG_UVLO_VTH_TURN_TO_00 << AW87XXX_PID_18_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_18_REG_UVLO_VTH_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_UVLO_VTH_DEFAULT	\
+	(AW87XXX_PID_18_REG_UVLO_VTH_DEFAULT_VALUE << AW87XXX_PID_18_REG_UVLO_VTH_START_BIT)
+
+/* REG_PD_CRS0 bit 1:0 (DFT3 0x63) */
+#define AW87XXX_PID_18_REG_PD_CRS0_START_BIT	(0)
+#define AW87XXX_PID_18_REG_PD_CRS0_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_PD_CRS0_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_PD_CRS0_BITS_LEN)-1) << AW87XXX_PID_18_REG_PD_CRS0_START_BIT))
+
+#define AW87XXX_PID_18_REG_PD_CRS0_ALL_OF_AGC1_AGC2_AND_AGC3_CROSS_ZERO_ENABLE	(0)
+#define AW87XXX_PID_18_REG_PD_CRS0_ALL_OF_AGC1_AGC2_AND_AGC3_CROSS_ZERO_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_CRS0_ALL_OF_AGC1_AGC2_AND_AGC3_CROSS_ZERO_ENABLE << AW87XXX_PID_18_REG_PD_CRS0_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_CRS0_BOTH_AGC2_AND_AGC3_CROSS_ZERO_ENABLE_AGC1_CROSS_ZERO_DISABLE	(1)
+#define AW87XXX_PID_18_REG_PD_CRS0_BOTH_AGC2_AND_AGC3_CROSS_ZERO_ENABLE_AGC1_CROSS_ZERO_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_CRS0_BOTH_AGC2_AND_AGC3_CROSS_ZERO_ENABLE_AGC1_CROSS_ZERO_DISABLE << AW87XXX_PID_18_REG_PD_CRS0_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_CRS0_ONLY_AGC3_CROSS_ZERO_ENABLE_AGC1_AND_AGC2_CROSS_ZERO_DISABLE	(2)
+#define AW87XXX_PID_18_REG_PD_CRS0_ONLY_AGC3_CROSS_ZERO_ENABLE_AGC1_AND_AGC2_CROSS_ZERO_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_CRS0_ONLY_AGC3_CROSS_ZERO_ENABLE_AGC1_AND_AGC2_CROSS_ZERO_DISABLE << AW87XXX_PID_18_REG_PD_CRS0_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_CRS0_ALL_OF_AGC1_AGC2_AND_AGC3_CROSS_ZERO_DISABLE	(3)
+#define AW87XXX_PID_18_REG_PD_CRS0_ALL_OF_AGC1_AGC2_AND_AGC3_CROSS_ZERO_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_PD_CRS0_ALL_OF_AGC1_AGC2_AND_AGC3_CROSS_ZERO_DISABLE << AW87XXX_PID_18_REG_PD_CRS0_START_BIT)
+
+#define AW87XXX_PID_18_REG_PD_CRS0_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_PD_CRS0_DEFAULT	\
+	(AW87XXX_PID_18_REG_PD_CRS0_DEFAULT_VALUE << AW87XXX_PID_18_REG_PD_CRS0_START_BIT)
+
+/* default value of DFT3 (0x63) */
+/* #define AW87XXX_PID_18_DFT3_DEFAULT		(0x00) */
+
+/* DFT4 (0x64) detail */
+/* REG_DEGLITCH_CP bit 7:6 (DFT4 0x64) */
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_START_BIT	(6)
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_DEGLITCH_CP_BITS_LEN)-1) << AW87XXX_PID_18_REG_DEGLITCH_CP_START_BIT))
+
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_3NS	(0)
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_3NS_VALUE	\
+	(AW87XXX_PID_18_REG_DEGLITCH_CP_3NS << AW87XXX_PID_18_REG_DEGLITCH_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_5NS	(1)
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_5NS_VALUE	\
+	(AW87XXX_PID_18_REG_DEGLITCH_CP_5NS << AW87XXX_PID_18_REG_DEGLITCH_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_1NS	(2)
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_1NS_VALUE	\
+	(AW87XXX_PID_18_REG_DEGLITCH_CP_1NS << AW87XXX_PID_18_REG_DEGLITCH_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_0NS	(3)
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_0NS_VALUE	\
+	(AW87XXX_PID_18_REG_DEGLITCH_CP_0NS << AW87XXX_PID_18_REG_DEGLITCH_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_DEGLITCH_CP_DEFAULT	\
+	(AW87XXX_PID_18_REG_DEGLITCH_CP_DEFAULT_VALUE << AW87XXX_PID_18_REG_DEGLITCH_CP_START_BIT)
+
+/* REG_EDGE_CP bit 5:4 (DFT4 0x64) */
+#define AW87XXX_PID_18_REG_EDGE_CP_START_BIT	(4)
+#define AW87XXX_PID_18_REG_EDGE_CP_BITS_LEN	(2)
+#define AW87XXX_PID_18_REG_EDGE_CP_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EDGE_CP_BITS_LEN)-1) << AW87XXX_PID_18_REG_EDGE_CP_START_BIT))
+
+#define AW87XXX_PID_18_REG_EDGE_CP_DEFAULT_14P8NS	(0)
+#define AW87XXX_PID_18_REG_EDGE_CP_DEFAULT_14P8NS_VALUE	\
+	(AW87XXX_PID_18_REG_EDGE_CP_DEFAULT_14P8NS << AW87XXX_PID_18_REG_EDGE_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EDGE_CP_MODERATE_13P5NS	(1)
+#define AW87XXX_PID_18_REG_EDGE_CP_MODERATE_13P5NS_VALUE	\
+	(AW87XXX_PID_18_REG_EDGE_CP_MODERATE_13P5NS << AW87XXX_PID_18_REG_EDGE_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EDGE_CP_SLOWEST_19P3NS	(2)
+#define AW87XXX_PID_18_REG_EDGE_CP_SLOWEST_19P3NS_VALUE	\
+	(AW87XXX_PID_18_REG_EDGE_CP_SLOWEST_19P3NS << AW87XXX_PID_18_REG_EDGE_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EDGE_CP_FASTEST_4P6NS00	(3)
+#define AW87XXX_PID_18_REG_EDGE_CP_FASTEST_4P6NS00_VALUE	\
+	(AW87XXX_PID_18_REG_EDGE_CP_FASTEST_4P6NS00 << AW87XXX_PID_18_REG_EDGE_CP_START_BIT)
+
+#define AW87XXX_PID_18_REG_EDGE_CP_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_EDGE_CP_DEFAULT	\
+	(AW87XXX_PID_18_REG_EDGE_CP_DEFAULT_VALUE << AW87XXX_PID_18_REG_EDGE_CP_START_BIT)
+
+/* REG_TESTSEL bit 3:0 (DFT4 0x64) */
+#define AW87XXX_PID_18_REG_TESTSEL_START_BIT	(0)
+#define AW87XXX_PID_18_REG_TESTSEL_BITS_LEN	(4)
+#define AW87XXX_PID_18_REG_TESTSEL_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_TESTSEL_BITS_LEN)-1) << AW87XXX_PID_18_REG_TESTSEL_START_BIT))
+
+#define AW87XXX_PID_18_REG_TESTSEL_VBG_FROM_BIAS	(0)
+#define AW87XXX_PID_18_REG_TESTSEL_VBG_FROM_BIAS_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_VBG_FROM_BIAS << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_VCOM1_FROM_PREAMP	(1)
+#define AW87XXX_PID_18_REG_TESTSEL_VCOM1_FROM_PREAMP_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_VCOM1_FROM_PREAMP << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_VREF_AGC_FROM_RAMP	(2)
+#define AW87XXX_PID_18_REG_TESTSEL_VREF_AGC_FROM_RAMP_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_VREF_AGC_FROM_RAMP << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_VREF_ADP_FROM_THGEN	(3)
+#define AW87XXX_PID_18_REG_TESTSEL_VREF_ADP_FROM_THGEN_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_VREF_ADP_FROM_THGEN << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_OC	(4)
+#define AW87XXX_PID_18_REG_TESTSEL_OC_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_OC << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_OT160	(5)
+#define AW87XXX_PID_18_REG_TESTSEL_OT160_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_OT160 << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_UVLO	(6)
+#define AW87XXX_PID_18_REG_TESTSEL_UVLO_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_UVLO << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_GT_P_TEST_FROM_GATEDRIVER	(7)
+#define AW87XXX_PID_18_REG_TESTSEL_GT_P_TEST_FROM_GATEDRIVER_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_GT_P_TEST_FROM_GATEDRIVER << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_GT_N_TEST_FROM_GATEDRIVER	(8)
+#define AW87XXX_PID_18_REG_TESTSEL_GT_N_TEST_FROM_GATEDRIVER_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_GT_N_TEST_FROM_GATEDRIVER << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_GT1_P_TEST_FROM_GATEDRIVER	(9)
+#define AW87XXX_PID_18_REG_TESTSEL_GT1_P_TEST_FROM_GATEDRIVER_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_GT1_P_TEST_FROM_GATEDRIVER << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_GT1_N_TEST_FROM_GATEDRIVER	(10)
+#define AW87XXX_PID_18_REG_TESTSEL_GT1_N_TEST_FROM_GATEDRIVER_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_GT1_N_TEST_FROM_GATEDRIVER << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_OVP0_TEST_FROM_OVP	(11)
+#define AW87XXX_PID_18_REG_TESTSEL_OVP0_TEST_FROM_OVP_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_OVP0_TEST_FROM_OVP << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_OVP1_TEST_FROM_OVP	(12)
+#define AW87XXX_PID_18_REG_TESTSEL_OVP1_TEST_FROM_OVP_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_OVP1_TEST_FROM_OVP << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_PORN_TEST_FROM_PORN	(13)
+#define AW87XXX_PID_18_REG_TESTSEL_PORN_TEST_FROM_PORN_VALUE	\
+	(AW87XXX_PID_18_REG_TESTSEL_PORN_TEST_FROM_PORN << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+#define AW87XXX_PID_18_REG_TESTSEL_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_TESTSEL_DEFAULT	\
+	(AW87XXX_PID_18_REG_TESTSEL_DEFAULT_VALUE << AW87XXX_PID_18_REG_TESTSEL_START_BIT)
+
+/* default value of DFT4 (0x64) */
+/* #define AW87XXX_PID_18_DFT4_DEFAULT		(0x00) */
+
+/* DFT5 (0x65) detail */
+/* FCLK_CS bit 5 (DFT5 0x65) */
+#define AW87XXX_PID_18_FCLK_CS_START_BIT	(5)
+#define AW87XXX_PID_18_FCLK_CS_BITS_LEN	(1)
+#define AW87XXX_PID_18_FCLK_CS_MASK		\
+	(~(((1<<AW87XXX_PID_18_FCLK_CS_BITS_LEN)-1) << AW87XXX_PID_18_FCLK_CS_START_BIT))
+
+#define AW87XXX_PID_18_FCLK_CS_CHOOSE_THE_CLOCK_SIGNALCLK_PA_FROM_THE_ANALOG_PART	(0)
+#define AW87XXX_PID_18_FCLK_CS_CHOOSE_THE_CLOCK_SIGNALCLK_PA_FROM_THE_ANALOG_PART_VALUE	\
+	(AW87XXX_PID_18_FCLK_CS_CHOOSE_THE_CLOCK_SIGNALCLK_PA_FROM_THE_ANALOG_PART << AW87XXX_PID_18_FCLK_CS_START_BIT)
+
+#define AW87XXX_PID_18_FCLK_CS_CHOOSE_THE_CLOCK_SIGNAL_GENERATED_BY_DIGITAL_PART_THEN_WRITE_0XA5_TO_THE_0X66_REGISTORGENERATE_A_PULSE_AFTER_EACH_WRITING	(1)
+#define AW87XXX_PID_18_FCLK_CS_CHOOSE_THE_CLOCK_SIGNAL_GENERATED_BY_DIGITAL_PART_THEN_WRITE_0XA5_TO_THE_0X66_REGISTORGENERATE_A_PULSE_AFTER_EACH_WRITING_VALUE	\
+	(AW87XXX_PID_18_FCLK_CS_CHOOSE_THE_CLOCK_SIGNAL_GENERATED_BY_DIGITAL_PART_THEN_WRITE_0XA5_TO_THE_0X66_REGISTORGENERATE_A_PULSE_AFTER_EACH_WRITING << AW87XXX_PID_18_FCLK_CS_START_BIT)
+
+#define AW87XXX_PID_18_FCLK_CS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_FCLK_CS_DEFAULT	\
+	(AW87XXX_PID_18_FCLK_CS_DEFAULT_VALUE << AW87XXX_PID_18_FCLK_CS_START_BIT)
+
+/* REG_OT_TEST bit 4 (DFT5 0x65) */
+#define AW87XXX_PID_18_REG_OT_TEST_START_BIT	(4)
+#define AW87XXX_PID_18_REG_OT_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_OT_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_OT_TEST_BITS_LEN)-1) << AW87XXX_PID_18_REG_OT_TEST_START_BIT))
+
+#define AW87XXX_PID_18_REG_OT_TEST_DISABLE_NOT_TO_TRANSFER_THE_OTN_TO_THE_TEST_BLOCK	(0)
+#define AW87XXX_PID_18_REG_OT_TEST_DISABLE_NOT_TO_TRANSFER_THE_OTN_TO_THE_TEST_BLOCK_VALUE	\
+	(AW87XXX_PID_18_REG_OT_TEST_DISABLE_NOT_TO_TRANSFER_THE_OTN_TO_THE_TEST_BLOCK << AW87XXX_PID_18_REG_OT_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_OT_TEST_ENABLE_TO_TRANSFER_THE_OTN_TO_THE_TEST_BLOCK	(1)
+#define AW87XXX_PID_18_REG_OT_TEST_ENABLE_TO_TRANSFER_THE_OTN_TO_THE_TEST_BLOCK_VALUE	\
+	(AW87XXX_PID_18_REG_OT_TEST_ENABLE_TO_TRANSFER_THE_OTN_TO_THE_TEST_BLOCK << AW87XXX_PID_18_REG_OT_TEST_START_BIT)
+
+#define AW87XXX_PID_18_REG_OT_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_OT_TEST_DEFAULT	\
+	(AW87XXX_PID_18_REG_OT_TEST_DEFAULT_VALUE << AW87XXX_PID_18_REG_OT_TEST_START_BIT)
+
+/* REG_EN_OC bit 3 (DFT5 0x65) */
+#define AW87XXX_PID_18_REG_EN_OC_START_BIT	(3)
+#define AW87XXX_PID_18_REG_EN_OC_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_EN_OC_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_EN_OC_BITS_LEN)-1) << AW87XXX_PID_18_REG_EN_OC_START_BIT))
+
+#define AW87XXX_PID_18_REG_EN_OC_TURN_OFF_THE_OC_BLOCK_FORCE_0C0	(0)
+#define AW87XXX_PID_18_REG_EN_OC_TURN_OFF_THE_OC_BLOCK_FORCE_0C0_VALUE	\
+	(AW87XXX_PID_18_REG_EN_OC_TURN_OFF_THE_OC_BLOCK_FORCE_0C0 << AW87XXX_PID_18_REG_EN_OC_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_OC_TURN_ON_THE_OC_BLOCK_FUNCTION	(1)
+#define AW87XXX_PID_18_REG_EN_OC_TURN_ON_THE_OC_BLOCK_FUNCTION_VALUE	\
+	(AW87XXX_PID_18_REG_EN_OC_TURN_ON_THE_OC_BLOCK_FUNCTION << AW87XXX_PID_18_REG_EN_OC_START_BIT)
+
+#define AW87XXX_PID_18_REG_EN_OC_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_18_REG_EN_OC_DEFAULT	\
+	(AW87XXX_PID_18_REG_EN_OC_DEFAULT_VALUE << AW87XXX_PID_18_REG_EN_OC_START_BIT)
+
+/* EN_RD bit 2 (DFT5 0x65) */
+#define AW87XXX_PID_18_EN_RD_START_BIT	(2)
+#define AW87XXX_PID_18_EN_RD_BITS_LEN	(1)
+#define AW87XXX_PID_18_EN_RD_MASK		\
+	(~(((1<<AW87XXX_PID_18_EN_RD_BITS_LEN)-1) << AW87XXX_PID_18_EN_RD_START_BIT))
+
+#define AW87XXX_PID_18_EN_RD_DISABLE	(0)
+#define AW87XXX_PID_18_EN_RD_DISABLE_VALUE	\
+	(AW87XXX_PID_18_EN_RD_DISABLE << AW87XXX_PID_18_EN_RD_START_BIT)
+
+#define AW87XXX_PID_18_EN_RD_ENABLE		(1)
+#define AW87XXX_PID_18_EN_RD_ENABLE_VALUE	\
+	(AW87XXX_PID_18_EN_RD_ENABLE << AW87XXX_PID_18_EN_RD_START_BIT)
+
+#define AW87XXX_PID_18_EN_RD_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_EN_RD_DEFAULT	\
+	(AW87XXX_PID_18_EN_RD_DEFAULT_VALUE << AW87XXX_PID_18_EN_RD_START_BIT)
+
+/* REG_FAST_VFAGC bit 1 (DFT5 0x65) */
+#define AW87XXX_PID_18_REG_FAST_VFAGC_START_BIT	(1)
+#define AW87XXX_PID_18_REG_FAST_VFAGC_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_FAST_VFAGC_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_FAST_VFAGC_BITS_LEN)-1) << AW87XXX_PID_18_REG_FAST_VFAGC_START_BIT))
+
+#define AW87XXX_PID_18_REG_FAST_VFAGC_DISABLE	(0)
+#define AW87XXX_PID_18_REG_FAST_VFAGC_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_FAST_VFAGC_DISABLE << AW87XXX_PID_18_REG_FAST_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_FAST_VFAGC_ENABLE	(1)
+#define AW87XXX_PID_18_REG_FAST_VFAGC_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_FAST_VFAGC_ENABLE << AW87XXX_PID_18_REG_FAST_VFAGC_START_BIT)
+
+#define AW87XXX_PID_18_REG_FAST_VFAGC_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_FAST_VFAGC_DEFAULT	\
+	(AW87XXX_PID_18_REG_FAST_VFAGC_DEFAULT_VALUE << AW87XXX_PID_18_REG_FAST_VFAGC_START_BIT)
+
+/* REG_FAST_HVDD bit 0 (DFT5 0x65) */
+#define AW87XXX_PID_18_REG_FAST_HVDD_START_BIT	(0)
+#define AW87XXX_PID_18_REG_FAST_HVDD_BITS_LEN	(1)
+#define AW87XXX_PID_18_REG_FAST_HVDD_MASK	\
+	(~(((1<<AW87XXX_PID_18_REG_FAST_HVDD_BITS_LEN)-1) << AW87XXX_PID_18_REG_FAST_HVDD_START_BIT))
+
+#define AW87XXX_PID_18_REG_FAST_HVDD_DISABLE	(0)
+#define AW87XXX_PID_18_REG_FAST_HVDD_DISABLE_VALUE	\
+	(AW87XXX_PID_18_REG_FAST_HVDD_DISABLE << AW87XXX_PID_18_REG_FAST_HVDD_START_BIT)
+
+#define AW87XXX_PID_18_REG_FAST_HVDD_ENABLE	(1)
+#define AW87XXX_PID_18_REG_FAST_HVDD_ENABLE_VALUE	\
+	(AW87XXX_PID_18_REG_FAST_HVDD_ENABLE << AW87XXX_PID_18_REG_FAST_HVDD_START_BIT)
+
+#define AW87XXX_PID_18_REG_FAST_HVDD_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_REG_FAST_HVDD_DEFAULT	\
+	(AW87XXX_PID_18_REG_FAST_HVDD_DEFAULT_VALUE << AW87XXX_PID_18_REG_FAST_HVDD_START_BIT)
+
+/* default value of DFT5 (0x65) */
+/* #define AW87XXX_PID_18_DFT5_DEFAULT		(0x08) */
+
+/* DFT6 (0x66) detail */
+/* Q_SHDN bit 7:4 (DFT6 0x66) */
+#define AW87XXX_PID_18_Q_SHDN_START_BIT	(4)
+#define AW87XXX_PID_18_Q_SHDN_BITS_LEN	(4)
+#define AW87XXX_PID_18_Q_SHDN_MASK		\
+	(~(((1<<AW87XXX_PID_18_Q_SHDN_BITS_LEN)-1) << AW87XXX_PID_18_Q_SHDN_START_BIT))
+
+#define AW87XXX_PID_18_Q_SHDN_MODE1		(0)
+#define AW87XXX_PID_18_Q_SHDN_MODE1_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE1 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE2		(1)
+#define AW87XXX_PID_18_Q_SHDN_MODE2_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE2 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE3		(2)
+#define AW87XXX_PID_18_Q_SHDN_MODE3_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE3 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE4		(3)
+#define AW87XXX_PID_18_Q_SHDN_MODE4_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE4 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE5		(4)
+#define AW87XXX_PID_18_Q_SHDN_MODE5_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE5 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE6		(5)
+#define AW87XXX_PID_18_Q_SHDN_MODE6_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE6 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE7		(6)
+#define AW87XXX_PID_18_Q_SHDN_MODE7_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE7 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE8		(7)
+#define AW87XXX_PID_18_Q_SHDN_MODE8_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE8 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE9		(8)
+#define AW87XXX_PID_18_Q_SHDN_MODE9_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE9 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_MODE10	(9)
+#define AW87XXX_PID_18_Q_SHDN_MODE10_VALUE	\
+	(AW87XXX_PID_18_Q_SHDN_MODE10 << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+#define AW87XXX_PID_18_Q_SHDN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_18_Q_SHDN_DEFAULT	\
+	(AW87XXX_PID_18_Q_SHDN_DEFAULT_VALUE << AW87XXX_PID_18_Q_SHDN_START_BIT)
+
+/* REG_FSS bit 3:0 (DFT6 0x66) */
+#define AW87XXX_PID_18_REG_FSS_START_BIT	(0)
+#define AW87XXX_PID_18_REG_FSS_BITS_LEN	(4)
+#define AW87XXX_PID_18_REG_FSS_MASK		\
+	(~(((1<<AW87XXX_PID_18_REG_FSS_BITS_LEN)-1) << AW87XXX_PID_18_REG_FSS_START_BIT))
+
+#define AW87XXX_PID_18_REG_FSS_1P408MHZ	(0)
+#define AW87XXX_PID_18_REG_FSS_1P408MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P408MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P432MHZ	(1)
+#define AW87XXX_PID_18_REG_FSS_1P432MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P432MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P456MHZ	(3)
+#define AW87XXX_PID_18_REG_FSS_1P456MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P456MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P48MHZ	(2)
+#define AW87XXX_PID_18_REG_FSS_1P48MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P48MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P504MHZ	(6)
+#define AW87XXX_PID_18_REG_FSS_1P504MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P504MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P528MHZ	(7)
+#define AW87XXX_PID_18_REG_FSS_1P528MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P528MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P552MHZ	(5)
+#define AW87XXX_PID_18_REG_FSS_1P552MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P552MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P576MHZ	(4)
+#define AW87XXX_PID_18_REG_FSS_1P576MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P576MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P6MHZ	(12)
+#define AW87XXX_PID_18_REG_FSS_1P6MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P6MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P627MHZ	(13)
+#define AW87XXX_PID_18_REG_FSS_1P627MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P627MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P655MHZ	(15)
+#define AW87XXX_PID_18_REG_FSS_1P655MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P655MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P682MHZ	(14)
+#define AW87XXX_PID_18_REG_FSS_1P682MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P682MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P71MHZ	(10)
+#define AW87XXX_PID_18_REG_FSS_1P71MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P71MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P737MHZ	(11)
+#define AW87XXX_PID_18_REG_FSS_1P737MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P737MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P765MHZ	(9)
+#define AW87XXX_PID_18_REG_FSS_1P765MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P765MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_1P792MHZ	(8)
+#define AW87XXX_PID_18_REG_FSS_1P792MHZ_VALUE	\
+	(AW87XXX_PID_18_REG_FSS_1P792MHZ << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+#define AW87XXX_PID_18_REG_FSS_DEFAULT_VALUE	(0x0C)
+#define AW87XXX_PID_18_REG_FSS_DEFAULT	\
+	(AW87XXX_PID_18_REG_FSS_DEFAULT_VALUE << AW87XXX_PID_18_REG_FSS_START_BIT)
+
+/* default value of DFT6 (0x66) */
+/* #define AW87XXX_PID_18_DFT6_DEFAULT		(0x0C) */
+
+/* detail information of registers end */
+
+#endif  /* #ifndef  __AW87XXX_PID_18_REG_H__ */
\ No newline at end of file
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_39_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_39_reg.h
new file mode 100644
index 000000000000..0dfd9751c39e
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_39_reg.h
@@ -0,0 +1,67 @@
+#ifndef __AW87XXX_PID_39_REG_H__
+#define __AW87XXX_PID_39_REG_H__
+
+#define AW87XXX_PID_39_REG_CHIPID		(0x00)
+#define AW87XXX_PID_39_REG_SYSCTRL		(0x01)
+#define AW87XXX_PID_39_REG_MODECTRL		(0x02)
+#define AW87XXX_PID_39_REG_CPOVP		(0x03)
+#define AW87XXX_PID_39_REG_CPP			(0x04)
+#define AW87XXX_PID_39_REG_GAIN			(0x05)
+#define AW87XXX_PID_39_REG_AGC3_PO		(0x06)
+#define AW87XXX_PID_39_REG_AGC3			(0x07)
+#define AW87XXX_PID_39_REG_AGC2_PO		(0x08)
+#define AW87XXX_PID_39_REG_AGC2			(0x09)
+#define AW87XXX_PID_39_REG_AGC1			(0x0A)
+#define AW87XXX_PID_39_REG_DFT1			(0x62)
+#define AW87XXX_PID_39_REG_DFT2			(0x63)
+#define AW87XXX_PID_39_REG_ENCRY		(0x64)
+
+#define AW87XXX_PID_39_MODECTRL_DEFAULT		(0xa0)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_39_softrst_access[2] = {0x00, 0xaa};
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_39_REG_MAX			(0x65)
+
+#define REG_NONE_ACCESS		(0)
+#define REG_RD_ACCESS		(1 << 0)
+#define REG_WR_ACCESS		(1 << 1)
+
+const unsigned char aw87xxx_pid_39_reg_access[AW87XXX_PID_39_REG_MAX] = {
+	[AW87XXX_PID_39_REG_CHIPID]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_39_REG_SYSCTRL]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_MODECTRL]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_CPOVP]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_CPP]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_GAIN]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_AGC3_PO]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_AGC3]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_AGC2_PO]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_AGC2]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_AGC1]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_39_REG_DFT1]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_39_REG_DFT2]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_39_REG_ENCRY]	= (REG_RD_ACCESS),
+};
+
+/* RCV_MODE bit 3 (MODECTRL 0x02) */
+#define AW87XXX_PID_39_REC_MODE_START_BIT	(3)
+#define AW87XXX_PID_39_REC_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_39_REC_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_39_REC_MODE_BITS_LEN)-1) << AW87XXX_PID_39_REC_MODE_START_BIT))
+
+#define AW87XXX_PID_39_REC_MODE_DISABLE	(0)
+#define AW87XXX_PID_39_REC_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_39_REC_MODE_DISABLE << AW87XXX_PID_39_REC_MODE_START_BIT)
+
+#define AW87XXX_PID_39_REC_MODE_ENABLE	(1)
+#define AW87XXX_PID_39_REC_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_39_REC_MODE_ENABLE << AW87XXX_PID_39_REC_MODE_START_BIT)
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_59_3x9_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_59_3x9_reg.h
new file mode 100644
index 000000000000..67f73477b892
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_59_3x9_reg.h
@@ -0,0 +1,93 @@
+/*
+ * @Descripttion: Header file of AW87XXX_PID_59_3X9_REG
+ * @version: V1.33
+ * @Author: zhaozhongbo
+ * @Date: 2021-03-10
+ * @LastEditors: Please set LastEditors
+ * @LastEditTime: 2021-03-10
+ */
+#ifndef __AW87XXX_PID_59_3X9_REG_H__
+#define __AW87XXX_PID_59_3X9_REG_H__
+
+#define AW87XXX_PID_59_3X9_REG_CHIPID		(0x00)
+#define AW87XXX_PID_59_3X9_REG_SYSCTRL		(0x01)
+#define AW87XXX_PID_59_3X9_REG_MDCRTL		(0x02)
+#define AW87XXX_PID_59_3X9_REG_CPOVP		(0x03)
+#define AW87XXX_PID_59_3X9_REG_CPP		(0x04)
+#define AW87XXX_PID_59_3X9_REG_PAG		(0x05)
+#define AW87XXX_PID_59_3X9_REG_AGC3PO		(0x06)
+#define AW87XXX_PID_59_3X9_REG_AGC3PA		(0x07)
+#define AW87XXX_PID_59_3X9_REG_AGC2PO		(0x08)
+#define AW87XXX_PID_59_3X9_REG_AGC2PA		(0x09)
+#define AW87XXX_PID_59_3X9_REG_AGC1PA		(0x0A)
+#define AW87XXX_PID_59_3X9_REG_SYSST		(0x59)
+#define AW87XXX_PID_59_3X9_REG_SYSINT		(0x60)
+#define AW87XXX_PID_59_3X9_REG_DFT_SYSCTRL	(0x61)
+#define AW87XXX_PID_59_3X9_REG_DFT_MDCTRL	(0x62)
+#define AW87XXX_PID_59_3X9_REG_DFT_CPOVP2	(0x63)
+#define AW87XXX_PID_59_3X9_REG_DFT_AGCPA	(0x64)
+#define AW87XXX_PID_59_3X9_REG_DFT_POFR		(0x65)
+#define AW87XXX_PID_59_3X9_REG_DFT_OC		(0x66)
+#define AW87XXX_PID_59_3X9_REG_DFT_OTA		(0x67)
+#define AW87XXX_PID_59_3X9_REG_DFT_REF		(0x68)
+#define AW87XXX_PID_59_3X9_REG_DFT_LDO		(0x69)
+#define AW87XXX_PID_59_3X9_REG_ENCR		(0x70)
+
+#define AW87XXX_PID_59_3X9_ENCR_DEFAULT		(0x00)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_59_3x9_softrst_access[2] = {0x00, 0xaa};
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_59_3X9_REG_MAX			(0x71)
+
+#define REG_NONE_ACCESS		(0)
+#define REG_RD_ACCESS		(1 << 0)
+#define REG_WR_ACCESS		(1 << 1)
+
+const unsigned char aw87xxx_pid_59_3x9_reg_access[AW87XXX_PID_59_3X9_REG_MAX] = {
+	[AW87XXX_PID_59_3X9_REG_CHIPID]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_SYSCTRL]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_MDCRTL]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_CPOVP]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_CPP]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_PAG]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_AGC3PO]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_AGC3PA]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_AGC2PO]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_AGC2PA]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_AGC1PA]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_SYSST]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_SYSINT]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_SYSCTRL]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_MDCTRL]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_CPOVP2]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_AGCPA]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_POFR]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_OC]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_OTA]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_REF]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_DFT_LDO]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_3X9_REG_ENCR]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+};
+
+/* SPK_MODE bit 2 (MDCRTL 0x02) */
+#define AW87XXX_PID_59_3X9_SPK_MODE_START_BIT	(2)
+#define AW87XXX_PID_59_3X9_SPK_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_59_3X9_SPK_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_59_3X9_SPK_MODE_BITS_LEN)-1) << AW87XXX_PID_59_3X9_SPK_MODE_START_BIT))
+
+#define AW87XXX_PID_59_3X9_SPK_MODE_DISABLE	(0)
+#define AW87XXX_PID_59_3X9_SPK_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_59_3X9_SPK_MODE_DISABLE << AW87XXX_PID_59_3X9_SPK_MODE_START_BIT)
+
+#define AW87XXX_PID_59_3X9_SPK_MODE_ENABLE	(1)
+#define AW87XXX_PID_59_3X9_SPK_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_59_3X9_SPK_MODE_ENABLE << AW87XXX_PID_59_3X9_SPK_MODE_START_BIT)
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_59_5x9_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_59_5x9_reg.h
new file mode 100644
index 000000000000..6a0cae4c49ac
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_59_5x9_reg.h
@@ -0,0 +1,94 @@
+/*
+ * @Descripttion: Header file of AW87XXX_PID_59_5X9_REG
+ * @version: V1.33
+ * @Author: zhaozhongbo
+ * @Date: 2021-03-10
+ * @LastEditors: Please set LastEditors
+ * @LastEditTime: 2021-03-10
+ */
+#ifndef __AW87XXX_PID_59_5X9_REG_H__
+#define __AW87XXX_PID_59_5X9_REG_H__
+
+
+#define AW87XXX_PID_59_5X9_REG_CHIPID		(0x00)
+#define AW87XXX_PID_59_5X9_REG_SYSCTRL		(0x01)
+#define AW87XXX_PID_59_5X9_REG_BATSAFE		(0x02)
+#define AW87XXX_PID_59_5X9_REG_BSTOVR		(0x03)
+#define AW87XXX_PID_59_5X9_REG_BSTVPR		(0x04)
+#define AW87XXX_PID_59_5X9_REG_PAGR		(0x05)
+#define AW87XXX_PID_59_5X9_REG_PAGC3OPR		(0x06)
+#define AW87XXX_PID_59_5X9_REG_PAGC3PR		(0x07)
+#define AW87XXX_PID_59_5X9_REG_PAGC2OPR		(0x08)
+#define AW87XXX_PID_59_5X9_REG_PAGC2PR		(0x09)
+#define AW87XXX_PID_59_5X9_REG_PAGC1PR		(0x0A)
+#define AW87XXX_PID_59_5X9_REG_SYSST		(0x58)
+#define AW87XXX_PID_59_5X9_REG_SYSINT		(0x59)
+#define AW87XXX_PID_59_5X9_REG_CPCR		(0x60)
+#define AW87XXX_PID_59_5X9_REG_DFT1R		(0x61)
+#define AW87XXX_PID_59_5X9_REG_DFT2R		(0x62)
+#define AW87XXX_PID_59_5X9_REG_DFT3R		(0x63)
+#define AW87XXX_PID_59_5X9_REG_DFT4R		(0x64)
+#define AW87XXX_PID_59_5X9_REG_DFT5R		(0x65)
+#define AW87XXX_PID_59_5X9_REG_DFT6R		(0x66)
+#define AW87XXX_PID_59_5X9_REG_DFT7R		(0x67)
+#define AW87XXX_PID_59_5X9_REG_DFT8R		(0x68)
+#define AW87XXX_PID_59_5X9_REG_ENCR		(0x69)
+
+#define AW87XXX_PID_59_5X9_ENCRY_DEFAULT	(0x00)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_59_5x9_softrst_access[2] = {0x00, 0xaa};
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_59_5X9_REG_MAX			(0x70)
+
+#define REG_NONE_ACCESS		(0)
+#define REG_RD_ACCESS		(1 << 0)
+#define REG_WR_ACCESS		(1 << 1)
+
+const unsigned char aw87xxx_pid_59_5x9_reg_access[AW87XXX_PID_59_5X9_REG_MAX] = {
+	[AW87XXX_PID_59_5X9_REG_CHIPID]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_SYSCTRL]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_BATSAFE]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_BSTOVR]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_BSTVPR]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_PAGR]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_PAGC3OPR]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_PAGC3PR]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_PAGC2OPR]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_PAGC2PR]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_PAGC1PR]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_SYSST]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_SYSINT]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_CPCR]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT1R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT2R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT3R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT4R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT5R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT6R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT7R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_DFT8R]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_59_5X9_REG_ENCR]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+};
+
+/* RCV_MODE bit 3 (SYSCTRL 0x01) */
+#define AW87XXX_PID_59_5X9_REC_MODE_START_BIT	(3)
+#define AW87XXX_PID_59_5X9_REC_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_59_5X9_REC_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_59_5X9_REC_MODE_BITS_LEN)-1) << AW87XXX_PID_59_5X9_REC_MODE_START_BIT))
+
+#define AW87XXX_PID_59_5X9_REC_MODE_DISABLE	(0)
+#define AW87XXX_PID_59_5X9_REC_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_59_5X9_REC_MODE_DISABLE << AW87XXX_PID_59_5X9_REC_MODE_START_BIT)
+
+#define AW87XXX_PID_59_5X9_REC_MODE_ENABLE	(1)
+#define AW87XXX_PID_59_5X9_REC_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_59_5X9_REC_MODE_ENABLE << AW87XXX_PID_59_5X9_REC_MODE_START_BIT)
+
+#endif
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_5a_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_5a_reg.h
new file mode 100644
index 000000000000..020bf5496d01
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_5a_reg.h
@@ -0,0 +1,4124 @@
+/*
+ * @Descripttion: Header file of AW87XXX_PID_5A_REG
+ * @version: V1.4
+ * @Author: zhaozhongbo
+ * @Date: 2021-03-10
+ * @LastEditors: Please set LastEditors
+ * @LastEditTime: 2021-03-10
+ */
+#ifndef __AW87XXX_PID_5A_REG_H__
+#define __AW87XXX_PID_5A_REG_H__
+
+/* registers list */
+#define AW87XXX_PID_5A_REG_ID_REG		(0x00)
+#define AW87XXX_PID_5A_REG_SYSCTRL_REG	(0x01)
+#define AW87XXX_PID_5A_REG_BATSAFE_REG	(0x02)
+#define AW87XXX_PID_5A_REG_BSTOVR_REG	(0x03)
+#define AW87XXX_PID_5A_REG_BSTCPR1_REG	(0x04)
+#define AW87XXX_PID_5A_REG_BSTCPR2_REG	(0x05)
+#define AW87XXX_PID_5A_REG_PAGR_REG		(0x06)
+#define AW87XXX_PID_5A_REG_PAGC3OPR_REG	(0x07)
+#define AW87XXX_PID_5A_REG_PAGC3PR_REG	(0x08)
+#define AW87XXX_PID_5A_REG_PAGC2OPR_REG	(0x09)
+#define AW87XXX_PID_5A_REG_PAGC2PR_REG	(0x0A)
+#define AW87XXX_PID_5A_REG_PAGC1PR_REG	(0x0B)
+#define AW87XXX_PID_5A_REG_ADP_MODE_REG	(0x0C)
+#define AW87XXX_PID_5A_REG_ADPBST_TIME1_REG	(0x0D)
+#define AW87XXX_PID_5A_REG_ADPBST_TIME2_REG	(0x0E)
+#define AW87XXX_PID_5A_REG_ADPBST_VTH_REG	(0x0F)
+#define AW87XXX_PID_5A_REG_BOOST_PAR_REG	(0x10)
+#define AW87XXX_PID_5A_REG_BOOST_VOUT_DET_REG	(0x57)
+#define AW87XXX_PID_5A_REG_SYSST_REG	(0x58)
+#define AW87XXX_PID_5A_REG_SYSINT_REG	(0x59)
+#define AW87XXX_PID_5A_REG_DFT1R_REG	(0x60)
+#define AW87XXX_PID_5A_REG_DFT2R_REG	(0x61)
+#define AW87XXX_PID_5A_REG_DFT3R_REG	(0x62)
+#define AW87XXX_PID_5A_REG_DFT4R_REG	(0x63)
+#define AW87XXX_PID_5A_REG_DFT5R_REG	(0x64)
+#define AW87XXX_PID_5A_REG_DFT6R_REG	(0x65)
+#define AW87XXX_PID_5A_REG_DFT7R_REG	(0x66)
+#define AW87XXX_PID_5A_REG_DFT8R_REG	(0x67)
+#define AW87XXX_PID_5A_REG_DFT9R_REG	(0x68)
+#define AW87XXX_PID_5A_REG_DFTAR_REG	(0x69)
+#define AW87XXX_PID_5A_REG_DFTBR_REG	(0x70)
+#define AW87XXX_PID_5A_REG_DFTCR_REG	(0x71)
+#define AW87XXX_PID_5A_REG_DFTDR_REG	(0x72)
+#define AW87XXX_PID_5A_REG_DFTER_REG	(0x73)
+#define AW87XXX_PID_5A_REG_DFTFR_REG	(0x74)
+#define AW87XXX_PID_5A_REG_test1_REG	(0x75)
+#define AW87XXX_PID_5A_REG_test2_REG	(0x76)
+#define AW87XXX_PID_5A_REG_ENCR_REG	(0x77)
+
+#define AW87XXX_PID_5A_DFT3R_DEFAULT	(0x02)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_5a_softrst_access[2] = {0x00, 0xaa};
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_5A_REG_MAX		(0x78)
+
+#define REG_NONE_ACCESS					(0)
+#define REG_RD_ACCESS					(1 << 0)
+#define REG_WR_ACCESS					(1 << 1)
+
+const unsigned char aw87xxx_pid_5a_reg_access[AW87XXX_PID_5A_REG_MAX] = {
+	[AW87XXX_PID_5A_REG_ID_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_5A_REG_SYSCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_BATSAFE_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_BSTOVR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_BSTCPR1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_BSTCPR2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_PAGR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_PAGC3OPR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_PAGC3PR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_PAGC2OPR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_PAGC2PR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_PAGC1PR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_ADP_MODE_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_ADPBST_TIME1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_ADPBST_TIME2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_ADPBST_VTH_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_BOOST_PAR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_BOOST_VOUT_DET_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_5A_REG_SYSST_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_5A_REG_SYSINT_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT1R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT2R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT3R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT4R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT5R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT6R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT7R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT8R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFT9R_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFTAR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFTBR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFTCR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFTDR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFTER_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_DFTFR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_test1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_test2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_5A_REG_ENCR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+};
+
+/* detail information of registers begin */
+/* ID (0x00) detail */
+/* IDCODE bit 7:0 (ID 0x00) */
+#define AW87XXX_PID_5A_REG_IDCODE_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_IDCODE_BITS_LEN	(8)
+#define AW87XXX_PID_5A_REG_IDCODE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_IDCODE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_IDCODE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_IDCODE_DEFAULT_VALUE	(0x5A)
+#define AW87XXX_PID_5A_REG_IDCODE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_IDCODE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_IDCODE_START_BIT)
+
+/* default value of ID (0x00) */
+/* #define AW87XXX_PID_5A_REG_ID_DEFAULT		(0x5A) */
+
+/* SYSCTRL (0x01) detail */
+/* EN_SW bit 6 (SYSCTRL 0x01) */
+#define AW87XXX_PID_5A_REG_EN_SW_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_EN_SW_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_SW_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_SW_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_SW_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_SW_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_SW_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_SW_DISABLE << AW87XXX_PID_5A_REG_EN_SW_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_SW_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_SW_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_SW_ENABLE << AW87XXX_PID_5A_REG_EN_SW_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_SW_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_SW_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_SW_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_SW_START_BIT)
+
+/* EN_CP bit 5 (SYSCTRL 0x01) */
+#define AW87XXX_PID_5A_REG_EN_CP_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_EN_CP_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_CP_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_CP_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_CP_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_CP_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_CP_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_CP_DISABLE << AW87XXX_PID_5A_REG_EN_CP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_CP_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_CP_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_CP_ENABLE << AW87XXX_PID_5A_REG_EN_CP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_CP_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_EN_CP_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_CP_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_CP_START_BIT)
+
+/* EN_BOOST bit 4 (SYSCTRL 0x01) */
+#define AW87XXX_PID_5A_REG_EN_BOOST_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_EN_BOOST_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_BOOST_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_BOOST_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_BOOST_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_BOOST_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_DISABLE << AW87XXX_PID_5A_REG_EN_BOOST_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_BOOST_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_ENABLE << AW87XXX_PID_5A_REG_EN_BOOST_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_EN_BOOST_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_BOOST_START_BIT)
+
+/* EN_PA bit 3 (SYSCTRL 0x01) */
+#define AW87XXX_PID_5A_REG_EN_PA_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_EN_PA_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_PA_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_PA_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_PA_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_PA_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_PA_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_PA_DISABLE << AW87XXX_PID_5A_REG_EN_PA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_PA_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_PA_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_PA_ENABLE << AW87XXX_PID_5A_REG_EN_PA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_PA_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_EN_PA_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_PA_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_PA_START_BIT)
+
+/* RCV_MODE bit 2 (SYSCTRL 0x01) */
+#define AW87XXX_PID_5A_REG_RCV_MODE_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_RCV_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_RCV_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_RCV_MODE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_RCV_MODE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_RCV_MODE_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_RCV_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_RCV_MODE_DISABLE << AW87XXX_PID_5A_REG_RCV_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RCV_MODE_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_RCV_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_RCV_MODE_ENABLE << AW87XXX_PID_5A_REG_RCV_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RCV_MODE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_RCV_MODE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_RCV_MODE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_RCV_MODE_START_BIT)
+
+/* EN_OVERLOAD bit 1 (SYSCTRL 0x01) */
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_OVERLOAD_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_OVERLOAD_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_DISABL	(0)
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_DISABL_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_OVERLOAD_DISABL << AW87XXX_PID_5A_REG_EN_OVERLOAD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_OVERLOAD_ENABLE << AW87XXX_PID_5A_REG_EN_OVERLOAD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_OVERLOAD_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_OVERLOAD_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_OVERLOAD_START_BIT)
+
+/* EN_HVBAT bit 0 (SYSCTRL 0x01) */
+#define AW87XXX_PID_5A_REG_EN_HVBAT_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_EN_HVBAT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_HVBAT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_HVBAT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_HVBAT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_HVBAT_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_HVBAT_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_HVBAT_DISABLE << AW87XXX_PID_5A_REG_EN_HVBAT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_HVBAT_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_HVBAT_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_HVBAT_ENABLE << AW87XXX_PID_5A_REG_EN_HVBAT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_HVBAT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_HVBAT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_HVBAT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_HVBAT_START_BIT)
+
+/* default value of SYSCTRL (0x01) */
+/* #define AW87XXX_PID_5A_REG_SYSCTRL_DEFAULT		(0x38) */
+
+/* BATSAFE (0x02) detail */
+/* BAT_SFGD_DEGLITCH bit 6:5 (BATSAFE 0x02) */
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_1MS	(0)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_1MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_1MS << AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_500US	(1)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_500US_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_500US << AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_200US	(2)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_200US_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_200US << AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_DISABLE	(3)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_DISABLE << AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BAT_SFGD_DEGLITCH_START_BIT)
+
+/* BAT_SFGD_VTH bit 4:3 (BATSAFE 0x02) */
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BAT_SFGD_VTH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BAT_SFGD_VTH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P3V	(0)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P3V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P3V << AW87XXX_PID_5A_REG_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P4V	(1)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P4V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P4V << AW87XXX_PID_5A_REG_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P5V	(2)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P5V << AW87XXX_PID_5A_REG_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P6V	(3)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P6V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_VTH_3P6V << AW87XXX_PID_5A_REG_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_VTH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_VTH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BAT_SFGD_VTH_START_BIT)
+
+/* EN_BAT_SFGD bit 2 (BATSAFE 0x02) */
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_BAT_SFGD_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_BAT_SFGD_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BAT_SFGD_DISABLE << AW87XXX_PID_5A_REG_EN_BAT_SFGD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BAT_SFGD_ENABLE << AW87XXX_PID_5A_REG_EN_BAT_SFGD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_BAT_SFGD_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_BAT_SFGD_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_BAT_SFGD_START_BIT)
+
+/* BAT_SFGD_LEVEL bit 1:0 (BATSAFE 0x02) */
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_5V	(0)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_5V << AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_5P5V	(1)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_5P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_5P5V << AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_6V	(2)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_6V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_6V << AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_6P5V	(3)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_6P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_6P5V << AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_DEFAULT_VALUE	(0x01)
+#define AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BAT_SFGD_LEVEL_START_BIT)
+
+/* default value of BATSAFE (0x02) */
+/* #define AW87XXX_PID_5A_REG_BATSAFE_DEFAULT		(0x09) */
+
+/* BSTOVR (0x03) detail */
+/* BST_VOUT bit 4:0 (BSTOVR 0x03) */
+#define AW87XXX_PID_5A_REG_BST_VOUT_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_VOUT_BITS_LEN	(5)
+#define AW87XXX_PID_5A_REG_BST_VOUT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_VOUT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_6P5V	(0)
+#define AW87XXX_PID_5A_REG_BST_VOUT_6P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_6P5V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_6P75V	(1)
+#define AW87XXX_PID_5A_REG_BST_VOUT_6P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_6P75V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P0V	(2)
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_7P0V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P25V	(3)
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_7P25V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P5V	(4)
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_7P5V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P75V	(5)
+#define AW87XXX_PID_5A_REG_BST_VOUT_7P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_7P75V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P0V	(6)
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_8P0V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P25V	(7)
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_8P25V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P5V	(8)
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_8P5V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P75V	(9)
+#define AW87XXX_PID_5A_REG_BST_VOUT_8P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_8P75V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P0V	(10)
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_9P0V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P25V	(11)
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_9P25V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P5V	(12)
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_9P5V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P75V	(13)
+#define AW87XXX_PID_5A_REG_BST_VOUT_9P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_9P75V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P0V	(14)
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_10P0V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P25V	(15)
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_10P25V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P5V	(16)
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_10P5V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P75V	(17)
+#define AW87XXX_PID_5A_REG_BST_VOUT_10P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_10P75V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P0V	(18)
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_11P0V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P25V	(19)
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_11P25V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P5V	(20)
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_11P5V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P75V	(21)
+#define AW87XXX_PID_5A_REG_BST_VOUT_11P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_11P75V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_12P0V	(22)
+#define AW87XXX_PID_5A_REG_BST_VOUT_12P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_12P0V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_12P25V	(23)
+#define AW87XXX_PID_5A_REG_BST_VOUT_12P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_12P25V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_12P5V	(24)
+#define AW87XXX_PID_5A_REG_BST_VOUT_12P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_12P5V << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_VOUT_DEFAULT_VALUE	(0x0C)
+#define AW87XXX_PID_5A_REG_BST_VOUT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_VOUT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_VOUT_START_BIT)
+
+/* default value of BSTOVR (0x03) */
+/* #define AW87XXX_PID_5A_REG_BSTOVR_DEFAULT		(0x0C) */
+
+/* BSTCPR1 (0x04) detail */
+/* BURST_HYS_SELA bit 7 (BSTCPR1 0x04) */
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BURST_HYS_SELA_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BURST_HYS_SELA_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_3P3MV	(0)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_3P3MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SELA_3P3MV << AW87XXX_PID_5A_REG_BURST_HYS_SELA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_5MV	(1)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_5MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SELA_5MV << AW87XXX_PID_5A_REG_BURST_HYS_SELA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_8P3MV	(2)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_8P3MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SELA_8P3MV << AW87XXX_PID_5A_REG_BURST_HYS_SELA_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_8P3MV	(3)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_8P3MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SELA_8P3MV << AW87XXX_PID_5A_REG_BURST_HYS_SELA_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SELA_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SELA_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BURST_HYS_SELA_START_BIT)
+
+/* BST_IPEAK_SS bit 6:5 (BSTCPR1 0x04) */
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_IPEAK_SS_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_IPEAK_SS_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_0P8A	(0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_0P8A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_SS_0P8A << AW87XXX_PID_5A_REG_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_1A	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_1A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_SS_1A << AW87XXX_PID_5A_REG_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_1P5A	(2)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_1P5A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_SS_1P5A << AW87XXX_PID_5A_REG_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_2A	(3)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_2A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_SS_2A << AW87XXX_PID_5A_REG_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_SS_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_SS_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_IPEAK_SS_START_BIT)
+
+/* BST_IPEAK_ADJ bit 4 (BSTCPR1 0x04) */
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_IPEAK	(0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_IPEAK_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_IPEAK << AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_IPEAK0P5A	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_IPEAK0P5A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_IPEAK0P5A << AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_IPEAK_ADJ_START_BIT)
+
+/* BST_IPEAK_LOWBAT_EN bit 3 (BSTCPR1 0x04) */
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_DISABLE << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_ENABLE << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_EN_START_BIT)
+
+/* BST_IPEAK_LOWBAT bit 2 (BSTCPR1 0x04) */
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_2P5A	(0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_2P5A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_2P5A << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_2P75A	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_2P75A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_2P75A << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_IPEAK_LOWBAT_START_BIT)
+
+/* BURST_HYS_SEL bit 1 (BSTCPR1 0x04) */
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BURST_HYS_SEL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BURST_HYS_SEL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_3P3MV	(0)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_3P3MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SEL_3P3MV << AW87XXX_PID_5A_REG_BURST_HYS_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_5MV	(1)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_5MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SEL_5MV << AW87XXX_PID_5A_REG_BURST_HYS_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_8P3MV	(2)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_8P3MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SEL_8P3MV << AW87XXX_PID_5A_REG_BURST_HYS_SEL_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_8P3MV	(3)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_8P3MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SEL_8P3MV << AW87XXX_PID_5A_REG_BURST_HYS_SEL_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BURST_HYS_SEL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BURST_HYS_SEL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BURST_HYS_SEL_START_BIT)
+
+/* BURST_MODE bit 0 (BSTCPR1 0x04) */
+#define AW87XXX_PID_5A_REG_BURST_MODE_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BURST_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BURST_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BURST_MODE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BURST_MODE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BURST_MODE_PVDD_DECIDE	(0)
+#define AW87XXX_PID_5A_REG_BURST_MODE_PVDD_DECIDE_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_MODE_PVDD_DECIDE << AW87XXX_PID_5A_REG_BURST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_MODE_BUEST_PEAK_DECIDE	(1)
+#define AW87XXX_PID_5A_REG_BURST_MODE_BUEST_PEAK_DECIDE_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_MODE_BUEST_PEAK_DECIDE << AW87XXX_PID_5A_REG_BURST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_MODE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BURST_MODE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BURST_MODE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BURST_MODE_START_BIT)
+
+/* default value of BSTCPR1 (0x04) */
+/* #define AW87XXX_PID_5A_REG_BSTCPR1_DEFAULT		(0x00) */
+
+/* BSTCPR2 (0x05) detail */
+/* BURST_PEAK bit 5:4 (BSTCPR2 0x05) */
+#define AW87XXX_PID_5A_REG_BURST_PEAK_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BURST_PEAK_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BURST_PEAK_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BURST_PEAK_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BURST_PEAK_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_660MV_HYS_800MV	(0)
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_660MV_HYS_800MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_660MV_HYS_800MV << AW87XXX_PID_5A_REG_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_730MV_HYS_890MV	(1)
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_730MV_HYS_890MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_730MV_HYS_890MV << AW87XXX_PID_5A_REG_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_780MV_HYS_930MV	(2)
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_780MV_HYS_930MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_780MV_HYS_930MV << AW87XXX_PID_5A_REG_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_810MV_HYS_970MV	(3)
+#define AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_810MV_HYS_970MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BURST_PEAK_CLAMP_810MV_HYS_970MV << AW87XXX_PID_5A_REG_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BURST_PEAK_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BURST_PEAK_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BURST_PEAK_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BURST_PEAK_START_BIT)
+
+/* BST_IPEAK bit 3:0 (BSTCPR2 0x05) */
+#define AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_IPEAK_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_1P5A	(0)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_1P5A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_1P5A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_1P75A	(1)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_1P75A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_1P75A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2A	(2)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_2A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2P25A	(3)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2P25A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_2P25A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2P5A	(4)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2P5A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_2P5A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2P75A	(5)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_2P75A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_2P75A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3A	(6)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_3A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3P25	(7)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3P25_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_3P25 << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3P5A	(8)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3P5A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_3P5A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3P75A	(9)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_3P75A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_3P75A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_4A	(10)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_4A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_4A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_4P25A	(11)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_4P25A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_4P25A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_4P5A	(12)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_4P5A_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_4P5A << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_IPEAK_DEFAULT_VALUE	(0x8)
+#define AW87XXX_PID_5A_REG_BST_IPEAK_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_IPEAK_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_IPEAK_START_BIT)
+
+/* default value of BSTCPR2 (0x05) */
+/* #define AW87XXX_PID_5A_REG_BSTCPR2_DEFAULT		(0x08) */
+
+/* PAGR (0x06) detail */
+/* PA_GAIN bit 4:0 (PAGR 0x06) */
+#define AW87XXX_PID_5A_REG_PA_GAIN_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_PA_GAIN_BITS_LEN	(5)
+#define AW87XXX_PID_5A_REG_PA_GAIN_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_GAIN_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_0DB	(0)
+#define AW87XXX_PID_5A_REG_PA_GAIN_0DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_0DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_1P5DB	(1)
+#define AW87XXX_PID_5A_REG_PA_GAIN_1P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_1P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_3DB	(2)
+#define AW87XXX_PID_5A_REG_PA_GAIN_3DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_3DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_4P5DB	(3)
+#define AW87XXX_PID_5A_REG_PA_GAIN_4P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_4P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_6DB	(4)
+#define AW87XXX_PID_5A_REG_PA_GAIN_6DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_6DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_7P5DB	(5)
+#define AW87XXX_PID_5A_REG_PA_GAIN_7P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_7P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_9DB	(6)
+#define AW87XXX_PID_5A_REG_PA_GAIN_9DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_9DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_10P5DB	(7)
+#define AW87XXX_PID_5A_REG_PA_GAIN_10P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_10P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_12DB	(8)
+#define AW87XXX_PID_5A_REG_PA_GAIN_12DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_12DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_13P5DB	(9)
+#define AW87XXX_PID_5A_REG_PA_GAIN_13P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_13P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_15DB	(10)
+#define AW87XXX_PID_5A_REG_PA_GAIN_15DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_15DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_16P5DB	(11)
+#define AW87XXX_PID_5A_REG_PA_GAIN_16P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_16P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_18DB	(12)
+#define AW87XXX_PID_5A_REG_PA_GAIN_18DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_18DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_19P5DB	(13)
+#define AW87XXX_PID_5A_REG_PA_GAIN_19P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_19P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_21DB	(14)
+#define AW87XXX_PID_5A_REG_PA_GAIN_21DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_21DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_22P5DB	(15)
+#define AW87XXX_PID_5A_REG_PA_GAIN_22P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_22P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_24DB	(16)
+#define AW87XXX_PID_5A_REG_PA_GAIN_24DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_24DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_25P5DB	(17)
+#define AW87XXX_PID_5A_REG_PA_GAIN_25P5DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_25P5DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_27DB	(18)
+#define AW87XXX_PID_5A_REG_PA_GAIN_27DB_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_27DB << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GAIN_DEFAULT_VALUE	(0x10)
+#define AW87XXX_PID_5A_REG_PA_GAIN_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_GAIN_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_GAIN_START_BIT)
+
+/* default value of PAGR (0x06) */
+/* #define AW87XXX_PID_5A_REG_PAGR_DEFAULT		(0x10) */
+
+/* PAGC3OPR (0x07) detail */
+/* PAVG_ADJ bit 7:5 (PAGC3OPR 0x07) */
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PAVG_ADJ_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_0P94PO	(0)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_0P94PO_VALUE	\
+	(AW87XXX_PID_5A_REG_PAVG_ADJ_0P94PO << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_0P97PO	(1)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_0P97PO_VALUE	\
+	(AW87XXX_PID_5A_REG_PAVG_ADJ_0P97PO << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P0PO	(2)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P0PO_VALUE	\
+	(AW87XXX_PID_5A_REG_PAVG_ADJ_1P0PO << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P03PO	(3)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P03PO_VALUE	\
+	(AW87XXX_PID_5A_REG_PAVG_ADJ_1P03PO << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P06PO	(4)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P06PO_VALUE	\
+	(AW87XXX_PID_5A_REG_PAVG_ADJ_1P06PO << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P09PO	(5)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_1P09PO_VALUE	\
+	(AW87XXX_PID_5A_REG_PAVG_ADJ_1P09PO << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_PAVG_ADJ_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PAVG_ADJ_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PAVG_ADJ_START_BIT)
+
+/* PD_AGC3 bit 4 (PAGC3OPR 0x07) */
+#define AW87XXX_PID_5A_REG_PD_AGC3_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_PD_AGC3_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PD_AGC3_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PD_AGC3_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PD_AGC3_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PD_AGC3_ENABLE	(0)
+#define AW87XXX_PID_5A_REG_PD_AGC3_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_AGC3_ENABLE << AW87XXX_PID_5A_REG_PD_AGC3_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_AGC3_DISABLE	(1)
+#define AW87XXX_PID_5A_REG_PD_AGC3_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_AGC3_DISABLE << AW87XXX_PID_5A_REG_PD_AGC3_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_AGC3_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_5A_REG_PD_AGC3_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PD_AGC3_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PD_AGC3_START_BIT)
+
+/* AGC3_OUTPUT_POWER bit 3:0 (PAGC3OPR 0x07) */
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P5W8_OHM	(0)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P5W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P5W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P6W8_OHM	(1)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P6W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P6W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P7W8_OHM	(2)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P7W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P7W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P8W8_OHM	(3)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P8W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P8W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P9W8_OHM	(4)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P9W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_0P9W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P0W8_OHM	(5)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P0W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P0W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P1W8_OHM	(6)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P1W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P1W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P2W8_OHM	(7)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P2W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P2W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P3W8_OHM	(8)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P3W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P3W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P4W8_OHM	(9)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P4W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P4W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P5W8_OHM	(10)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P5W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P5W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P6W8_OHM	(11)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P6W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P6W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P7W8_OHM	(12)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P7W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P7W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P8W8_OHM	(13)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P8W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P8W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P9W8_OHM	(14)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P9W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_1P9W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_2P0W8_OHM	(15)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_2P0W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_2P0W8_OHM << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC3_OUTPUT_POWER_START_BIT)
+
+/* default value of PAGC3OPR (0x07) */
+/* #define AW87XXX_PID_5A_REG_PAGC3OPR_DEFAULT		(0x43) */
+
+/* PAGC3PR (0x08) detail */
+/* AGC3_REL_TIME bit 7:5 (PAGC3PR 0x08) */
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC3_REL_TIME_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_5P12MSDB	(0)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_5P12MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_5P12MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_10P24MSDB	(1)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_10P24MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_10P24MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_20P48MSDB	(2)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_20P48MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_20P48MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_40P96MSDB	(3)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_40P96MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_40P96MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_81P92MSDB	(4)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_81P92MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_81P92MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_163P84MSDB	(5)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_163P84MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_163P84MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_327P68MSDB	(6)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_327P68MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_327P68MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_655P36MSDB	(7)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_655P36MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_655P36MSDB << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_AGC3_REL_TIME_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC3_REL_TIME_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC3_REL_TIME_START_BIT)
+
+/* AGC3_ATT_TIME bit 4:2 (PAGC3PR 0x08) */
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC3_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_1P28MSDB	(0)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_1P28MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_1P28MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_2P56MSDB	(1)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_2P56MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_2P56MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_10P24MSDB	(2)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_10P24MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_10P24MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_40P96MSDB	(3)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_40P96MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_40P96MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_82MSDB	(4)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_82MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_82MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_164MSDB	(5)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_164MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_164MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_328MSDB	(6)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_328MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_328MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_656MSDB	(7)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_656MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_656MSDB << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_5A_REG_AGC3_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC3_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC3_ATT_TIME_START_BIT)
+
+/* AGC3_FIRST_ATT_TIME bit 1:0 (PAGC3PR 0x08) */
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_5P12MS	(0)
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_5P12MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_5P12MS << AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_10P24MS	(1)
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_10P24MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_10P24MS << AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_20P48MS	(2)
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_20P48MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_20P48MS << AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_41MS	(3)
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_41MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_41MS << AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC3_FIRST_ATT_TIME_START_BIT)
+
+/* default value of PAGC3PR (0x08) */
+/* #define AW87XXX_PID_5A_REG_PAGC3PR_DEFAULT		(0x4E) */
+
+/* PAGC2OPR (0x09) detail */
+/* AGC2_OUTPUT_POWER bit 3:0 (PAGC2OPR 0x09) */
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P0W8_OHM	(0)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P0W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P0W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P2W8_OHM	(1)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P2W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P2W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P4W8_OHM	(2)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P4W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P4W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P6W8_OHM	(3)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P6W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P6W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P8W8_OHM	(4)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P8W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_1P8W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P0W8_OHM	(5)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P0W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P0W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P2W8_OHM	(6)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P2W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P2W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P4W8_OHM	(7)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P4W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P4W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P6W8_OHM	(8)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P6W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P6W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P8W8_OHM	(9)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P8W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_2P8W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_3P0W8_OHM	(10)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_3P0W8_OHM_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_3P0W8_OHM << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_AGC2_OFF	(11)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_AGC2_OFF_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_AGC2_OFF << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC2_OUTPUT_POWER_START_BIT)
+
+/* default value of PAGC2OPR (0x09) */
+/* #define AW87XXX_PID_5A_REG_PAGC2OPR_DEFAULT		(0x03) */
+
+/* PAGC2PR (0x0A) detail */
+/* AGC2_ATT_TIME bit 4:2 (PAGC2PR 0x0A) */
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC2_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P16MSDB	(0)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P16MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P16MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P32MSDB	(1)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P32MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P32MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P64MSDB	(2)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P64MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_0P64MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_2P56MSDB	(3)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_2P56MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_2P56MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_10P24MSDB	(4)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_10P24MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_10P24MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_40P96MSDB	(5)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_40P96MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_40P96MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_82MSDB	(6)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_82MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_82MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_164MSDB	(7)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_164MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_164MSDB << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_AGC2_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC2_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC2_ATT_TIME_START_BIT)
+
+/* AGC2_FIRST_ATT_TIME bit 1:0 (PAGC2PR 0x0A) */
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_0P08MS	(0)
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_0P08MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_0P08MS << AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_0P32MS	(1)
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_0P32MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_0P32MS << AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_1P28MS	(2)
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_1P28MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_1P28MS << AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_5P12MS	(3)
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_5P12MS_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_5P12MS << AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC2_FIRST_ATT_TIME_START_BIT)
+
+/* default value of PAGC2PR (0x0A) */
+/* #define AW87XXX_PID_5A_REG_PAGC2PR_DEFAULT		(0x08) */
+
+/* PAGC1PR (0x0B) detail */
+/* AGC1_OUTPUT_LEVEL bit 6:3 (PAGC1PR 0x0B) */
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5V	(0)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P2V	(1)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P2V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P2V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P4V	(2)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P4V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P4V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P6V	(3)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P6V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P6V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P8V	(4)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P8V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_5P8V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P0V	(5)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P0V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P2V	(6)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P2V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P2V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P4V	(7)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P4V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P4V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P6V	(8)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P6V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P6V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P8V	(9)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P8V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_6P8V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7V	(10)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P2V	(11)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P2V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P2V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P4V	(12)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P4V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P4V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P6V	(13)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P6V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P6V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P8V	(14)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P8V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_7P8V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_8V	(15)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_8V_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_8V << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_DEFAULT_VALUE	(0x9)
+#define AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC1_OUTPUT_LEVEL_START_BIT)
+
+/* AGC1_ATT_TIME bit 2:1 (PAGC1PR 0x0B) */
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC1_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P04MSDB	(0)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P04MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P04MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P08MSDB	(1)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P08MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P08MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P16MSDB	(2)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P16MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P16MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P32MSDB	(3)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P32MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P32MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P02MSDB	(4)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P02MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P02MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P01MSDB	(5)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P01MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P01MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P005MSDB	(6)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P005MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P005MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P005MSDB	(7)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P005MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_0P005MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC1_ATT_TIME_START_BIT)
+
+/* PD_AGC1 bit 0 (PAGC1PR 0x0B) */
+#define AW87XXX_PID_5A_REG_PD_AGC1_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_PD_AGC1_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PD_AGC1_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PD_AGC1_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PD_AGC1_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PD_AGC1_ENABLE	(0)
+#define AW87XXX_PID_5A_REG_PD_AGC1_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_AGC1_ENABLE << AW87XXX_PID_5A_REG_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_AGC1_DISABLE	(1)
+#define AW87XXX_PID_5A_REG_PD_AGC1_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_AGC1_DISABLE << AW87XXX_PID_5A_REG_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_AGC1_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PD_AGC1_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PD_AGC1_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PD_AGC1_START_BIT)
+
+/* default value of PAGC1PR (0x0B) */
+/* #define AW87XXX_PID_5A_REG_PAGC1PR_DEFAULT		(0x4A) */
+
+/* ADP_MODE (0x0C) detail */
+/* AGC1_ATT_TIMEA bit 3 (ADP_MODE 0x0C) */
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P04MSDB	(0)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P04MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P04MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P08MSDB	(1)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P08MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P08MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P16MSDB	(2)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P16MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P16MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P32MSDB	(3)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P32MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P32MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P02MSDB	(4)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P02MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P02MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P01MSDB	(5)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P01MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P01MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P005MSDB	(6)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P005MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P005MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P005MSDB	(7)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P005MSDB_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_0P005MSDB << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC1_ATT_TIMEA_START_BIT)
+
+/* ADPBOOST_MODE bit 2:0 (ADP_MODE 0x0C) */
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_ADPBOOST_MODE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_PASS_THROUGH	(0)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_PASS_THROUGH_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_PASS_THROUGH << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_FORCE_BOOST	(1)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_FORCE_BOOST_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_FORCE_BOOST << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD1	(2)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD1_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD1 << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD2	(3)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD2_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD2 << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD3	(4)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD3_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD3 << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD4	(5)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD4_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD4 << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD5	(6)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD5_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD5 << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD2	(7)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD2_VALUE	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_MD2 << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_5A_REG_ADPBOOST_MODE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_ADPBOOST_MODE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_ADPBOOST_MODE_START_BIT)
+
+/* default value of ADP_MODE (0x0C) */
+/* #define AW87XXX_PID_5A_REG_ADP_MODE_DEFAULT		(0x03) */
+
+/* ADPBST_TIME1 (0x0D) detail */
+/* ADP_BST_TIME_2W bit 7:4 (ADPBST_TIME1 0x0D) */
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_BITS_LEN)-1) << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT))
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_1P25MS	(0)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_1P25MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_1P25MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_2P5MS	(1)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_2P5MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_2P5MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_5MS	(2)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_5MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_5MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_10MS	(3)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_10MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_10MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_15MS	(4)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_15MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_15MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_20MS	(5)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_20MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_20MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_30MS	(6)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_30MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_30MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_40MS	(7)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_40MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_40MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_65MS	(8)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_65MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_65MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_80MS	(9)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_80MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_80MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_100MS	(10)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_100MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_100MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_120MS	(11)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_120MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_120MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_140MS	(12)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_140MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_140MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_160MS	(13)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_160MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_160MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_320MS	(14)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_320MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_320MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_480MS	(15)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_480MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_480MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_DEFAULT_VALUE	(0xD)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_DEFAULT	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_DEFAULT_VALUE << AW87XXX_PID_5A_REG_ADP_BST_TIME_2W_START_BIT)
+
+/* ADP_BST_TIME_0P4W bit 3:0 (ADPBST_TIME1 0x0D) */
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_BITS_LEN)-1) << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT))
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_1P25MS	(0)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_1P25MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_1P25MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_2P5MS	(1)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_2P5MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_2P5MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_5MS	(2)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_5MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_5MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_10MS	(3)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_10MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_10MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_15MS	(4)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_15MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_15MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_20MS	(5)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_20MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_20MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_30MS	(6)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_30MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_30MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_40MS	(7)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_40MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_40MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_65MS	(8)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_65MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_65MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_80MS	(9)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_80MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_80MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_100MS	(10)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_100MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_100MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_120MS	(11)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_120MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_120MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_140MS	(12)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_140MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_140MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_160MS	(13)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_160MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_160MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_320MS	(14)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_320MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_320MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_480MS	(15)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_480MS_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_480MS << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_DEFAULT_VALUE	(0xD)
+#define AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_DEFAULT	\
+	(AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_DEFAULT_VALUE << AW87XXX_PID_5A_REG_ADP_BST_TIME_0P4W_START_BIT)
+
+/* default value of ADPBST_TIME1 (0x0D) */
+/* #define AW87XXX_PID_5A_REG_ADPBST_TIME1_DEFAULT		(0xDD) */
+
+/* ADPBST_TIME2 (0x0E) detail */
+/* BST_UP_DT bit 7:4 (ADPBST_TIME2 0x0E) */
+#define AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_UP_DT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P005MS	(0)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P005MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P005MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P01MS	(1)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P01MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P01MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P015MS	(2)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P015MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P015MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P02MS	(3)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P02MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P02MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P03MS	(4)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P03MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P03MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P04MS	(5)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P04MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P04MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P05MS	(6)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P05MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P05MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P06MS	(7)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P06MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P06MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P07MS	(8)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P07MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P07MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P08MS	(9)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P08MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P08MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P10MS	(10)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P10MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P10MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P16MS	(11)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P16MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P16MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P20MS	(12)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P20MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P20MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P32MS	(13)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P32MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P32MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P64MS	(14)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_0P64MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_0P64MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_1P28MS	(15)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_1P28MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_1P28MS << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_UP_DT_DEFAULT_VALUE	(0x7)
+#define AW87XXX_PID_5A_REG_BST_UP_DT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_UP_DT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_UP_DT_START_BIT)
+
+/* BST_DOWN_TD bit 3:0 (ADPBST_TIME2 0x0E) */
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_BITS_LEN	(4)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_DOWN_TD_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P01MS	(0)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P01MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P01MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P02MS	(1)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P02MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P02MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P04MS	(2)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P04MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P04MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P08MS	(3)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P08MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P08MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P12MS	(4)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P12MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P12MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P16MS	(5)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P16MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P16MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P24MS	(6)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P24MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P24MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P32MS	(7)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P32MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P32MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P64MS	(8)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P64MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P64MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P96MS	(9)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_0P96MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_0P96MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_1P28MS	(10)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_1P28MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_1P28MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_1P60MS	(11)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_1P60MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_1P60MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_1P92MS	(12)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_1P92MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_1P92MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_2P56MS	(13)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_2P56MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_2P56MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_5P12MS	(14)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_5P12MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_5P12MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_10P24MS	(15)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_10P24MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_10P24MS << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_DEFAULT_VALUE	(0xA)
+#define AW87XXX_PID_5A_REG_BST_DOWN_TD_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_DOWN_TD_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_DOWN_TD_START_BIT)
+
+/* default value of ADPBST_TIME2 (0x0E) */
+/* #define AW87XXX_PID_5A_REG_ADPBST_TIME2_DEFAULT		(0x7A) */
+
+/* ADPBST_VTH (0x0F) detail */
+/* ADP_LOW_STEP bit 7:6 (ADPBST_VTH 0x0F) */
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_ADP_LOW_STEP_BITS_LEN)-1) << AW87XXX_PID_5A_REG_ADP_LOW_STEP_START_BIT))
+
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00000	(0)
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00000_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00000 << AW87XXX_PID_5A_REG_ADP_LOW_STEP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00001	(1)
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00001_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00001 << AW87XXX_PID_5A_REG_ADP_LOW_STEP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00010	(2)
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00010_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00010 << AW87XXX_PID_5A_REG_ADP_LOW_STEP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00011	(3)
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00011_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_LOW_STEP_1ST_BST_OUT00011 << AW87XXX_PID_5A_REG_ADP_LOW_STEP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_ADP_LOW_STEP_DEFAULT	\
+	(AW87XXX_PID_5A_REG_ADP_LOW_STEP_DEFAULT_VALUE << AW87XXX_PID_5A_REG_ADP_LOW_STEP_START_BIT)
+
+/* SET_BOOST_VTH2 bit 5:3 (ADPBST_VTH 0x0F) */
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_SET_BOOST_VTH2_BITS_LEN)-1) << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT))
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P2W	(0)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P2W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P2W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P4W	(1)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P4W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P4W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P6W	(2)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P6W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P6W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P8W	(3)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P8W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_1P8W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P0W	(4)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P0W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P0W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P2W	(5)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P2W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P2W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P4W	(6)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P4W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P4W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P4W	(7)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P4W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_2P4W << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH2_DEFAULT	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH2_DEFAULT_VALUE << AW87XXX_PID_5A_REG_SET_BOOST_VTH2_START_BIT)
+
+/* SET_BOOST_VTH1 bit 2:0 (ADPBST_VTH 0x0F) */
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_SET_BOOST_VTH1_BITS_LEN)-1) << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT))
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P1W	(0)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P1W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P1W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P2W	(1)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P2W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P2W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P3W	(2)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P3W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P3W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P4W	(3)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P4W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P4W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P5W	(4)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P5W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P5W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W	(5)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W	(6)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W	(7)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W_VALUE	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_0P6W << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_5A_REG_SET_BOOST_VTH1_DEFAULT	\
+	(AW87XXX_PID_5A_REG_SET_BOOST_VTH1_DEFAULT_VALUE << AW87XXX_PID_5A_REG_SET_BOOST_VTH1_START_BIT)
+
+/* default value of ADPBST_VTH (0x0F) */
+/* #define AW87XXX_PID_5A_REG_ADPBST_VTH_DEFAULT		(0x23) */
+
+/* BOOST_PAR (0x10) detail */
+/* CLKDLY_SELECT bit 7 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CLKDLY_SELECT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CLKDLY_SELECT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_DELAY_CLK_CHOOSE_CLK_DLY	(0)
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_DELAY_CLK_CHOOSE_CLK_DLY_VALUE	\
+	(AW87XXX_PID_5A_REG_CLKDLY_SELECT_DELAY_CLK_CHOOSE_CLK_DLY << AW87XXX_PID_5A_REG_CLKDLY_SELECT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_DELAY_CLK_CHOOSE_MAXIM_DUTY	(1)
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_DELAY_CLK_CHOOSE_MAXIM_DUTY_VALUE	\
+	(AW87XXX_PID_5A_REG_CLKDLY_SELECT_DELAY_CLK_CHOOSE_MAXIM_DUTY << AW87XXX_PID_5A_REG_CLKDLY_SELECT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_CLKDLY_SELECT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CLKDLY_SELECT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CLKDLY_SELECT_START_BIT)
+
+/* CPOK_VBGOK bit 6 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CPOK_VBGOK_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CPOK_VBGOK_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_ENABLE_CPOK	(0)
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_ENABLE_CPOK_VALUE	\
+	(AW87XXX_PID_5A_REG_CPOK_VBGOK_ENABLE_CPOK << AW87XXX_PID_5A_REG_CPOK_VBGOK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_ENABLE_VBGOK	(1)
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_ENABLE_VBGOK_VALUE	\
+	(AW87XXX_PID_5A_REG_CPOK_VBGOK_ENABLE_VBGOK << AW87XXX_PID_5A_REG_CPOK_VBGOK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_CPOK_VBGOK_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CPOK_VBGOK_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CPOK_VBGOK_START_BIT)
+
+/* EN_LOWBAT_ADJ bit 5 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_DISABLE << AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_ENABLE << AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_LOWBAT_ADJ_START_BIT)
+
+/* EN_ADP_MODE1_DEGLITCH bit 4 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_DISABLE << AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_ENABLE << AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_ADP_MODE1_DEGLITCH_START_BIT)
+
+/* EN_VCLAMP_MIN_VTH bit 3 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_DISABLE << AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_ENABLE << AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_VCLAMP_MIN_VTH_START_BIT)
+
+/* SS_ADP_BIAS bit 2 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_SS_ADP_BIAS_BITS_LEN)-1) << AW87XXX_PID_5A_REG_SS_ADP_BIAS_START_BIT))
+
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_SS_ADP_BIAS_4UA	(0)
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_SS_ADP_BIAS_4UA_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_ADP_BIAS_SS_ADP_BIAS_4UA << AW87XXX_PID_5A_REG_SS_ADP_BIAS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_SS_ADP_BIAS_8UA	(1)
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_SS_ADP_BIAS_8UA_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_ADP_BIAS_SS_ADP_BIAS_8UA << AW87XXX_PID_5A_REG_SS_ADP_BIAS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_SS_ADP_BIAS_DEFAULT	\
+	(AW87XXX_PID_5A_REG_SS_ADP_BIAS_DEFAULT_VALUE << AW87XXX_PID_5A_REG_SS_ADP_BIAS_START_BIT)
+
+/* BOOST_VTH1_0P1W_0P2W bit 1 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BOOST_VTH1_0P1W	(0)
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BOOST_VTH1_0P1W_VALUE	\
+	(AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BOOST_VTH1_0P1W << AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BOOST_VTH1_0P2W	(1)
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BOOST_VTH1_0P2W_VALUE	\
+	(AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_BOOST_VTH1_0P2W << AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BOOST_VTH1_0P1W_0P2W_START_BIT)
+
+/* EN_LOWBAT_BOOST_VTH1 bit 0 (BOOST_PAR 0x10) */
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_DISABLE << AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_ENABLE << AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_LOWBAT_BOOST_VTH1_START_BIT)
+
+/* default value of BOOST_PAR (0x10) */
+/* #define AW87XXX_PID_5A_REG_BOOST_PAR_DEFAULT		(0x08) */
+
+/* BOOST_VOUT_DET (0x57) detail */
+/* ADP_BOOST_VOUT bit 4:0 (BOOST_VOUT_DET 0x57) */
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_BITS_LEN	(5)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_6P5V	(0)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_6P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_6P5V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_6P75V	(1)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_6P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_6P75V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P0V	(2)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P0V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P25V	(3)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P25V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P5V	(4)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P5V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P75V	(5)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_7P75V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P0V	(6)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P0V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P25V	(7)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P25V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P5V	(8)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P5V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P75V	(9)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_8P75V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P0V	(10)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P0V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P25V	(11)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P25V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P5V	(12)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P5V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P75V	(13)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_9P75V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P0V	(14)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P0V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P25V	(15)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P25V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P5V	(16)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P5V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P75V	(17)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_10P75V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P0V	(18)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P0V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P25V	(19)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P25V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P5V	(20)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P5V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P75V	(21)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_11P75V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P0V	(22)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P0V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P0V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P25V	(23)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P25V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P5V	(24)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_12P5V << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_DEFAULT_VALUE	(0x0C)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_ADP_BOOST_VOUT_START_BIT)
+
+/* default value of BOOST_VOUT_DET (0x57) */
+/* #define AW87XXX_PID_5A_REG_BOOST_VOUT_DET_DEFAULT		(0x0C) */
+
+/* SYSST (0x58) detail */
+/* UVLO_S bit 7 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_UVLO_S_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_UVLO_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_UVLO_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_UVLO_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_UVLO_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_UVLO_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_UVLO_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_S_NORMAL_OPERATION << AW87XXX_PID_5A_REG_UVLO_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_S_VBAT_UNDER_VOLTAGE	(1)
+#define AW87XXX_PID_5A_REG_UVLO_S_VBAT_UNDER_VOLTAGE_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_S_VBAT_UNDER_VOLTAGE << AW87XXX_PID_5A_REG_UVLO_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_UVLO_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_UVLO_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_UVLO_S_START_BIT)
+
+/* LOW_BATT_S bit 6 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_LOW_BATT_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_LOW_BATT_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_LOW_BATT_S_NORMAL_OPERATION << AW87XXX_PID_5A_REG_LOW_BATT_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_LOW_VBAT_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_LOW_VBAT_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_LOW_BATT_S_LOW_VBAT_DETECTED << AW87XXX_PID_5A_REG_LOW_BATT_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_LOW_BATT_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_LOW_BATT_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_LOW_BATT_S_START_BIT)
+
+/* BST_OVP_S bit 5 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_BST_OVP_S_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_BST_OVP_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP_S_NORMAL_OPERATION << AW87XXX_PID_5A_REG_BST_OVP_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP_S_BOOST_OVER_VOLTAGE_PROTECTION	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP_S_BOOST_OVER_VOLTAGE_PROTECTION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP_S_BOOST_OVER_VOLTAGE_PROTECTION << AW87XXX_PID_5A_REG_BST_OVP_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_OVP_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP_S_START_BIT)
+
+/* BST_OVP2_S bit 4 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP2_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP2_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_S_NORMAL_OPERATION << AW87XXX_PID_5A_REG_BST_OVP2_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_BOOST_HEAVY_LOAD_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_BOOST_HEAVY_LOAD_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_S_BOOST_HEAVY_LOAD_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_BST_OVP2_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP2_S_START_BIT)
+
+/* BST_SCP_S bit 3 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_BST_SCP_S_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_SCP_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_SCP_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_SCP_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_SCP_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_SCP_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_BST_SCP_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SCP_S_NORMAL_OPERATION << AW87XXX_PID_5A_REG_BST_SCP_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SCP_S_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_BST_SCP_S_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SCP_S_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_BST_SCP_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SCP_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_SCP_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_SCP_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_SCP_S_START_BIT)
+
+/* PA_OC_S bit 2 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_PA_OC_S_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_PA_OC_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PA_OC_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_OC_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_OC_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_OC_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_PA_OC_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_S_NORMAL_OPERATION << AW87XXX_PID_5A_REG_PA_OC_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_S_PA_OVER_CURRENT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_PA_OC_S_PA_OVER_CURRENT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_S_PA_OVER_CURRENT_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_PA_OC_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_PA_OC_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_OC_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_OC_S_START_BIT)
+
+/* OT160_S bit 1 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_OT160_S_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_OT160_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_OT160_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_OT160_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_OT160_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_OT160_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_OT160_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_OT160_S_NORMAL_OPERATION << AW87XXX_PID_5A_REG_OT160_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_OT160_S_PA_OVER_TEMPRETURE_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_OT160_S_PA_OVER_TEMPRETURE_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_OT160_S_PA_OVER_TEMPRETURE_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_OT160_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_OT160_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_OT160_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_OT160_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_OT160_S_START_BIT)
+
+/* ADP_BOOST_S bit 0 (SYSST 0x58) */
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_ADP_BOOST_S_BITS_LEN)-1) << AW87XXX_PID_5A_REG_ADP_BOOST_S_START_BIT))
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_DIRECT_MODE	(0)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_DIRECT_MODE_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_S_DIRECT_MODE << AW87XXX_PID_5A_REG_ADP_BOOST_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_BOOST_MODE	(1)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_BOOST_MODE_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_S_BOOST_MODE << AW87XXX_PID_5A_REG_ADP_BOOST_S_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_S_DEFAULT	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_S_DEFAULT_VALUE << AW87XXX_PID_5A_REG_ADP_BOOST_S_START_BIT)
+
+/* default value of SYSST (0x58) */
+/* #define AW87XXX_PID_5A_REG_SYSST_DEFAULT		(0xFF) */
+
+/* SYSINT (0x59) detail */
+/* UVLO_I bit 7 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_UVLO_I_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_UVLO_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_UVLO_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_UVLO_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_UVLO_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_UVLO_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_UVLO_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_I_NORMAL_OPERATION << AW87XXX_PID_5A_REG_UVLO_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_I_VBAT_UNDER_VOLTAGE	(1)
+#define AW87XXX_PID_5A_REG_UVLO_I_VBAT_UNDER_VOLTAGE_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_I_VBAT_UNDER_VOLTAGE << AW87XXX_PID_5A_REG_UVLO_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_UVLO_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_UVLO_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_UVLO_I_START_BIT)
+
+/* LOW_BATT_I bit 6 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_LOW_BATT_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_LOW_BATT_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_LOW_BATT_I_NORMAL_OPERATION << AW87XXX_PID_5A_REG_LOW_BATT_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_LOW_VBAT_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_LOW_VBAT_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_LOW_BATT_I_LOW_VBAT_DETECTED << AW87XXX_PID_5A_REG_LOW_BATT_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_LOW_BATT_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_LOW_BATT_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_LOW_BATT_I_START_BIT)
+
+/* BST_OVP_I bit 5 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_BST_OVP_I_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_BST_OVP_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP_I_NORMAL_OPERATION << AW87XXX_PID_5A_REG_BST_OVP_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP_I_BOOST_OVER_VOLTAGE_PROTECTION	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP_I_BOOST_OVER_VOLTAGE_PROTECTION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP_I_BOOST_OVER_VOLTAGE_PROTECTION << AW87XXX_PID_5A_REG_BST_OVP_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_OVP_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP_I_START_BIT)
+
+/* BST_OVP2_I bit 4 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP2_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP2_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_I_NORMAL_OPERATION << AW87XXX_PID_5A_REG_BST_OVP2_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_BOOST_HEAVY_LOAD_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_BOOST_HEAVY_LOAD_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_I_BOOST_HEAVY_LOAD_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_BST_OVP2_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP2_I_START_BIT)
+
+/* BST_SCP_I bit 3 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_BST_SCP_I_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_SCP_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_SCP_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_SCP_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_SCP_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_SCP_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_BST_SCP_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SCP_I_NORMAL_OPERATION << AW87XXX_PID_5A_REG_BST_SCP_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SCP_I_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_BST_SCP_I_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SCP_I_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_BST_SCP_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SCP_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_SCP_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_SCP_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_SCP_I_START_BIT)
+
+/* PA_OC_I bit 2 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_PA_OC_I_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_PA_OC_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PA_OC_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_OC_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_OC_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_OC_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_PA_OC_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_I_NORMAL_OPERATION << AW87XXX_PID_5A_REG_PA_OC_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_I_PA_OVER_CURRENT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_PA_OC_I_PA_OVER_CURRENT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_I_PA_OVER_CURRENT_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_PA_OC_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_PA_OC_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_OC_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_OC_I_START_BIT)
+
+/* OT160_I bit 1 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_OT160_I_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_OT160_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_OT160_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_OT160_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_OT160_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_OT160_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_5A_REG_OT160_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_5A_REG_OT160_I_NORMAL_OPERATION << AW87XXX_PID_5A_REG_OT160_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_OT160_I_PA_OVER_TEMPRETURE_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_5A_REG_OT160_I_PA_OVER_TEMPRETURE_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_5A_REG_OT160_I_PA_OVER_TEMPRETURE_PROTECTION_DETECTED << AW87XXX_PID_5A_REG_OT160_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_OT160_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_OT160_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_OT160_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_OT160_I_START_BIT)
+
+/* ADP_BOOST_I bit 0 (SYSINT 0x59) */
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_ADP_BOOST_I_BITS_LEN)-1) << AW87XXX_PID_5A_REG_ADP_BOOST_I_START_BIT))
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_DIRECT_MODE	(0)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_DIRECT_MODE_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_I_DIRECT_MODE << AW87XXX_PID_5A_REG_ADP_BOOST_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_BOOST_MODE	(1)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_BOOST_MODE_VALUE	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_I_BOOST_MODE << AW87XXX_PID_5A_REG_ADP_BOOST_I_START_BIT)
+
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_ADP_BOOST_I_DEFAULT	\
+	(AW87XXX_PID_5A_REG_ADP_BOOST_I_DEFAULT_VALUE << AW87XXX_PID_5A_REG_ADP_BOOST_I_START_BIT)
+
+/* default value of SYSINT (0x59) */
+/* #define AW87XXX_PID_5A_REG_SYSINT_DEFAULT		(0xFF) */
+
+/* DFT1R (0x60) detail */
+/* CP_FREQ bit 7:6 (DFT1R 0x60) */
+#define AW87XXX_PID_5A_REG_CP_FREQ_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_CP_FREQ_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_CP_FREQ_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CP_FREQ_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CP_FREQ_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CP_FREQ_4P8MHZ	(0)
+#define AW87XXX_PID_5A_REG_CP_FREQ_4P8MHZ_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_FREQ_4P8MHZ << AW87XXX_PID_5A_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_FREQ_6P4MHZ	(1)
+#define AW87XXX_PID_5A_REG_CP_FREQ_6P4MHZ_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_FREQ_6P4MHZ << AW87XXX_PID_5A_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_FREQ_8P0MHZ	(2)
+#define AW87XXX_PID_5A_REG_CP_FREQ_8P0MHZ_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_FREQ_8P0MHZ << AW87XXX_PID_5A_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_FREQ_9P6MHZ	(3)
+#define AW87XXX_PID_5A_REG_CP_FREQ_9P6MHZ_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_FREQ_9P6MHZ << AW87XXX_PID_5A_REG_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_FREQ_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_CP_FREQ_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CP_FREQ_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CP_FREQ_START_BIT)
+
+/* CP_LDO bit 5:4 (DFT1R 0x60) */
+#define AW87XXX_PID_5A_REG_CP_LDO_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_CP_LDO_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_CP_LDO_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CP_LDO_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CP_LDO_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CP_LDO_4P75V	(0)
+#define AW87XXX_PID_5A_REG_CP_LDO_4P75V_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_LDO_4P75V << AW87XXX_PID_5A_REG_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_LDO_5V	(1)
+#define AW87XXX_PID_5A_REG_CP_LDO_5V_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_LDO_5V << AW87XXX_PID_5A_REG_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_LDO_5P25V	(2)
+#define AW87XXX_PID_5A_REG_CP_LDO_5P25V_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_LDO_5P25V << AW87XXX_PID_5A_REG_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_LDO_5P5V	(3)
+#define AW87XXX_PID_5A_REG_CP_LDO_5P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_LDO_5P5V << AW87XXX_PID_5A_REG_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_LDO_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_CP_LDO_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CP_LDO_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CP_LDO_START_BIT)
+
+/* CP_VOS bit 3:2 (DFT1R 0x60) */
+#define AW87XXX_PID_5A_REG_CP_VOS_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_CP_VOS_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_CP_VOS_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CP_VOS_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CP_VOS_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CP_VOS_0MV	(0)
+#define AW87XXX_PID_5A_REG_CP_VOS_0MV_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_VOS_0MV << AW87XXX_PID_5A_REG_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_VOS_50MV	(1)
+#define AW87XXX_PID_5A_REG_CP_VOS_50MV_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_VOS_50MV << AW87XXX_PID_5A_REG_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_VOS_100MV	(2)
+#define AW87XXX_PID_5A_REG_CP_VOS_100MV_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_VOS_100MV << AW87XXX_PID_5A_REG_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_VOS_150MV	(3)
+#define AW87XXX_PID_5A_REG_CP_VOS_150MV_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_VOS_150MV << AW87XXX_PID_5A_REG_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_VOS_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_CP_VOS_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CP_VOS_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CP_VOS_START_BIT)
+
+/* CPOK_TM bit 1 (DFT1R 0x60) */
+#define AW87XXX_PID_5A_REG_CPOK_TM_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_CPOK_TM_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_CPOK_TM_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CPOK_TM_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CPOK_TM_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CPOK_TM_0P6MS	(0)
+#define AW87XXX_PID_5A_REG_CPOK_TM_0P6MS_VALUE	\
+	(AW87XXX_PID_5A_REG_CPOK_TM_0P6MS << AW87XXX_PID_5A_REG_CPOK_TM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CPOK_TM_1MS	(1)
+#define AW87XXX_PID_5A_REG_CPOK_TM_1MS_VALUE	\
+	(AW87XXX_PID_5A_REG_CPOK_TM_1MS << AW87XXX_PID_5A_REG_CPOK_TM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CPOK_TM_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_CPOK_TM_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CPOK_TM_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CPOK_TM_START_BIT)
+
+/* CP_DDT bit 0 (DFT1R 0x60) */
+#define AW87XXX_PID_5A_REG_CP_DDT_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_CP_DDT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_CP_DDT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CP_DDT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CP_DDT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CP_DDT_0NS	(0)
+#define AW87XXX_PID_5A_REG_CP_DDT_0NS_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_DDT_0NS << AW87XXX_PID_5A_REG_CP_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_DDT_10NS	(1)
+#define AW87XXX_PID_5A_REG_CP_DDT_10NS_VALUE	\
+	(AW87XXX_PID_5A_REG_CP_DDT_10NS << AW87XXX_PID_5A_REG_CP_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CP_DDT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_CP_DDT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CP_DDT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CP_DDT_START_BIT)
+
+/* default value of DFT1R (0x60) */
+/* #define AW87XXX_PID_5A_REG_DFT1R_DEFAULT		(0x66) */
+
+/* DFT2R (0x61) detail */
+/* BOOST_VCLAMP_SS bit 7:6 (DFT2R 0x61) */
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_680MV840MV	(0)
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_680MV840MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_680MV840MV << AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_780MV930MV	(1)
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_780MV930MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_780MV930MV << AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_1070MV1225MV	(2)
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_1070MV1225MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_1070MV1225MV << AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_1350MV1500MV	(3)
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_1350MV1500MV_VALUE	\
+	(AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_CLAMP_1350MV1500MV << AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BOOST_VCLAMP_SS_START_BIT)
+
+/* BST_KICK_ITH bit 5:4 (DFT2R 0x61) */
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_KICK_ITH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_KICK_ITH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P5KOHM	(0)
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P5KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P5KOHM << AW87XXX_PID_5A_REG_BST_KICK_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P25KOHM	(1)
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P25KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P25KOHM << AW87XXX_PID_5A_REG_BST_KICK_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P167KOHM	(2)
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P167KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P167KOHM << AW87XXX_PID_5A_REG_BST_KICK_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P125KOHM	(3)
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P125KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_KICK_ITH_PVDD0P125KOHM << AW87XXX_PID_5A_REG_BST_KICK_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_KICK_ITH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_KICK_ITH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_KICK_ITH_START_BIT)
+
+/* BST_EA_CUR bit 3 (DFT2R 0x61) */
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_EA_CUR_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_EA_CUR_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_1UA	(0)
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_1UA_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EA_CUR_1UA << AW87XXX_PID_5A_REG_BST_EA_CUR_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_4UA	(1)
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_4UA_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EA_CUR_4UA << AW87XXX_PID_5A_REG_BST_EA_CUR_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_EA_CUR_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_EA_CUR_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_EA_CUR_START_BIT)
+
+/* BST_CK_MODE bit 2 (DFT2R 0x61) */
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_CK_MODE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_CK_MODE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_1P6MHZ	(0)
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_1P6MHZ_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_CK_MODE_1P6MHZ << AW87XXX_PID_5A_REG_BST_CK_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_2P0MHZ	(1)
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_2P0MHZ_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_CK_MODE_2P0MHZ << AW87XXX_PID_5A_REG_BST_CK_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_CK_MODE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_CK_MODE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_CK_MODE_START_BIT)
+
+/* BST_COMPMAX bit 1:0 (DFT2R 0x61) */
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_COMPMAX_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_COMPMAX_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P2V	(0)
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P2V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_COMPMAX_2P2V << AW87XXX_PID_5A_REG_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P4V	(1)
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P4V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_COMPMAX_2P4V << AW87XXX_PID_5A_REG_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P6V	(2)
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P6V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_COMPMAX_2P6V << AW87XXX_PID_5A_REG_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P8V	(3)
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_2P8V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_COMPMAX_2P8V << AW87XXX_PID_5A_REG_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_COMPMAX_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_COMPMAX_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_COMPMAX_START_BIT)
+
+/* default value of DFT2R (0x61) */
+/* #define AW87XXX_PID_5A_REG_DFT2R_DEFAULT		(0x18) */
+
+/* DFT3R (0x62) detail */
+/* BST_PWM_SHORT bit 7 (DFT3R 0x62) */
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_PWM_SHORT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_PWM_SHORT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_VBSTBELOWVDD	(0)
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_VBSTBELOWVDD_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_PWM_SHORT_VBSTBELOWVDD << AW87XXX_PID_5A_REG_BST_PWM_SHORT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_VBSTBELOWVDDMINUS_VTH	(1)
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_VBSTBELOWVDDMINUS_VTH_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_PWM_SHORT_VBSTBELOWVDDMINUS_VTH << AW87XXX_PID_5A_REG_BST_PWM_SHORT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_PWM_SHORT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_PWM_SHORT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_PWM_SHORT_START_BIT)
+
+/* BST_SLOPE bit 6:5 (DFT3R 0x62) */
+#define AW87XXX_PID_5A_REG_BST_SLOPE_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_SLOPE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_SLOPE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE	(0)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P25	(1)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P25_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P25 << AW87XXX_PID_5A_REG_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P5	(2)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P5_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P5 << AW87XXX_PID_5A_REG_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P75	(3)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P75_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_ISLOPE1P75 << AW87XXX_PID_5A_REG_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_SLOPE_START_BIT)
+
+/* BST_LOOPC bit 4 (DFT3R 0x62) */
+#define AW87XXX_PID_5A_REG_BST_LOOPC_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_LOOPC_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_LOOPC_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_LOOPC_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_LOOPC_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_LOOPC_28PF	(0)
+#define AW87XXX_PID_5A_REG_BST_LOOPC_28PF_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LOOPC_28PF << AW87XXX_PID_5A_REG_BST_LOOPC_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_LOOPC_50PF	(1)
+#define AW87XXX_PID_5A_REG_BST_LOOPC_50PF_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LOOPC_50PF << AW87XXX_PID_5A_REG_BST_LOOPC_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_LOOPC_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_LOOPC_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_LOOPC_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_LOOPC_START_BIT)
+
+/* BST_OS_WIDTH bit 3:2 (DFT3R 0x62) */
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OS_WIDTH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OS_WIDTH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_40NS	(0)
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_40NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OS_WIDTH_40NS << AW87XXX_PID_5A_REG_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_30NS	(1)
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_30NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OS_WIDTH_30NS << AW87XXX_PID_5A_REG_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_50NS	(2)
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_50NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OS_WIDTH_50NS << AW87XXX_PID_5A_REG_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_60NS	(3)
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_60NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OS_WIDTH_60NS << AW87XXX_PID_5A_REG_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_OS_WIDTH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OS_WIDTH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OS_WIDTH_START_BIT)
+
+/* BST_LOOPR bit 1:0 (DFT3R 0x62) */
+#define AW87XXX_PID_5A_REG_BST_LOOPR_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_LOOPR_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_LOOPR_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_LOOPR_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_LOOPR_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_LOOPR_320K	(0)
+#define AW87XXX_PID_5A_REG_BST_LOOPR_320K_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LOOPR_320K << AW87XXX_PID_5A_REG_BST_LOOPR_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_LOOPR_160K	(1)
+#define AW87XXX_PID_5A_REG_BST_LOOPR_160K_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LOOPR_160K << AW87XXX_PID_5A_REG_BST_LOOPR_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_LOOPR_480K	(2)
+#define AW87XXX_PID_5A_REG_BST_LOOPR_480K_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LOOPR_480K << AW87XXX_PID_5A_REG_BST_LOOPR_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_BST_LOOPR_320K	(3)
+#define AW87XXX_PID_5A_REG_BST_LOOPR_320K_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LOOPR_320K << AW87XXX_PID_5A_REG_BST_LOOPR_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_BST_LOOPR_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_BST_LOOPR_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_LOOPR_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_LOOPR_START_BIT)
+
+/* default value of DFT3R (0x62) */
+/* #define AW87XXX_PID_5A_REG_DFT3R_DEFAULT		(0x02) */
+
+/* DFT4R (0x63) detail */
+/* BST_BURST_IN_DELAY bit 7:6 (DFT4R 0x63) */
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_8US	(0)
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_8US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_8US << AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_12US	(1)
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_12US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_12US << AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_4US	(2)
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_4US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_4US << AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_2US	(3)
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_2US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_2US << AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_BURST_IN_DELAY_START_BIT)
+
+/* BST_BURST_OUT_DELAY bit 5:4 (DFT4R 0x63) */
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_2US	(0)
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_2US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_2US << AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_4US	(1)
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_4US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_4US << AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_1P3US	(2)
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_1P3US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_1P3US << AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_1P0US	(3)
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_1P0US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_1P0US << AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_BURST_OUT_DELAY_START_BIT)
+
+/* BST_EN_DELAY bit 3:2 (DFT4R 0x63) */
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_EN_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_EN_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_8NS	(0)
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_8NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EN_DELAY_8NS << AW87XXX_PID_5A_REG_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_80NS	(1)
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_80NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EN_DELAY_80NS << AW87XXX_PID_5A_REG_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_130NS	(2)
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_130NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EN_DELAY_130NS << AW87XXX_PID_5A_REG_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_200NS	(3)
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_200NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EN_DELAY_200NS << AW87XXX_PID_5A_REG_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_BST_EN_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_EN_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_EN_DELAY_START_BIT)
+
+/* BST_GD_DELAY bit 1:0 (DFT4R 0x63) */
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_GD_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_GD_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_1P2NS_LS_1P2NS	(0)
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_1P2NS_LS_1P2NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_1P2NS_LS_1P2NS << AW87XXX_PID_5A_REG_BST_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_1P2NS_LS_2P5NS	(1)
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_1P2NS_LS_2P5NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_1P2NS_LS_2P5NS << AW87XXX_PID_5A_REG_BST_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_2P5NS_LS_1P2NS	(2)
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_2P5NS_LS_1P2NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_2P5NS_LS_1P2NS << AW87XXX_PID_5A_REG_BST_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_2P5NS_LS_2P5NS	(3)
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_2P5NS_LS_2P5NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_GD_DELAY_HS_2P5NS_LS_2P5NS << AW87XXX_PID_5A_REG_BST_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_GD_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_GD_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_GD_DELAY_START_BIT)
+
+/* default value of DFT4R (0x63) */
+/* #define AW87XXX_PID_5A_REG_DFT4R_DEFAULT		(0x08) */
+
+/* DFT5R (0x64) detail */
+/* PA_FLT_SR bit 7 (DFT5R 0x64) */
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_FLT_SR_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_FLT_SR_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_ENABLE	(0)
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_FLT_SR_ENABLE << AW87XXX_PID_5A_REG_PA_FLT_SR_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_DISABLE	(1)
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_FLT_SR_DISABLE << AW87XXX_PID_5A_REG_PA_FLT_SR_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PA_FLT_SR_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_FLT_SR_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_FLT_SR_START_BIT)
+
+/* AGC1_VTH_SEL bit 6:5 (DFT5R 0x64) */
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_AGC1_VTH_SEL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_AGC1_VTH_SEL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN	(0)
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN << AW87XXX_PID_5A_REG_AGC1_VTH_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_THGEN	(1)
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_THGEN_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_VTH_SEL_THGEN << AW87XXX_PID_5A_REG_AGC1_VTH_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN_AND_THGEN	(2)
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN_AND_THGEN_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN_AND_THGEN << AW87XXX_PID_5A_REG_AGC1_VTH_SEL_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN_AND_THGEN	(3)
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN_AND_THGEN_VALUE	\
+	(AW87XXX_PID_5A_REG_AGC1_VTH_SEL_RAMP_GEN_AND_THGEN << AW87XXX_PID_5A_REG_AGC1_VTH_SEL_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_AGC1_VTH_SEL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_AGC1_VTH_SEL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_AGC1_VTH_SEL_START_BIT)
+
+/* BST_OVP2_EN bit 4 (DFT5R 0x64) */
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP2_EN_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP2_EN_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_EN_DISABLE << AW87XXX_PID_5A_REG_BST_OVP2_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_EN_ENABLE << AW87XXX_PID_5A_REG_BST_OVP2_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_OVP2_EN_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_EN_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP2_EN_START_BIT)
+
+/* BST_OVP2_ITH bit 3:2 (DFT5R 0x64) */
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP2_ITH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP2_ITH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P5KOHM	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P5KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P5KOHM << AW87XXX_PID_5A_REG_BST_OVP2_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P25KOHM	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P25KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P25KOHM << AW87XXX_PID_5A_REG_BST_OVP2_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P167KOHM	(2)
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P167KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P167KOHM << AW87XXX_PID_5A_REG_BST_OVP2_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P125KOHM	(3)
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P125KOHM_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_ITH_PVDD0P125KOHM << AW87XXX_PID_5A_REG_BST_OVP2_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_ITH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_ITH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP2_ITH_START_BIT)
+
+/* BST_OVP2_VTH bit 1:0 (DFT5R 0x64) */
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP2_VTH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP2_VTH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_13V_9V	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_13V_9V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_VTH_13V_9V << AW87XXX_PID_5A_REG_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_13P5V_9P5V	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_13P5V_9P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_VTH_13P5V_9P5V << AW87XXX_PID_5A_REG_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_14V_10V	(2)
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_14V_10V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_VTH_14V_10V << AW87XXX_PID_5A_REG_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_14P5V_10P5V	(3)
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_14P5V_10P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_VTH_14P5V_10P5V << AW87XXX_PID_5A_REG_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_OVP2_VTH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP2_VTH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP2_VTH_START_BIT)
+
+/* default value of DFT5R (0x64) */
+/* #define AW87XXX_PID_5A_REG_DFT5R_DEFAULT		(0x45) */
+
+/* DFT6R (0x65) detail */
+/* POWER_SAVE_DLY_SELECT bit 7 (DFT6R 0x65) */
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_HIGH_VOLTAGE_TRIGGER	(0)
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_HIGH_VOLTAGE_TRIGGER_VALUE	\
+	(AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_HIGH_VOLTAGE_TRIGGER << AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_CLK_RISING_EDGE_TRIGGER	(1)
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_CLK_RISING_EDGE_TRIGGER_VALUE	\
+	(AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_CLK_RISING_EDGE_TRIGGER << AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_POWER_SAVE_DLY_SELECT_START_BIT)
+
+/* PA_OPD bit 6 (DFT6R 0x65) */
+#define AW87XXX_PID_5A_REG_PA_OPD_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_PA_OPD_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PA_OPD_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_OPD_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_OPD_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_OPD_OUTPUT_FLOATING	(0)
+#define AW87XXX_PID_5A_REG_PA_OPD_OUTPUT_FLOATING_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OPD_OUTPUT_FLOATING << AW87XXX_PID_5A_REG_PA_OPD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OPD_OUTPUT_TIED_TO_GND	(1)
+#define AW87XXX_PID_5A_REG_PA_OPD_OUTPUT_TIED_TO_GND_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OPD_OUTPUT_TIED_TO_GND << AW87XXX_PID_5A_REG_PA_OPD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OPD_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_PA_OPD_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_OPD_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_OPD_START_BIT)
+
+/* CLK_OCP_SEL bit 5 (DFT6R 0x65) */
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_CLK_OCP_SEL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_CLK_OCP_SEL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_160MS	(0)
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_160MS_VALUE	\
+	(AW87XXX_PID_5A_REG_CLK_OCP_SEL_160MS << AW87XXX_PID_5A_REG_CLK_OCP_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_640MS	(1)
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_640MS_VALUE	\
+	(AW87XXX_PID_5A_REG_CLK_OCP_SEL_640MS << AW87XXX_PID_5A_REG_CLK_OCP_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_CLK_OCP_SEL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_CLK_OCP_SEL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_CLK_OCP_SEL_START_BIT)
+
+/* BST_SKIP_EN bit 4 (DFT6R 0x65) */
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_SKIP_EN_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_SKIP_EN_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SKIP_EN_DISABLE << AW87XXX_PID_5A_REG_BST_SKIP_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SKIP_EN_ENABLE << AW87XXX_PID_5A_REG_BST_SKIP_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_SKIP_EN_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_SKIP_EN_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_SKIP_EN_START_BIT)
+
+/* BST_OVP_DEGLITCH_SEL bit 3 (DFT6R 0x65) */
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_NO_DEGLITCH	(0)
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_NO_DEGLITCH_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_NO_DEGLITCH << AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_DEGLITCH_300NS	(1)
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_DEGLITCH_300NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_DEGLITCH_300NS << AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_OVP_DEGLITCH_SEL_START_BIT)
+
+/* BST_NCD_ITH bit 2:1 (DFT6R 0x65) */
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_NCD_ITH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_NCD_ITH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_150MA	(0)
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_150MA_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_NCD_ITH_150MA << AW87XXX_PID_5A_REG_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_200MA	(1)
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_200MA_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_NCD_ITH_200MA << AW87XXX_PID_5A_REG_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_250MA	(2)
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_250MA_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_NCD_ITH_250MA << AW87XXX_PID_5A_REG_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_300MA	(3)
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_300MA_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_NCD_ITH_300MA << AW87XXX_PID_5A_REG_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_NCD_ITH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_NCD_ITH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_NCD_ITH_START_BIT)
+
+/* BST_LMD_VTH bit 0 (DFT6R 0x65) */
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_LMD_VTH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_LMD_VTH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_HIGH_SIDE_VDD	(0)
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_HIGH_SIDE_VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LMD_VTH_HIGH_SIDE_VDD << AW87XXX_PID_5A_REG_BST_LMD_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_LOW_SIDE_VDD	(1)
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_LOW_SIDE_VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_LMD_VTH_LOW_SIDE_VDD << AW87XXX_PID_5A_REG_BST_LMD_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_LMD_VTH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_LMD_VTH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_LMD_VTH_START_BIT)
+
+/* default value of DFT6R (0x65) */
+/* #define AW87XXX_PID_5A_REG_DFT6R_DEFAULT		(0x53) */
+
+/* DFT7R (0x66) detail */
+
+/* PA_OC_DT bit 4:3 (DFT7R 0x66) */
+#define AW87XXX_PID_5A_REG_PA_OC_DT_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_PA_OC_DT_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_PA_OC_DT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_OC_DT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_OC_DT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_OC_DT_80NS	(0)
+#define AW87XXX_PID_5A_REG_PA_OC_DT_80NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_DT_80NS << AW87XXX_PID_5A_REG_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_DT_150NS	(1)
+#define AW87XXX_PID_5A_REG_PA_OC_DT_150NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_DT_150NS << AW87XXX_PID_5A_REG_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_DT_210NS	(2)
+#define AW87XXX_PID_5A_REG_PA_OC_DT_210NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_DT_210NS << AW87XXX_PID_5A_REG_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_DT_240NS	(3)
+#define AW87XXX_PID_5A_REG_PA_OC_DT_240NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OC_DT_240NS << AW87XXX_PID_5A_REG_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OC_DT_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_PA_OC_DT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_OC_DT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_OC_DT_START_BIT)
+
+/* PA_RAMP_AGC1 bit 2:1 (DFT7R 0x66) */
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_RAMP_AGC1_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_RAMP_AGC1_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P8VDD	(0)
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P8VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P8VDD << AW87XXX_PID_5A_REG_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P825VDD	(1)
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P825VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P825VDD << AW87XXX_PID_5A_REG_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P85VDD	(2)
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P85VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P85VDD << AW87XXX_PID_5A_REG_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P875VDD	(3)
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P875VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_RAMP_AGC1_0P875VDD << AW87XXX_PID_5A_REG_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PA_RAMP_AGC1_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_RAMP_AGC1_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_RAMP_AGC1_START_BIT)
+
+/* PA_OCSWD bit 0 (DFT7R 0x66) */
+#define AW87XXX_PID_5A_REG_PA_OCSWD_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_PA_OCSWD_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PA_OCSWD_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_OCSWD_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_OCSWD_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_OCSWD_THROUGH_GATEDRIVER	(0)
+#define AW87XXX_PID_5A_REG_PA_OCSWD_THROUGH_GATEDRIVER_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OCSWD_THROUGH_GATEDRIVER << AW87XXX_PID_5A_REG_PA_OCSWD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OCSWD_THROUGH_SWITCH_MOS	(1)
+#define AW87XXX_PID_5A_REG_PA_OCSWD_THROUGH_SWITCH_MOS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_OCSWD_THROUGH_SWITCH_MOS << AW87XXX_PID_5A_REG_PA_OCSWD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_OCSWD_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PA_OCSWD_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_OCSWD_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_OCSWD_START_BIT)
+
+/* default value of DFT7R (0x66) */
+/* #define AW87XXX_PID_5A_REG_DFT7R_DEFAULT		(0x70) */
+
+/* DFT8R (0x67) detail */
+/* PA_GD_DELAY bit 7:6 (DFT8R 0x67) */
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_GD_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_GD_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_1P2NS_LS_1P2NS	(0)
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_1P2NS_LS_1P2NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_1P2NS_LS_1P2NS << AW87XXX_PID_5A_REG_PA_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_1P2NS_LS_2P5NS	(1)
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_1P2NS_LS_2P5NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_1P2NS_LS_2P5NS << AW87XXX_PID_5A_REG_PA_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_2P5NS_LS_1P2NS	(2)
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_2P5NS_LS_1P2NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_2P5NS_LS_1P2NS << AW87XXX_PID_5A_REG_PA_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_2P5NS_LS_2P5NS	(3)
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_2P5NS_LS_2P5NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GD_DELAY_HS_2P5NS_LS_2P5NS << AW87XXX_PID_5A_REG_PA_GD_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PA_GD_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_GD_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_GD_DELAY_START_BIT)
+
+/* PA_GD_DGT bit 5 (DFT8R 0x67) */
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_GD_DGT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_GD_DGT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_4NS	(0)
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_4NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GD_DGT_4NS << AW87XXX_PID_5A_REG_PA_GD_DGT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_5P5NS	(1)
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_5P5NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GD_DGT_5P5NS << AW87XXX_PID_5A_REG_PA_GD_DGT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PA_GD_DGT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_GD_DGT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_GD_DGT_START_BIT)
+
+/* PA_PORT bit 4:3 (DFT8R 0x67) */
+#define AW87XXX_PID_5A_REG_PA_PORT_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_PA_PORT_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_PA_PORT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_PORT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_PORT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_PORT_80MS	(0)
+#define AW87XXX_PID_5A_REG_PA_PORT_80MS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_PORT_80MS << AW87XXX_PID_5A_REG_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_PORT_40MS	(1)
+#define AW87XXX_PID_5A_REG_PA_PORT_40MS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_PORT_40MS << AW87XXX_PID_5A_REG_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_PORT_20MS	(2)
+#define AW87XXX_PID_5A_REG_PA_PORT_20MS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_PORT_20MS << AW87XXX_PID_5A_REG_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_PORT_10MS	(3)
+#define AW87XXX_PID_5A_REG_PA_PORT_10MS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_PORT_10MS << AW87XXX_PID_5A_REG_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_PORT_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_PA_PORT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_PORT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_PORT_START_BIT)
+
+/* EN_AGC1_ADP bit 2 (DFT8R 0x67) */
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_AGC1_ADP_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_AGC1_ADP_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_AGC_CROSSZERO_AS_BEFORE	(0)
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_AGC_CROSSZERO_AS_BEFORE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_AGC1_ADP_AGC_CROSSZERO_AS_BEFORE << AW87XXX_PID_5A_REG_EN_AGC1_ADP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_AGC_CROSSZERO_ADAPTIVELY	(1)
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_AGC_CROSSZERO_ADAPTIVELY_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_AGC1_ADP_AGC_CROSSZERO_ADAPTIVELY << AW87XXX_PID_5A_REG_EN_AGC1_ADP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_AGC1_ADP_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_AGC1_ADP_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_AGC1_ADP_START_BIT)
+
+/* PD_CROSSZERO bit 1:0 (DFT8R 0x67) */
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PD_CROSSZERO_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PD_CROSSZERO_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC1AGC2_AND_AGC3_CROSS_ZERO	(0)
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC1AGC2_AND_AGC3_CROSS_ZERO_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC1AGC2_AND_AGC3_CROSS_ZERO << AW87XXX_PID_5A_REG_PD_CROSSZERO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC2AGC3_CROSS_ZERO_DISABLE_AGC1_CROSS_ZERO	(1)
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC2AGC3_CROSS_ZERO_DISABLE_AGC1_CROSS_ZERO_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC2AGC3_CROSS_ZERO_DISABLE_AGC1_CROSS_ZERO << AW87XXX_PID_5A_REG_PD_CROSSZERO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC3_CROSS_ZERO_DISABLE_AGC1AGC2_CROSS_ZERO	(2)
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC3_CROSS_ZERO_DISABLE_AGC1AGC2_CROSS_ZERO_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_CROSSZERO_ENABLE_AGC3_CROSS_ZERO_DISABLE_AGC1AGC2_CROSS_ZERO << AW87XXX_PID_5A_REG_PD_CROSSZERO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_DISABLE_AGC1AGC2_AND_AGC3_CROSS_ZERO	(3)
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_DISABLE_AGC1AGC2_AND_AGC3_CROSS_ZERO_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_CROSSZERO_DISABLE_AGC1AGC2_AND_AGC3_CROSS_ZERO << AW87XXX_PID_5A_REG_PD_CROSSZERO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PD_CROSSZERO_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PD_CROSSZERO_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PD_CROSSZERO_START_BIT)
+
+/* default value of DFT8R (0x67) */
+/* #define AW87XXX_PID_5A_REG_DFT8R_DEFAULT		(0x08) */
+
+/* DFT9R (0x68) detail */
+/* EN_BOOST_VCLAMP_SS bit 7 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_DISABLE << AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_ENABLE << AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_BOOST_VCLAMP_SS_START_BIT)
+
+/* EN_BOOST_PLDO bit 6 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_BOOST_PLDO_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_BOOST_PLDO_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_SET_VDD	(0)
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_SET_VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_PLDO_SET_VDD << AW87XXX_PID_5A_REG_EN_BOOST_PLDO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_SET_PVLDO	(1)
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_SET_PVLDO_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_PLDO_SET_PVLDO << AW87XXX_PID_5A_REG_EN_BOOST_PLDO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_BOOST_PLDO_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_BOOST_PLDO_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_BOOST_PLDO_START_BIT)
+
+/* EN_CLAMP bit 5 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_EN_CLAMP_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_EN_CLAMP_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_CLAMP_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_CLAMP_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_CLAMP_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_CLAMP_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_CLAMP_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_CLAMP_DISABLE << AW87XXX_PID_5A_REG_EN_CLAMP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_CLAMP_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_CLAMP_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_CLAMP_ENABLE << AW87XXX_PID_5A_REG_EN_CLAMP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_CLAMP_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_EN_CLAMP_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_CLAMP_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_CLAMP_START_BIT)
+
+/* EN_VBG_PASS bit 4 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_VBG_PASS_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_VBG_PASS_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_VBG_PASS_DISABLE << AW87XXX_PID_5A_REG_EN_VBG_PASS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_VBG_PASS_ENABLE << AW87XXX_PID_5A_REG_EN_VBG_PASS_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_VBG_PASS_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_VBG_PASS_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_VBG_PASS_START_BIT)
+
+/* SS_SOFT_IPEAK_ADP bit 3 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_BITS_LEN)-1) << AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_START_BIT))
+
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_DISABLE << AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_ENABLE << AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_DEFAULT	\
+	(AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_DEFAULT_VALUE << AW87XXX_PID_5A_REG_SS_SOFT_IPEAK_ADP_START_BIT)
+
+/* EN_ADP_IPEAK bit 2 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_ADP_IPEAK_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_ADP_IPEAK_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_ADP_IPEAK_DISABLE << AW87XXX_PID_5A_REG_EN_ADP_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_ADP_IPEAK_ENABLE << AW87XXX_PID_5A_REG_EN_ADP_IPEAK_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_ADP_IPEAK_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_ADP_IPEAK_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_ADP_IPEAK_START_BIT)
+
+/* SEL_FINISH_ID bit 1 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_SEL_FINISH_ID_BITS_LEN)-1) << AW87XXX_PID_5A_REG_SEL_FINISH_ID_START_BIT))
+
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_MODE1_DELAY	(0)
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_MODE1_DELAY_VALUE	\
+	(AW87XXX_PID_5A_REG_SEL_FINISH_ID_MODE1_DELAY << AW87XXX_PID_5A_REG_SEL_FINISH_ID_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_LIMIT_SS_FINISH	(1)
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_LIMIT_SS_FINISH_VALUE	\
+	(AW87XXX_PID_5A_REG_SEL_FINISH_ID_LIMIT_SS_FINISH << AW87XXX_PID_5A_REG_SEL_FINISH_ID_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_SEL_FINISH_ID_DEFAULT	\
+	(AW87XXX_PID_5A_REG_SEL_FINISH_ID_DEFAULT_VALUE << AW87XXX_PID_5A_REG_SEL_FINISH_ID_START_BIT)
+
+/* SS_FINISH_SELECTED bit 0 (DFT9R 0x68) */
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_BITS_LEN)-1) << AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_START_BIT))
+
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_3US	(0)
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_3US_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_3US << AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_0US	(1)
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_0US_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_0US << AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_DEFAULT	\
+	(AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_DEFAULT_VALUE << AW87XXX_PID_5A_REG_SS_FINISH_SELECTED_START_BIT)
+
+/* default value of DFT9R (0x68) */
+/* #define AW87XXX_PID_5A_REG_DFT9R_DEFAULT		(0x21) */
+
+/* DFTAR (0x69) detail */
+/* HWM_DELAY_INITIAL bit 7:6 (DFTAR 0x69) */
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_104NS	(0)
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_104NS_VALUE	\
+	(AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_104NS << AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_63NS	(1)
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_63NS_VALUE	\
+	(AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_63NS << AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_56NS	(2)
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_56NS_VALUE	\
+	(AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_56NS << AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_42NS	(3)
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_42NS_VALUE	\
+	(AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_42NS << AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_HWM_DELAY_INITIAL_START_BIT)
+
+/* BST_DFPWM bit 5:3 (DFTAR 0x69) */
+#define AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_DFPWM_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_2P5US	(0)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_2P5US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_2P5US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_5US	(1)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_5US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_5US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_10US	(2)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_10US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_10US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_20US	(3)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_20US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_20US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_40US	(4)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_40US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_40US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_80US	(5)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_80US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_80US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_160US	(6)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_160US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_160US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_320US	(7)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_320US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_320US << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_DFPWM_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_5A_REG_BST_DFPWM_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_DFPWM_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_DFPWM_START_BIT)
+
+/* BST_SOFT_DELAY bit 2:0 (DFTAR 0x69) */
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_SOFT_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_40US	(0)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_40US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_40US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_80US	(1)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_80US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_80US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_160US	(2)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_160US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_160US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_320US	(3)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_320US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_320US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_1280US	(4)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_1280US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_1280US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_2560US	(5)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_2560US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_2560US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_5120US	(6)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_5120US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_5120US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_10240US	(7)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_10240US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_10240US << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_5A_REG_BST_SOFT_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_SOFT_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_SOFT_DELAY_START_BIT)
+
+/* default value of DFTAR (0x69) */
+/* #define AW87XXX_PID_5A_REG_DFTAR_DEFAULT		(0xA4) */
+
+/* DFTBR (0x70) detail */
+/* BST_CLK_DIV bit 4 (DFTBR 0x70) */
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_CLK_DIV_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_CLK_DIV_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_DIV_BY_4	(0)
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_DIV_BY_4_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_CLK_DIV_DIV_BY_4 << AW87XXX_PID_5A_REG_BST_CLK_DIV_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_DIV_BY_2	(1)
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_DIV_BY_2_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_CLK_DIV_DIV_BY_2 << AW87XXX_PID_5A_REG_BST_CLK_DIV_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_CLK_DIV_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_CLK_DIV_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_CLK_DIV_START_BIT)
+
+/* RAMP_1SPW_VC bit 3:2 (DFTBR 0x70) */
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_RAMP_1SPW_VC_BITS_LEN)-1) << AW87XXX_PID_5A_REG_RAMP_1SPW_VC_START_BIT))
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P37VDD	(0)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P37VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P37VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P39VDD	(1)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P39VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P39VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P33VDD	(2)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P33VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P33VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P35VDD	(3)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P35VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VC_VC0P35VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VC_DEFAULT	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VC_DEFAULT_VALUE << AW87XXX_PID_5A_REG_RAMP_1SPW_VC_START_BIT)
+
+/* RAMP_1SPW_VL bit 1:0 (DFTBR 0x70) */
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_RAMP_1SPW_VL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_RAMP_1SPW_VL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P16VDD	(0)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P16VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P16VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P18VDD	(1)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P18VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P18VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P20VDD	(2)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P20VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P20VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P14VDD	(3)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P14VDD_VALUE	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VL_VC0P14VDD << AW87XXX_PID_5A_REG_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_RAMP_1SPW_VL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_RAMP_1SPW_VL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_RAMP_1SPW_VL_START_BIT)
+
+/* default value of DFTBR (0x70) */
+/* #define AW87XXX_PID_5A_REG_DFTBR_DEFAULT		(0x1C) */
+
+/* DFTCR (0x71) detail */
+/* DT_EN bit 7 (DFTCR 0x71) */
+#define AW87XXX_PID_5A_REG_DT_EN_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_DT_EN_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_DT_EN_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_DT_EN_BITS_LEN)-1) << AW87XXX_PID_5A_REG_DT_EN_START_BIT))
+
+#define AW87XXX_PID_5A_REG_DT_EN_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_DT_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_DT_EN_DISABLE << AW87XXX_PID_5A_REG_DT_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_DT_EN_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_DT_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_DT_EN_ENABLE << AW87XXX_PID_5A_REG_DT_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_DT_EN_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_DT_EN_DEFAULT	\
+	(AW87XXX_PID_5A_REG_DT_EN_DEFAULT_VALUE << AW87XXX_PID_5A_REG_DT_EN_START_BIT)
+
+/* BST_TD bit 6:4 (DFTCR 0x71) */
+#define AW87XXX_PID_5A_REG_BST_TD_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_BST_TD_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_BST_TD_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_TD_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_TD_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_TD_0P08MS	(0)
+#define AW87XXX_PID_5A_REG_BST_TD_0P08MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_0P08MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_0P16MS	(1)
+#define AW87XXX_PID_5A_REG_BST_TD_0P16MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_0P16MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_0P32MS	(2)
+#define AW87XXX_PID_5A_REG_BST_TD_0P32MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_0P32MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_0P64MS	(3)
+#define AW87XXX_PID_5A_REG_BST_TD_0P64MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_0P64MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_1P28MS	(4)
+#define AW87XXX_PID_5A_REG_BST_TD_1P28MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_1P28MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_2P56MS	(5)
+#define AW87XXX_PID_5A_REG_BST_TD_2P56MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_2P56MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_5P12MS	(6)
+#define AW87XXX_PID_5A_REG_BST_TD_5P12MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_5P12MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_10P24MS	(7)
+#define AW87XXX_PID_5A_REG_BST_TD_10P24MS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_TD_10P24MS << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_TD_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_TD_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_TD_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_TD_START_BIT)
+
+/* BST_GTDR_DDT bit 3 (DFTCR 0x71) */
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_GTDR_DDT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_GTDR_DDT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_3NS	(0)
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_3NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_GTDR_DDT_3NS << AW87XXX_PID_5A_REG_BST_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_6NS	(1)
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_6NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_GTDR_DDT_6NS << AW87XXX_PID_5A_REG_BST_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_GTDR_DDT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_GTDR_DDT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_GTDR_DDT_START_BIT)
+
+/* BST_EN_RSQN_DLY bit 2 (DFTCR 0x71) */
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_DISABLE << AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_ENABLE << AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_EN_RSQN_DLY_START_BIT)
+
+/* BST_RSQN_DLY bit 1:0 (DFTCR 0x71) */
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_RSQN_DLY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_RSQN_DLY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_15NS	(0)
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_15NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_RSQN_DLY_15NS << AW87XXX_PID_5A_REG_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_25NS	(1)
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_25NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_RSQN_DLY_25NS << AW87XXX_PID_5A_REG_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_35NS	(2)
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_35NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_RSQN_DLY_35NS << AW87XXX_PID_5A_REG_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_45NS	(3)
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_45NS_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_RSQN_DLY_45NS << AW87XXX_PID_5A_REG_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_BST_RSQN_DLY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_RSQN_DLY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_RSQN_DLY_START_BIT)
+
+/* default value of DFTCR (0x71) */
+/* #define AW87XXX_PID_5A_REG_DFTCR_DEFAULT		(0x10) */
+
+/* DFTDR (0x72) detail */
+/* DLY_EN bit 7 (DFTDR 0x72) */
+#define AW87XXX_PID_5A_REG_DLY_EN_START_BIT	(7)
+#define AW87XXX_PID_5A_REG_DLY_EN_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_DLY_EN_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_DLY_EN_BITS_LEN)-1) << AW87XXX_PID_5A_REG_DLY_EN_START_BIT))
+
+#define AW87XXX_PID_5A_REG_DLY_EN_NO_DELAY	(0)
+#define AW87XXX_PID_5A_REG_DLY_EN_NO_DELAY_VALUE	\
+	(AW87XXX_PID_5A_REG_DLY_EN_NO_DELAY << AW87XXX_PID_5A_REG_DLY_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_DLY_EN_DELAY_TWO_CLOCK	(1)
+#define AW87XXX_PID_5A_REG_DLY_EN_DELAY_TWO_CLOCK_VALUE	\
+	(AW87XXX_PID_5A_REG_DLY_EN_DELAY_TWO_CLOCK << AW87XXX_PID_5A_REG_DLY_EN_START_BIT)
+
+#define AW87XXX_PID_5A_REG_DLY_EN_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_DLY_EN_DEFAULT	\
+	(AW87XXX_PID_5A_REG_DLY_EN_DEFAULT_VALUE << AW87XXX_PID_5A_REG_DLY_EN_START_BIT)
+
+/* DOWNSIGNAL_SEL bit 6 (DFTDR 0x72) */
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_SET_160MS	(0)
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_SET_160MS_VALUE	\
+	(AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_SET_160MS << AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_SET_640MS	(1)
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_SET_640MS_VALUE	\
+	(AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_SET_640MS << AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_DOWNSIGNAL_SEL_START_BIT)
+
+/* BST_SLOPE_LIMIT bit 5:3 (DFTDR 0x72) */
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_BITS_LEN	(3)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0SLOPE	(0)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P25SLOPE	(1)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P25SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P25SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P5SLOPE	(2)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P5SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P5SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P25SLOPE	(3)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P25SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P25SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P5SLOPE	(4)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P5SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P5SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P75SLOPE	(5)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P75SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_0P75SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P25SLOPE	(6)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P25SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P25SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P75SLOPE	(7)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P75SLOPE_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_1P75SLOPE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_SLOPE_LIMIT_START_BIT)
+
+/* MODEL_START_DELAY bit 2:1 (DFTDR 0x72) */
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_MODEL_START_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_MODEL_START_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_20US	(0)
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_20US_VALUE	\
+	(AW87XXX_PID_5A_REG_MODEL_START_DELAY_20US << AW87XXX_PID_5A_REG_MODEL_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_0US	(1)
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_0US_VALUE	\
+	(AW87XXX_PID_5A_REG_MODEL_START_DELAY_0US << AW87XXX_PID_5A_REG_MODEL_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_5US	(2)
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_5US_VALUE	\
+	(AW87XXX_PID_5A_REG_MODEL_START_DELAY_5US << AW87XXX_PID_5A_REG_MODEL_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_2P5US	(3)
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_2P5US_VALUE	\
+	(AW87XXX_PID_5A_REG_MODEL_START_DELAY_2P5US << AW87XXX_PID_5A_REG_MODEL_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_MODEL_START_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_MODEL_START_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_MODEL_START_DELAY_START_BIT)
+
+/* PEAK_LIMIT_SS_CAP bit 0 (DFTDR 0x72) */
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_350FF	(0)
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_350FF_VALUE	\
+	(AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_350FF << AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_0FF	(1)
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_0FF_VALUE	\
+	(AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_0FF << AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PEAK_LIMIT_SS_CAP_START_BIT)
+
+/* default value of DFTDR (0x72) */
+/* #define AW87XXX_PID_5A_REG_DFTDR_DEFAULT		(0xA0) */
+
+/* DFTER (0x73) detail */
+/* BST_SS_TIME bit 7:6 (DFTER 0x73) */
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_BST_SS_TIME_BITS_LEN)-1) << AW87XXX_PID_5A_REG_BST_SS_TIME_START_BIT))
+
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_35US	(0)
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_35US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SS_TIME_35US << AW87XXX_PID_5A_REG_BST_SS_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_56US	(1)
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_56US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SS_TIME_56US << AW87XXX_PID_5A_REG_BST_SS_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_76US	(2)
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_76US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SS_TIME_76US << AW87XXX_PID_5A_REG_BST_SS_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_107US	(3)
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_107US_VALUE	\
+	(AW87XXX_PID_5A_REG_BST_SS_TIME_107US << AW87XXX_PID_5A_REG_BST_SS_TIME_START_BIT)
+
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_BST_SS_TIME_DEFAULT	\
+	(AW87XXX_PID_5A_REG_BST_SS_TIME_DEFAULT_VALUE << AW87XXX_PID_5A_REG_BST_SS_TIME_START_BIT)
+
+/* PD_UVLO bit 5 (DFTER 0x73) */
+#define AW87XXX_PID_5A_REG_PD_UVLO_START_BIT	(5)
+#define AW87XXX_PID_5A_REG_PD_UVLO_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PD_UVLO_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PD_UVLO_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PD_UVLO_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PD_UVLO_ENABLE	(0)
+#define AW87XXX_PID_5A_REG_PD_UVLO_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_UVLO_ENABLE << AW87XXX_PID_5A_REG_PD_UVLO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_UVLO_DISABLE	(1)
+#define AW87XXX_PID_5A_REG_PD_UVLO_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_UVLO_DISABLE << AW87XXX_PID_5A_REG_PD_UVLO_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_UVLO_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PD_UVLO_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PD_UVLO_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PD_UVLO_START_BIT)
+
+/* UVLO_VTH bit 4:3 (DFTER 0x73) */
+#define AW87XXX_PID_5A_REG_UVLO_VTH_START_BIT	(3)
+#define AW87XXX_PID_5A_REG_UVLO_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_UVLO_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_UVLO_VTH_BITS_LEN)-1) << AW87XXX_PID_5A_REG_UVLO_VTH_START_BIT))
+
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P6V_VL2P5V	(0)
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P6V_VL2P5V_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_VTH_VH2P6V_VL2P5V << AW87XXX_PID_5A_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P7V_VL2P6V	(1)
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P7V_VL2P6V_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_VTH_VH2P7V_VL2P6V << AW87XXX_PID_5A_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P8V_VL2P7V	(2)
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P8V_VL2P7V_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_VTH_VH2P8V_VL2P7V << AW87XXX_PID_5A_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P9V_VL2P8V	(3)
+#define AW87XXX_PID_5A_REG_UVLO_VTH_VH2P9V_VL2P8V_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_VTH_VH2P9V_VL2P8V << AW87XXX_PID_5A_REG_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_VTH_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_5A_REG_UVLO_VTH_DEFAULT	\
+	(AW87XXX_PID_5A_REG_UVLO_VTH_DEFAULT_VALUE << AW87XXX_PID_5A_REG_UVLO_VTH_START_BIT)
+
+/* UVLO_DT bit 2 (DFTER 0x73) */
+#define AW87XXX_PID_5A_REG_UVLO_DT_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_UVLO_DT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_UVLO_DT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_UVLO_DT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_UVLO_DT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_UVLO_DT_3US	(0)
+#define AW87XXX_PID_5A_REG_UVLO_DT_3US_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_DT_3US << AW87XXX_PID_5A_REG_UVLO_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_DT_10US	(1)
+#define AW87XXX_PID_5A_REG_UVLO_DT_10US_VALUE	\
+	(AW87XXX_PID_5A_REG_UVLO_DT_10US << AW87XXX_PID_5A_REG_UVLO_DT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_UVLO_DT_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_5A_REG_UVLO_DT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_UVLO_DT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_UVLO_DT_START_BIT)
+
+/* OC_DISABLE bit 1 (DFTER 0x73) */
+#define AW87XXX_PID_5A_REG_OC_DISABLE_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_OC_DISABLE_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_OC_DISABLE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_OC_DISABLE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_OC_DISABLE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_OC_DISABLE_ENABLE	(0)
+#define AW87XXX_PID_5A_REG_OC_DISABLE_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_OC_DISABLE_ENABLE << AW87XXX_PID_5A_REG_OC_DISABLE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_OC_DISABLE_SHUTDOWN	(1)
+#define AW87XXX_PID_5A_REG_OC_DISABLE_SHUTDOWN_VALUE	\
+	(AW87XXX_PID_5A_REG_OC_DISABLE_SHUTDOWN << AW87XXX_PID_5A_REG_OC_DISABLE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_OC_DISABLE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_OC_DISABLE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_OC_DISABLE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_OC_DISABLE_START_BIT)
+
+/* PD_OT bit 0 (DFTER 0x73) */
+#define AW87XXX_PID_5A_REG_PD_OT_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_PD_OT_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_PD_OT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PD_OT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PD_OT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PD_OT_ENABLE	(0)
+#define AW87XXX_PID_5A_REG_PD_OT_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_OT_ENABLE << AW87XXX_PID_5A_REG_PD_OT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_OT_SHUTDOWN	(1)
+#define AW87XXX_PID_5A_REG_PD_OT_SHUTDOWN_VALUE	\
+	(AW87XXX_PID_5A_REG_PD_OT_SHUTDOWN << AW87XXX_PID_5A_REG_PD_OT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PD_OT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PD_OT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PD_OT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PD_OT_START_BIT)
+
+/* default value of DFTER (0x73) */
+/* #define AW87XXX_PID_5A_REG_DFTER_DEFAULT		(0x54) */
+
+/* DFTFR (0x74) detail */
+/* EN_SWF bit 6 (DFTFR 0x74) */
+#define AW87XXX_PID_5A_REG_EN_SWF_START_BIT	(6)
+#define AW87XXX_PID_5A_REG_EN_SWF_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_SWF_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_SWF_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_SWF_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_SWF_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_SWF_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_SWF_DISABLE << AW87XXX_PID_5A_REG_EN_SWF_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_SWF_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_SWF_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_SWF_ENABLE << AW87XXX_PID_5A_REG_EN_SWF_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_SWF_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_SWF_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_SWF_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_SWF_START_BIT)
+
+/* SS_CONTROL bit 5:4 (DFTFR 0x74) */
+#define AW87XXX_PID_5A_REG_SS_CONTROL_START_BIT	(4)
+#define AW87XXX_PID_5A_REG_SS_CONTROL_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_SS_CONTROL_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_SS_CONTROL_BITS_LEN)-1) << AW87XXX_PID_5A_REG_SS_CONTROL_START_BIT))
+
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SPREAD_SPECTRUM	(0)
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SPREAD_SPECTRUM_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_CONTROL_SPREAD_SPECTRUM << AW87XXX_PID_5A_REG_SS_CONTROL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SW20111	(1)
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SW20111_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_CONTROL_SW20111 << AW87XXX_PID_5A_REG_SS_CONTROL_START_BIT)
+
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SW20000	(2)
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SW20000_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_CONTROL_SW20000 << AW87XXX_PID_5A_REG_SS_CONTROL_START_BIT)
+/*
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SW20111	(3)
+#define AW87XXX_PID_5A_REG_SS_CONTROL_SW20111_VALUE	\
+	(AW87XXX_PID_5A_REG_SS_CONTROL_SW20111 << AW87XXX_PID_5A_REG_SS_CONTROL_START_BIT)
+*/
+#define AW87XXX_PID_5A_REG_SS_CONTROL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_SS_CONTROL_DEFAULT	\
+	(AW87XXX_PID_5A_REG_SS_CONTROL_DEFAULT_VALUE << AW87XXX_PID_5A_REG_SS_CONTROL_START_BIT)
+
+/* PA_GTDR_DDT bit 3:2 (DFTFR 0x74) */
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_START_BIT	(2)
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_BITS_LEN	(2)
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_PA_GTDR_DDT_BITS_LEN)-1) << AW87XXX_PID_5A_REG_PA_GTDR_DDT_START_BIT))
+
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_12NS	(0)
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_12NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GTDR_DDT_12NS << AW87XXX_PID_5A_REG_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_13NS	(1)
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_13NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GTDR_DDT_13NS << AW87XXX_PID_5A_REG_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_14NS	(2)
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_14NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GTDR_DDT_14NS << AW87XXX_PID_5A_REG_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_15NS	(3)
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_15NS_VALUE	\
+	(AW87XXX_PID_5A_REG_PA_GTDR_DDT_15NS << AW87XXX_PID_5A_REG_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_PA_GTDR_DDT_DEFAULT	\
+	(AW87XXX_PID_5A_REG_PA_GTDR_DDT_DEFAULT_VALUE << AW87XXX_PID_5A_REG_PA_GTDR_DDT_START_BIT)
+
+/* EN_HWM_DELAY bit 1 (DFTFR 0x74) */
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_START_BIT	(1)
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_HWM_DELAY_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_HWM_DELAY_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_HWM_DELAY_DISABLE << AW87XXX_PID_5A_REG_EN_HWM_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_HWM_DELAY_ENABLE << AW87XXX_PID_5A_REG_EN_HWM_DELAY_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_HWM_DELAY_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_HWM_DELAY_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_HWM_DELAY_START_BIT)
+
+/* EN_HW_MODE bit 0 (DFTFR 0x74) */
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_START_BIT	(0)
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_5A_REG_EN_HW_MODE_BITS_LEN)-1) << AW87XXX_PID_5A_REG_EN_HW_MODE_START_BIT))
+
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_DISABLE	(0)
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_HW_MODE_DISABLE << AW87XXX_PID_5A_REG_EN_HW_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_ENABLE	(1)
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_5A_REG_EN_HW_MODE_ENABLE << AW87XXX_PID_5A_REG_EN_HW_MODE_START_BIT)
+
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_5A_REG_EN_HW_MODE_DEFAULT	\
+	(AW87XXX_PID_5A_REG_EN_HW_MODE_DEFAULT_VALUE << AW87XXX_PID_5A_REG_EN_HW_MODE_START_BIT)
+
+/* default value of DFTFR (0x74) */
+/* #define AW87XXX_PID_5A_REG_DFTFR_DEFAULT		(0x00) */
+
+/* detail information of registers end */
+
+#endif  /* #ifndef  __AW87XXX_PID_5A_REG_H__ */
\ No newline at end of file
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_60_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_60_reg.h
new file mode 100644
index 000000000000..ba47dd1ff546
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_60_reg.h
@@ -0,0 +1,5246 @@
+#ifndef __AW87XXX_PID_60_REG_H__
+#define __AW87XXX_PID_60_REG_H__
+
+/* registers list */
+#define AW87XXX_PID_60_ID_REG			(0x00)
+#define AW87XXX_PID_60_SYSCTRL_REG		(0x01)
+#define AW87XXX_PID_60_BSTOVR_REG		(0x02)
+#define AW87XXX_PID_60_PEAKLIMIT_REG		(0x03)
+#define AW87XXX_PID_60_ADPSET_REG		(0x04)
+#define AW87XXX_PID_60_PAG_REG			(0x05)
+#define AW87XXX_PID_60_AGC1PA_REG		(0x06)
+#define AW87XXX_PID_60_AGC2PA_REG		(0x07)
+#define AW87XXX_PID_60_AGC3PA_REG		(0x08)
+#define AW87XXX_PID_60_AGC3P_REG		(0x09)
+#define AW87XXX_PID_60_LOW_BAT_REG		(0x0A)
+#define AW87XXX_PID_60_BSTOUT_REG		(0x0B)
+#define AW87XXX_PID_60_SYSST_REG		(0x59)
+#define AW87XXX_PID_60_SYSINT_REG		(0x60)
+#define AW87XXX_PID_60_BURST_CON_REG		(0x61)
+#define AW87XXX_PID_60_BST_BIAS_REG		(0x62)
+#define AW87XXX_PID_60_BST_EA_REG		(0x63)
+#define AW87XXX_PID_60_BST_DE_SOFT_REG		(0x64)
+#define AW87XXX_PID_60_BST_BURST_KICK_REG	(0x65)
+#define AW87XXX_PID_60_BST_CON1_REG		(0x66)
+#define AW87XXX_PID_60_BST_OVP_REG		(0x67)
+#define AW87XXX_PID_60_LINE_MODE_REG		(0x68)
+#define AW87XXX_PID_60_BST_ISEN_REG		(0x69)
+#define AW87XXX_PID_60_BST_PEAK_REG		(0x6A)
+#define AW87XXX_PID_60_BST_PEAK2_REG		(0x6B)
+#define AW87XXX_PID_60_OFFTIME_REG		(0x6C)
+#define AW87XXX_PID_60_ADPBST_REG		(0x6D)
+#define AW87XXX_PID_60_OTA_REG			(0x6E)
+#define AW87XXX_PID_60_RAMPGEN_REG		(0x6F)
+#define AW87XXX_PID_60_CLASSD_SYSCTRL_REG	(0x70)
+#define AW87XXX_PID_60_GTDR_REG			(0x71)
+#define AW87XXX_PID_60_OC_REG			(0x72)
+#define AW87XXX_PID_60_AGC_CON_REG		(0x73)
+#define AW87XXX_PID_60_NG_REG			(0x74)
+#define AW87XXX_PID_60_NG2_REG			(0x75)
+#define AW87XXX_PID_60_NG3_REG			(0x76)
+#define AW87XXX_PID_60_CP_REG			(0x77)
+#define AW87XXX_PID_60_TEST_GTDR_REG		(0x78)
+#define AW87XXX_PID_60_TEST_BST_REG		(0x79)
+#define AW87XXX_PID_60_TEST_MODE_REG		(0x7A)
+#define AW87XXX_PID_60_TEST_CON_REG		(0x7B)
+#define AW87XXX_PID_60_ENCR_REG			(0x7C)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_60_softrst_access[2] = {0x00, 0xaa};
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_60_REG_MAX			(0x7D)
+
+#define REG_NONE_ACCESS					(0)
+#define REG_RD_ACCESS					(1 << 0)
+#define REG_WR_ACCESS					(1 << 1)
+#define AW87XXX_PID_60_ESD_REG_VAL			(0x91)
+
+const unsigned char aw87xxx_pid_60_reg_access[AW87XXX_PID_60_REG_MAX] = {
+	[AW87XXX_PID_60_ID_REG]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_60_SYSCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BSTOVR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_PEAKLIMIT_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_ADPSET_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_PAG_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_AGC1PA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_AGC2PA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_AGC3PA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_AGC3P_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_LOW_BAT_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BSTOUT_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_60_SYSST_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_60_SYSINT_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_60_BURST_CON_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_BIAS_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_EA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_DE_SOFT_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_BURST_KICK_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_CON1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_OVP_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_LINE_MODE_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_ISEN_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_PEAK_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_BST_PEAK2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_OFFTIME_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_ADPBST_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_OTA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_RAMPGEN_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_CLASSD_SYSCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_GTDR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_OC_REG]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_AGC_CON_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_NG_REG]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_NG2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_NG3_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_CP_REG]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_TEST_GTDR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_TEST_BST_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_TEST_MODE_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_TEST_CON_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_60_ENCR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+};
+
+/* detail information of registers begin */
+/* ID (0x00) detail */
+/* IDCODE bit 7:0 (ID 0x00) */
+#define AW87XXX_PID_60_IDCODE_START_BIT	(0)
+#define AW87XXX_PID_60_IDCODE_BITS_LEN	(8)
+#define AW87XXX_PID_60_IDCODE_MASK		\
+	(~(((1<<AW87XXX_PID_60_IDCODE_BITS_LEN)-1) << AW87XXX_PID_60_IDCODE_START_BIT))
+
+#define AW87XXX_PID_60_IDCODE_DEFAULT_VALUE	(0x60)
+#define AW87XXX_PID_60_IDCODE_DEFAULT	\
+	(AW87XXX_PID_60_IDCODE_DEFAULT_VALUE << AW87XXX_PID_60_IDCODE_START_BIT)
+
+/* default value of ID (0x00) */
+/* #define AW87XXX_PID_60_ID_DEFAULT		(0x60) */
+
+/* SYSCTRL (0x01) detail */
+/* EN_HVBAT bit 0 (SYSCTRL 0x01) */
+#define AW87XXX_PID_60_EN_HVBAT_START_BIT	(0)
+#define AW87XXX_PID_60_EN_HVBAT_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_HVBAT_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_HVBAT_BITS_LEN)-1) << AW87XXX_PID_60_EN_HVBAT_START_BIT))
+
+#define AW87XXX_PID_60_EN_HVBAT_DISABLE	(0)
+#define AW87XXX_PID_60_EN_HVBAT_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_HVBAT_DISABLE << AW87XXX_PID_60_EN_HVBAT_START_BIT)
+
+#define AW87XXX_PID_60_EN_HVBAT_ENABLE	(1)
+#define AW87XXX_PID_60_EN_HVBAT_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_HVBAT_ENABLE << AW87XXX_PID_60_EN_HVBAT_START_BIT)
+
+#define AW87XXX_PID_60_EN_HVBAT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_HVBAT_DEFAULT	\
+	(AW87XXX_PID_60_EN_HVBAT_DEFAULT_VALUE << AW87XXX_PID_60_EN_HVBAT_START_BIT)
+
+/* RCV_MODE bit 1 (SYSCTRL 0x01) */
+#define AW87XXX_PID_60_RCV_MODE_START_BIT	(1)
+#define AW87XXX_PID_60_RCV_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_60_RCV_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_60_RCV_MODE_BITS_LEN)-1) << AW87XXX_PID_60_RCV_MODE_START_BIT))
+
+#define AW87XXX_PID_60_RCV_MODE_DISABLE	(0)
+#define AW87XXX_PID_60_RCV_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_60_RCV_MODE_DISABLE << AW87XXX_PID_60_RCV_MODE_START_BIT)
+
+#define AW87XXX_PID_60_RCV_MODE_ENABLE	(1)
+#define AW87XXX_PID_60_RCV_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_60_RCV_MODE_ENABLE << AW87XXX_PID_60_RCV_MODE_START_BIT)
+
+#define AW87XXX_PID_60_RCV_MODE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_RCV_MODE_DEFAULT	\
+	(AW87XXX_PID_60_RCV_MODE_DEFAULT_VALUE << AW87XXX_PID_60_RCV_MODE_START_BIT)
+
+/* EN_PA bit 2 (SYSCTRL 0x01) */
+#define AW87XXX_PID_60_EN_PA_START_BIT	(2)
+#define AW87XXX_PID_60_EN_PA_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_PA_MASK		\
+	(~(((1<<AW87XXX_PID_60_EN_PA_BITS_LEN)-1) << AW87XXX_PID_60_EN_PA_START_BIT))
+
+#define AW87XXX_PID_60_EN_PA_DISABLE	(0)
+#define AW87XXX_PID_60_EN_PA_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_PA_DISABLE << AW87XXX_PID_60_EN_PA_START_BIT)
+
+#define AW87XXX_PID_60_EN_PA_ENABLE		(1)
+#define AW87XXX_PID_60_EN_PA_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_PA_ENABLE << AW87XXX_PID_60_EN_PA_START_BIT)
+
+#define AW87XXX_PID_60_EN_PA_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_EN_PA_DEFAULT	\
+	(AW87XXX_PID_60_EN_PA_DEFAULT_VALUE << AW87XXX_PID_60_EN_PA_START_BIT)
+
+/* EN_BOOST bit 3 (SYSCTRL 0x01) */
+#define AW87XXX_PID_60_EN_BOOST_START_BIT	(3)
+#define AW87XXX_PID_60_EN_BOOST_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_BOOST_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_BOOST_BITS_LEN)-1) << AW87XXX_PID_60_EN_BOOST_START_BIT))
+
+#define AW87XXX_PID_60_EN_BOOST_DISABLE	(0)
+#define AW87XXX_PID_60_EN_BOOST_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_BOOST_DISABLE << AW87XXX_PID_60_EN_BOOST_START_BIT)
+
+#define AW87XXX_PID_60_EN_BOOST_ENABLE	(1)
+#define AW87XXX_PID_60_EN_BOOST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_BOOST_ENABLE << AW87XXX_PID_60_EN_BOOST_START_BIT)
+
+#define AW87XXX_PID_60_EN_BOOST_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_EN_BOOST_DEFAULT	\
+	(AW87XXX_PID_60_EN_BOOST_DEFAULT_VALUE << AW87XXX_PID_60_EN_BOOST_START_BIT)
+
+/* EN_CP bit 4 (SYSCTRL 0x01) */
+#define AW87XXX_PID_60_EN_CP_START_BIT	(4)
+#define AW87XXX_PID_60_EN_CP_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_CP_MASK		\
+	(~(((1<<AW87XXX_PID_60_EN_CP_BITS_LEN)-1) << AW87XXX_PID_60_EN_CP_START_BIT))
+
+#define AW87XXX_PID_60_EN_CP_DISABLE	(0)
+#define AW87XXX_PID_60_EN_CP_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_CP_DISABLE << AW87XXX_PID_60_EN_CP_START_BIT)
+
+#define AW87XXX_PID_60_EN_CP_ENABLE		(1)
+#define AW87XXX_PID_60_EN_CP_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_CP_ENABLE << AW87XXX_PID_60_EN_CP_START_BIT)
+
+#define AW87XXX_PID_60_EN_CP_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_EN_CP_DEFAULT	\
+	(AW87XXX_PID_60_EN_CP_DEFAULT_VALUE << AW87XXX_PID_60_EN_CP_START_BIT)
+
+/* EN_SW bit 5 (SYSCTRL 0x01) */
+#define AW87XXX_PID_60_EN_SW_START_BIT	(5)
+#define AW87XXX_PID_60_EN_SW_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_SW_MASK		\
+	(~(((1<<AW87XXX_PID_60_EN_SW_BITS_LEN)-1) << AW87XXX_PID_60_EN_SW_START_BIT))
+
+#define AW87XXX_PID_60_EN_SW_DISABLE	(0)
+#define AW87XXX_PID_60_EN_SW_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_SW_DISABLE << AW87XXX_PID_60_EN_SW_START_BIT)
+
+#define AW87XXX_PID_60_EN_SW_ENABLE		(1)
+#define AW87XXX_PID_60_EN_SW_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_SW_ENABLE << AW87XXX_PID_60_EN_SW_START_BIT)
+
+#define AW87XXX_PID_60_EN_SW_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_SW_DEFAULT	\
+	(AW87XXX_PID_60_EN_SW_DEFAULT_VALUE << AW87XXX_PID_60_EN_SW_START_BIT)
+
+/* default value of SYSCTRL (0x01) */
+/* #define AW87XXX_PID_60_SYSCTRL_DEFAULT		(0x1C) */
+
+/* BSTOVR (0x02) detail */
+/* CP_FREQ bit 6:5 (BSTOVR 0x02) */
+#define AW87XXX_PID_60_CP_FREQ_START_BIT	(5)
+#define AW87XXX_PID_60_CP_FREQ_BITS_LEN	(2)
+#define AW87XXX_PID_60_CP_FREQ_MASK		\
+	(~(((1<<AW87XXX_PID_60_CP_FREQ_BITS_LEN)-1) << AW87XXX_PID_60_CP_FREQ_START_BIT))
+
+#define AW87XXX_PID_60_CP_FREQ_4P8MHZ	(0)
+#define AW87XXX_PID_60_CP_FREQ_4P8MHZ_VALUE	\
+	(AW87XXX_PID_60_CP_FREQ_4P8MHZ << AW87XXX_PID_60_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_60_CP_FREQ_6P4MHZ	(1)
+#define AW87XXX_PID_60_CP_FREQ_6P4MHZ_VALUE	\
+	(AW87XXX_PID_60_CP_FREQ_6P4MHZ << AW87XXX_PID_60_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_60_CP_FREQ_8P0MHZ	(2)
+#define AW87XXX_PID_60_CP_FREQ_8P0MHZ_VALUE	\
+	(AW87XXX_PID_60_CP_FREQ_8P0MHZ << AW87XXX_PID_60_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_60_CP_FREQ_9P6MHZ	(3)
+#define AW87XXX_PID_60_CP_FREQ_9P6MHZ_VALUE	\
+	(AW87XXX_PID_60_CP_FREQ_9P6MHZ << AW87XXX_PID_60_CP_FREQ_START_BIT)
+
+#define AW87XXX_PID_60_CP_FREQ_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_CP_FREQ_DEFAULT	\
+	(AW87XXX_PID_60_CP_FREQ_DEFAULT_VALUE << AW87XXX_PID_60_CP_FREQ_START_BIT)
+
+/* BST_VOUT bit 4:0 (BSTOVR 0x02) */
+#define AW87XXX_PID_60_BST_VOUT_START_BIT	(0)
+#define AW87XXX_PID_60_BST_VOUT_BITS_LEN	(5)
+#define AW87XXX_PID_60_BST_VOUT_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_VOUT_BITS_LEN)-1) << AW87XXX_PID_60_BST_VOUT_START_BIT))
+
+#define AW87XXX_PID_60_BST_VOUT_4P75V	(0)
+#define AW87XXX_PID_60_BST_VOUT_4P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_4P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_5P0V	(1)
+#define AW87XXX_PID_60_BST_VOUT_5P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_5P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_5P25V	(2)
+#define AW87XXX_PID_60_BST_VOUT_5P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_5P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_5P5V	(3)
+#define AW87XXX_PID_60_BST_VOUT_5P5V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_5P5V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_5P75V	(4)
+#define AW87XXX_PID_60_BST_VOUT_5P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_5P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_6P0V	(5)
+#define AW87XXX_PID_60_BST_VOUT_6P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_6P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_6P25V	(6)
+#define AW87XXX_PID_60_BST_VOUT_6P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_6P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_6P5V	(7)
+#define AW87XXX_PID_60_BST_VOUT_6P5V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_6P5V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_6P75V	(8)
+#define AW87XXX_PID_60_BST_VOUT_6P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_6P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_7P0V	(9)
+#define AW87XXX_PID_60_BST_VOUT_7P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_7P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_7P25V	(10)
+#define AW87XXX_PID_60_BST_VOUT_7P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_7P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_7P5V	(11)
+#define AW87XXX_PID_60_BST_VOUT_7P5V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_7P5V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_7P75V	(12)
+#define AW87XXX_PID_60_BST_VOUT_7P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_7P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_8P0V	(13)
+#define AW87XXX_PID_60_BST_VOUT_8P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_8P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_8P25V	(14)
+#define AW87XXX_PID_60_BST_VOUT_8P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_8P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_8P5V	(15)
+#define AW87XXX_PID_60_BST_VOUT_8P5V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_8P5V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_8P75V	(16)
+#define AW87XXX_PID_60_BST_VOUT_8P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_8P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_9P0V	(17)
+#define AW87XXX_PID_60_BST_VOUT_9P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_9P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_9P25V	(18)
+#define AW87XXX_PID_60_BST_VOUT_9P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_9P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_9P5V	(19)
+#define AW87XXX_PID_60_BST_VOUT_9P5V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_9P5V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_9P75V	(20)
+#define AW87XXX_PID_60_BST_VOUT_9P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_9P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_10P0V	(21)
+#define AW87XXX_PID_60_BST_VOUT_10P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_10P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_10P25V	(22)
+#define AW87XXX_PID_60_BST_VOUT_10P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_10P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_10P5V	(23)
+#define AW87XXX_PID_60_BST_VOUT_10P5V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_10P5V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_10P75V	(24)
+#define AW87XXX_PID_60_BST_VOUT_10P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_10P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_11P0V	(25)
+#define AW87XXX_PID_60_BST_VOUT_11P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_11P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_11P25V	(26)
+#define AW87XXX_PID_60_BST_VOUT_11P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_11P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_11P5V	(27)
+#define AW87XXX_PID_60_BST_VOUT_11P5V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_11P5V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_11P75V	(28)
+#define AW87XXX_PID_60_BST_VOUT_11P75V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_11P75V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_12P0V	(29)
+#define AW87XXX_PID_60_BST_VOUT_12P0V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_12P0V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_12P25V	(30)
+#define AW87XXX_PID_60_BST_VOUT_12P25V_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_12P25V << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_DEFAULT_VALUE	(0x11)
+#define AW87XXX_PID_60_BST_VOUT_DEFAULT	\
+	(AW87XXX_PID_60_BST_VOUT_DEFAULT_VALUE << AW87XXX_PID_60_BST_VOUT_START_BIT)
+
+/* default value of BSTOVR (0x02) */
+/* #define AW87XXX_PID_60_BSTOVR_DEFAULT		(0x31) */
+
+/* PEAKLIMIT (0x03) detail */
+/* BST_OVP2_VTH bit 6:4 (PEAKLIMIT 0x03) */
+#define AW87XXX_PID_60_BST_OVP2_VTH_START_BIT	(4)
+#define AW87XXX_PID_60_BST_OVP2_VTH_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_OVP2_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP2_VTH_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_13P5V	(0)
+#define AW87XXX_PID_60_BST_OVP2_VTH_13P5V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_13P5V << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_14P0	(1)
+#define AW87XXX_PID_60_BST_OVP2_VTH_14P0_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_14P0 << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_14P5V	(2)
+#define AW87XXX_PID_60_BST_OVP2_VTH_14P5V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_14P5V << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_15P0V	(3)
+#define AW87XXX_PID_60_BST_OVP2_VTH_15P0V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_15P0V << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_9P0V	(4)
+#define AW87XXX_PID_60_BST_OVP2_VTH_9P0V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_9P0V << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_9P5V	(5)
+#define AW87XXX_PID_60_BST_OVP2_VTH_9P5V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_9P5V << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_10P0V	(6)
+#define AW87XXX_PID_60_BST_OVP2_VTH_10P0V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_10P0V << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_10P5V	(7)
+#define AW87XXX_PID_60_BST_OVP2_VTH_10P5V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_10P5V << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_VTH_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_OVP2_VTH_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP2_VTH_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP2_VTH_START_BIT)
+
+/* BST_IPEAK bit 3:0 (PEAKLIMIT 0x03) */
+#define AW87XXX_PID_60_BST_IPEAK_START_BIT	(0)
+#define AW87XXX_PID_60_BST_IPEAK_BITS_LEN	(4)
+#define AW87XXX_PID_60_BST_IPEAK_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_IPEAK_BITS_LEN)-1) << AW87XXX_PID_60_BST_IPEAK_START_BIT))
+
+#define AW87XXX_PID_60_BST_IPEAK_1P5A	(0)
+#define AW87XXX_PID_60_BST_IPEAK_1P5A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_1P5A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_1P75A	(1)
+#define AW87XXX_PID_60_BST_IPEAK_1P75A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_1P75A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_2P0A	(2)
+#define AW87XXX_PID_60_BST_IPEAK_2P0A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_2P0A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_2P25A	(3)
+#define AW87XXX_PID_60_BST_IPEAK_2P25A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_2P25A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_2P5A	(4)
+#define AW87XXX_PID_60_BST_IPEAK_2P5A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_2P5A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_2P75A	(5)
+#define AW87XXX_PID_60_BST_IPEAK_2P75A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_2P75A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_3P0A	(6)
+#define AW87XXX_PID_60_BST_IPEAK_3P0A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_3P0A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_3P25A	(7)
+#define AW87XXX_PID_60_BST_IPEAK_3P25A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_3P25A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_3P5A	(8)
+#define AW87XXX_PID_60_BST_IPEAK_3P5A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_3P5A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_3P75A	(9)
+#define AW87XXX_PID_60_BST_IPEAK_3P75A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_3P75A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_4A		(10)
+#define AW87XXX_PID_60_BST_IPEAK_4A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_4A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_4P25A	(11)
+#define AW87XXX_PID_60_BST_IPEAK_4P25A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_4P25A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_4P50A	(12)
+#define AW87XXX_PID_60_BST_IPEAK_4P50A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_4P50A << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_DEFAULT_VALUE	(9)
+#define AW87XXX_PID_60_BST_IPEAK_DEFAULT	\
+	(AW87XXX_PID_60_BST_IPEAK_DEFAULT_VALUE << AW87XXX_PID_60_BST_IPEAK_START_BIT)
+
+/* default value of PEAKLIMIT (0x03) */
+/* #define AW87XXX_PID_60_PEAKLIMIT_DEFAULT		(0x09) */
+
+/* ADPSET (0x04) detail */
+/* EN_ADP_BST bit 6 (ADPSET 0x04) */
+#define AW87XXX_PID_60_EN_ADP_BST_START_BIT	(6)
+#define AW87XXX_PID_60_EN_ADP_BST_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_ADP_BST_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_ADP_BST_BITS_LEN)-1) << AW87XXX_PID_60_EN_ADP_BST_START_BIT))
+
+#define AW87XXX_PID_60_EN_ADP_BST_DISABLE	(0)
+#define AW87XXX_PID_60_EN_ADP_BST_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_ADP_BST_DISABLE << AW87XXX_PID_60_EN_ADP_BST_START_BIT)
+
+#define AW87XXX_PID_60_EN_ADP_BST_ENABLE	(1)
+#define AW87XXX_PID_60_EN_ADP_BST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_ADP_BST_ENABLE << AW87XXX_PID_60_EN_ADP_BST_START_BIT)
+
+#define AW87XXX_PID_60_EN_ADP_BST_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_EN_ADP_BST_DEFAULT	\
+	(AW87XXX_PID_60_EN_ADP_BST_DEFAULT_VALUE << AW87XXX_PID_60_EN_ADP_BST_START_BIT)
+
+/* ADP_BOOST_MODE bit 5:3 (ADPSET 0x04) */
+#define AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT	(3)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_BITS_LEN	(3)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_60_ADP_BOOST_MODE_BITS_LEN)-1) << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT))
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_RCV	(0)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_RCV_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_RCV << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_FORCE_BOOST	(1)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_FORCE_BOOST_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_FORCE_BOOST << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_OSBOSD	(2)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_OSBOSD_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_OSBOSD << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_TSBTSD	(3)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_TSBTSD_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_TSBTSD << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_TSBOSD	(4)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_TSBOSD_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_TSBOSD << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_MSBOSD	(5)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_MSBOSD_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_MSBOSD << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_MSBTSD	(6)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_MSBTSD_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_MSBTSD << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_MSBMSD	(7)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_MSBMSD_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_MSBMSD << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_MODE_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_ADP_BOOST_MODE_DEFAULT	\
+	(AW87XXX_PID_60_ADP_BOOST_MODE_DEFAULT_VALUE << AW87XXX_PID_60_ADP_BOOST_MODE_START_BIT)
+
+/* SET_BOOST_VTH2 bit 2:0 (ADPSET 0x04) */
+#define AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT	(0)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_BITS_LEN	(3)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_MASK	\
+	(~(((1<<AW87XXX_PID_60_SET_BOOST_VTH2_BITS_LEN)-1) << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT))
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P2W	(0)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P2W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_1P2W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P4W	(1)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P4W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_1P4W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P6W	(2)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P6W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_1P6W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P8W	(3)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_1P8W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_1P8W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P0W	(4)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P0W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_2P0W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P2W	(5)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P2W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_2P2W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P4W	(6)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P4W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_2P4W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P6W	(7)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_2P6W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_2P6W << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH2_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_60_SET_BOOST_VTH2_DEFAULT	\
+	(AW87XXX_PID_60_SET_BOOST_VTH2_DEFAULT_VALUE << AW87XXX_PID_60_SET_BOOST_VTH2_START_BIT)
+
+/* default value of ADPSET (0x04) */
+/* #define AW87XXX_PID_60_ADPSET_DEFAULT		(0x54) */
+
+/* PAG (0x05) detail */
+/* SET_BOOST_VTH1 bit 6:5 (PAG 0x05) */
+#define AW87XXX_PID_60_SET_BOOST_VTH1_START_BIT	(5)
+#define AW87XXX_PID_60_SET_BOOST_VTH1_BITS_LEN	(2)
+#define AW87XXX_PID_60_SET_BOOST_VTH1_MASK	\
+	(~(((1<<AW87XXX_PID_60_SET_BOOST_VTH1_BITS_LEN)-1) << AW87XXX_PID_60_SET_BOOST_VTH1_START_BIT))
+
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P1W	(0)
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P1W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH1_0P1W << AW87XXX_PID_60_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P2W	(1)
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P2W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH1_0P2W << AW87XXX_PID_60_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P3W	(2)
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P3W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH1_0P3W << AW87XXX_PID_60_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P4W	(3)
+#define AW87XXX_PID_60_SET_BOOST_VTH1_0P4W_VALUE	\
+	(AW87XXX_PID_60_SET_BOOST_VTH1_0P4W << AW87XXX_PID_60_SET_BOOST_VTH1_START_BIT)
+
+#define AW87XXX_PID_60_SET_BOOST_VTH1_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_SET_BOOST_VTH1_DEFAULT	\
+	(AW87XXX_PID_60_SET_BOOST_VTH1_DEFAULT_VALUE << AW87XXX_PID_60_SET_BOOST_VTH1_START_BIT)
+
+/* PA_GAIN bit 4:0 (PAG 0x05) */
+#define AW87XXX_PID_60_PA_GAIN_START_BIT	(0)
+#define AW87XXX_PID_60_PA_GAIN_BITS_LEN	(5)
+#define AW87XXX_PID_60_PA_GAIN_MASK		\
+	(~(((1<<AW87XXX_PID_60_PA_GAIN_BITS_LEN)-1) << AW87XXX_PID_60_PA_GAIN_START_BIT))
+
+#define AW87XXX_PID_60_PA_GAIN_0DB		(0)
+#define AW87XXX_PID_60_PA_GAIN_0DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_0DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_1P5DB	(1)
+#define AW87XXX_PID_60_PA_GAIN_1P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_1P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_3DB		(2)
+#define AW87XXX_PID_60_PA_GAIN_3DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_3DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_4P5DB	(3)
+#define AW87XXX_PID_60_PA_GAIN_4P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_4P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_6DB		(4)
+#define AW87XXX_PID_60_PA_GAIN_6DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_6DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_7P5DB	(5)
+#define AW87XXX_PID_60_PA_GAIN_7P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_7P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_9DB		(6)
+#define AW87XXX_PID_60_PA_GAIN_9DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_9DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_10P5DB	(7)
+#define AW87XXX_PID_60_PA_GAIN_10P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_10P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_12DB		(8)
+#define AW87XXX_PID_60_PA_GAIN_12DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_12DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_13P5DB	(9)
+#define AW87XXX_PID_60_PA_GAIN_13P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_13P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_15DB		(10)
+#define AW87XXX_PID_60_PA_GAIN_15DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_15DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_16P5DB	(11)
+#define AW87XXX_PID_60_PA_GAIN_16P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_16P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_18DB		(12)
+#define AW87XXX_PID_60_PA_GAIN_18DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_18DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_19P5DB	(13)
+#define AW87XXX_PID_60_PA_GAIN_19P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_19P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_21DB		(14)
+#define AW87XXX_PID_60_PA_GAIN_21DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_21DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_22P5DB	(15)
+#define AW87XXX_PID_60_PA_GAIN_22P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_22P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_24DB		(16)
+#define AW87XXX_PID_60_PA_GAIN_24DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_24DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_25P5DB	(17)
+#define AW87XXX_PID_60_PA_GAIN_25P5DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_25P5DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_PA_GAIN_27DB		(18)
+#define AW87XXX_PID_60_PA_GAIN_27DB_VALUE	\
+	(AW87XXX_PID_60_PA_GAIN_27DB << AW87XXX_PID_60_PA_GAIN_START_BIT)
+/*
+Fix me here:
+reg_addr:0x05, reg_name:PAG, field_name:PA_GAIN, content:when RCV_MODE=0��PA_GAIN  default�� 10000
+maybe need to fix manually
+*/
+#define AW87XXX_PID_60_PA_GAIN_DEFAULT_VALUE	(0x10)
+#define AW87XXX_PID_60_PA_GAIN_DEFAULT	\
+	(AW87XXX_PID_60_PA_GAIN_DEFAULT_VALUE << AW87XXX_PID_60_PA_GAIN_START_BIT)
+
+/* default value of PAG (0x05) */
+/* #define AW87XXX_PID_60_PAG_DEFAULT		(0x50) */
+
+/* AGC1PA (0x06) detail */
+/* PD_AGC1 bit 7 (AGC1PA 0x06) */
+#define AW87XXX_PID_60_PD_AGC1_START_BIT	(7)
+#define AW87XXX_PID_60_PD_AGC1_BITS_LEN	(1)
+#define AW87XXX_PID_60_PD_AGC1_MASK		\
+	(~(((1<<AW87XXX_PID_60_PD_AGC1_BITS_LEN)-1) << AW87XXX_PID_60_PD_AGC1_START_BIT))
+
+#define AW87XXX_PID_60_PD_AGC1_ENABLE	(0)
+#define AW87XXX_PID_60_PD_AGC1_ENABLE_VALUE	\
+	(AW87XXX_PID_60_PD_AGC1_ENABLE << AW87XXX_PID_60_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_60_PD_AGC1_DISABLE	(1)
+#define AW87XXX_PID_60_PD_AGC1_DISABLE_VALUE	\
+	(AW87XXX_PID_60_PD_AGC1_DISABLE << AW87XXX_PID_60_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_60_PD_AGC1_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PD_AGC1_DEFAULT	\
+	(AW87XXX_PID_60_PD_AGC1_DEFAULT_VALUE << AW87XXX_PID_60_PD_AGC1_START_BIT)
+
+/* AGC1_OUTPUT_LEVEL bit 6:3 (AGC1PA 0x06) */
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT	(3)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_BITS_LEN	(4)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_BITS_LEN)-1) << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT))
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5V	(0)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P2V	(1)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P2V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P2V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P4V	(2)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P4V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P4V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P6V	(3)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P6V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P6V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P8V	(4)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P8V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_5P8V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P0V	(5)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P0V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P0V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P2V	(6)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P2V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P2V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P4V	(7)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P4V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P4V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P6V	(8)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P6V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P6V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P8V	(9)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P8V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_6P8V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7V	(10)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P2V	(11)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P2V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P2V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P4V	(12)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P4V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P4V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P6V	(13)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P6V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P6V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P8V	(14)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P8V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_7P8V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_8V	(15)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_8V_VALUE	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_8V << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_DEFAULT_VALUE	(0x9)
+#define AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_DEFAULT	\
+	(AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_DEFAULT_VALUE << AW87XXX_PID_60_AGC1_OUTPUT_LEVEL_START_BIT)
+
+/* AGC1_ATT_TIME bit 2:0 (AGC1PA 0x06) */
+#define AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT	(0)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_BITS_LEN	(3)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC1_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P04MSDB	(0)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P04MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P04MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P08MSDB	(1)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P08MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P08MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P16MSDB	(2)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P16MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P16MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P32MSDB	(3)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P32MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P32MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P02MSDB	(4)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P02MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P02MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P01MSDB	(5)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P01MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P01MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P005MSDB	(6)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P005MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P005MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+/*
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P005MSDB	(7)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_0P005MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_0P005MSDB << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+*/
+
+#define AW87XXX_PID_60_AGC1_ATT_TIME_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_AGC1_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_60_AGC1_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_60_AGC1_ATT_TIME_START_BIT)
+
+/* default value of AGC1PA (0x06) */
+/* #define AW87XXX_PID_60_AGC1PA_DEFAULT		(0x49) */
+
+/* AGC2PA (0x07) detail */
+/* AGC2_OUTPUT_POWER bit 6:3 (AGC2PA 0x07) */
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT	(3)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_BITS_LEN	(4)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC2_OUTPUT_POWER_BITS_LEN)-1) << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT))
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P0W4_OHM	(0)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P0W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P0W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P4W4_OHM	(1)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P4W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P4W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P8W4_OHM	(2)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P8W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_2P8W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_3P2W4_OHM	(3)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_3P2W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_3P2W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_3P6W4_OHM	(4)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_3P6W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_3P6W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P0W4_OHM	(5)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P0W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P0W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P4W4_OHM	(6)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P4W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P4W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P8W4_OHM	(7)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P8W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_4P8W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_5P2W4_OHM	(8)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_5P2W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_5P2W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_5P6W4_OHM	(9)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_5P6W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_5P6W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_6W4_OHM	(10)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_6W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_6W4_OHM << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_AGC2_OFF	(11)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_AGC2_OFF_VALUE	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_AGC2_OFF << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_60_AGC2_OUTPUT_POWER_DEFAULT	\
+	(AW87XXX_PID_60_AGC2_OUTPUT_POWER_DEFAULT_VALUE << AW87XXX_PID_60_AGC2_OUTPUT_POWER_START_BIT)
+
+/* AGC2_ATT_TIME bit 2:0 (AGC2PA 0x07) */
+#define AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT	(0)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_BITS_LEN	(3)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC2_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_0P16MSDB	(0)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_0P16MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_0P16MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_0P32MSDB	(1)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_0P32MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_0P32MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_0P64MSDB	(2)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_0P64MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_0P64MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_2P56MSDB	(3)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_2P56MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_2P56MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_10P24MSDB	(4)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_10P24MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_10P24MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_40P96MSDB	(5)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_40P96MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_40P96MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_82MSDB	(6)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_82MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_82MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_164MSDB	(7)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_164MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_164MSDB << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_ATT_TIME_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_AGC2_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_60_AGC2_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_60_AGC2_ATT_TIME_START_BIT)
+
+/* default value of AGC2PA (0x07) */
+/* #define AW87XXX_PID_60_AGC2PA_DEFAULT		(0x1A) */
+
+/* AGC3PA (0x08) detail */
+/* PD_AGC3 bit 4 (AGC3PA 0x08) */
+#define AW87XXX_PID_60_PD_AGC3_START_BIT	(4)
+#define AW87XXX_PID_60_PD_AGC3_BITS_LEN	(1)
+#define AW87XXX_PID_60_PD_AGC3_MASK		\
+	(~(((1<<AW87XXX_PID_60_PD_AGC3_BITS_LEN)-1) << AW87XXX_PID_60_PD_AGC3_START_BIT))
+
+#define AW87XXX_PID_60_PD_AGC3_ENABLE	(0)
+#define AW87XXX_PID_60_PD_AGC3_ENABLE_VALUE	\
+	(AW87XXX_PID_60_PD_AGC3_ENABLE << AW87XXX_PID_60_PD_AGC3_START_BIT)
+
+#define AW87XXX_PID_60_PD_AGC3_DISABLE	(1)
+#define AW87XXX_PID_60_PD_AGC3_DISABLE_VALUE	\
+	(AW87XXX_PID_60_PD_AGC3_DISABLE << AW87XXX_PID_60_PD_AGC3_START_BIT)
+
+#define AW87XXX_PID_60_PD_AGC3_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PD_AGC3_DEFAULT	\
+	(AW87XXX_PID_60_PD_AGC3_DEFAULT_VALUE << AW87XXX_PID_60_PD_AGC3_START_BIT)
+
+/* EN_HW_MODE bit 5 (AGC3PA 0x08) */
+#define AW87XXX_PID_60_EN_HW_MODE_START_BIT	(5)
+#define AW87XXX_PID_60_EN_HW_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_HW_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_HW_MODE_BITS_LEN)-1) << AW87XXX_PID_60_EN_HW_MODE_START_BIT))
+
+#define AW87XXX_PID_60_EN_HW_MODE_DISABLE	(0)
+#define AW87XXX_PID_60_EN_HW_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_HW_MODE_DISABLE << AW87XXX_PID_60_EN_HW_MODE_START_BIT)
+
+#define AW87XXX_PID_60_EN_HW_MODE_ENABLE	(1)
+#define AW87XXX_PID_60_EN_HW_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_HW_MODE_ENABLE << AW87XXX_PID_60_EN_HW_MODE_START_BIT)
+
+#define AW87XXX_PID_60_EN_HW_MODE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_HW_MODE_DEFAULT	\
+	(AW87XXX_PID_60_EN_HW_MODE_DEFAULT_VALUE << AW87XXX_PID_60_EN_HW_MODE_START_BIT)
+
+/* AGC3_OUTPUT_POWER bit 3:0 (AGC3PA 0x08) */
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT	(0)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_BITS_LEN	(4)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC3_OUTPUT_POWER_BITS_LEN)-1) << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT))
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P0W4_OHM	(0)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P0W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P0W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P2W4_OHM	(1)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P2W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P2W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P4W4_OHM	(2)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P4W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P4W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P6W4_OHM	(3)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P6W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P6W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P8W4_OHM	(4)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P8W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_1P8W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P0W4_OHM	(5)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P0W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P0W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P2W4_OHM	(6)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P2W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P2W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P4W4_OHM	(7)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P4W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P4W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P6W4_OHM	(8)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P6W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P6W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P8W4_OHM	(9)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P8W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_2P8W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P0W4_OHM	(10)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P0W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P0W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P2W4_OHM	(11)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P2W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P2W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P4W4_OHM	(12)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P4W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P4W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P6W4_OHM	(13)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P6W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P6W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P8W4_OHM	(14)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P8W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_3P8W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_4P0W4_OHM	(15)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_4P0W4_OHM_VALUE	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_4P0W4_OHM << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_60_AGC3_OUTPUT_POWER_DEFAULT	\
+	(AW87XXX_PID_60_AGC3_OUTPUT_POWER_DEFAULT_VALUE << AW87XXX_PID_60_AGC3_OUTPUT_POWER_START_BIT)
+
+/* default value of AGC3PA (0x08) */
+/* #define AW87XXX_PID_60_AGC3PA_DEFAULT		(0x03) */
+
+/* AGC3P (0x09) detail */
+/* AGC3_REL_TIME bit 7:5 (AGC3P 0x09) */
+#define AW87XXX_PID_60_AGC3_REL_TIME_START_BIT	(5)
+#define AW87XXX_PID_60_AGC3_REL_TIME_BITS_LEN	(3)
+#define AW87XXX_PID_60_AGC3_REL_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC3_REL_TIME_BITS_LEN)-1) << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT))
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_5P12MSDB	(0)
+#define AW87XXX_PID_60_AGC3_REL_TIME_5P12MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_5P12MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_10P24MSDB	(1)
+#define AW87XXX_PID_60_AGC3_REL_TIME_10P24MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_10P24MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_20P48MSDB	(2)
+#define AW87XXX_PID_60_AGC3_REL_TIME_20P48MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_20P48MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_40P96MSDB	(3)
+#define AW87XXX_PID_60_AGC3_REL_TIME_40P96MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_40P96MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_81P92MSDB	(4)
+#define AW87XXX_PID_60_AGC3_REL_TIME_81P92MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_81P92MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_163P84MSDB	(5)
+#define AW87XXX_PID_60_AGC3_REL_TIME_163P84MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_163P84MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_327P68MSDB	(6)
+#define AW87XXX_PID_60_AGC3_REL_TIME_327P68MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_327P68MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_655P36MSDB	(7)
+#define AW87XXX_PID_60_AGC3_REL_TIME_655P36MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_655P36MSDB << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_REL_TIME_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_AGC3_REL_TIME_DEFAULT	\
+	(AW87XXX_PID_60_AGC3_REL_TIME_DEFAULT_VALUE << AW87XXX_PID_60_AGC3_REL_TIME_START_BIT)
+
+/* AGC3_ATT_TIME bit 4:2 (AGC3P 0x09) */
+#define AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT	(2)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_BITS_LEN	(3)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC3_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_1P28MSDB	(0)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_1P28MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_1P28MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_2P56MSDB	(1)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_2P56MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_2P56MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_10P24MSDB	(2)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_10P24MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_10P24MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_40P96MSDB	(3)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_40P96MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_40P96MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_82MSDB	(4)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_82MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_82MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_164MSDB	(5)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_164MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_164MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_328MSDB	(6)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_328MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_328MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_656MSDB	(7)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_656MSDB_VALUE	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_656MSDB << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_ATT_TIME_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_60_AGC3_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_60_AGC3_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_60_AGC3_ATT_TIME_START_BIT)
+
+/* AGC3_FIRST_ATT_TIME bit 1:0 (AGC3P 0x09) */
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_START_BIT	(0)
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_BITS_LEN	(2)
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_5P12MS	(0)
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_5P12MS_VALUE	\
+	(AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_5P12MS << AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_10P24MS	(1)
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_10P24MS_VALUE	\
+	(AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_10P24MS << AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_20P48MS	(2)
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_20P48MS_VALUE	\
+	(AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_20P48MS << AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_41MS	(3)
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_41MS_VALUE	\
+	(AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_41MS << AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_60_AGC3_FIRST_ATT_TIME_START_BIT)
+
+/* default value of AGC3P (0x09) */
+/* #define AW87XXX_PID_60_AGC3P_DEFAULT		(0x4E) */
+
+/* LOW_BAT (0x0A) detail */
+/* EN_BAT_SFGD bit 6 (LOW_BAT 0x0A) */
+#define AW87XXX_PID_60_EN_BAT_SFGD_START_BIT	(6)
+#define AW87XXX_PID_60_EN_BAT_SFGD_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_BAT_SFGD_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_BAT_SFGD_BITS_LEN)-1) << AW87XXX_PID_60_EN_BAT_SFGD_START_BIT))
+
+#define AW87XXX_PID_60_EN_BAT_SFGD_DISABLE	(0)
+#define AW87XXX_PID_60_EN_BAT_SFGD_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_BAT_SFGD_DISABLE << AW87XXX_PID_60_EN_BAT_SFGD_START_BIT)
+
+#define AW87XXX_PID_60_EN_BAT_SFGD_ENABLE	(1)
+#define AW87XXX_PID_60_EN_BAT_SFGD_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_BAT_SFGD_ENABLE << AW87XXX_PID_60_EN_BAT_SFGD_START_BIT)
+
+#define AW87XXX_PID_60_EN_BAT_SFGD_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_BAT_SFGD_DEFAULT	\
+	(AW87XXX_PID_60_EN_BAT_SFGD_DEFAULT_VALUE << AW87XXX_PID_60_EN_BAT_SFGD_START_BIT)
+
+/* BAT_SFGD_VTH bit 5:4 (LOW_BAT 0x0A) */
+#define AW87XXX_PID_60_BAT_SFGD_VTH_START_BIT	(4)
+#define AW87XXX_PID_60_BAT_SFGD_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_60_BAT_SFGD_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BAT_SFGD_VTH_BITS_LEN)-1) << AW87XXX_PID_60_BAT_SFGD_VTH_START_BIT))
+
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P3V	(0)
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P3V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_VTH_3P3V << AW87XXX_PID_60_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P4V	(1)
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P4V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_VTH_3P4V << AW87XXX_PID_60_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P5V	(2)
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P5V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_VTH_3P5V << AW87XXX_PID_60_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P6V	(3)
+#define AW87XXX_PID_60_BAT_SFGD_VTH_3P6V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_VTH_3P6V << AW87XXX_PID_60_BAT_SFGD_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_VTH_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BAT_SFGD_VTH_DEFAULT	\
+	(AW87XXX_PID_60_BAT_SFGD_VTH_DEFAULT_VALUE << AW87XXX_PID_60_BAT_SFGD_VTH_START_BIT)
+
+/* BAT_SFGD_LEVEL bit 3:2 (LOW_BAT 0x0A) */
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_START_BIT	(2)
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_BITS_LEN	(2)
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_BAT_SFGD_LEVEL_BITS_LEN)-1) << AW87XXX_PID_60_BAT_SFGD_LEVEL_START_BIT))
+
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_5V	(0)
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_5V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_LEVEL_5V << AW87XXX_PID_60_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_5P5V	(1)
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_5P5V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_LEVEL_5P5V << AW87XXX_PID_60_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_6V	(2)
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_6V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_LEVEL_6V << AW87XXX_PID_60_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_6P5V	(3)
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_6P5V_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_LEVEL_6P5V << AW87XXX_PID_60_BAT_SFGD_LEVEL_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_DEFAULT_VALUE	(0x01)
+#define AW87XXX_PID_60_BAT_SFGD_LEVEL_DEFAULT	\
+	(AW87XXX_PID_60_BAT_SFGD_LEVEL_DEFAULT_VALUE << AW87XXX_PID_60_BAT_SFGD_LEVEL_START_BIT)
+
+/* BAT_SFGD_DEGLITCH bit 1:0 (LOW_BAT 0x0A) */
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_START_BIT	(0)
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_BITS_LEN	(2)
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BAT_SFGD_DEGLITCH_BITS_LEN)-1) << AW87XXX_PID_60_BAT_SFGD_DEGLITCH_START_BIT))
+
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_1MS	(0)
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_1MS_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_DEGLITCH_1MS << AW87XXX_PID_60_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_500US	(1)
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_500US_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_DEGLITCH_500US << AW87XXX_PID_60_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_200US	(2)
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_200US_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_DEGLITCH_200US << AW87XXX_PID_60_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_DISABLE	(3)
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BAT_SFGD_DEGLITCH_DISABLE << AW87XXX_PID_60_BAT_SFGD_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_BAT_SFGD_DEGLITCH_DEFAULT	\
+	(AW87XXX_PID_60_BAT_SFGD_DEGLITCH_DEFAULT_VALUE << AW87XXX_PID_60_BAT_SFGD_DEGLITCH_START_BIT)
+
+/* default value of LOW_BAT (0x0A) */
+/* #define AW87XXX_PID_60_LOW_BAT_DEFAULT		(0x14) */
+
+/* BSTOUT (0x0B) detail */
+/* ADP_BOOST_VOUT bit 4:0 (BSTOUT 0x0B) */
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT	(0)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_BITS_LEN	(5)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_MASK	\
+	(~(((1<<AW87XXX_PID_60_ADP_BOOST_VOUT_BITS_LEN)-1) << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT))
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_4P75V	(0)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_4P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_4P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P0V	(1)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_5P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P25V	(2)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_5P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P5V	(3)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P5V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_5P5V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P75V	(4)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_5P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_5P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P0V	(5)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_6P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P25V	(6)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_6P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P5V	(7)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P5V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_6P5V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P75V	(8)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_6P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_6P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P0V	(9)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_7P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P25V	(10)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_7P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P5V	(11)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P5V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_7P5V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P75V	(12)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_7P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_7P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P0V	(13)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_8P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P25V	(14)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_8P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P5V	(15)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P5V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_8P5V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P75V	(16)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_8P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_8P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P0V	(17)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_9P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P25V	(18)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_9P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P5V	(19)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P5V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_9P5V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P75V	(20)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_9P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_9P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P0V	(21)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_10P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P25V	(22)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_10P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P5V	(23)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P5V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_10P5V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P75V	(24)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_10P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_10P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P0V	(25)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_11P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P25V	(26)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_11P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P5V	(27)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P5V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_11P5V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P75V	(28)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_11P75V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_11P75V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_12P0V	(29)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_12P0V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_12P0V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_12P25V	(30)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_12P25V_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_12P25V << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_DEFAULT_VALUE	(0x11)
+#define AW87XXX_PID_60_ADP_BOOST_VOUT_DEFAULT	\
+	(AW87XXX_PID_60_ADP_BOOST_VOUT_DEFAULT_VALUE << AW87XXX_PID_60_ADP_BOOST_VOUT_START_BIT)
+
+/* default value of BSTOUT (0x0B) */
+/* #define AW87XXX_PID_60_BSTOUT_DEFAULT		(0x11) */
+
+/* SYSST (0x59) detail */
+/* ADP_BOOST_S bit 0 (SYSST 0x59) */
+#define AW87XXX_PID_60_ADP_BOOST_S_START_BIT	(0)
+#define AW87XXX_PID_60_ADP_BOOST_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_ADP_BOOST_S_MASK	\
+	(~(((1<<AW87XXX_PID_60_ADP_BOOST_S_BITS_LEN)-1) << AW87XXX_PID_60_ADP_BOOST_S_START_BIT))
+
+#define AW87XXX_PID_60_ADP_BOOST_S_DIRECT_MODE	(0)
+#define AW87XXX_PID_60_ADP_BOOST_S_DIRECT_MODE_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_S_DIRECT_MODE << AW87XXX_PID_60_ADP_BOOST_S_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_S_BOOST_MODE	(1)
+#define AW87XXX_PID_60_ADP_BOOST_S_BOOST_MODE_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_S_BOOST_MODE << AW87XXX_PID_60_ADP_BOOST_S_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_ADP_BOOST_S_DEFAULT	\
+	(AW87XXX_PID_60_ADP_BOOST_S_DEFAULT_VALUE << AW87XXX_PID_60_ADP_BOOST_S_START_BIT)
+
+/* OT160_S bit 1 (SYSST 0x59) */
+#define AW87XXX_PID_60_OT160_S_START_BIT	(1)
+#define AW87XXX_PID_60_OT160_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_OT160_S_MASK		\
+	(~(((1<<AW87XXX_PID_60_OT160_S_BITS_LEN)-1) << AW87XXX_PID_60_OT160_S_START_BIT))
+
+#define AW87XXX_PID_60_OT160_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_OT160_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_OT160_S_NORMAL_OPERATION << AW87XXX_PID_60_OT160_S_START_BIT)
+
+#define AW87XXX_PID_60_OT160_S_PA_OVER_TEMPRETURE_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_OT160_S_PA_OVER_TEMPRETURE_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_OT160_S_PA_OVER_TEMPRETURE_PROTECTION_DETECTED << AW87XXX_PID_60_OT160_S_START_BIT)
+
+#define AW87XXX_PID_60_OT160_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_OT160_S_DEFAULT	\
+	(AW87XXX_PID_60_OT160_S_DEFAULT_VALUE << AW87XXX_PID_60_OT160_S_START_BIT)
+
+/* PA_OC_S bit 2 (SYSST 0x59) */
+#define AW87XXX_PID_60_PA_OC_S_START_BIT	(2)
+#define AW87XXX_PID_60_PA_OC_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_PA_OC_S_MASK		\
+	(~(((1<<AW87XXX_PID_60_PA_OC_S_BITS_LEN)-1) << AW87XXX_PID_60_PA_OC_S_START_BIT))
+
+#define AW87XXX_PID_60_PA_OC_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_PA_OC_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_PA_OC_S_NORMAL_OPERATION << AW87XXX_PID_60_PA_OC_S_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_S_PA_OVER_CURRENT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_PA_OC_S_PA_OVER_CURRENT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_PA_OC_S_PA_OVER_CURRENT_PROTECTION_DETECTED << AW87XXX_PID_60_PA_OC_S_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_PA_OC_S_DEFAULT	\
+	(AW87XXX_PID_60_PA_OC_S_DEFAULT_VALUE << AW87XXX_PID_60_PA_OC_S_START_BIT)
+
+/* BST_SCP_S bit 3 (SYSST 0x59) */
+#define AW87XXX_PID_60_BST_SCP_S_START_BIT	(3)
+#define AW87XXX_PID_60_BST_SCP_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_SCP_S_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SCP_S_BITS_LEN)-1) << AW87XXX_PID_60_BST_SCP_S_START_BIT))
+
+#define AW87XXX_PID_60_BST_SCP_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_BST_SCP_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_S_NORMAL_OPERATION << AW87XXX_PID_60_BST_SCP_S_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_S_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_BST_SCP_S_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_S_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED << AW87XXX_PID_60_BST_SCP_S_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BST_SCP_S_DEFAULT	\
+	(AW87XXX_PID_60_BST_SCP_S_DEFAULT_VALUE << AW87XXX_PID_60_BST_SCP_S_START_BIT)
+
+/* BST_OVP2_S bit 4 (SYSST 0x59) */
+#define AW87XXX_PID_60_BST_OVP2_S_START_BIT	(4)
+#define AW87XXX_PID_60_BST_OVP2_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP2_S_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP2_S_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP2_S_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP2_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_BST_OVP2_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_S_NORMAL_OPERATION << AW87XXX_PID_60_BST_OVP2_S_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_S_BOOST_HEAVY_LOAD_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_BST_OVP2_S_BOOST_HEAVY_LOAD_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_S_BOOST_HEAVY_LOAD_PROTECTION_DETECTED << AW87XXX_PID_60_BST_OVP2_S_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BST_OVP2_S_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP2_S_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP2_S_START_BIT)
+
+/* BST_OVP_S bit 5 (SYSST 0x59) */
+#define AW87XXX_PID_60_BST_OVP_S_START_BIT	(5)
+#define AW87XXX_PID_60_BST_OVP_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP_S_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP_S_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP_S_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_BST_OVP_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_S_NORMAL_OPERATION << AW87XXX_PID_60_BST_OVP_S_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_S_BOOST_OVER_VOLTAGE_PROTECTION	(1)
+#define AW87XXX_PID_60_BST_OVP_S_BOOST_OVER_VOLTAGE_PROTECTION_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_S_BOOST_OVER_VOLTAGE_PROTECTION << AW87XXX_PID_60_BST_OVP_S_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BST_OVP_S_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP_S_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP_S_START_BIT)
+
+/* LOW_BATT_S bit 6 (SYSST 0x59) */
+#define AW87XXX_PID_60_LOW_BATT_S_START_BIT	(6)
+#define AW87XXX_PID_60_LOW_BATT_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_LOW_BATT_S_MASK	\
+	(~(((1<<AW87XXX_PID_60_LOW_BATT_S_BITS_LEN)-1) << AW87XXX_PID_60_LOW_BATT_S_START_BIT))
+
+#define AW87XXX_PID_60_LOW_BATT_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_LOW_BATT_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_LOW_BATT_S_NORMAL_OPERATION << AW87XXX_PID_60_LOW_BATT_S_START_BIT)
+
+#define AW87XXX_PID_60_LOW_BATT_S_LOW_VBAT_DETECTED	(1)
+#define AW87XXX_PID_60_LOW_BATT_S_LOW_VBAT_DETECTED_VALUE	\
+	(AW87XXX_PID_60_LOW_BATT_S_LOW_VBAT_DETECTED << AW87XXX_PID_60_LOW_BATT_S_START_BIT)
+
+#define AW87XXX_PID_60_LOW_BATT_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_LOW_BATT_S_DEFAULT	\
+	(AW87XXX_PID_60_LOW_BATT_S_DEFAULT_VALUE << AW87XXX_PID_60_LOW_BATT_S_START_BIT)
+
+/* UVLO_S bit 7 (SYSST 0x59) */
+#define AW87XXX_PID_60_UVLO_S_START_BIT	(7)
+#define AW87XXX_PID_60_UVLO_S_BITS_LEN	(1)
+#define AW87XXX_PID_60_UVLO_S_MASK		\
+	(~(((1<<AW87XXX_PID_60_UVLO_S_BITS_LEN)-1) << AW87XXX_PID_60_UVLO_S_START_BIT))
+
+#define AW87XXX_PID_60_UVLO_S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_UVLO_S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_UVLO_S_NORMAL_OPERATION << AW87XXX_PID_60_UVLO_S_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_S_VBAT_UNDER_VOLTAGE	(1)
+#define AW87XXX_PID_60_UVLO_S_VBAT_UNDER_VOLTAGE_VALUE	\
+	(AW87XXX_PID_60_UVLO_S_VBAT_UNDER_VOLTAGE << AW87XXX_PID_60_UVLO_S_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_S_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_UVLO_S_DEFAULT	\
+	(AW87XXX_PID_60_UVLO_S_DEFAULT_VALUE << AW87XXX_PID_60_UVLO_S_START_BIT)
+
+/* default value of SYSST (0x59) */
+/* #define AW87XXX_PID_60_SYSST_DEFAULT		(0xFF) */
+
+/* SYSINT (0x60) detail */
+/* ADP_BOOST_I bit 0 (SYSINT 0x60) */
+#define AW87XXX_PID_60_ADP_BOOST_I_START_BIT	(0)
+#define AW87XXX_PID_60_ADP_BOOST_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_ADP_BOOST_I_MASK	\
+	(~(((1<<AW87XXX_PID_60_ADP_BOOST_I_BITS_LEN)-1) << AW87XXX_PID_60_ADP_BOOST_I_START_BIT))
+
+#define AW87XXX_PID_60_ADP_BOOST_I_DIRECT_MODE	(0)
+#define AW87XXX_PID_60_ADP_BOOST_I_DIRECT_MODE_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_I_DIRECT_MODE << AW87XXX_PID_60_ADP_BOOST_I_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_I_BOOST_MODE	(1)
+#define AW87XXX_PID_60_ADP_BOOST_I_BOOST_MODE_VALUE	\
+	(AW87XXX_PID_60_ADP_BOOST_I_BOOST_MODE << AW87XXX_PID_60_ADP_BOOST_I_START_BIT)
+
+#define AW87XXX_PID_60_ADP_BOOST_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_ADP_BOOST_I_DEFAULT	\
+	(AW87XXX_PID_60_ADP_BOOST_I_DEFAULT_VALUE << AW87XXX_PID_60_ADP_BOOST_I_START_BIT)
+
+/* OT160_I bit 1 (SYSINT 0x60) */
+#define AW87XXX_PID_60_OT160_I_START_BIT	(1)
+#define AW87XXX_PID_60_OT160_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_OT160_I_MASK		\
+	(~(((1<<AW87XXX_PID_60_OT160_I_BITS_LEN)-1) << AW87XXX_PID_60_OT160_I_START_BIT))
+
+#define AW87XXX_PID_60_OT160_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_OT160_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_OT160_I_NORMAL_OPERATION << AW87XXX_PID_60_OT160_I_START_BIT)
+
+#define AW87XXX_PID_60_OT160_I_PA_OVER_TEMPRETURE_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_OT160_I_PA_OVER_TEMPRETURE_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_OT160_I_PA_OVER_TEMPRETURE_PROTECTION_DETECTED << AW87XXX_PID_60_OT160_I_START_BIT)
+
+#define AW87XXX_PID_60_OT160_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_OT160_I_DEFAULT	\
+	(AW87XXX_PID_60_OT160_I_DEFAULT_VALUE << AW87XXX_PID_60_OT160_I_START_BIT)
+
+/* PA_OC_I bit 2 (SYSINT 0x60) */
+#define AW87XXX_PID_60_PA_OC_I_START_BIT	(2)
+#define AW87XXX_PID_60_PA_OC_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_PA_OC_I_MASK		\
+	(~(((1<<AW87XXX_PID_60_PA_OC_I_BITS_LEN)-1) << AW87XXX_PID_60_PA_OC_I_START_BIT))
+
+#define AW87XXX_PID_60_PA_OC_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_PA_OC_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_PA_OC_I_NORMAL_OPERATION << AW87XXX_PID_60_PA_OC_I_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_I_PA_OVER_CURRENT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_PA_OC_I_PA_OVER_CURRENT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_PA_OC_I_PA_OVER_CURRENT_PROTECTION_DETECTED << AW87XXX_PID_60_PA_OC_I_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_PA_OC_I_DEFAULT	\
+	(AW87XXX_PID_60_PA_OC_I_DEFAULT_VALUE << AW87XXX_PID_60_PA_OC_I_START_BIT)
+
+/* BST_SCP_I bit 3 (SYSINT 0x60) */
+#define AW87XXX_PID_60_BST_SCP_I_START_BIT	(3)
+#define AW87XXX_PID_60_BST_SCP_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_SCP_I_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SCP_I_BITS_LEN)-1) << AW87XXX_PID_60_BST_SCP_I_START_BIT))
+
+#define AW87XXX_PID_60_BST_SCP_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_BST_SCP_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_I_NORMAL_OPERATION << AW87XXX_PID_60_BST_SCP_I_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_I_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_BST_SCP_I_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_I_BOOST_SHORT_CIRCUIT_PROTECTION_DETECTED << AW87XXX_PID_60_BST_SCP_I_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BST_SCP_I_DEFAULT	\
+	(AW87XXX_PID_60_BST_SCP_I_DEFAULT_VALUE << AW87XXX_PID_60_BST_SCP_I_START_BIT)
+
+/* BST_OVP2_I bit 4 (SYSINT 0x60) */
+#define AW87XXX_PID_60_BST_OVP2_I_START_BIT	(4)
+#define AW87XXX_PID_60_BST_OVP2_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP2_I_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP2_I_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP2_I_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP2_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_BST_OVP2_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_I_NORMAL_OPERATION << AW87XXX_PID_60_BST_OVP2_I_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_I_BOOST_HEAVY_LOAD_PROTECTION_DETECTED	(1)
+#define AW87XXX_PID_60_BST_OVP2_I_BOOST_HEAVY_LOAD_PROTECTION_DETECTED_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_I_BOOST_HEAVY_LOAD_PROTECTION_DETECTED << AW87XXX_PID_60_BST_OVP2_I_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BST_OVP2_I_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP2_I_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP2_I_START_BIT)
+
+/* BST_OVP_I bit 5 (SYSINT 0x60) */
+#define AW87XXX_PID_60_BST_OVP_I_START_BIT	(5)
+#define AW87XXX_PID_60_BST_OVP_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP_I_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP_I_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP_I_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_BST_OVP_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_I_NORMAL_OPERATION << AW87XXX_PID_60_BST_OVP_I_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_I_BOOST_OVER_VOLTAGE_PROTECTION	(1)
+#define AW87XXX_PID_60_BST_OVP_I_BOOST_OVER_VOLTAGE_PROTECTION_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_I_BOOST_OVER_VOLTAGE_PROTECTION << AW87XXX_PID_60_BST_OVP_I_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BST_OVP_I_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP_I_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP_I_START_BIT)
+
+/* LOW_BATT_I bit 6 (SYSINT 0x60) */
+#define AW87XXX_PID_60_LOW_BATT_I_START_BIT	(6)
+#define AW87XXX_PID_60_LOW_BATT_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_LOW_BATT_I_MASK	\
+	(~(((1<<AW87XXX_PID_60_LOW_BATT_I_BITS_LEN)-1) << AW87XXX_PID_60_LOW_BATT_I_START_BIT))
+
+#define AW87XXX_PID_60_LOW_BATT_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_LOW_BATT_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_LOW_BATT_I_NORMAL_OPERATION << AW87XXX_PID_60_LOW_BATT_I_START_BIT)
+
+#define AW87XXX_PID_60_LOW_BATT_I_LOW_VBAT_DETECTED	(1)
+#define AW87XXX_PID_60_LOW_BATT_I_LOW_VBAT_DETECTED_VALUE	\
+	(AW87XXX_PID_60_LOW_BATT_I_LOW_VBAT_DETECTED << AW87XXX_PID_60_LOW_BATT_I_START_BIT)
+
+#define AW87XXX_PID_60_LOW_BATT_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_LOW_BATT_I_DEFAULT	\
+	(AW87XXX_PID_60_LOW_BATT_I_DEFAULT_VALUE << AW87XXX_PID_60_LOW_BATT_I_START_BIT)
+
+/* UVLO_I bit 7 (SYSINT 0x60) */
+#define AW87XXX_PID_60_UVLO_I_START_BIT	(7)
+#define AW87XXX_PID_60_UVLO_I_BITS_LEN	(1)
+#define AW87XXX_PID_60_UVLO_I_MASK		\
+	(~(((1<<AW87XXX_PID_60_UVLO_I_BITS_LEN)-1) << AW87XXX_PID_60_UVLO_I_START_BIT))
+
+#define AW87XXX_PID_60_UVLO_I_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_60_UVLO_I_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_60_UVLO_I_NORMAL_OPERATION << AW87XXX_PID_60_UVLO_I_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_I_VBAT_UNDER_VOLTAGE	(1)
+#define AW87XXX_PID_60_UVLO_I_VBAT_UNDER_VOLTAGE_VALUE	\
+	(AW87XXX_PID_60_UVLO_I_VBAT_UNDER_VOLTAGE << AW87XXX_PID_60_UVLO_I_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_I_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_UVLO_I_DEFAULT	\
+	(AW87XXX_PID_60_UVLO_I_DEFAULT_VALUE << AW87XXX_PID_60_UVLO_I_START_BIT)
+
+/* default value of SYSINT (0x60) */
+/* #define AW87XXX_PID_60_SYSINT_DEFAULT		(0xFF) */
+
+/* BURST_CON (0x61) detail */
+/* BURST_PEAK bit 7:5 (BURST_CON 0x61) */
+#define AW87XXX_PID_60_BURST_PEAK_START_BIT	(5)
+#define AW87XXX_PID_60_BURST_PEAK_BITS_LEN	(3)
+#define AW87XXX_PID_60_BURST_PEAK_MASK	\
+	(~(((1<<AW87XXX_PID_60_BURST_PEAK_BITS_LEN)-1) << AW87XXX_PID_60_BURST_PEAK_START_BIT))
+
+#define AW87XXX_PID_60_BURST_PEAK_10MA	(0)
+#define AW87XXX_PID_60_BURST_PEAK_10MA_VALUE	\
+	(AW87XXX_PID_60_BURST_PEAK_10MA << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_BURST_PEAK_20MA	(1)
+#define AW87XXX_PID_60_BURST_PEAK_20MA_VALUE	\
+	(AW87XXX_PID_60_BURST_PEAK_20MA << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_BURST_PEAK_30MA	(2)
+#define AW87XXX_PID_60_BURST_PEAK_30MA_VALUE	\
+	(AW87XXX_PID_60_BURST_PEAK_30MA << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_BURST_PEAK_50MA	(3)
+#define AW87XXX_PID_60_BURST_PEAK_50MA_VALUE	\
+	(AW87XXX_PID_60_BURST_PEAK_50MA << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_BURST_PEAK_70MA	(4)
+#define AW87XXX_PID_60_BURST_PEAK_70MA_VALUE	\
+	(AW87XXX_PID_60_BURST_PEAK_70MA << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_BURST_PEAK_130MA	(5)
+#define AW87XXX_PID_60_BURST_PEAK_130MA_VALUE	\
+	(AW87XXX_PID_60_BURST_PEAK_130MA << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_BURST_PEAK_160MA	(7)
+#define AW87XXX_PID_60_BURST_PEAK_160MA_VALUE	\
+	(AW87XXX_PID_60_BURST_PEAK_160MA << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_BURST_PEAK_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_60_BURST_PEAK_DEFAULT	\
+	(AW87XXX_PID_60_BURST_PEAK_DEFAULT_VALUE << AW87XXX_PID_60_BURST_PEAK_START_BIT)
+
+/* BST_BURST_SS bit 4:2 (BURST_CON 0x61) */
+#define AW87XXX_PID_60_BST_BURST_SS_START_BIT	(2)
+#define AW87XXX_PID_60_BST_BURST_SS_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_BURST_SS_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_BURST_SS_BITS_LEN)-1) << AW87XXX_PID_60_BST_BURST_SS_START_BIT))
+
+#define AW87XXX_PID_60_BST_BURST_SS_700MV	(0)
+#define AW87XXX_PID_60_BST_BURST_SS_700MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_700MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_760MV	(1)
+#define AW87XXX_PID_60_BST_BURST_SS_760MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_760MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_820MV	(2)
+#define AW87XXX_PID_60_BST_BURST_SS_820MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_820MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_880MV	(3)
+#define AW87XXX_PID_60_BST_BURST_SS_880MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_880MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_940MV	(4)
+#define AW87XXX_PID_60_BST_BURST_SS_940MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_940MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_1000MV	(5)
+#define AW87XXX_PID_60_BST_BURST_SS_1000MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_1000MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_1060MV	(6)
+#define AW87XXX_PID_60_BST_BURST_SS_1060MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_1060MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_1120MV	(7)
+#define AW87XXX_PID_60_BST_BURST_SS_1120MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SS_1120MV << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SS_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_BURST_SS_DEFAULT	\
+	(AW87XXX_PID_60_BST_BURST_SS_DEFAULT_VALUE << AW87XXX_PID_60_BST_BURST_SS_START_BIT)
+
+/* BST_COMPMAX bit 1:0 (BURST_CON 0x61) */
+#define AW87XXX_PID_60_BST_COMPMAX_START_BIT	(0)
+#define AW87XXX_PID_60_BST_COMPMAX_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_COMPMAX_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_COMPMAX_BITS_LEN)-1) << AW87XXX_PID_60_BST_COMPMAX_START_BIT))
+
+#define AW87XXX_PID_60_BST_COMPMAX_2P0V	(0)
+#define AW87XXX_PID_60_BST_COMPMAX_2P0V_VALUE	\
+	(AW87XXX_PID_60_BST_COMPMAX_2P0V << AW87XXX_PID_60_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_60_BST_COMPMAX_2P2V	(1)
+#define AW87XXX_PID_60_BST_COMPMAX_2P2V_VALUE	\
+	(AW87XXX_PID_60_BST_COMPMAX_2P2V << AW87XXX_PID_60_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_60_BST_COMPMAX_2P3V	(2)
+#define AW87XXX_PID_60_BST_COMPMAX_2P3V_VALUE	\
+	(AW87XXX_PID_60_BST_COMPMAX_2P3V << AW87XXX_PID_60_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_60_BST_COMPMAX_2P4V	(3)
+#define AW87XXX_PID_60_BST_COMPMAX_2P4V_VALUE	\
+	(AW87XXX_PID_60_BST_COMPMAX_2P4V << AW87XXX_PID_60_BST_COMPMAX_START_BIT)
+
+#define AW87XXX_PID_60_BST_COMPMAX_DEFAULT_VALUE	(3)
+#define AW87XXX_PID_60_BST_COMPMAX_DEFAULT	\
+	(AW87XXX_PID_60_BST_COMPMAX_DEFAULT_VALUE << AW87XXX_PID_60_BST_COMPMAX_START_BIT)
+
+/* default value of BURST_CON (0x61) */
+/* #define AW87XXX_PID_60_BURST_CON_DEFAULT		(0x47) */
+
+/* BST_BIAS (0x62) detail */
+/* BST_EA_CUR bit 0 (BST_BIAS 0x62) */
+#define AW87XXX_PID_60_BST_EA_CUR_START_BIT	(0)
+#define AW87XXX_PID_60_BST_EA_CUR_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_EA_CUR_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_EA_CUR_BITS_LEN)-1) << AW87XXX_PID_60_BST_EA_CUR_START_BIT))
+
+#define AW87XXX_PID_60_BST_EA_CUR_1UA	(0)
+#define AW87XXX_PID_60_BST_EA_CUR_1UA_VALUE	\
+	(AW87XXX_PID_60_BST_EA_CUR_1UA << AW87XXX_PID_60_BST_EA_CUR_START_BIT)
+
+#define AW87XXX_PID_60_BST_EA_CUR_2UA	(1)
+#define AW87XXX_PID_60_BST_EA_CUR_2UA_VALUE	\
+	(AW87XXX_PID_60_BST_EA_CUR_2UA << AW87XXX_PID_60_BST_EA_CUR_START_BIT)
+
+#define AW87XXX_PID_60_BST_EA_CUR_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_EA_CUR_DEFAULT	\
+	(AW87XXX_PID_60_BST_EA_CUR_DEFAULT_VALUE << AW87XXX_PID_60_BST_EA_CUR_START_BIT)
+
+/* BST_BURST_SSMD bit 5 (BST_BIAS 0x62) */
+#define AW87XXX_PID_60_BST_BURST_SSMD_START_BIT	(5)
+#define AW87XXX_PID_60_BST_BURST_SSMD_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_BURST_SSMD_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_BURST_SSMD_BITS_LEN)-1) << AW87XXX_PID_60_BST_BURST_SSMD_START_BIT))
+
+#define AW87XXX_PID_60_BST_BURST_SSMD_SLOW	(0)
+#define AW87XXX_PID_60_BST_BURST_SSMD_SLOW_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SSMD_SLOW << AW87XXX_PID_60_BST_BURST_SSMD_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SSMD_FAST	(1)
+#define AW87XXX_PID_60_BST_BURST_SSMD_FAST_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_SSMD_FAST << AW87XXX_PID_60_BST_BURST_SSMD_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_SSMD_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_BURST_SSMD_DEFAULT	\
+	(AW87XXX_PID_60_BST_BURST_SSMD_DEFAULT_VALUE << AW87XXX_PID_60_BST_BURST_SSMD_START_BIT)
+
+/* BST_NCD_ITH bit 7:6 (BST_BIAS 0x62) */
+#define AW87XXX_PID_60_BST_NCD_ITH_START_BIT	(6)
+#define AW87XXX_PID_60_BST_NCD_ITH_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_NCD_ITH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_NCD_ITH_BITS_LEN)-1) << AW87XXX_PID_60_BST_NCD_ITH_START_BIT))
+
+#define AW87XXX_PID_60_BST_NCD_ITH_170MA	(0)
+#define AW87XXX_PID_60_BST_NCD_ITH_170MA_VALUE	\
+	(AW87XXX_PID_60_BST_NCD_ITH_170MA << AW87XXX_PID_60_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_NCD_ITH_220MA	(1)
+#define AW87XXX_PID_60_BST_NCD_ITH_220MA_VALUE	\
+	(AW87XXX_PID_60_BST_NCD_ITH_220MA << AW87XXX_PID_60_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_NCD_ITH_280MA	(2)
+#define AW87XXX_PID_60_BST_NCD_ITH_280MA_VALUE	\
+	(AW87XXX_PID_60_BST_NCD_ITH_280MA << AW87XXX_PID_60_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_NCD_ITH_340MA	(3)
+#define AW87XXX_PID_60_BST_NCD_ITH_340MA_VALUE	\
+	(AW87XXX_PID_60_BST_NCD_ITH_340MA << AW87XXX_PID_60_BST_NCD_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_NCD_ITH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_NCD_ITH_DEFAULT	\
+	(AW87XXX_PID_60_BST_NCD_ITH_DEFAULT_VALUE << AW87XXX_PID_60_BST_NCD_ITH_START_BIT)
+
+/* BST_VOUT_TRIM bit 4:3 (BST_BIAS 0x62) */
+#define AW87XXX_PID_60_BST_VOUT_TRIM_START_BIT	(3)
+#define AW87XXX_PID_60_BST_VOUT_TRIM_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_VOUT_TRIM_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_VOUT_TRIM_BITS_LEN)-1) << AW87XXX_PID_60_BST_VOUT_TRIM_START_BIT))
+
+#define AW87XXX_PID_60_BST_VOUT_TRIM_25UA	(0)
+#define AW87XXX_PID_60_BST_VOUT_TRIM_25UA_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_TRIM_25UA << AW87XXX_PID_60_BST_VOUT_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_TRIM_24UA	(1)
+#define AW87XXX_PID_60_BST_VOUT_TRIM_24UA_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_TRIM_24UA << AW87XXX_PID_60_BST_VOUT_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_TRIM_25P5UA	(2)
+#define AW87XXX_PID_60_BST_VOUT_TRIM_25P5UA_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_TRIM_25P5UA << AW87XXX_PID_60_BST_VOUT_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_TRIM_24P5UA	(3)
+#define AW87XXX_PID_60_BST_VOUT_TRIM_24P5UA_VALUE	\
+	(AW87XXX_PID_60_BST_VOUT_TRIM_24P5UA << AW87XXX_PID_60_BST_VOUT_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_VOUT_TRIM_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_VOUT_TRIM_DEFAULT	\
+	(AW87XXX_PID_60_BST_VOUT_TRIM_DEFAULT_VALUE << AW87XXX_PID_60_BST_VOUT_TRIM_START_BIT)
+
+/* BST_BURST_IN bit 2:1 (BST_BIAS 0x62) */
+#define AW87XXX_PID_60_BST_BURST_IN_START_BIT	(1)
+#define AW87XXX_PID_60_BST_BURST_IN_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_BURST_IN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_BURST_IN_BITS_LEN)-1) << AW87XXX_PID_60_BST_BURST_IN_START_BIT))
+
+#define AW87XXX_PID_60_BST_BURST_IN_3MV	(0)
+#define AW87XXX_PID_60_BST_BURST_IN_3MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_3MV << AW87XXX_PID_60_BST_BURST_IN_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_5MV	(1)
+#define AW87XXX_PID_60_BST_BURST_IN_5MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_5MV << AW87XXX_PID_60_BST_BURST_IN_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_7MV	(2)
+#define AW87XXX_PID_60_BST_BURST_IN_7MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_7MV << AW87XXX_PID_60_BST_BURST_IN_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_9MV	(3)
+#define AW87XXX_PID_60_BST_BURST_IN_9MV_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_9MV << AW87XXX_PID_60_BST_BURST_IN_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_BURST_IN_DEFAULT	\
+	(AW87XXX_PID_60_BST_BURST_IN_DEFAULT_VALUE << AW87XXX_PID_60_BST_BURST_IN_START_BIT)
+
+/* default value of BST_BIAS (0x62) */
+/* #define AW87XXX_PID_60_BST_BIAS_DEFAULT		(0x60) */
+
+/* BST_EA (0x63) detail */
+/* BST_LOW_CLAMP_EN bit 2 (BST_EA 0x63) */
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_START_BIT	(2)
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_LOW_CLAMP_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_LOW_CLAMP_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_DISABLE	(0)
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_LOW_CLAMP_EN_DISABLE << AW87XXX_PID_60_BST_LOW_CLAMP_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_ENABLE	(1)
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_LOW_CLAMP_EN_ENABLE << AW87XXX_PID_60_BST_LOW_CLAMP_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_LOW_CLAMP_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_LOW_CLAMP_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_LOW_CLAMP_EN_START_BIT)
+
+/* EN_VOUT_DIV bit 7 (BST_EA 0x63) */
+#define AW87XXX_PID_60_EN_VOUT_DIV_START_BIT	(7)
+#define AW87XXX_PID_60_EN_VOUT_DIV_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_VOUT_DIV_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_VOUT_DIV_BITS_LEN)-1) << AW87XXX_PID_60_EN_VOUT_DIV_START_BIT))
+
+#define AW87XXX_PID_60_EN_VOUT_DIV_DISABLE	(0)
+#define AW87XXX_PID_60_EN_VOUT_DIV_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_VOUT_DIV_DISABLE << AW87XXX_PID_60_EN_VOUT_DIV_START_BIT)
+
+#define AW87XXX_PID_60_EN_VOUT_DIV_ENABLE	(1)
+#define AW87XXX_PID_60_EN_VOUT_DIV_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_VOUT_DIV_ENABLE << AW87XXX_PID_60_EN_VOUT_DIV_START_BIT)
+
+#define AW87XXX_PID_60_EN_VOUT_DIV_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_EN_VOUT_DIV_DEFAULT	\
+	(AW87XXX_PID_60_EN_VOUT_DIV_DEFAULT_VALUE << AW87XXX_PID_60_EN_VOUT_DIV_START_BIT)
+
+/* BST_BURST_OUT_DELAY bit 6:5 (BST_EA 0x63) */
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_START_BIT	(5)
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_BURST_OUT_DELAY_BITS_LEN)-1) << AW87XXX_PID_60_BST_BURST_OUT_DELAY_START_BIT))
+
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_2P8US	(0)
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_2P8US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_OUT_DELAY_2P8US << AW87XXX_PID_60_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_8P1US	(1)
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_8P1US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_OUT_DELAY_8P1US << AW87XXX_PID_60_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_1P2US	(2)
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_1P2US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_OUT_DELAY_1P2US << AW87XXX_PID_60_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_1P8US	(3)
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_1P8US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_OUT_DELAY_1P8US << AW87XXX_PID_60_BST_BURST_OUT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_BURST_OUT_DELAY_DEFAULT	\
+	(AW87XXX_PID_60_BST_BURST_OUT_DELAY_DEFAULT_VALUE << AW87XXX_PID_60_BST_BURST_OUT_DELAY_START_BIT)
+
+/* BST_BURST_IN_DELAY bit 4:3 (BST_EA 0x63) */
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_START_BIT	(3)
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_BURST_IN_DELAY_BITS_LEN)-1) << AW87XXX_PID_60_BST_BURST_IN_DELAY_START_BIT))
+
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_7P4US	(0)
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_7P4US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_DELAY_7P4US << AW87XXX_PID_60_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_14P6US	(1)
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_14P6US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_DELAY_14P6US << AW87XXX_PID_60_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_3P7US	(2)
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_3P7US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_DELAY_3P7US << AW87XXX_PID_60_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_5US	(3)
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_5US_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_IN_DELAY_5US << AW87XXX_PID_60_BST_BURST_IN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_BURST_IN_DELAY_DEFAULT	\
+	(AW87XXX_PID_60_BST_BURST_IN_DELAY_DEFAULT_VALUE << AW87XXX_PID_60_BST_BURST_IN_DELAY_START_BIT)
+
+/* BST_LOOPR bit 1:0 (BST_EA 0x63) */
+#define AW87XXX_PID_60_BST_LOOPR_START_BIT	(0)
+#define AW87XXX_PID_60_BST_LOOPR_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_LOOPR_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_LOOPR_BITS_LEN)-1) << AW87XXX_PID_60_BST_LOOPR_START_BIT))
+
+#define AW87XXX_PID_60_BST_LOOPR_100K	(0)
+#define AW87XXX_PID_60_BST_LOOPR_100K_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPR_100K << AW87XXX_PID_60_BST_LOOPR_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOOPR_200K	(1)
+#define AW87XXX_PID_60_BST_LOOPR_200K_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPR_200K << AW87XXX_PID_60_BST_LOOPR_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOOPR_400K	(2)
+#define AW87XXX_PID_60_BST_LOOPR_400K_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPR_400K << AW87XXX_PID_60_BST_LOOPR_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOOPR_500K	(3)
+#define AW87XXX_PID_60_BST_LOOPR_500K_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPR_500K << AW87XXX_PID_60_BST_LOOPR_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOOPR_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_60_BST_LOOPR_DEFAULT	\
+	(AW87XXX_PID_60_BST_LOOPR_DEFAULT_VALUE << AW87XXX_PID_60_BST_LOOPR_START_BIT)
+
+/* default value of BST_EA (0x63) */
+/* #define AW87XXX_PID_60_BST_EA_DEFAULT		(0x86) */
+
+/* BST_DE_SOFT (0x64) detail */
+/* EN_ADP_PEAK bit 0 (BST_DE_SOFT 0x64) */
+#define AW87XXX_PID_60_EN_ADP_PEAK_START_BIT	(0)
+#define AW87XXX_PID_60_EN_ADP_PEAK_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_ADP_PEAK_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_ADP_PEAK_BITS_LEN)-1) << AW87XXX_PID_60_EN_ADP_PEAK_START_BIT))
+
+#define AW87XXX_PID_60_EN_ADP_PEAK_DISABLE	(0)
+#define AW87XXX_PID_60_EN_ADP_PEAK_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_ADP_PEAK_DISABLE << AW87XXX_PID_60_EN_ADP_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_EN_ADP_PEAK_ENABLE	(1)
+#define AW87XXX_PID_60_EN_ADP_PEAK_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_ADP_PEAK_ENABLE << AW87XXX_PID_60_EN_ADP_PEAK_START_BIT)
+
+#define AW87XXX_PID_60_EN_ADP_PEAK_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_EN_ADP_PEAK_DEFAULT	\
+	(AW87XXX_PID_60_EN_ADP_PEAK_DEFAULT_VALUE << AW87XXX_PID_60_EN_ADP_PEAK_START_BIT)
+
+/* BST_SOFT_MODE_EN bit 7 (BST_DE_SOFT 0x64) */
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_START_BIT	(7)
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SOFT_MODE_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_SOFT_MODE_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_DISABLE	(0)
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_SOFT_MODE_EN_DISABLE << AW87XXX_PID_60_BST_SOFT_MODE_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_ENABLE	(1)
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_SOFT_MODE_EN_ENABLE << AW87XXX_PID_60_BST_SOFT_MODE_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_SOFT_MODE_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_SOFT_MODE_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_SOFT_MODE_EN_START_BIT)
+
+/* BST_LOOPC bit 6:5 (BST_DE_SOFT 0x64) */
+#define AW87XXX_PID_60_BST_LOOPC_START_BIT	(5)
+#define AW87XXX_PID_60_BST_LOOPC_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_LOOPC_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_LOOPC_BITS_LEN)-1) << AW87XXX_PID_60_BST_LOOPC_START_BIT))
+
+#define AW87XXX_PID_60_BST_LOOPC_30PF	(0)
+#define AW87XXX_PID_60_BST_LOOPC_30PF_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPC_30PF << AW87XXX_PID_60_BST_LOOPC_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOOPC_40PF	(1)
+#define AW87XXX_PID_60_BST_LOOPC_40PF_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPC_40PF << AW87XXX_PID_60_BST_LOOPC_START_BIT)
+
+/*
+#define AW87XXX_PID_60_BST_LOOPC_40PF	(2)
+#define AW87XXX_PID_60_BST_LOOPC_40PF_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPC_40PF << AW87XXX_PID_60_BST_LOOPC_START_BIT)
+*/
+
+#define AW87XXX_PID_60_BST_LOOPC_50PF	(3)
+#define AW87XXX_PID_60_BST_LOOPC_50PF_VALUE	\
+	(AW87XXX_PID_60_BST_LOOPC_50PF << AW87XXX_PID_60_BST_LOOPC_START_BIT)
+
+#define AW87XXX_PID_60_BST_LOOPC_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_LOOPC_DEFAULT	\
+	(AW87XXX_PID_60_BST_LOOPC_DEFAULT_VALUE << AW87XXX_PID_60_BST_LOOPC_START_BIT)
+
+/* BST_SEL_DFPWM bit 4:3 (BST_DE_SOFT 0x64) */
+#define AW87XXX_PID_60_BST_SEL_DFPWM_START_BIT	(3)
+#define AW87XXX_PID_60_BST_SEL_DFPWM_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_SEL_DFPWM_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SEL_DFPWM_BITS_LEN)-1) << AW87XXX_PID_60_BST_SEL_DFPWM_START_BIT))
+
+#define AW87XXX_PID_60_BST_SEL_DFPWM_32US_00_2US	(0)
+#define AW87XXX_PID_60_BST_SEL_DFPWM_32US_00_2US_VALUE	\
+	(AW87XXX_PID_60_BST_SEL_DFPWM_32US_00_2US << AW87XXX_PID_60_BST_SEL_DFPWM_START_BIT)
+
+#define AW87XXX_PID_60_BST_SEL_DFPWM_64US_01_CLK	(1)
+#define AW87XXX_PID_60_BST_SEL_DFPWM_64US_01_CLK_VALUE	\
+	(AW87XXX_PID_60_BST_SEL_DFPWM_64US_01_CLK << AW87XXX_PID_60_BST_SEL_DFPWM_START_BIT)
+
+#define AW87XXX_PID_60_BST_SEL_DFPWM_128US_10_4US	(2)
+#define AW87XXX_PID_60_BST_SEL_DFPWM_128US_10_4US_VALUE	\
+	(AW87XXX_PID_60_BST_SEL_DFPWM_128US_10_4US << AW87XXX_PID_60_BST_SEL_DFPWM_START_BIT)
+
+#define AW87XXX_PID_60_BST_SEL_DFPWM_256US_11_8US	(3)
+#define AW87XXX_PID_60_BST_SEL_DFPWM_256US_11_8US_VALUE	\
+	(AW87XXX_PID_60_BST_SEL_DFPWM_256US_11_8US << AW87XXX_PID_60_BST_SEL_DFPWM_START_BIT)
+
+#define AW87XXX_PID_60_BST_SEL_DFPWM_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_SEL_DFPWM_DEFAULT	\
+	(AW87XXX_PID_60_BST_SEL_DFPWM_DEFAULT_VALUE << AW87XXX_PID_60_BST_SEL_DFPWM_START_BIT)
+
+/* BST_SOFT_DELAY bit 2:1 (BST_DE_SOFT 0x64) */
+#define AW87XXX_PID_60_BST_SOFT_DELAY_START_BIT	(1)
+#define AW87XXX_PID_60_BST_SOFT_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_SOFT_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SOFT_DELAY_BITS_LEN)-1) << AW87XXX_PID_60_BST_SOFT_DELAY_START_BIT))
+
+#define AW87XXX_PID_60_BST_SOFT_DELAY_40US_001280U	(0)
+#define AW87XXX_PID_60_BST_SOFT_DELAY_40US_001280U_VALUE	\
+	(AW87XXX_PID_60_BST_SOFT_DELAY_40US_001280U << AW87XXX_PID_60_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_SOFT_DELAY_80US_012560U	(1)
+#define AW87XXX_PID_60_BST_SOFT_DELAY_80US_012560U_VALUE	\
+	(AW87XXX_PID_60_BST_SOFT_DELAY_80US_012560U << AW87XXX_PID_60_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_SOFT_DELAY_160US_105120U	(2)
+#define AW87XXX_PID_60_BST_SOFT_DELAY_160US_105120U_VALUE	\
+	(AW87XXX_PID_60_BST_SOFT_DELAY_160US_105120U << AW87XXX_PID_60_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_SOFT_DELAY_320US_1110240U	(3)
+#define AW87XXX_PID_60_BST_SOFT_DELAY_320US_1110240U_VALUE	\
+	(AW87XXX_PID_60_BST_SOFT_DELAY_320US_1110240U << AW87XXX_PID_60_BST_SOFT_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_SOFT_DELAY_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_SOFT_DELAY_DEFAULT	\
+	(AW87XXX_PID_60_BST_SOFT_DELAY_DEFAULT_VALUE << AW87XXX_PID_60_BST_SOFT_DELAY_START_BIT)
+
+/* default value of BST_DE_SOFT (0x64) */
+/* #define AW87XXX_PID_60_BST_DE_SOFT_DEFAULT		(0x09) */
+
+/* BST_BURST_KICK (0x65) detail */
+/* EN_TRANS_ERROR bit 0 (BST_BURST_KICK 0x65) */
+#define AW87XXX_PID_60_EN_TRANS_ERROR_START_BIT	(0)
+#define AW87XXX_PID_60_EN_TRANS_ERROR_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_TRANS_ERROR_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_TRANS_ERROR_BITS_LEN)-1) << AW87XXX_PID_60_EN_TRANS_ERROR_START_BIT))
+
+#define AW87XXX_PID_60_EN_TRANS_ERROR_DISABLE	(0)
+#define AW87XXX_PID_60_EN_TRANS_ERROR_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_TRANS_ERROR_DISABLE << AW87XXX_PID_60_EN_TRANS_ERROR_START_BIT)
+
+#define AW87XXX_PID_60_EN_TRANS_ERROR_ENABLE	(1)
+#define AW87XXX_PID_60_EN_TRANS_ERROR_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_TRANS_ERROR_ENABLE << AW87XXX_PID_60_EN_TRANS_ERROR_START_BIT)
+
+#define AW87XXX_PID_60_EN_TRANS_ERROR_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_EN_TRANS_ERROR_DEFAULT	\
+	(AW87XXX_PID_60_EN_TRANS_ERROR_DEFAULT_VALUE << AW87XXX_PID_60_EN_TRANS_ERROR_START_BIT)
+
+/* BST_SCP_VTH bit 1 (BST_BURST_KICK 0x65) */
+#define AW87XXX_PID_60_BST_SCP_VTH_START_BIT	(1)
+#define AW87XXX_PID_60_BST_SCP_VTH_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_SCP_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SCP_VTH_BITS_LEN)-1) << AW87XXX_PID_60_BST_SCP_VTH_START_BIT))
+
+#define AW87XXX_PID_60_BST_SCP_VTH_HIGH_SIDE_VDD	(0)
+#define AW87XXX_PID_60_BST_SCP_VTH_HIGH_SIDE_VDD_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_VTH_HIGH_SIDE_VDD << AW87XXX_PID_60_BST_SCP_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_VTH_LOW_SIDE_VDD	(1)
+#define AW87XXX_PID_60_BST_SCP_VTH_LOW_SIDE_VDD_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_VTH_LOW_SIDE_VDD << AW87XXX_PID_60_BST_SCP_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_VTH_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_SCP_VTH_DEFAULT	\
+	(AW87XXX_PID_60_BST_SCP_VTH_DEFAULT_VALUE << AW87XXX_PID_60_BST_SCP_VTH_START_BIT)
+
+/* BST_SKIP_EN bit 6 (BST_BURST_KICK 0x65) */
+#define AW87XXX_PID_60_BST_SKIP_EN_START_BIT	(6)
+#define AW87XXX_PID_60_BST_SKIP_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_SKIP_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SKIP_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_SKIP_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_SKIP_EN_ENABLE	(0)
+#define AW87XXX_PID_60_BST_SKIP_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_SKIP_EN_ENABLE << AW87XXX_PID_60_BST_SKIP_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_SKIP_EN_DISABLE	(1)
+#define AW87XXX_PID_60_BST_SKIP_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_SKIP_EN_DISABLE << AW87XXX_PID_60_BST_SKIP_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_SKIP_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_SKIP_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_SKIP_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_SKIP_EN_START_BIT)
+
+/* BST_ADBK_COMP_ADJ bit 7 (BST_BURST_KICK 0x65) */
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_START_BIT	(7)
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_ADBK_COMP_ADJ_BITS_LEN)-1) << AW87XXX_PID_60_BST_ADBK_COMP_ADJ_START_BIT))
+
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_14UA	(0)
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_14UA_VALUE	\
+	(AW87XXX_PID_60_BST_ADBK_COMP_ADJ_14UA << AW87XXX_PID_60_BST_ADBK_COMP_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_10UA	(1)
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_10UA_VALUE	\
+	(AW87XXX_PID_60_BST_ADBK_COMP_ADJ_10UA << AW87XXX_PID_60_BST_ADBK_COMP_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_ADBK_COMP_ADJ_DEFAULT	\
+	(AW87XXX_PID_60_BST_ADBK_COMP_ADJ_DEFAULT_VALUE << AW87XXX_PID_60_BST_ADBK_COMP_ADJ_START_BIT)
+
+/* BST_OVP2_ITH bit 5:4 (BST_BURST_KICK 0x65) */
+#define AW87XXX_PID_60_BST_OVP2_ITH_START_BIT	(4)
+#define AW87XXX_PID_60_BST_OVP2_ITH_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_OVP2_ITH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP2_ITH_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP2_ITH_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP2_ITH_32MA	(0)
+#define AW87XXX_PID_60_BST_OVP2_ITH_32MA_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_ITH_32MA << AW87XXX_PID_60_BST_OVP2_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_ITH_48MA	(1)
+#define AW87XXX_PID_60_BST_OVP2_ITH_48MA_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_ITH_48MA << AW87XXX_PID_60_BST_OVP2_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_ITH_64MA	(2)
+#define AW87XXX_PID_60_BST_OVP2_ITH_64MA_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_ITH_64MA << AW87XXX_PID_60_BST_OVP2_ITH_START_BIT)
+
+/*
+#define AW87XXX_PID_60_BST_OVP2_ITH_64MA	(3)
+#define AW87XXX_PID_60_BST_OVP2_ITH_64MA_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_ITH_64MA << AW87XXX_PID_60_BST_OVP2_ITH_START_BIT)
+*/
+
+#define AW87XXX_PID_60_BST_OVP2_ITH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_OVP2_ITH_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP2_ITH_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP2_ITH_START_BIT)
+
+/* BST_KICK_ITH bit 3:2 (BST_BURST_KICK 0x65) */
+#define AW87XXX_PID_60_BST_KICK_ITH_START_BIT	(2)
+#define AW87XXX_PID_60_BST_KICK_ITH_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_KICK_ITH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_KICK_ITH_BITS_LEN)-1) << AW87XXX_PID_60_BST_KICK_ITH_START_BIT))
+
+#define AW87XXX_PID_60_BST_KICK_ITH_32MA	(0)
+#define AW87XXX_PID_60_BST_KICK_ITH_32MA_VALUE	\
+	(AW87XXX_PID_60_BST_KICK_ITH_32MA << AW87XXX_PID_60_BST_KICK_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_KICK_ITH_48MA	(1)
+#define AW87XXX_PID_60_BST_KICK_ITH_48MA_VALUE	\
+	(AW87XXX_PID_60_BST_KICK_ITH_48MA << AW87XXX_PID_60_BST_KICK_ITH_START_BIT)
+
+#define AW87XXX_PID_60_BST_KICK_ITH_64MA	(2)
+#define AW87XXX_PID_60_BST_KICK_ITH_64MA_VALUE	\
+	(AW87XXX_PID_60_BST_KICK_ITH_64MA << AW87XXX_PID_60_BST_KICK_ITH_START_BIT)
+
+/*
+#define AW87XXX_PID_60_BST_KICK_ITH_64MA	(3)
+#define AW87XXX_PID_60_BST_KICK_ITH_64MA_VALUE	\
+	(AW87XXX_PID_60_BST_KICK_ITH_64MA << AW87XXX_PID_60_BST_KICK_ITH_START_BIT)
+*/
+
+#define AW87XXX_PID_60_BST_KICK_ITH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_KICK_ITH_DEFAULT	\
+	(AW87XXX_PID_60_BST_KICK_ITH_DEFAULT_VALUE << AW87XXX_PID_60_BST_KICK_ITH_START_BIT)
+
+/* default value of BST_BURST_KICK (0x65) */
+/* #define AW87XXX_PID_60_BST_BURST_KICK_DEFAULT		(0x14) */
+
+/* BST_CON1 (0x66) detail */
+/* BST_GTDR_DDT bit 2 (BST_CON1 0x66) */
+#define AW87XXX_PID_60_BST_GTDR_DDT_START_BIT	(2)
+#define AW87XXX_PID_60_BST_GTDR_DDT_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_GTDR_DDT_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_GTDR_DDT_BITS_LEN)-1) << AW87XXX_PID_60_BST_GTDR_DDT_START_BIT))
+
+#define AW87XXX_PID_60_BST_GTDR_DDT_9NS	(0)
+#define AW87XXX_PID_60_BST_GTDR_DDT_9NS_VALUE	\
+	(AW87XXX_PID_60_BST_GTDR_DDT_9NS << AW87XXX_PID_60_BST_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_60_BST_GTDR_DDT_12NS	(1)
+#define AW87XXX_PID_60_BST_GTDR_DDT_12NS_VALUE	\
+	(AW87XXX_PID_60_BST_GTDR_DDT_12NS << AW87XXX_PID_60_BST_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_60_BST_GTDR_DDT_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_GTDR_DDT_DEFAULT	\
+	(AW87XXX_PID_60_BST_GTDR_DDT_DEFAULT_VALUE << AW87XXX_PID_60_BST_GTDR_DDT_START_BIT)
+
+/* EN_ADP_MODE1_DEGLITCH bit 3 (BST_CON1 0x66) */
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_START_BIT	(3)
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_BITS_LEN)-1) << AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_START_BIT))
+
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_DISABLE	(0)
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_DISABLE << AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_ENABLE	(1)
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_ENABLE << AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_START_BIT)
+
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_DEFAULT	\
+	(AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_DEFAULT_VALUE << AW87XXX_PID_60_EN_ADP_MODE1_DEGLITCH_START_BIT)
+
+/* SS_FINISH_SELECT bit 6 (BST_CON1 0x66) */
+#define AW87XXX_PID_60_SS_FINISH_SELECT_START_BIT	(6)
+#define AW87XXX_PID_60_SS_FINISH_SELECT_BITS_LEN	(1)
+#define AW87XXX_PID_60_SS_FINISH_SELECT_MASK	\
+	(~(((1<<AW87XXX_PID_60_SS_FINISH_SELECT_BITS_LEN)-1) << AW87XXX_PID_60_SS_FINISH_SELECT_START_BIT))
+
+#define AW87XXX_PID_60_SS_FINISH_SELECT_NOT_USE	(0)
+#define AW87XXX_PID_60_SS_FINISH_SELECT_NOT_USE_VALUE	\
+	(AW87XXX_PID_60_SS_FINISH_SELECT_NOT_USE << AW87XXX_PID_60_SS_FINISH_SELECT_START_BIT)
+
+#define AW87XXX_PID_60_SS_FINISH_SELECT_USE	(1)
+#define AW87XXX_PID_60_SS_FINISH_SELECT_USE_VALUE	\
+	(AW87XXX_PID_60_SS_FINISH_SELECT_USE << AW87XXX_PID_60_SS_FINISH_SELECT_START_BIT)
+
+#define AW87XXX_PID_60_SS_FINISH_SELECT_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_SS_FINISH_SELECT_DEFAULT	\
+	(AW87XXX_PID_60_SS_FINISH_SELECT_DEFAULT_VALUE << AW87XXX_PID_60_SS_FINISH_SELECT_START_BIT)
+
+/* BST_GDRV_TEST bit 7 (BST_CON1 0x66) */
+#define AW87XXX_PID_60_BST_GDRV_TEST_START_BIT	(7)
+#define AW87XXX_PID_60_BST_GDRV_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_GDRV_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_GDRV_TEST_BITS_LEN)-1) << AW87XXX_PID_60_BST_GDRV_TEST_START_BIT))
+
+#define AW87XXX_PID_60_BST_GDRV_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_BST_GDRV_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_BST_GDRV_TEST_DIABLE << AW87XXX_PID_60_BST_GDRV_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_GDRV_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_GDRV_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_GDRV_TEST_ENABLE << AW87XXX_PID_60_BST_GDRV_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_GDRV_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_GDRV_TEST_DEFAULT	\
+	(AW87XXX_PID_60_BST_GDRV_TEST_DEFAULT_VALUE << AW87XXX_PID_60_BST_GDRV_TEST_START_BIT)
+
+/* BST_EN_DELAY bit 5:4 (BST_CON1 0x66) */
+#define AW87XXX_PID_60_BST_EN_DELAY_START_BIT	(4)
+#define AW87XXX_PID_60_BST_EN_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_EN_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_EN_DELAY_BITS_LEN)-1) << AW87XXX_PID_60_BST_EN_DELAY_START_BIT))
+
+#define AW87XXX_PID_60_BST_EN_DELAY_8N	(0)
+#define AW87XXX_PID_60_BST_EN_DELAY_8N_VALUE	\
+	(AW87XXX_PID_60_BST_EN_DELAY_8N << AW87XXX_PID_60_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_EN_DELAY_80N	(1)
+#define AW87XXX_PID_60_BST_EN_DELAY_80N_VALUE	\
+	(AW87XXX_PID_60_BST_EN_DELAY_80N << AW87XXX_PID_60_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_EN_DELAY_130N	(2)
+#define AW87XXX_PID_60_BST_EN_DELAY_130N_VALUE	\
+	(AW87XXX_PID_60_BST_EN_DELAY_130N << AW87XXX_PID_60_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_EN_DELAY_200N	(3)
+#define AW87XXX_PID_60_BST_EN_DELAY_200N_VALUE	\
+	(AW87XXX_PID_60_BST_EN_DELAY_200N << AW87XXX_PID_60_BST_EN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_BST_EN_DELAY_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_EN_DELAY_DEFAULT	\
+	(AW87XXX_PID_60_BST_EN_DELAY_DEFAULT_VALUE << AW87XXX_PID_60_BST_EN_DELAY_START_BIT)
+
+/* BST_SRC bit 1:0 (BST_CON1 0x66) */
+#define AW87XXX_PID_60_BST_SRC_START_BIT	(0)
+#define AW87XXX_PID_60_BST_SRC_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_SRC_MASK		\
+	(~(((1<<AW87XXX_PID_60_BST_SRC_BITS_LEN)-1) << AW87XXX_PID_60_BST_SRC_START_BIT))
+
+#define AW87XXX_PID_60_BST_SRC_3NS		(0)
+#define AW87XXX_PID_60_BST_SRC_3NS_VALUE	\
+	(AW87XXX_PID_60_BST_SRC_3NS << AW87XXX_PID_60_BST_SRC_START_BIT)
+
+#define AW87XXX_PID_60_BST_SRC_4NS		(1)
+#define AW87XXX_PID_60_BST_SRC_4NS_VALUE	\
+	(AW87XXX_PID_60_BST_SRC_4NS << AW87XXX_PID_60_BST_SRC_START_BIT)
+
+#define AW87XXX_PID_60_BST_SRC_7NS		(2)
+#define AW87XXX_PID_60_BST_SRC_7NS_VALUE	\
+	(AW87XXX_PID_60_BST_SRC_7NS << AW87XXX_PID_60_BST_SRC_START_BIT)
+
+#define AW87XXX_PID_60_BST_SRC_15NS		(3)
+#define AW87XXX_PID_60_BST_SRC_15NS_VALUE	\
+	(AW87XXX_PID_60_BST_SRC_15NS << AW87XXX_PID_60_BST_SRC_START_BIT)
+
+#define AW87XXX_PID_60_BST_SRC_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_SRC_DEFAULT	\
+	(AW87XXX_PID_60_BST_SRC_DEFAULT_VALUE << AW87XXX_PID_60_BST_SRC_START_BIT)
+
+/* default value of BST_CON1 (0x66) */
+/* #define AW87XXX_PID_60_BST_CON1_DEFAULT		(0x10) */
+
+/* BST_OVP (0x67) detail */
+/* BST_OVP_VTH bit 0 (BST_OVP 0x67) */
+#define AW87XXX_PID_60_BST_OVP_VTH_START_BIT	(0)
+#define AW87XXX_PID_60_BST_OVP_VTH_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP_VTH_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP_VTH_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP_VTH_3MINUS6P875V	(0)
+#define AW87XXX_PID_60_BST_OVP_VTH_3MINUS6P875V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_VTH_3MINUS6P875V << AW87XXX_PID_60_BST_OVP_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_VTH_7MINUS11V	(1)
+#define AW87XXX_PID_60_BST_OVP_VTH_7MINUS11V_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_VTH_7MINUS11V << AW87XXX_PID_60_BST_OVP_VTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_VTH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_OVP_VTH_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP_VTH_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP_VTH_START_BIT)
+
+/* BST_VFB_EN bit 1 (BST_OVP 0x67) */
+#define AW87XXX_PID_60_BST_VFB_EN_START_BIT	(1)
+#define AW87XXX_PID_60_BST_VFB_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_VFB_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_VFB_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_VFB_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_VFB_EN_DISABLE	(0)
+#define AW87XXX_PID_60_BST_VFB_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_VFB_EN_DISABLE << AW87XXX_PID_60_BST_VFB_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_VFB_EN_ENABLE	(1)
+#define AW87XXX_PID_60_BST_VFB_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_VFB_EN_ENABLE << AW87XXX_PID_60_BST_VFB_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_VFB_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_VFB_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_VFB_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_VFB_EN_START_BIT)
+
+/* BST_FORCE_PWM bit 2 (BST_OVP 0x67) */
+#define AW87XXX_PID_60_BST_FORCE_PWM_START_BIT	(2)
+#define AW87XXX_PID_60_BST_FORCE_PWM_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_FORCE_PWM_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_FORCE_PWM_BITS_LEN)-1) << AW87XXX_PID_60_BST_FORCE_PWM_START_BIT))
+
+#define AW87XXX_PID_60_BST_FORCE_PWM_DISABLE	(0)
+#define AW87XXX_PID_60_BST_FORCE_PWM_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_FORCE_PWM_DISABLE << AW87XXX_PID_60_BST_FORCE_PWM_START_BIT)
+
+#define AW87XXX_PID_60_BST_FORCE_PWM_ENABLE	(1)
+#define AW87XXX_PID_60_BST_FORCE_PWM_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_FORCE_PWM_ENABLE << AW87XXX_PID_60_BST_FORCE_PWM_START_BIT)
+
+#define AW87XXX_PID_60_BST_FORCE_PWM_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_FORCE_PWM_DEFAULT	\
+	(AW87XXX_PID_60_BST_FORCE_PWM_DEFAULT_VALUE << AW87XXX_PID_60_BST_FORCE_PWM_START_BIT)
+
+/* BST_OVP2_EN bit 5 (BST_OVP 0x67) */
+#define AW87XXX_PID_60_BST_OVP2_EN_START_BIT	(5)
+#define AW87XXX_PID_60_BST_OVP2_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP2_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP2_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP2_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP2_EN_DISABLE	(0)
+#define AW87XXX_PID_60_BST_OVP2_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_EN_DISABLE << AW87XXX_PID_60_BST_OVP2_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_EN_ENABLE	(1)
+#define AW87XXX_PID_60_BST_OVP2_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_OVP2_EN_ENABLE << AW87XXX_PID_60_BST_OVP2_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP2_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_OVP2_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP2_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP2_EN_START_BIT)
+
+/* BST_OVP_DEGLI_SEL bit 6 (BST_OVP 0x67) */
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_START_BIT	(6)
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP_DEGLI_SEL_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP_DEGLI_SEL_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_NO_DEGLITCH	(0)
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_NO_DEGLITCH_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_DEGLI_SEL_NO_DEGLITCH << AW87XXX_PID_60_BST_OVP_DEGLI_SEL_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_DEGLITCH	(1)
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_DEGLITCH_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_DEGLI_SEL_DEGLITCH << AW87XXX_PID_60_BST_OVP_DEGLI_SEL_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_OVP_DEGLI_SEL_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP_DEGLI_SEL_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP_DEGLI_SEL_START_BIT)
+
+/* BST_CLK_DIV bit 7 (BST_OVP 0x67) */
+#define AW87XXX_PID_60_BST_CLK_DIV_START_BIT	(7)
+#define AW87XXX_PID_60_BST_CLK_DIV_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_CLK_DIV_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_CLK_DIV_BITS_LEN)-1) << AW87XXX_PID_60_BST_CLK_DIV_START_BIT))
+
+#define AW87XXX_PID_60_BST_CLK_DIV_DIV_BY_4	(0)
+#define AW87XXX_PID_60_BST_CLK_DIV_DIV_BY_4_VALUE	\
+	(AW87XXX_PID_60_BST_CLK_DIV_DIV_BY_4 << AW87XXX_PID_60_BST_CLK_DIV_START_BIT)
+
+#define AW87XXX_PID_60_BST_CLK_DIV_DIV_BY_2	(1)
+#define AW87XXX_PID_60_BST_CLK_DIV_DIV_BY_2_VALUE	\
+	(AW87XXX_PID_60_BST_CLK_DIV_DIV_BY_2 << AW87XXX_PID_60_BST_CLK_DIV_START_BIT)
+
+#define AW87XXX_PID_60_BST_CLK_DIV_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_CLK_DIV_DEFAULT	\
+	(AW87XXX_PID_60_BST_CLK_DIV_DEFAULT_VALUE << AW87XXX_PID_60_BST_CLK_DIV_START_BIT)
+
+/* BURST_HYS_EN bit 4:3 (BST_OVP 0x67) */
+#define AW87XXX_PID_60_BURST_HYS_EN_START_BIT	(3)
+#define AW87XXX_PID_60_BURST_HYS_EN_BITS_LEN	(2)
+#define AW87XXX_PID_60_BURST_HYS_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BURST_HYS_EN_BITS_LEN)-1) << AW87XXX_PID_60_BURST_HYS_EN_START_BIT))
+
+#define AW87XXX_PID_60_BURST_HYS_EN_OUT	(0)
+#define AW87XXX_PID_60_BURST_HYS_EN_OUT_VALUE	\
+	(AW87XXX_PID_60_BURST_HYS_EN_OUT << AW87XXX_PID_60_BURST_HYS_EN_START_BIT)
+
+#define AW87XXX_PID_60_BURST_HYS_EN_IN	(1)
+#define AW87XXX_PID_60_BURST_HYS_EN_IN_VALUE	\
+	(AW87XXX_PID_60_BURST_HYS_EN_IN << AW87XXX_PID_60_BURST_HYS_EN_START_BIT)
+
+#define AW87XXX_PID_60_BURST_HYS_EN_OUT_AND_IN	(3)
+#define AW87XXX_PID_60_BURST_HYS_EN_OUT_AND_IN_VALUE	\
+	(AW87XXX_PID_60_BURST_HYS_EN_OUT_AND_IN << AW87XXX_PID_60_BURST_HYS_EN_START_BIT)
+
+#define AW87XXX_PID_60_BURST_HYS_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BURST_HYS_EN_DEFAULT	\
+	(AW87XXX_PID_60_BURST_HYS_EN_DEFAULT_VALUE << AW87XXX_PID_60_BURST_HYS_EN_START_BIT)
+
+/* default value of BST_OVP (0x67) */
+/* #define AW87XXX_PID_60_BST_OVP_DEFAULT		(0x81) */
+
+/* LINE_MODE (0x68) detail */
+/* BST_EN_RSQN_DLY bit 0 (LINE_MODE 0x68) */
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_START_BIT	(0)
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_EN_RSQN_DLY_BITS_LEN)-1) << AW87XXX_PID_60_BST_EN_RSQN_DLY_START_BIT))
+
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_DISABLE	(0)
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_EN_RSQN_DLY_DISABLE << AW87XXX_PID_60_BST_EN_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_ENABLE	(1)
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_EN_RSQN_DLY_ENABLE << AW87XXX_PID_60_BST_EN_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_EN_RSQN_DLY_DEFAULT	\
+	(AW87XXX_PID_60_BST_EN_RSQN_DLY_DEFAULT_VALUE << AW87XXX_PID_60_BST_EN_RSQN_DLY_START_BIT)
+
+/* BST_FORCE_BOOST bit 1 (LINE_MODE 0x68) */
+#define AW87XXX_PID_60_BST_FORCE_BOOST_START_BIT	(1)
+#define AW87XXX_PID_60_BST_FORCE_BOOST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_FORCE_BOOST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_FORCE_BOOST_BITS_LEN)-1) << AW87XXX_PID_60_BST_FORCE_BOOST_START_BIT))
+
+#define AW87XXX_PID_60_BST_FORCE_BOOST_DISABLE	(0)
+#define AW87XXX_PID_60_BST_FORCE_BOOST_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_FORCE_BOOST_DISABLE << AW87XXX_PID_60_BST_FORCE_BOOST_START_BIT)
+
+#define AW87XXX_PID_60_BST_FORCE_BOOST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_FORCE_BOOST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_FORCE_BOOST_ENABLE << AW87XXX_PID_60_BST_FORCE_BOOST_START_BIT)
+
+#define AW87XXX_PID_60_BST_FORCE_BOOST_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_FORCE_BOOST_DEFAULT	\
+	(AW87XXX_PID_60_BST_FORCE_BOOST_DEFAULT_VALUE << AW87XXX_PID_60_BST_FORCE_BOOST_START_BIT)
+
+/* BST_PWM_SHORT bit 2 (LINE_MODE 0x68) */
+#define AW87XXX_PID_60_BST_PWM_SHORT_START_BIT	(2)
+#define AW87XXX_PID_60_BST_PWM_SHORT_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_PWM_SHORT_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_PWM_SHORT_BITS_LEN)-1) << AW87XXX_PID_60_BST_PWM_SHORT_START_BIT))
+
+#define AW87XXX_PID_60_BST_PWM_SHORT_VBSTBELOWVDD	(0)
+#define AW87XXX_PID_60_BST_PWM_SHORT_VBSTBELOWVDD_VALUE	\
+	(AW87XXX_PID_60_BST_PWM_SHORT_VBSTBELOWVDD << AW87XXX_PID_60_BST_PWM_SHORT_START_BIT)
+
+#define AW87XXX_PID_60_BST_PWM_SHORT_VBSTBELOWVDDMINUSVTH	(1)
+#define AW87XXX_PID_60_BST_PWM_SHORT_VBSTBELOWVDDMINUSVTH_VALUE	\
+	(AW87XXX_PID_60_BST_PWM_SHORT_VBSTBELOWVDDMINUSVTH << AW87XXX_PID_60_BST_PWM_SHORT_START_BIT)
+
+#define AW87XXX_PID_60_BST_PWM_SHORT_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_PWM_SHORT_DEFAULT	\
+	(AW87XXX_PID_60_BST_PWM_SHORT_DEFAULT_VALUE << AW87XXX_PID_60_BST_PWM_SHORT_START_BIT)
+
+/* BST_OS_WIDTH bit 7:5 (LINE_MODE 0x68) */
+#define AW87XXX_PID_60_BST_OS_WIDTH_START_BIT	(5)
+#define AW87XXX_PID_60_BST_OS_WIDTH_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_OS_WIDTH_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OS_WIDTH_BITS_LEN)-1) << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT))
+
+#define AW87XXX_PID_60_BST_OS_WIDTH_10NS	(0)
+#define AW87XXX_PID_60_BST_OS_WIDTH_10NS_VALUE	\
+	(AW87XXX_PID_60_BST_OS_WIDTH_10NS << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OS_WIDTH_20NS	(1)
+#define AW87XXX_PID_60_BST_OS_WIDTH_20NS_VALUE	\
+	(AW87XXX_PID_60_BST_OS_WIDTH_20NS << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OS_WIDTH_30NS	(2)
+#define AW87XXX_PID_60_BST_OS_WIDTH_30NS_VALUE	\
+	(AW87XXX_PID_60_BST_OS_WIDTH_30NS << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OS_WIDTH_40NS	(3)
+#define AW87XXX_PID_60_BST_OS_WIDTH_40NS_VALUE	\
+	(AW87XXX_PID_60_BST_OS_WIDTH_40NS << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OS_WIDTH_50NS	(4)
+#define AW87XXX_PID_60_BST_OS_WIDTH_50NS_VALUE	\
+	(AW87XXX_PID_60_BST_OS_WIDTH_50NS << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OS_WIDTH_60NS	(5)
+#define AW87XXX_PID_60_BST_OS_WIDTH_60NS_VALUE	\
+	(AW87XXX_PID_60_BST_OS_WIDTH_60NS << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT)
+
+#define AW87XXX_PID_60_BST_OS_WIDTH_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_OS_WIDTH_DEFAULT	\
+	(AW87XXX_PID_60_BST_OS_WIDTH_DEFAULT_VALUE << AW87XXX_PID_60_BST_OS_WIDTH_START_BIT)
+
+/* MODE1_START_DELAY bit 4:3 (LINE_MODE 0x68) */
+#define AW87XXX_PID_60_MODE1_START_DELAY_START_BIT	(3)
+#define AW87XXX_PID_60_MODE1_START_DELAY_BITS_LEN	(2)
+#define AW87XXX_PID_60_MODE1_START_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_60_MODE1_START_DELAY_BITS_LEN)-1) << AW87XXX_PID_60_MODE1_START_DELAY_START_BIT))
+
+#define AW87XXX_PID_60_MODE1_START_DELAY_15P6US	(0)
+#define AW87XXX_PID_60_MODE1_START_DELAY_15P6US_VALUE	\
+	(AW87XXX_PID_60_MODE1_START_DELAY_15P6US << AW87XXX_PID_60_MODE1_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_MODE1_START_DELAY_11P4US	(1)
+#define AW87XXX_PID_60_MODE1_START_DELAY_11P4US_VALUE	\
+	(AW87XXX_PID_60_MODE1_START_DELAY_11P4US << AW87XXX_PID_60_MODE1_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_MODE1_START_DELAY_7P3US	(2)
+#define AW87XXX_PID_60_MODE1_START_DELAY_7P3US_VALUE	\
+	(AW87XXX_PID_60_MODE1_START_DELAY_7P3US << AW87XXX_PID_60_MODE1_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_MODE1_START_DELAY_3US	(3)
+#define AW87XXX_PID_60_MODE1_START_DELAY_3US_VALUE	\
+	(AW87XXX_PID_60_MODE1_START_DELAY_3US << AW87XXX_PID_60_MODE1_START_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_MODE1_START_DELAY_DEFAULT_VALUE	(3)
+#define AW87XXX_PID_60_MODE1_START_DELAY_DEFAULT	\
+	(AW87XXX_PID_60_MODE1_START_DELAY_DEFAULT_VALUE << AW87XXX_PID_60_MODE1_START_DELAY_START_BIT)
+
+/* default value of LINE_MODE (0x68) */
+/* #define AW87XXX_PID_60_LINE_MODE_DEFAULT		(0x3F) */
+
+/* BST_ISEN (0x69) detail */
+/* BST_RSQN_DLY bit 7:5 (BST_ISEN 0x69) */
+#define AW87XXX_PID_60_BST_RSQN_DLY_START_BIT	(5)
+#define AW87XXX_PID_60_BST_RSQN_DLY_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_RSQN_DLY_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_RSQN_DLY_BITS_LEN)-1) << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT))
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_15NS	(0)
+#define AW87XXX_PID_60_BST_RSQN_DLY_15NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_15NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_25NS	(1)
+#define AW87XXX_PID_60_BST_RSQN_DLY_25NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_25NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_35NS	(2)
+#define AW87XXX_PID_60_BST_RSQN_DLY_35NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_35NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_45NS	(3)
+#define AW87XXX_PID_60_BST_RSQN_DLY_45NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_45NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_70NS	(4)
+#define AW87XXX_PID_60_BST_RSQN_DLY_70NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_70NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_80NS	(5)
+#define AW87XXX_PID_60_BST_RSQN_DLY_80NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_80NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_90NS	(6)
+#define AW87XXX_PID_60_BST_RSQN_DLY_90NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_90NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_100NS	(7)
+#define AW87XXX_PID_60_BST_RSQN_DLY_100NS_VALUE	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_100NS << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+#define AW87XXX_PID_60_BST_RSQN_DLY_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_60_BST_RSQN_DLY_DEFAULT	\
+	(AW87XXX_PID_60_BST_RSQN_DLY_DEFAULT_VALUE << AW87XXX_PID_60_BST_RSQN_DLY_START_BIT)
+
+/* BST_SLOPE bit 4:3 (BST_ISEN 0x69) */
+#define AW87XXX_PID_60_BST_SLOPE_START_BIT	(3)
+#define AW87XXX_PID_60_BST_SLOPE_BITS_LEN	(2)
+#define AW87XXX_PID_60_BST_SLOPE_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SLOPE_BITS_LEN)-1) << AW87XXX_PID_60_BST_SLOPE_START_BIT))
+
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1	(0)
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_ISLOPE1 << AW87XXX_PID_60_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1P25	(1)
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1P25_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_ISLOPE1P25 << AW87XXX_PID_60_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1P5	(2)
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1P5_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_ISLOPE1P5 << AW87XXX_PID_60_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1P75	(3)
+#define AW87XXX_PID_60_BST_SLOPE_ISLOPE1P75_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_ISLOPE1P75 << AW87XXX_PID_60_BST_SLOPE_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_SLOPE_DEFAULT	\
+	(AW87XXX_PID_60_BST_SLOPE_DEFAULT_VALUE << AW87XXX_PID_60_BST_SLOPE_START_BIT)
+
+/* BST_SLOPE_LIMIT bit 2:0 (BST_ISEN 0x69) */
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT	(0)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SLOPE_LIMIT_BITS_LEN)-1) << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT))
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_0P50ISLOPE	(0)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_0P50ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_0P50ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_0P75ISLOPE	(1)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_0P75ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_0P75ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P00ISLOPE	(2)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P00ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_1P00ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P25ISLOPE	(3)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P25ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_1P25ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P50ISLOPE	(4)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P50ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_1P50ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P75ISLOPE	(5)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_1P75ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_1P75ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_2P00ISLOPE	(6)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_2P00ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_2P00ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_2P25ISLOPE	(7)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_2P25ISLOPE_VALUE	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_2P25ISLOPE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_60_BST_SLOPE_LIMIT_DEFAULT	\
+	(AW87XXX_PID_60_BST_SLOPE_LIMIT_DEFAULT_VALUE << AW87XXX_PID_60_BST_SLOPE_LIMIT_START_BIT)
+
+/* default value of BST_ISEN (0x69) */
+/* #define AW87XXX_PID_60_BST_ISEN_DEFAULT		(0x42) */
+
+/* BST_PEAK (0x6A) detail */
+/* BST_IPEAK_LOWBAT bit 3 (BST_PEAK 0x6A) */
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_START_BIT	(3)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_IPEAK_LOWBAT_BITS_LEN)-1) << AW87XXX_PID_60_BST_IPEAK_LOWBAT_START_BIT))
+
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_2P5A	(0)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_2P5A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_LOWBAT_2P5A << AW87XXX_PID_60_BST_IPEAK_LOWBAT_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_2P75A	(1)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_2P75A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_LOWBAT_2P75A << AW87XXX_PID_60_BST_IPEAK_LOWBAT_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_DEFAULT	\
+	(AW87XXX_PID_60_BST_IPEAK_LOWBAT_DEFAULT_VALUE << AW87XXX_PID_60_BST_IPEAK_LOWBAT_START_BIT)
+
+/* BST_IPEAK_LOWBAT_EN bit 4 (BST_PEAK 0x6A) */
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_START_BIT	(4)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_DISABLE	(0)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_DISABLE << AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_ENABLE	(1)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_ENABLE << AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_IPEAK_LOWBAT_EN_START_BIT)
+
+/* BST_IPEAK_ADJ bit 5 (BST_PEAK 0x6A) */
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_START_BIT	(5)
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_IPEAK_ADJ_BITS_LEN)-1) << AW87XXX_PID_60_BST_IPEAK_ADJ_START_BIT))
+
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_IPEAK	(0)
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_IPEAK_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_ADJ_IPEAK << AW87XXX_PID_60_BST_IPEAK_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_IPEAK0P5A	(1)
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_IPEAK0P5A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_ADJ_IPEAK0P5A << AW87XXX_PID_60_BST_IPEAK_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_IPEAK_ADJ_DEFAULT	\
+	(AW87XXX_PID_60_BST_IPEAK_ADJ_DEFAULT_VALUE << AW87XXX_PID_60_BST_IPEAK_ADJ_START_BIT)
+
+/* BACK_ADPT_R400K bit 6 (BST_PEAK 0x6A) */
+#define AW87XXX_PID_60_BACK_ADPT_R400K_START_BIT	(6)
+#define AW87XXX_PID_60_BACK_ADPT_R400K_BITS_LEN	(1)
+#define AW87XXX_PID_60_BACK_ADPT_R400K_MASK	\
+	(~(((1<<AW87XXX_PID_60_BACK_ADPT_R400K_BITS_LEN)-1) << AW87XXX_PID_60_BACK_ADPT_R400K_START_BIT))
+
+#define AW87XXX_PID_60_BACK_ADPT_R400K_DISABLE	(0)
+#define AW87XXX_PID_60_BACK_ADPT_R400K_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BACK_ADPT_R400K_DISABLE << AW87XXX_PID_60_BACK_ADPT_R400K_START_BIT)
+
+#define AW87XXX_PID_60_BACK_ADPT_R400K_ENABALE	(1)
+#define AW87XXX_PID_60_BACK_ADPT_R400K_ENABALE_VALUE	\
+	(AW87XXX_PID_60_BACK_ADPT_R400K_ENABALE << AW87XXX_PID_60_BACK_ADPT_R400K_START_BIT)
+
+#define AW87XXX_PID_60_BACK_ADPT_R400K_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BACK_ADPT_R400K_DEFAULT	\
+	(AW87XXX_PID_60_BACK_ADPT_R400K_DEFAULT_VALUE << AW87XXX_PID_60_BACK_ADPT_R400K_START_BIT)
+
+/* SEL_FINISH_ID bit 7 (BST_PEAK 0x6A) */
+#define AW87XXX_PID_60_SEL_FINISH_ID_START_BIT	(7)
+#define AW87XXX_PID_60_SEL_FINISH_ID_BITS_LEN	(1)
+#define AW87XXX_PID_60_SEL_FINISH_ID_MASK	\
+	(~(((1<<AW87XXX_PID_60_SEL_FINISH_ID_BITS_LEN)-1) << AW87XXX_PID_60_SEL_FINISH_ID_START_BIT))
+
+#define AW87XXX_PID_60_SEL_FINISH_ID_MODE1START	(0)
+#define AW87XXX_PID_60_SEL_FINISH_ID_MODE1START_VALUE	\
+	(AW87XXX_PID_60_SEL_FINISH_ID_MODE1START << AW87XXX_PID_60_SEL_FINISH_ID_START_BIT)
+
+#define AW87XXX_PID_60_SEL_FINISH_ID_LIMITSSFINISH	(1)
+#define AW87XXX_PID_60_SEL_FINISH_ID_LIMITSSFINISH_VALUE	\
+	(AW87XXX_PID_60_SEL_FINISH_ID_LIMITSSFINISH << AW87XXX_PID_60_SEL_FINISH_ID_START_BIT)
+
+#define AW87XXX_PID_60_SEL_FINISH_ID_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_SEL_FINISH_ID_DEFAULT	\
+	(AW87XXX_PID_60_SEL_FINISH_ID_DEFAULT_VALUE << AW87XXX_PID_60_SEL_FINISH_ID_START_BIT)
+
+/* BST_IPEAK_SS bit 2:0 (BST_PEAK 0x6A) */
+#define AW87XXX_PID_60_BST_IPEAK_SS_START_BIT	(0)
+#define AW87XXX_PID_60_BST_IPEAK_SS_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_IPEAK_SS_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_IPEAK_SS_BITS_LEN)-1) << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT))
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_0P8A	(0)
+#define AW87XXX_PID_60_BST_IPEAK_SS_0P8A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_0P8A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P0A	(1)
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P0A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_1P0A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P25A	(2)
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P25A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_1P25A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P5A	(3)
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P5A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_1P5A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P75A	(4)
+#define AW87XXX_PID_60_BST_IPEAK_SS_1P75A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_1P75A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_2A	(5)
+#define AW87XXX_PID_60_BST_IPEAK_SS_2A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_2A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_2P25A	(6)
+#define AW87XXX_PID_60_BST_IPEAK_SS_2P25A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_2P25A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_2P50A	(7)
+#define AW87XXX_PID_60_BST_IPEAK_SS_2P50A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_2P50A << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_SS_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_60_BST_IPEAK_SS_DEFAULT	\
+	(AW87XXX_PID_60_BST_IPEAK_SS_DEFAULT_VALUE << AW87XXX_PID_60_BST_IPEAK_SS_START_BIT)
+
+/* default value of BST_PEAK (0x6A) */
+/* #define AW87XXX_PID_60_BST_PEAK_DEFAULT		(0x01) */
+
+/* BST_PEAK2 (0x6B) detail */
+/* BST_BACK_EN bit 2 (BST_PEAK2 0x6B) */
+#define AW87XXX_PID_60_BST_BACK_EN_START_BIT	(2)
+#define AW87XXX_PID_60_BST_BACK_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_BACK_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_BACK_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_BACK_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_BACK_EN_ENABLE	(0)
+#define AW87XXX_PID_60_BST_BACK_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_BACK_EN_ENABLE << AW87XXX_PID_60_BST_BACK_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_BACK_EN_DIABALE	(1)
+#define AW87XXX_PID_60_BST_BACK_EN_DIABALE_VALUE	\
+	(AW87XXX_PID_60_BST_BACK_EN_DIABALE << AW87XXX_PID_60_BST_BACK_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_BACK_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_BACK_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_BACK_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_BACK_EN_START_BIT)
+
+/* BST_IPEAK_ITH_EN bit 3 (BST_PEAK2 0x6B) */
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_START_BIT	(3)
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_IPEAK_ITH_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_IPEAK_ITH_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_IPEAK	(0)
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_IPEAK_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_ITH_EN_IPEAK << AW87XXX_PID_60_BST_IPEAK_ITH_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_IPEAKMINUS0P5A	(1)
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_IPEAKMINUS0P5A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_ITH_EN_IPEAKMINUS0P5A << AW87XXX_PID_60_BST_IPEAK_ITH_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_IPEAK_ITH_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_IPEAK_ITH_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_IPEAK_ITH_EN_START_BIT)
+
+/* BST_IPEAK_TRIM bit 7:4 (BST_PEAK2 0x6B) */
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT	(4)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_BITS_LEN	(4)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_IPEAK_TRIM_BITS_LEN)-1) << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT))
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0A	(0)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_0A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0P2A	(1)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0P2A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_0P2A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0P4A	(2)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0P4A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_0P4A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0P6A	(3)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_0P6A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_0P6A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P6A	(8)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P6A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P6A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P4A	(9)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P4A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P4A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P2A	(10)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P2A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1P2A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1A	(11)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS1A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P8A	(12)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P8A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P8A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P6A	(13)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P6A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P6A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P4A	(14)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P4A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P4A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P2A	(15)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P2A_VALUE	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_MINUS0P2A << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_IPEAK_TRIM_DEFAULT	\
+	(AW87XXX_PID_60_BST_IPEAK_TRIM_DEFAULT_VALUE << AW87XXX_PID_60_BST_IPEAK_TRIM_START_BIT)
+
+/* OFFTIME_OUT_C bit 1:0 (BST_PEAK2 0x6B) */
+#define AW87XXX_PID_60_OFFTIME_OUT_C_START_BIT	(0)
+#define AW87XXX_PID_60_OFFTIME_OUT_C_BITS_LEN	(2)
+#define AW87XXX_PID_60_OFFTIME_OUT_C_MASK	\
+	(~(((1<<AW87XXX_PID_60_OFFTIME_OUT_C_BITS_LEN)-1) << AW87XXX_PID_60_OFFTIME_OUT_C_START_BIT))
+
+#define AW87XXX_PID_60_OFFTIME_OUT_C_0	(0)
+#define AW87XXX_PID_60_OFFTIME_OUT_C_0_VALUE	\
+	(AW87XXX_PID_60_OFFTIME_OUT_C_0 << AW87XXX_PID_60_OFFTIME_OUT_C_START_BIT)
+
+#define AW87XXX_PID_60_OFFTIME_OUT_C_1	(1)
+#define AW87XXX_PID_60_OFFTIME_OUT_C_1_VALUE	\
+	(AW87XXX_PID_60_OFFTIME_OUT_C_1 << AW87XXX_PID_60_OFFTIME_OUT_C_START_BIT)
+
+#define AW87XXX_PID_60_OFFTIME_OUT_C_2	(2)
+#define AW87XXX_PID_60_OFFTIME_OUT_C_2_VALUE	\
+	(AW87XXX_PID_60_OFFTIME_OUT_C_2 << AW87XXX_PID_60_OFFTIME_OUT_C_START_BIT)
+
+#define AW87XXX_PID_60_OFFTIME_OUT_C_3	(3)
+#define AW87XXX_PID_60_OFFTIME_OUT_C_3_VALUE	\
+	(AW87XXX_PID_60_OFFTIME_OUT_C_3 << AW87XXX_PID_60_OFFTIME_OUT_C_START_BIT)
+
+#define AW87XXX_PID_60_OFFTIME_OUT_C_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_60_OFFTIME_OUT_C_DEFAULT	\
+	(AW87XXX_PID_60_OFFTIME_OUT_C_DEFAULT_VALUE << AW87XXX_PID_60_OFFTIME_OUT_C_START_BIT)
+
+/* default value of BST_PEAK2 (0x6B) */
+/* #define AW87XXX_PID_60_BST_PEAK2_DEFAULT		(0x02) */
+
+/* OFFTIME (0x6C) detail */
+/* HEAD_ROOM bit 3 (OFFTIME 0x6C) */
+#define AW87XXX_PID_60_HEAD_ROOM_START_BIT	(3)
+#define AW87XXX_PID_60_HEAD_ROOM_BITS_LEN	(1)
+#define AW87XXX_PID_60_HEAD_ROOM_MASK	\
+	(~(((1<<AW87XXX_PID_60_HEAD_ROOM_BITS_LEN)-1) << AW87XXX_PID_60_HEAD_ROOM_START_BIT))
+
+#define AW87XXX_PID_60_HEAD_ROOM_1P5V	(0)
+#define AW87XXX_PID_60_HEAD_ROOM_1P5V_VALUE	\
+	(AW87XXX_PID_60_HEAD_ROOM_1P5V << AW87XXX_PID_60_HEAD_ROOM_START_BIT)
+
+#define AW87XXX_PID_60_HEAD_ROOM_2P0V	(1)
+#define AW87XXX_PID_60_HEAD_ROOM_2P0V_VALUE	\
+	(AW87XXX_PID_60_HEAD_ROOM_2P0V << AW87XXX_PID_60_HEAD_ROOM_START_BIT)
+
+#define AW87XXX_PID_60_HEAD_ROOM_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_HEAD_ROOM_DEFAULT	\
+	(AW87XXX_PID_60_HEAD_ROOM_DEFAULT_VALUE << AW87XXX_PID_60_HEAD_ROOM_START_BIT)
+
+/* BST_OFFTIME_EN bit 7 (OFFTIME 0x6C) */
+#define AW87XXX_PID_60_BST_OFFTIME_EN_START_BIT	(7)
+#define AW87XXX_PID_60_BST_OFFTIME_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OFFTIME_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OFFTIME_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_OFFTIME_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_OFFTIME_EN_ENABLE	(0)
+#define AW87XXX_PID_60_BST_OFFTIME_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_EN_ENABLE << AW87XXX_PID_60_BST_OFFTIME_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_EN_DIABALE	(1)
+#define AW87XXX_PID_60_BST_OFFTIME_EN_DIABALE_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_EN_DIABALE << AW87XXX_PID_60_BST_OFFTIME_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_OFFTIME_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_OFFTIME_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_OFFTIME_EN_START_BIT)
+
+/* BST_OFFTIME bit 6:4 (OFFTIME 0x6C) */
+#define AW87XXX_PID_60_BST_OFFTIME_START_BIT	(4)
+#define AW87XXX_PID_60_BST_OFFTIME_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_OFFTIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OFFTIME_BITS_LEN)-1) << AW87XXX_PID_60_BST_OFFTIME_START_BIT))
+
+#define AW87XXX_PID_60_BST_OFFTIME_4	(0)
+#define AW87XXX_PID_60_BST_OFFTIME_4_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_4 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_3	(1)
+#define AW87XXX_PID_60_BST_OFFTIME_3_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_3 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_2	(2)
+#define AW87XXX_PID_60_BST_OFFTIME_2_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_2 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_1	(3)
+#define AW87XXX_PID_60_BST_OFFTIME_1_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_1 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_0	(4)
+#define AW87XXX_PID_60_BST_OFFTIME_0_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_0 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_MINUS1	(5)
+#define AW87XXX_PID_60_BST_OFFTIME_MINUS1_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_MINUS1 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_MINUS2	(6)
+#define AW87XXX_PID_60_BST_OFFTIME_MINUS2_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_MINUS2 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_MINUS3	(7)
+#define AW87XXX_PID_60_BST_OFFTIME_MINUS3_VALUE	\
+	(AW87XXX_PID_60_BST_OFFTIME_MINUS3 << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+#define AW87XXX_PID_60_BST_OFFTIME_DEFAULT_VALUE	(4)
+#define AW87XXX_PID_60_BST_OFFTIME_DEFAULT	\
+	(AW87XXX_PID_60_BST_OFFTIME_DEFAULT_VALUE << AW87XXX_PID_60_BST_OFFTIME_START_BIT)
+
+/* BST_OUT_VTH0 bit 2:0 (OFFTIME 0x6C) */
+#define AW87XXX_PID_60_BST_OUT_VTH0_START_BIT	(0)
+#define AW87XXX_PID_60_BST_OUT_VTH0_BITS_LEN	(3)
+#define AW87XXX_PID_60_BST_OUT_VTH0_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OUT_VTH0_BITS_LEN)-1) << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT))
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_6P5V	(0)
+#define AW87XXX_PID_60_BST_OUT_VTH0_6P5V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_6P5V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_6P75V	(1)
+#define AW87XXX_PID_60_BST_OUT_VTH0_6P75V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_6P75V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P0V	(2)
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P0V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_7P0V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P25V	(3)
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P25V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_7P25V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P5V	(4)
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P5V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_7P5V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P75V	(5)
+#define AW87XXX_PID_60_BST_OUT_VTH0_7P75V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_7P75V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_8P0V	(6)
+#define AW87XXX_PID_60_BST_OUT_VTH0_8P0V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_8P0V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_8P25V	(7)
+#define AW87XXX_PID_60_BST_OUT_VTH0_8P25V_VALUE	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_8P25V << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+#define AW87XXX_PID_60_BST_OUT_VTH0_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_BST_OUT_VTH0_DEFAULT	\
+	(AW87XXX_PID_60_BST_OUT_VTH0_DEFAULT_VALUE << AW87XXX_PID_60_BST_OUT_VTH0_START_BIT)
+
+/* default value of OFFTIME (0x6C) */
+/* #define AW87XXX_PID_60_OFFTIME_DEFAULT		(0x41) */
+
+/* ADPBST (0x6D) detail */
+/* REG_CLK_CP_OTA bit 0 (ADPBST 0x6D) */
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_START_BIT	(0)
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_CLK_CP_OTA_BITS_LEN)-1) << AW87XXX_PID_60_REG_CLK_CP_OTA_START_BIT))
+
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_200KHZ	(0)
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_200KHZ_VALUE	\
+	(AW87XXX_PID_60_REG_CLK_CP_OTA_200KHZ << AW87XXX_PID_60_REG_CLK_CP_OTA_START_BIT)
+
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_400KHZ	(1)
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_400KHZ_VALUE	\
+	(AW87XXX_PID_60_REG_CLK_CP_OTA_400KHZ << AW87XXX_PID_60_REG_CLK_CP_OTA_START_BIT)
+
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_REG_CLK_CP_OTA_DEFAULT	\
+	(AW87XXX_PID_60_REG_CLK_CP_OTA_DEFAULT_VALUE << AW87XXX_PID_60_REG_CLK_CP_OTA_START_BIT)
+
+/* OTA_MD2 bit 1 (ADPBST 0x6D) */
+#define AW87XXX_PID_60_OTA_MD2_START_BIT	(1)
+#define AW87XXX_PID_60_OTA_MD2_BITS_LEN	(1)
+#define AW87XXX_PID_60_OTA_MD2_MASK		\
+	(~(((1<<AW87XXX_PID_60_OTA_MD2_BITS_LEN)-1) << AW87XXX_PID_60_OTA_MD2_START_BIT))
+
+#define AW87XXX_PID_60_OTA_MD2_DISABLE	(0)
+#define AW87XXX_PID_60_OTA_MD2_DISABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_MD2_DISABLE << AW87XXX_PID_60_OTA_MD2_START_BIT)
+
+#define AW87XXX_PID_60_OTA_MD2_ENABLE	(1)
+#define AW87XXX_PID_60_OTA_MD2_ENABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_MD2_ENABLE << AW87XXX_PID_60_OTA_MD2_START_BIT)
+
+#define AW87XXX_PID_60_OTA_MD2_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_OTA_MD2_DEFAULT	\
+	(AW87XXX_PID_60_OTA_MD2_DEFAULT_VALUE << AW87XXX_PID_60_OTA_MD2_START_BIT)
+
+/* OTA_RCV bit 2 (ADPBST 0x6D) */
+#define AW87XXX_PID_60_OTA_RCV_START_BIT	(2)
+#define AW87XXX_PID_60_OTA_RCV_BITS_LEN	(1)
+#define AW87XXX_PID_60_OTA_RCV_MASK		\
+	(~(((1<<AW87XXX_PID_60_OTA_RCV_BITS_LEN)-1) << AW87XXX_PID_60_OTA_RCV_START_BIT))
+
+#define AW87XXX_PID_60_OTA_RCV_DISABLE	(0)
+#define AW87XXX_PID_60_OTA_RCV_DISABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_RCV_DISABLE << AW87XXX_PID_60_OTA_RCV_START_BIT)
+
+#define AW87XXX_PID_60_OTA_RCV_ENABLE	(1)
+#define AW87XXX_PID_60_OTA_RCV_ENABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_RCV_ENABLE << AW87XXX_PID_60_OTA_RCV_START_BIT)
+
+#define AW87XXX_PID_60_OTA_RCV_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_OTA_RCV_DEFAULT	\
+	(AW87XXX_PID_60_OTA_RCV_DEFAULT_VALUE << AW87XXX_PID_60_OTA_RCV_START_BIT)
+
+/* MSBM_VDD_SEL bit 3 (ADPBST 0x6D) */
+#define AW87XXX_PID_60_MSBM_VDD_SEL_START_BIT	(3)
+#define AW87XXX_PID_60_MSBM_VDD_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_60_MSBM_VDD_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_MSBM_VDD_SEL_BITS_LEN)-1) << AW87XXX_PID_60_MSBM_VDD_SEL_START_BIT))
+
+#define AW87XXX_PID_60_MSBM_VDD_SEL_VDDBELOW4P5V	(0)
+#define AW87XXX_PID_60_MSBM_VDD_SEL_VDDBELOW4P5V_VALUE	\
+	(AW87XXX_PID_60_MSBM_VDD_SEL_VDDBELOW4P5V << AW87XXX_PID_60_MSBM_VDD_SEL_START_BIT)
+
+#define AW87XXX_PID_60_MSBM_VDD_SEL_VDDABOVE4P5V	(1)
+#define AW87XXX_PID_60_MSBM_VDD_SEL_VDDABOVE4P5V_VALUE	\
+	(AW87XXX_PID_60_MSBM_VDD_SEL_VDDABOVE4P5V << AW87XXX_PID_60_MSBM_VDD_SEL_START_BIT)
+
+#define AW87XXX_PID_60_MSBM_VDD_SEL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_MSBM_VDD_SEL_DEFAULT	\
+	(AW87XXX_PID_60_MSBM_VDD_SEL_DEFAULT_VALUE << AW87XXX_PID_60_MSBM_VDD_SEL_START_BIT)
+
+/* DLY_EN bit 4 (ADPBST 0x6D) */
+#define AW87XXX_PID_60_DLY_EN_START_BIT	(4)
+#define AW87XXX_PID_60_DLY_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_DLY_EN_MASK		\
+	(~(((1<<AW87XXX_PID_60_DLY_EN_BITS_LEN)-1) << AW87XXX_PID_60_DLY_EN_START_BIT))
+
+#define AW87XXX_PID_60_DLY_EN_DISABLE	(0)
+#define AW87XXX_PID_60_DLY_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_DLY_EN_DISABLE << AW87XXX_PID_60_DLY_EN_START_BIT)
+
+#define AW87XXX_PID_60_DLY_EN_ENABLE	(1)
+#define AW87XXX_PID_60_DLY_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_DLY_EN_ENABLE << AW87XXX_PID_60_DLY_EN_START_BIT)
+
+#define AW87XXX_PID_60_DLY_EN_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_DLY_EN_DEFAULT	\
+	(AW87XXX_PID_60_DLY_EN_DEFAULT_VALUE << AW87XXX_PID_60_DLY_EN_START_BIT)
+
+/* DLY_CLK_EN bit 5 (ADPBST 0x6D) */
+#define AW87XXX_PID_60_DLY_CLK_EN_START_BIT	(5)
+#define AW87XXX_PID_60_DLY_CLK_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_DLY_CLK_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_DLY_CLK_EN_BITS_LEN)-1) << AW87XXX_PID_60_DLY_CLK_EN_START_BIT))
+
+#define AW87XXX_PID_60_DLY_CLK_EN_DISABLE	(0)
+#define AW87XXX_PID_60_DLY_CLK_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_DLY_CLK_EN_DISABLE << AW87XXX_PID_60_DLY_CLK_EN_START_BIT)
+
+#define AW87XXX_PID_60_DLY_CLK_EN_ENABLE	(1)
+#define AW87XXX_PID_60_DLY_CLK_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_DLY_CLK_EN_ENABLE << AW87XXX_PID_60_DLY_CLK_EN_START_BIT)
+
+#define AW87XXX_PID_60_DLY_CLK_EN_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_DLY_CLK_EN_DEFAULT	\
+	(AW87XXX_PID_60_DLY_CLK_EN_DEFAULT_VALUE << AW87XXX_PID_60_DLY_CLK_EN_START_BIT)
+
+/* ADPBST_FALL_TIME bit 7:6 (ADPBST 0x6D) */
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_START_BIT	(6)
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_BITS_LEN	(2)
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_ADPBST_FALL_TIME_BITS_LEN)-1) << AW87XXX_PID_60_ADPBST_FALL_TIME_START_BIT))
+
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_5MS	(0)
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_5MS_VALUE	\
+	(AW87XXX_PID_60_ADPBST_FALL_TIME_5MS << AW87XXX_PID_60_ADPBST_FALL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_10MS	(1)
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_10MS_VALUE	\
+	(AW87XXX_PID_60_ADPBST_FALL_TIME_10MS << AW87XXX_PID_60_ADPBST_FALL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_20MS	(2)
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_20MS_VALUE	\
+	(AW87XXX_PID_60_ADPBST_FALL_TIME_20MS << AW87XXX_PID_60_ADPBST_FALL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_40MS	(3)
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_40MS_VALUE	\
+	(AW87XXX_PID_60_ADPBST_FALL_TIME_40MS << AW87XXX_PID_60_ADPBST_FALL_TIME_START_BIT)
+
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_ADPBST_FALL_TIME_DEFAULT	\
+	(AW87XXX_PID_60_ADPBST_FALL_TIME_DEFAULT_VALUE << AW87XXX_PID_60_ADPBST_FALL_TIME_START_BIT)
+
+/* default value of ADPBST (0x6D) */
+/* #define AW87XXX_PID_60_ADPBST_DEFAULT		(0x50) */
+
+/* OTA (0x6E) detail */
+/* EN_SWF bit 0 (OTA 0x6E) */
+#define AW87XXX_PID_60_EN_SWF_START_BIT	(0)
+#define AW87XXX_PID_60_EN_SWF_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_SWF_MASK		\
+	(~(((1<<AW87XXX_PID_60_EN_SWF_BITS_LEN)-1) << AW87XXX_PID_60_EN_SWF_START_BIT))
+
+#define AW87XXX_PID_60_EN_SWF_DISABLE	(0)
+#define AW87XXX_PID_60_EN_SWF_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_SWF_DISABLE << AW87XXX_PID_60_EN_SWF_START_BIT)
+
+#define AW87XXX_PID_60_EN_SWF_ENABLE	(1)
+#define AW87XXX_PID_60_EN_SWF_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_SWF_ENABLE << AW87XXX_PID_60_EN_SWF_START_BIT)
+
+#define AW87XXX_PID_60_EN_SWF_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_SWF_DEFAULT	\
+	(AW87XXX_PID_60_EN_SWF_DEFAULT_VALUE << AW87XXX_PID_60_EN_SWF_START_BIT)
+
+/* BST_CK_MODE bit 1 (OTA 0x6E) */
+#define AW87XXX_PID_60_BST_CK_MODE_START_BIT	(1)
+#define AW87XXX_PID_60_BST_CK_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_CK_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_CK_MODE_BITS_LEN)-1) << AW87XXX_PID_60_BST_CK_MODE_START_BIT))
+
+#define AW87XXX_PID_60_BST_CK_MODE_1P6MHZ	(0)
+#define AW87XXX_PID_60_BST_CK_MODE_1P6MHZ_VALUE	\
+	(AW87XXX_PID_60_BST_CK_MODE_1P6MHZ << AW87XXX_PID_60_BST_CK_MODE_START_BIT)
+
+#define AW87XXX_PID_60_BST_CK_MODE_2P0MHZ	(1)
+#define AW87XXX_PID_60_BST_CK_MODE_2P0MHZ_VALUE	\
+	(AW87XXX_PID_60_BST_CK_MODE_2P0MHZ << AW87XXX_PID_60_BST_CK_MODE_START_BIT)
+
+#define AW87XXX_PID_60_BST_CK_MODE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_BST_CK_MODE_DEFAULT	\
+	(AW87XXX_PID_60_BST_CK_MODE_DEFAULT_VALUE << AW87XXX_PID_60_BST_CK_MODE_START_BIT)
+
+/* EN_LOOP_GBW_REDUCE bit 2 (OTA 0x6E) */
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_START_BIT	(2)
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_BITS_LEN)-1) << AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_START_BIT))
+
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_DISABLE	(0)
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_DISABLE << AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_START_BIT)
+
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_ENABLE	(1)
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_ENABLE << AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_START_BIT)
+
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_DEFAULT	\
+	(AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_DEFAULT_VALUE << AW87XXX_PID_60_EN_LOOP_GBW_REDUCE_START_BIT)
+
+/* EN_DEM_POWER bit 3 (OTA 0x6E) */
+#define AW87XXX_PID_60_EN_DEM_POWER_START_BIT	(3)
+#define AW87XXX_PID_60_EN_DEM_POWER_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_DEM_POWER_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_DEM_POWER_BITS_LEN)-1) << AW87XXX_PID_60_EN_DEM_POWER_START_BIT))
+
+#define AW87XXX_PID_60_EN_DEM_POWER_DISABLE	(0)
+#define AW87XXX_PID_60_EN_DEM_POWER_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_DEM_POWER_DISABLE << AW87XXX_PID_60_EN_DEM_POWER_START_BIT)
+
+#define AW87XXX_PID_60_EN_DEM_POWER_ENABLE	(1)
+#define AW87XXX_PID_60_EN_DEM_POWER_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_DEM_POWER_ENABLE << AW87XXX_PID_60_EN_DEM_POWER_START_BIT)
+
+#define AW87XXX_PID_60_EN_DEM_POWER_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_DEM_POWER_DEFAULT	\
+	(AW87XXX_PID_60_EN_DEM_POWER_DEFAULT_VALUE << AW87XXX_PID_60_EN_DEM_POWER_START_BIT)
+
+/* REG_CLK_CP_RIN bit 4 (OTA 0x6E) */
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_START_BIT	(4)
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_CLK_CP_RIN_BITS_LEN)-1) << AW87XXX_PID_60_REG_CLK_CP_RIN_START_BIT))
+
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_200KHZ	(0)
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_200KHZ_VALUE	\
+	(AW87XXX_PID_60_REG_CLK_CP_RIN_200KHZ << AW87XXX_PID_60_REG_CLK_CP_RIN_START_BIT)
+
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_400KHZ	(1)
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_400KHZ_VALUE	\
+	(AW87XXX_PID_60_REG_CLK_CP_RIN_400KHZ << AW87XXX_PID_60_REG_CLK_CP_RIN_START_BIT)
+
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_REG_CLK_CP_RIN_DEFAULT	\
+	(AW87XXX_PID_60_REG_CLK_CP_RIN_DEFAULT_VALUE << AW87XXX_PID_60_REG_CLK_CP_RIN_START_BIT)
+
+/* EN_DEM bit 5 (OTA 0x6E) */
+#define AW87XXX_PID_60_EN_DEM_START_BIT	(5)
+#define AW87XXX_PID_60_EN_DEM_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_DEM_MASK		\
+	(~(((1<<AW87XXX_PID_60_EN_DEM_BITS_LEN)-1) << AW87XXX_PID_60_EN_DEM_START_BIT))
+
+#define AW87XXX_PID_60_EN_DEM_DISABLE	(0)
+#define AW87XXX_PID_60_EN_DEM_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_DEM_DISABLE << AW87XXX_PID_60_EN_DEM_START_BIT)
+
+#define AW87XXX_PID_60_EN_DEM_ENABLE	(1)
+#define AW87XXX_PID_60_EN_DEM_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_DEM_ENABLE << AW87XXX_PID_60_EN_DEM_START_BIT)
+
+#define AW87XXX_PID_60_EN_DEM_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_DEM_DEFAULT	\
+	(AW87XXX_PID_60_EN_DEM_DEFAULT_VALUE << AW87XXX_PID_60_EN_DEM_START_BIT)
+
+/* OTA_NG bit 6 (OTA 0x6E) */
+#define AW87XXX_PID_60_OTA_NG_START_BIT	(6)
+#define AW87XXX_PID_60_OTA_NG_BITS_LEN	(1)
+#define AW87XXX_PID_60_OTA_NG_MASK		\
+	(~(((1<<AW87XXX_PID_60_OTA_NG_BITS_LEN)-1) << AW87XXX_PID_60_OTA_NG_START_BIT))
+
+#define AW87XXX_PID_60_OTA_NG_DISABLE	(0)
+#define AW87XXX_PID_60_OTA_NG_DISABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_NG_DISABLE << AW87XXX_PID_60_OTA_NG_START_BIT)
+
+#define AW87XXX_PID_60_OTA_NG_ENABLE	(1)
+#define AW87XXX_PID_60_OTA_NG_ENABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_NG_ENABLE << AW87XXX_PID_60_OTA_NG_START_BIT)
+
+#define AW87XXX_PID_60_OTA_NG_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_OTA_NG_DEFAULT	\
+	(AW87XXX_PID_60_OTA_NG_DEFAULT_VALUE << AW87XXX_PID_60_OTA_NG_START_BIT)
+
+/* OTA_CP bit 7 (OTA 0x6E) */
+#define AW87XXX_PID_60_OTA_CP_START_BIT	(7)
+#define AW87XXX_PID_60_OTA_CP_BITS_LEN	(1)
+#define AW87XXX_PID_60_OTA_CP_MASK		\
+	(~(((1<<AW87XXX_PID_60_OTA_CP_BITS_LEN)-1) << AW87XXX_PID_60_OTA_CP_START_BIT))
+
+#define AW87XXX_PID_60_OTA_CP_DISABLE	(0)
+#define AW87XXX_PID_60_OTA_CP_DISABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_CP_DISABLE << AW87XXX_PID_60_OTA_CP_START_BIT)
+
+#define AW87XXX_PID_60_OTA_CP_ENABLE	(1)
+#define AW87XXX_PID_60_OTA_CP_ENABLE_VALUE	\
+	(AW87XXX_PID_60_OTA_CP_ENABLE << AW87XXX_PID_60_OTA_CP_START_BIT)
+
+#define AW87XXX_PID_60_OTA_CP_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_OTA_CP_DEFAULT	\
+	(AW87XXX_PID_60_OTA_CP_DEFAULT_VALUE << AW87XXX_PID_60_OTA_CP_START_BIT)
+
+/* default value of OTA (0x6E) */
+/* #define AW87XXX_PID_60_OTA_DEFAULT		(0xC0) */
+
+/* RAMPGEN (0x6F) detail */
+/* RAMP_1SPW_VL bit 7:6 (RAMPGEN 0x6F) */
+#define AW87XXX_PID_60_RAMP_1SPW_VL_START_BIT	(6)
+#define AW87XXX_PID_60_RAMP_1SPW_VL_BITS_LEN	(2)
+#define AW87XXX_PID_60_RAMP_1SPW_VL_MASK	\
+	(~(((1<<AW87XXX_PID_60_RAMP_1SPW_VL_BITS_LEN)-1) << AW87XXX_PID_60_RAMP_1SPW_VL_START_BIT))
+
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P16VDD	(0)
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P16VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VL_0P16VDD << AW87XXX_PID_60_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P18VDD	(1)
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P18VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VL_0P18VDD << AW87XXX_PID_60_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P20VDD	(2)
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P20VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VL_0P20VDD << AW87XXX_PID_60_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P14VDD	(3)
+#define AW87XXX_PID_60_RAMP_1SPW_VL_0P14VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VL_0P14VDD << AW87XXX_PID_60_RAMP_1SPW_VL_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_RAMP_1SPW_VL_DEFAULT	\
+	(AW87XXX_PID_60_RAMP_1SPW_VL_DEFAULT_VALUE << AW87XXX_PID_60_RAMP_1SPW_VL_START_BIT)
+
+/* PA_RAMP_AGC1 bit 5:4 (RAMPGEN 0x6F) */
+#define AW87XXX_PID_60_PA_RAMP_AGC1_START_BIT	(4)
+#define AW87XXX_PID_60_PA_RAMP_AGC1_BITS_LEN	(2)
+#define AW87XXX_PID_60_PA_RAMP_AGC1_MASK	\
+	(~(((1<<AW87XXX_PID_60_PA_RAMP_AGC1_BITS_LEN)-1) << AW87XXX_PID_60_PA_RAMP_AGC1_START_BIT))
+
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P8VDD	(0)
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P8VDD_VALUE	\
+	(AW87XXX_PID_60_PA_RAMP_AGC1_0P8VDD << AW87XXX_PID_60_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P825VDD	(1)
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P825VDD_VALUE	\
+	(AW87XXX_PID_60_PA_RAMP_AGC1_0P825VDD << AW87XXX_PID_60_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P85VDD	(2)
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P85VDD_VALUE	\
+	(AW87XXX_PID_60_PA_RAMP_AGC1_0P85VDD << AW87XXX_PID_60_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P875VDD	(3)
+#define AW87XXX_PID_60_PA_RAMP_AGC1_0P875VDD_VALUE	\
+	(AW87XXX_PID_60_PA_RAMP_AGC1_0P875VDD << AW87XXX_PID_60_PA_RAMP_AGC1_START_BIT)
+
+#define AW87XXX_PID_60_PA_RAMP_AGC1_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PA_RAMP_AGC1_DEFAULT	\
+	(AW87XXX_PID_60_PA_RAMP_AGC1_DEFAULT_VALUE << AW87XXX_PID_60_PA_RAMP_AGC1_START_BIT)
+
+/* RAMP_1SPW_VC bit 3:2 (RAMPGEN 0x6F) */
+#define AW87XXX_PID_60_RAMP_1SPW_VC_START_BIT	(2)
+#define AW87XXX_PID_60_RAMP_1SPW_VC_BITS_LEN	(2)
+#define AW87XXX_PID_60_RAMP_1SPW_VC_MASK	\
+	(~(((1<<AW87XXX_PID_60_RAMP_1SPW_VC_BITS_LEN)-1) << AW87XXX_PID_60_RAMP_1SPW_VC_START_BIT))
+
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P37VDD	(0)
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P37VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VC_0P37VDD << AW87XXX_PID_60_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P39VDD	(1)
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P39VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VC_0P39VDD << AW87XXX_PID_60_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P33VDD	(2)
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P33VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VC_0P33VDD << AW87XXX_PID_60_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P35VDD	(3)
+#define AW87XXX_PID_60_RAMP_1SPW_VC_0P35VDD_VALUE	\
+	(AW87XXX_PID_60_RAMP_1SPW_VC_0P35VDD << AW87XXX_PID_60_RAMP_1SPW_VC_START_BIT)
+
+#define AW87XXX_PID_60_RAMP_1SPW_VC_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_60_RAMP_1SPW_VC_DEFAULT	\
+	(AW87XXX_PID_60_RAMP_1SPW_VC_DEFAULT_VALUE << AW87XXX_PID_60_RAMP_1SPW_VC_START_BIT)
+
+/* SS_CONTROL bit 1:0 (RAMPGEN 0x6F) */
+#define AW87XXX_PID_60_SS_CONTROL_START_BIT	(0)
+#define AW87XXX_PID_60_SS_CONTROL_BITS_LEN	(2)
+#define AW87XXX_PID_60_SS_CONTROL_MASK	\
+	(~(((1<<AW87XXX_PID_60_SS_CONTROL_BITS_LEN)-1) << AW87XXX_PID_60_SS_CONTROL_START_BIT))
+
+#define AW87XXX_PID_60_SS_CONTROL_SS_MODE	(0)
+#define AW87XXX_PID_60_SS_CONTROL_SS_MODE_VALUE	\
+	(AW87XXX_PID_60_SS_CONTROL_SS_MODE << AW87XXX_PID_60_SS_CONTROL_START_BIT)
+
+#define AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE111	(1)
+#define AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE111_VALUE	\
+	(AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE111 << AW87XXX_PID_60_SS_CONTROL_START_BIT)
+
+#define AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE000	(2)
+#define AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE000_VALUE	\
+	(AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE000 << AW87XXX_PID_60_SS_CONTROL_START_BIT)
+
+#define AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE011	(3)
+#define AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE011_VALUE	\
+	(AW87XXX_PID_60_SS_CONTROL_SWBELOW20ABOVE011 << AW87XXX_PID_60_SS_CONTROL_START_BIT)
+
+#define AW87XXX_PID_60_SS_CONTROL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_SS_CONTROL_DEFAULT	\
+	(AW87XXX_PID_60_SS_CONTROL_DEFAULT_VALUE << AW87XXX_PID_60_SS_CONTROL_START_BIT)
+
+/* default value of RAMPGEN (0x6F) */
+/* #define AW87XXX_PID_60_RAMPGEN_DEFAULT		(0x0C) */
+
+/* CLASSD_SYSCTRL (0x70) detail */
+/* SEL_VCOM1 bit 0 (CLASSD_SYSCTRL 0x70) */
+#define AW87XXX_PID_60_SEL_VCOM1_START_BIT	(0)
+#define AW87XXX_PID_60_SEL_VCOM1_BITS_LEN	(1)
+#define AW87XXX_PID_60_SEL_VCOM1_MASK	\
+	(~(((1<<AW87XXX_PID_60_SEL_VCOM1_BITS_LEN)-1) << AW87XXX_PID_60_SEL_VCOM1_START_BIT))
+
+#define AW87XXX_PID_60_SEL_VCOM1_VCOM11P4V	(0)
+#define AW87XXX_PID_60_SEL_VCOM1_VCOM11P4V_VALUE	\
+	(AW87XXX_PID_60_SEL_VCOM1_VCOM11P4V << AW87XXX_PID_60_SEL_VCOM1_START_BIT)
+
+/*
+#define AW87XXX_PID_60_SEL_VCOM1_VCOM11P4V	(1)
+#define AW87XXX_PID_60_SEL_VCOM1_VCOM11P4V_VALUE	\
+	(AW87XXX_PID_60_SEL_VCOM1_VCOM11P4V << AW87XXX_PID_60_SEL_VCOM1_START_BIT)
+*/
+/*
+Fix me here:
+reg_addr:0x70, reg_name:CLASSD_SYSCTRL, field_name:SEL_VCOM1, content:When ENOTA=0:
+maybe need to fix manually
+*/
+#define AW87XXX_PID_60_SEL_VCOM1_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_SEL_VCOM1_DEFAULT	\
+	(AW87XXX_PID_60_SEL_VCOM1_DEFAULT_VALUE << AW87XXX_PID_60_SEL_VCOM1_START_BIT)
+
+/* PA_OPD bit 1 (CLASSD_SYSCTRL 0x70) */
+#define AW87XXX_PID_60_PA_OPD_START_BIT	(1)
+#define AW87XXX_PID_60_PA_OPD_BITS_LEN	(1)
+#define AW87XXX_PID_60_PA_OPD_MASK		\
+	(~(((1<<AW87XXX_PID_60_PA_OPD_BITS_LEN)-1) << AW87XXX_PID_60_PA_OPD_START_BIT))
+
+#define AW87XXX_PID_60_PA_OPD_FLOATING	(0)
+#define AW87XXX_PID_60_PA_OPD_FLOATING_VALUE	\
+	(AW87XXX_PID_60_PA_OPD_FLOATING << AW87XXX_PID_60_PA_OPD_START_BIT)
+
+#define AW87XXX_PID_60_PA_OPD_TIED_TO_GND	(1)
+#define AW87XXX_PID_60_PA_OPD_TIED_TO_GND_VALUE	\
+	(AW87XXX_PID_60_PA_OPD_TIED_TO_GND << AW87XXX_PID_60_PA_OPD_START_BIT)
+
+#define AW87XXX_PID_60_PA_OPD_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_PA_OPD_DEFAULT	\
+	(AW87XXX_PID_60_PA_OPD_DEFAULT_VALUE << AW87XXX_PID_60_PA_OPD_START_BIT)
+
+/* CLK_OCP_SEL bit 4 (CLASSD_SYSCTRL 0x70) */
+#define AW87XXX_PID_60_CLK_OCP_SEL_START_BIT	(4)
+#define AW87XXX_PID_60_CLK_OCP_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_60_CLK_OCP_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_CLK_OCP_SEL_BITS_LEN)-1) << AW87XXX_PID_60_CLK_OCP_SEL_START_BIT))
+
+#define AW87XXX_PID_60_CLK_OCP_SEL_160MS	(0)
+#define AW87XXX_PID_60_CLK_OCP_SEL_160MS_VALUE	\
+	(AW87XXX_PID_60_CLK_OCP_SEL_160MS << AW87XXX_PID_60_CLK_OCP_SEL_START_BIT)
+
+#define AW87XXX_PID_60_CLK_OCP_SEL_SHUTDOWN	(1)
+#define AW87XXX_PID_60_CLK_OCP_SEL_SHUTDOWN_VALUE	\
+	(AW87XXX_PID_60_CLK_OCP_SEL_SHUTDOWN << AW87XXX_PID_60_CLK_OCP_SEL_START_BIT)
+
+#define AW87XXX_PID_60_CLK_OCP_SEL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_CLK_OCP_SEL_DEFAULT	\
+	(AW87XXX_PID_60_CLK_OCP_SEL_DEFAULT_VALUE << AW87XXX_PID_60_CLK_OCP_SEL_START_BIT)
+
+/* PD_OT bit 5 (CLASSD_SYSCTRL 0x70) */
+#define AW87XXX_PID_60_PD_OT_START_BIT	(5)
+#define AW87XXX_PID_60_PD_OT_BITS_LEN	(1)
+#define AW87XXX_PID_60_PD_OT_MASK		\
+	(~(((1<<AW87XXX_PID_60_PD_OT_BITS_LEN)-1) << AW87XXX_PID_60_PD_OT_START_BIT))
+
+#define AW87XXX_PID_60_PD_OT_ENABLE		(0)
+#define AW87XXX_PID_60_PD_OT_ENABLE_VALUE	\
+	(AW87XXX_PID_60_PD_OT_ENABLE << AW87XXX_PID_60_PD_OT_START_BIT)
+
+#define AW87XXX_PID_60_PD_OT_SHUTDOWN	(1)
+#define AW87XXX_PID_60_PD_OT_SHUTDOWN_VALUE	\
+	(AW87XXX_PID_60_PD_OT_SHUTDOWN << AW87XXX_PID_60_PD_OT_START_BIT)
+
+#define AW87XXX_PID_60_PD_OT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PD_OT_DEFAULT	\
+	(AW87XXX_PID_60_PD_OT_DEFAULT_VALUE << AW87XXX_PID_60_PD_OT_START_BIT)
+
+/* PA_FLT_SR bit 6 (CLASSD_SYSCTRL 0x70) */
+#define AW87XXX_PID_60_PA_FLT_SR_START_BIT	(6)
+#define AW87XXX_PID_60_PA_FLT_SR_BITS_LEN	(1)
+#define AW87XXX_PID_60_PA_FLT_SR_MASK	\
+	(~(((1<<AW87XXX_PID_60_PA_FLT_SR_BITS_LEN)-1) << AW87XXX_PID_60_PA_FLT_SR_START_BIT))
+
+#define AW87XXX_PID_60_PA_FLT_SR_ADD_RC	(0)
+#define AW87XXX_PID_60_PA_FLT_SR_ADD_RC_VALUE	\
+	(AW87XXX_PID_60_PA_FLT_SR_ADD_RC << AW87XXX_PID_60_PA_FLT_SR_START_BIT)
+
+#define AW87XXX_PID_60_PA_FLT_SR_NO_RC	(1)
+#define AW87XXX_PID_60_PA_FLT_SR_NO_RC_VALUE	\
+	(AW87XXX_PID_60_PA_FLT_SR_NO_RC << AW87XXX_PID_60_PA_FLT_SR_START_BIT)
+
+#define AW87XXX_PID_60_PA_FLT_SR_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PA_FLT_SR_DEFAULT	\
+	(AW87XXX_PID_60_PA_FLT_SR_DEFAULT_VALUE << AW87XXX_PID_60_PA_FLT_SR_START_BIT)
+
+/* PA_PORT bit 3:2 (CLASSD_SYSCTRL 0x70) */
+#define AW87XXX_PID_60_PA_PORT_START_BIT	(2)
+#define AW87XXX_PID_60_PA_PORT_BITS_LEN	(2)
+#define AW87XXX_PID_60_PA_PORT_MASK		\
+	(~(((1<<AW87XXX_PID_60_PA_PORT_BITS_LEN)-1) << AW87XXX_PID_60_PA_PORT_START_BIT))
+
+#define AW87XXX_PID_60_PA_PORT_80MS		(0)
+#define AW87XXX_PID_60_PA_PORT_80MS_VALUE	\
+	(AW87XXX_PID_60_PA_PORT_80MS << AW87XXX_PID_60_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_60_PA_PORT_40MS		(1)
+#define AW87XXX_PID_60_PA_PORT_40MS_VALUE	\
+	(AW87XXX_PID_60_PA_PORT_40MS << AW87XXX_PID_60_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_60_PA_PORT_20MS		(2)
+#define AW87XXX_PID_60_PA_PORT_20MS_VALUE	\
+	(AW87XXX_PID_60_PA_PORT_20MS << AW87XXX_PID_60_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_60_PA_PORT_10MS		(3)
+#define AW87XXX_PID_60_PA_PORT_10MS_VALUE	\
+	(AW87XXX_PID_60_PA_PORT_10MS << AW87XXX_PID_60_PA_PORT_START_BIT)
+
+#define AW87XXX_PID_60_PA_PORT_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_PA_PORT_DEFAULT	\
+	(AW87XXX_PID_60_PA_PORT_DEFAULT_VALUE << AW87XXX_PID_60_PA_PORT_START_BIT)
+
+/* default value of CLASSD_SYSCTRL (0x70) */
+/* #define AW87XXX_PID_60_CLASSD_SYSCTRL_DEFAULT		(0x07) */
+
+/* GTDR (0x71) detail */
+/* REG_DUTY_T bit 0 (GTDR 0x71) */
+#define AW87XXX_PID_60_REG_DUTY_T_START_BIT	(0)
+#define AW87XXX_PID_60_REG_DUTY_T_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_DUTY_T_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_DUTY_T_BITS_LEN)-1) << AW87XXX_PID_60_REG_DUTY_T_START_BIT))
+
+#define AW87XXX_PID_60_REG_DUTY_T_DISABLE	(0)
+#define AW87XXX_PID_60_REG_DUTY_T_DISABLE_VALUE	\
+	(AW87XXX_PID_60_REG_DUTY_T_DISABLE << AW87XXX_PID_60_REG_DUTY_T_START_BIT)
+
+#define AW87XXX_PID_60_REG_DUTY_T_ENALBE	(1)
+#define AW87XXX_PID_60_REG_DUTY_T_ENALBE_VALUE	\
+	(AW87XXX_PID_60_REG_DUTY_T_ENALBE << AW87XXX_PID_60_REG_DUTY_T_START_BIT)
+
+#define AW87XXX_PID_60_REG_DUTY_T_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_REG_DUTY_T_DEFAULT	\
+	(AW87XXX_PID_60_REG_DUTY_T_DEFAULT_VALUE << AW87XXX_PID_60_REG_DUTY_T_START_BIT)
+
+/* LN_DELAY bit 1 (GTDR 0x71) */
+#define AW87XXX_PID_60_LN_DELAY_START_BIT	(1)
+#define AW87XXX_PID_60_LN_DELAY_BITS_LEN	(1)
+#define AW87XXX_PID_60_LN_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_60_LN_DELAY_BITS_LEN)-1) << AW87XXX_PID_60_LN_DELAY_START_BIT))
+
+#define AW87XXX_PID_60_LN_DELAY_DISABLE	(0)
+#define AW87XXX_PID_60_LN_DELAY_DISABLE_VALUE	\
+	(AW87XXX_PID_60_LN_DELAY_DISABLE << AW87XXX_PID_60_LN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_LN_DELAY_ENABLE	(1)
+#define AW87XXX_PID_60_LN_DELAY_ENABLE_VALUE	\
+	(AW87XXX_PID_60_LN_DELAY_ENABLE << AW87XXX_PID_60_LN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_LN_DELAY_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_LN_DELAY_DEFAULT	\
+	(AW87XXX_PID_60_LN_DELAY_DEFAULT_VALUE << AW87XXX_PID_60_LN_DELAY_START_BIT)
+
+/* HN_DELAY bit 2 (GTDR 0x71) */
+#define AW87XXX_PID_60_HN_DELAY_START_BIT	(2)
+#define AW87XXX_PID_60_HN_DELAY_BITS_LEN	(1)
+#define AW87XXX_PID_60_HN_DELAY_MASK	\
+	(~(((1<<AW87XXX_PID_60_HN_DELAY_BITS_LEN)-1) << AW87XXX_PID_60_HN_DELAY_START_BIT))
+
+#define AW87XXX_PID_60_HN_DELAY_DISABLE	(0)
+#define AW87XXX_PID_60_HN_DELAY_DISABLE_VALUE	\
+	(AW87XXX_PID_60_HN_DELAY_DISABLE << AW87XXX_PID_60_HN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_HN_DELAY_ENABLE	(1)
+#define AW87XXX_PID_60_HN_DELAY_ENABLE_VALUE	\
+	(AW87XXX_PID_60_HN_DELAY_ENABLE << AW87XXX_PID_60_HN_DELAY_START_BIT)
+
+#define AW87XXX_PID_60_HN_DELAY_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_HN_DELAY_DEFAULT	\
+	(AW87XXX_PID_60_HN_DELAY_DEFAULT_VALUE << AW87XXX_PID_60_HN_DELAY_START_BIT)
+
+/* REG_T_EDGE bit 3 (GTDR 0x71) */
+#define AW87XXX_PID_60_REG_T_EDGE_START_BIT	(3)
+#define AW87XXX_PID_60_REG_T_EDGE_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_T_EDGE_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_T_EDGE_BITS_LEN)-1) << AW87XXX_PID_60_REG_T_EDGE_START_BIT))
+
+#define AW87XXX_PID_60_REG_T_EDGE_5NS	(0)
+#define AW87XXX_PID_60_REG_T_EDGE_5NS_VALUE	\
+	(AW87XXX_PID_60_REG_T_EDGE_5NS << AW87XXX_PID_60_REG_T_EDGE_START_BIT)
+
+#define AW87XXX_PID_60_REG_T_EDGE_15NS	(1)
+#define AW87XXX_PID_60_REG_T_EDGE_15NS_VALUE	\
+	(AW87XXX_PID_60_REG_T_EDGE_15NS << AW87XXX_PID_60_REG_T_EDGE_START_BIT)
+
+#define AW87XXX_PID_60_REG_T_EDGE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_REG_T_EDGE_DEFAULT	\
+	(AW87XXX_PID_60_REG_T_EDGE_DEFAULT_VALUE << AW87XXX_PID_60_REG_T_EDGE_START_BIT)
+
+/* REG_L_PULL bit 4 (GTDR 0x71) */
+#define AW87XXX_PID_60_REG_L_PULL_START_BIT	(4)
+#define AW87XXX_PID_60_REG_L_PULL_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_L_PULL_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_L_PULL_BITS_LEN)-1) << AW87XXX_PID_60_REG_L_PULL_START_BIT))
+
+#define AW87XXX_PID_60_REG_L_PULL_7NS	(0)
+#define AW87XXX_PID_60_REG_L_PULL_7NS_VALUE	\
+	(AW87XXX_PID_60_REG_L_PULL_7NS << AW87XXX_PID_60_REG_L_PULL_START_BIT)
+
+#define AW87XXX_PID_60_REG_L_PULL_14NS	(1)
+#define AW87XXX_PID_60_REG_L_PULL_14NS_VALUE	\
+	(AW87XXX_PID_60_REG_L_PULL_14NS << AW87XXX_PID_60_REG_L_PULL_START_BIT)
+
+#define AW87XXX_PID_60_REG_L_PULL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_REG_L_PULL_DEFAULT	\
+	(AW87XXX_PID_60_REG_L_PULL_DEFAULT_VALUE << AW87XXX_PID_60_REG_L_PULL_START_BIT)
+
+/* REG_L_PUD bit 5 (GTDR 0x71) */
+#define AW87XXX_PID_60_REG_L_PUD_START_BIT	(5)
+#define AW87XXX_PID_60_REG_L_PUD_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_L_PUD_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_L_PUD_BITS_LEN)-1) << AW87XXX_PID_60_REG_L_PUD_START_BIT))
+
+#define AW87XXX_PID_60_REG_L_PUD_0NS	(0)
+#define AW87XXX_PID_60_REG_L_PUD_0NS_VALUE	\
+	(AW87XXX_PID_60_REG_L_PUD_0NS << AW87XXX_PID_60_REG_L_PUD_START_BIT)
+
+#define AW87XXX_PID_60_REG_L_PUD_3NS	(1)
+#define AW87XXX_PID_60_REG_L_PUD_3NS_VALUE	\
+	(AW87XXX_PID_60_REG_L_PUD_3NS << AW87XXX_PID_60_REG_L_PUD_START_BIT)
+
+#define AW87XXX_PID_60_REG_L_PUD_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_REG_L_PUD_DEFAULT	\
+	(AW87XXX_PID_60_REG_L_PUD_DEFAULT_VALUE << AW87XXX_PID_60_REG_L_PUD_START_BIT)
+
+/* REG_HNG_PULL bit 6 (GTDR 0x71) */
+#define AW87XXX_PID_60_REG_HNG_PULL_START_BIT	(6)
+#define AW87XXX_PID_60_REG_HNG_PULL_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_HNG_PULL_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_HNG_PULL_BITS_LEN)-1) << AW87XXX_PID_60_REG_HNG_PULL_START_BIT))
+
+#define AW87XXX_PID_60_REG_HNG_PULL_DISABLE	(0)
+#define AW87XXX_PID_60_REG_HNG_PULL_DISABLE_VALUE	\
+	(AW87XXX_PID_60_REG_HNG_PULL_DISABLE << AW87XXX_PID_60_REG_HNG_PULL_START_BIT)
+
+#define AW87XXX_PID_60_REG_HNG_PULL_ENABLE	(1)
+#define AW87XXX_PID_60_REG_HNG_PULL_ENABLE_VALUE	\
+	(AW87XXX_PID_60_REG_HNG_PULL_ENABLE << AW87XXX_PID_60_REG_HNG_PULL_START_BIT)
+
+#define AW87XXX_PID_60_REG_HNG_PULL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_REG_HNG_PULL_DEFAULT	\
+	(AW87XXX_PID_60_REG_HNG_PULL_DEFAULT_VALUE << AW87XXX_PID_60_REG_HNG_PULL_START_BIT)
+
+/* REG_DUTY_VTH bit 7 (GTDR 0x71) */
+#define AW87XXX_PID_60_REG_DUTY_VTH_START_BIT	(7)
+#define AW87XXX_PID_60_REG_DUTY_VTH_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_DUTY_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_DUTY_VTH_BITS_LEN)-1) << AW87XXX_PID_60_REG_DUTY_VTH_START_BIT))
+
+#define AW87XXX_PID_60_REG_DUTY_VTH_POVTH0	(0)
+#define AW87XXX_PID_60_REG_DUTY_VTH_POVTH0_VALUE	\
+	(AW87XXX_PID_60_REG_DUTY_VTH_POVTH0 << AW87XXX_PID_60_REG_DUTY_VTH_START_BIT)
+
+#define AW87XXX_PID_60_REG_DUTY_VTH_POVTH4	(1)
+#define AW87XXX_PID_60_REG_DUTY_VTH_POVTH4_VALUE	\
+	(AW87XXX_PID_60_REG_DUTY_VTH_POVTH4 << AW87XXX_PID_60_REG_DUTY_VTH_START_BIT)
+
+#define AW87XXX_PID_60_REG_DUTY_VTH_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_REG_DUTY_VTH_DEFAULT	\
+	(AW87XXX_PID_60_REG_DUTY_VTH_DEFAULT_VALUE << AW87XXX_PID_60_REG_DUTY_VTH_START_BIT)
+
+/* default value of GTDR (0x71) */
+/* #define AW87XXX_PID_60_GTDR_DEFAULT		(0x27) */
+
+/* OC (0x72) detail */
+/* REG_SHORT_GUARD bit 7 (OC 0x72) */
+#define AW87XXX_PID_60_REG_SHORT_GUARD_START_BIT	(7)
+#define AW87XXX_PID_60_REG_SHORT_GUARD_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_SHORT_GUARD_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_SHORT_GUARD_BITS_LEN)-1) << AW87XXX_PID_60_REG_SHORT_GUARD_START_BIT))
+
+#define AW87XXX_PID_60_REG_SHORT_GUARD_DISABLE	(0)
+#define AW87XXX_PID_60_REG_SHORT_GUARD_DISABLE_VALUE	\
+	(AW87XXX_PID_60_REG_SHORT_GUARD_DISABLE << AW87XXX_PID_60_REG_SHORT_GUARD_START_BIT)
+
+#define AW87XXX_PID_60_REG_SHORT_GUARD_ENALBE	(1)
+#define AW87XXX_PID_60_REG_SHORT_GUARD_ENALBE_VALUE	\
+	(AW87XXX_PID_60_REG_SHORT_GUARD_ENALBE << AW87XXX_PID_60_REG_SHORT_GUARD_START_BIT)
+
+#define AW87XXX_PID_60_REG_SHORT_GUARD_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_REG_SHORT_GUARD_DEFAULT	\
+	(AW87XXX_PID_60_REG_SHORT_GUARD_DEFAULT_VALUE << AW87XXX_PID_60_REG_SHORT_GUARD_START_BIT)
+
+/* PA_GTDR_DDT bit 6:5 (OC 0x72) */
+#define AW87XXX_PID_60_PA_GTDR_DDT_START_BIT	(5)
+#define AW87XXX_PID_60_PA_GTDR_DDT_BITS_LEN	(2)
+#define AW87XXX_PID_60_PA_GTDR_DDT_MASK	\
+	(~(((1<<AW87XXX_PID_60_PA_GTDR_DDT_BITS_LEN)-1) << AW87XXX_PID_60_PA_GTDR_DDT_START_BIT))
+
+#define AW87XXX_PID_60_PA_GTDR_DDT_12NS	(0)
+#define AW87XXX_PID_60_PA_GTDR_DDT_12NS_VALUE	\
+	(AW87XXX_PID_60_PA_GTDR_DDT_12NS << AW87XXX_PID_60_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_60_PA_GTDR_DDT_13NS	(1)
+#define AW87XXX_PID_60_PA_GTDR_DDT_13NS_VALUE	\
+	(AW87XXX_PID_60_PA_GTDR_DDT_13NS << AW87XXX_PID_60_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_60_PA_GTDR_DDT_14NS	(2)
+#define AW87XXX_PID_60_PA_GTDR_DDT_14NS_VALUE	\
+	(AW87XXX_PID_60_PA_GTDR_DDT_14NS << AW87XXX_PID_60_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_60_PA_GTDR_DDT_15NS	(3)
+#define AW87XXX_PID_60_PA_GTDR_DDT_15NS_VALUE	\
+	(AW87XXX_PID_60_PA_GTDR_DDT_15NS << AW87XXX_PID_60_PA_GTDR_DDT_START_BIT)
+
+#define AW87XXX_PID_60_PA_GTDR_DDT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PA_GTDR_DDT_DEFAULT	\
+	(AW87XXX_PID_60_PA_GTDR_DDT_DEFAULT_VALUE << AW87XXX_PID_60_PA_GTDR_DDT_START_BIT)
+
+/* PA_OC_ITH bit 4:2 (OC 0x72) */
+#define AW87XXX_PID_60_PA_OC_ITH_START_BIT	(2)
+#define AW87XXX_PID_60_PA_OC_ITH_BITS_LEN	(3)
+#define AW87XXX_PID_60_PA_OC_ITH_MASK	\
+	(~(((1<<AW87XXX_PID_60_PA_OC_ITH_BITS_LEN)-1) << AW87XXX_PID_60_PA_OC_ITH_START_BIT))
+
+#define AW87XXX_PID_60_PA_OC_ITH_3P4A	(0)
+#define AW87XXX_PID_60_PA_OC_ITH_3P4A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_3P4A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_3P8A	(1)
+#define AW87XXX_PID_60_PA_OC_ITH_3P8A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_3P8A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_4P2A	(2)
+#define AW87XXX_PID_60_PA_OC_ITH_4P2A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_4P2A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_4P6A	(3)
+#define AW87XXX_PID_60_PA_OC_ITH_4P6A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_4P6A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_5P0A	(4)
+#define AW87XXX_PID_60_PA_OC_ITH_5P0A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_5P0A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_5P4A	(5)
+#define AW87XXX_PID_60_PA_OC_ITH_5P4A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_5P4A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_5P7A	(6)
+#define AW87XXX_PID_60_PA_OC_ITH_5P7A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_5P7A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_6P0A	(7)
+#define AW87XXX_PID_60_PA_OC_ITH_6P0A_VALUE	\
+	(AW87XXX_PID_60_PA_OC_ITH_6P0A << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_ITH_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_60_PA_OC_ITH_DEFAULT	\
+	(AW87XXX_PID_60_PA_OC_ITH_DEFAULT_VALUE << AW87XXX_PID_60_PA_OC_ITH_START_BIT)
+
+/* PA_OC_DT bit 1:0 (OC 0x72) */
+#define AW87XXX_PID_60_PA_OC_DT_START_BIT	(0)
+#define AW87XXX_PID_60_PA_OC_DT_BITS_LEN	(2)
+#define AW87XXX_PID_60_PA_OC_DT_MASK	\
+	(~(((1<<AW87XXX_PID_60_PA_OC_DT_BITS_LEN)-1) << AW87XXX_PID_60_PA_OC_DT_START_BIT))
+
+#define AW87XXX_PID_60_PA_OC_DT_20NS	(0)
+#define AW87XXX_PID_60_PA_OC_DT_20NS_VALUE	\
+	(AW87XXX_PID_60_PA_OC_DT_20NS << AW87XXX_PID_60_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_DT_40NS	(1)
+#define AW87XXX_PID_60_PA_OC_DT_40NS_VALUE	\
+	(AW87XXX_PID_60_PA_OC_DT_40NS << AW87XXX_PID_60_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_DT_60NS	(2)
+#define AW87XXX_PID_60_PA_OC_DT_60NS_VALUE	\
+	(AW87XXX_PID_60_PA_OC_DT_60NS << AW87XXX_PID_60_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_DT_80NS	(3)
+#define AW87XXX_PID_60_PA_OC_DT_80NS_VALUE	\
+	(AW87XXX_PID_60_PA_OC_DT_80NS << AW87XXX_PID_60_PA_OC_DT_START_BIT)
+
+#define AW87XXX_PID_60_PA_OC_DT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PA_OC_DT_DEFAULT	\
+	(AW87XXX_PID_60_PA_OC_DT_DEFAULT_VALUE << AW87XXX_PID_60_PA_OC_DT_START_BIT)
+
+/* default value of OC (0x72) */
+/* #define AW87XXX_PID_60_OC_DEFAULT		(0x8C) */
+
+/* AGC_CON (0x73) detail */
+/* PA_OCSWD bit 6 (AGC_CON 0x73) */
+#define AW87XXX_PID_60_PA_OCSWD_START_BIT	(6)
+#define AW87XXX_PID_60_PA_OCSWD_BITS_LEN	(1)
+#define AW87XXX_PID_60_PA_OCSWD_MASK	\
+	(~(((1<<AW87XXX_PID_60_PA_OCSWD_BITS_LEN)-1) << AW87XXX_PID_60_PA_OCSWD_START_BIT))
+
+#define AW87XXX_PID_60_PA_OCSWD_FAST	(0)
+#define AW87XXX_PID_60_PA_OCSWD_FAST_VALUE	\
+	(AW87XXX_PID_60_PA_OCSWD_FAST << AW87XXX_PID_60_PA_OCSWD_START_BIT)
+
+#define AW87XXX_PID_60_PA_OCSWD_SLOW	(1)
+#define AW87XXX_PID_60_PA_OCSWD_SLOW_VALUE	\
+	(AW87XXX_PID_60_PA_OCSWD_SLOW << AW87XXX_PID_60_PA_OCSWD_START_BIT)
+
+#define AW87XXX_PID_60_PA_OCSWD_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_PA_OCSWD_DEFAULT	\
+	(AW87XXX_PID_60_PA_OCSWD_DEFAULT_VALUE << AW87XXX_PID_60_PA_OCSWD_START_BIT)
+
+/* PD_CROSSZERO bit 5:4 (AGC_CON 0x73) */
+#define AW87XXX_PID_60_PD_CROSSZERO_START_BIT	(4)
+#define AW87XXX_PID_60_PD_CROSSZERO_BITS_LEN	(2)
+#define AW87XXX_PID_60_PD_CROSSZERO_MASK	\
+	(~(((1<<AW87XXX_PID_60_PD_CROSSZERO_BITS_LEN)-1) << AW87XXX_PID_60_PD_CROSSZERO_START_BIT))
+
+#define AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC2_3_CROSSZERO	(0)
+#define AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC2_3_CROSSZERO_VALUE	\
+	(AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC2_3_CROSSZERO << AW87XXX_PID_60_PD_CROSSZERO_START_BIT)
+
+#define AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC23_CROSSZERO	(1)
+#define AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC23_CROSSZERO_VALUE	\
+	(AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC23_CROSSZERO << AW87XXX_PID_60_PD_CROSSZERO_START_BIT)
+
+/*
+#define AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC23_CROSSZERO	(2)
+#define AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC23_CROSSZERO_VALUE	\
+	(AW87XXX_PID_60_PD_CROSSZERO_ENABLE_AGC1_WHEN_PVDD_IS_RISINGAGC23_CROSSZERO << AW87XXX_PID_60_PD_CROSSZERO_START_BIT)
+*/
+
+#define AW87XXX_PID_60_PD_CROSSZERO_AGC123_CROSSZERO	(3)
+#define AW87XXX_PID_60_PD_CROSSZERO_AGC123_CROSSZERO_VALUE	\
+	(AW87XXX_PID_60_PD_CROSSZERO_AGC123_CROSSZERO << AW87XXX_PID_60_PD_CROSSZERO_START_BIT)
+/*
+Fix me here:
+reg_addr:0x73, reg_name:AGC_CON, field_name:PD_CROSSZERO, content:AGC cross_zero adaptively When EN_AGC1_ADP=1
+maybe need to fix manually
+*/
+#define AW87XXX_PID_60_PD_CROSSZERO_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PD_CROSSZERO_DEFAULT	\
+	(AW87XXX_PID_60_PD_CROSSZERO_DEFAULT_VALUE << AW87XXX_PID_60_PD_CROSSZERO_START_BIT)
+
+/* AGC1_VTH_SEL bit 3:2 (AGC_CON 0x73) */
+#define AW87XXX_PID_60_AGC1_VTH_SEL_START_BIT	(2)
+#define AW87XXX_PID_60_AGC1_VTH_SEL_BITS_LEN	(2)
+#define AW87XXX_PID_60_AGC1_VTH_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC1_VTH_SEL_BITS_LEN)-1) << AW87XXX_PID_60_AGC1_VTH_SEL_START_BIT))
+
+#define AW87XXX_PID_60_AGC1_VTH_SEL_RAMPGEN	(0)
+#define AW87XXX_PID_60_AGC1_VTH_SEL_RAMPGEN_VALUE	\
+	(AW87XXX_PID_60_AGC1_VTH_SEL_RAMPGEN << AW87XXX_PID_60_AGC1_VTH_SEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_VTH_SEL_THGEN	(1)
+#define AW87XXX_PID_60_AGC1_VTH_SEL_THGEN_VALUE	\
+	(AW87XXX_PID_60_AGC1_VTH_SEL_THGEN << AW87XXX_PID_60_AGC1_VTH_SEL_START_BIT)
+
+#define AW87XXX_PID_60_AGC1_VTH_SEL_BOTH_MIN	(2)
+#define AW87XXX_PID_60_AGC1_VTH_SEL_BOTH_MIN_VALUE	\
+	(AW87XXX_PID_60_AGC1_VTH_SEL_BOTH_MIN << AW87XXX_PID_60_AGC1_VTH_SEL_START_BIT)
+
+/*
+#define AW87XXX_PID_60_AGC1_VTH_SEL_BOTH_MIN	(3)
+#define AW87XXX_PID_60_AGC1_VTH_SEL_BOTH_MIN_VALUE	\
+	(AW87XXX_PID_60_AGC1_VTH_SEL_BOTH_MIN << AW87XXX_PID_60_AGC1_VTH_SEL_START_BIT)
+*/
+
+#define AW87XXX_PID_60_AGC1_VTH_SEL_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_AGC1_VTH_SEL_DEFAULT	\
+	(AW87XXX_PID_60_AGC1_VTH_SEL_DEFAULT_VALUE << AW87XXX_PID_60_AGC1_VTH_SEL_START_BIT)
+
+/* AGC2_FIRST_ATT_TIME bit 1:0 (AGC_CON 0x73) */
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_START_BIT	(0)
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_BITS_LEN	(2)
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_MASK	\
+	(~(((1<<AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_BITS_LEN)-1) << AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_START_BIT))
+
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_0P08MS	(0)
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_0P08MS_VALUE	\
+	(AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_0P08MS << AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_0P32MS	(1)
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_0P32MS_VALUE	\
+	(AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_0P32MS << AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_1P28MS	(2)
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_1P28MS_VALUE	\
+	(AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_1P28MS << AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_5P12MS	(3)
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_5P12MS_VALUE	\
+	(AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_5P12MS << AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_START_BIT)
+
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_DEFAULT	\
+	(AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_DEFAULT_VALUE << AW87XXX_PID_60_AGC2_FIRST_ATT_TIME_START_BIT)
+
+/* default value of AGC_CON (0x73) */
+/* #define AW87XXX_PID_60_AGC_CON_DEFAULT		(0x48) */
+
+/* NG (0x74) detail */
+/* NG_MODE bit 2 (NG 0x74) */
+#define AW87XXX_PID_60_NG_MODE_START_BIT	(2)
+#define AW87XXX_PID_60_NG_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_60_NG_MODE_MASK		\
+	(~(((1<<AW87XXX_PID_60_NG_MODE_BITS_LEN)-1) << AW87XXX_PID_60_NG_MODE_START_BIT))
+
+#define AW87XXX_PID_60_NG_MODE_MODE1	(0)
+#define AW87XXX_PID_60_NG_MODE_MODE1_VALUE	\
+	(AW87XXX_PID_60_NG_MODE_MODE1 << AW87XXX_PID_60_NG_MODE_START_BIT)
+
+#define AW87XXX_PID_60_NG_MODE_MODE2	(1)
+#define AW87XXX_PID_60_NG_MODE_MODE2_VALUE	\
+	(AW87XXX_PID_60_NG_MODE_MODE2 << AW87XXX_PID_60_NG_MODE_START_BIT)
+
+#define AW87XXX_PID_60_NG_MODE_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_NG_MODE_DEFAULT	\
+	(AW87XXX_PID_60_NG_MODE_DEFAULT_VALUE << AW87XXX_PID_60_NG_MODE_START_BIT)
+
+/* EN_NG bit 3 (NG 0x74) */
+#define AW87XXX_PID_60_EN_NG_START_BIT	(3)
+#define AW87XXX_PID_60_EN_NG_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_NG_MASK		\
+	(~(((1<<AW87XXX_PID_60_EN_NG_BITS_LEN)-1) << AW87XXX_PID_60_EN_NG_START_BIT))
+
+#define AW87XXX_PID_60_EN_NG_DISABLE	(0)
+#define AW87XXX_PID_60_EN_NG_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_NG_DISABLE << AW87XXX_PID_60_EN_NG_START_BIT)
+
+#define AW87XXX_PID_60_EN_NG_ENABLE		(1)
+#define AW87XXX_PID_60_EN_NG_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_NG_ENABLE << AW87XXX_PID_60_EN_NG_START_BIT)
+
+#define AW87XXX_PID_60_EN_NG_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_NG_DEFAULT	\
+	(AW87XXX_PID_60_EN_NG_DEFAULT_VALUE << AW87XXX_PID_60_EN_NG_START_BIT)
+
+/* EN_AGC1_ADP bit 7 (NG 0x74) */
+#define AW87XXX_PID_60_EN_AGC1_ADP_START_BIT	(7)
+#define AW87XXX_PID_60_EN_AGC1_ADP_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_AGC1_ADP_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_AGC1_ADP_BITS_LEN)-1) << AW87XXX_PID_60_EN_AGC1_ADP_START_BIT))
+
+#define AW87XXX_PID_60_EN_AGC1_ADP_AGC_CROSSZERO_AS_BEFORE	(0)
+#define AW87XXX_PID_60_EN_AGC1_ADP_AGC_CROSSZERO_AS_BEFORE_VALUE	\
+	(AW87XXX_PID_60_EN_AGC1_ADP_AGC_CROSSZERO_AS_BEFORE << AW87XXX_PID_60_EN_AGC1_ADP_START_BIT)
+
+#define AW87XXX_PID_60_EN_AGC1_ADP_AGC_CROSSZERO_ADAPTIVELY	(1)
+#define AW87XXX_PID_60_EN_AGC1_ADP_AGC_CROSSZERO_ADAPTIVELY_VALUE	\
+	(AW87XXX_PID_60_EN_AGC1_ADP_AGC_CROSSZERO_ADAPTIVELY << AW87XXX_PID_60_EN_AGC1_ADP_START_BIT)
+
+#define AW87XXX_PID_60_EN_AGC1_ADP_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_AGC1_ADP_DEFAULT	\
+	(AW87XXX_PID_60_EN_AGC1_ADP_DEFAULT_VALUE << AW87XXX_PID_60_EN_AGC1_ADP_START_BIT)
+
+/* PAVG_ADJ bit 6:4 (NG 0x74) */
+#define AW87XXX_PID_60_PAVG_ADJ_START_BIT	(4)
+#define AW87XXX_PID_60_PAVG_ADJ_BITS_LEN	(3)
+#define AW87XXX_PID_60_PAVG_ADJ_MASK	\
+	(~(((1<<AW87XXX_PID_60_PAVG_ADJ_BITS_LEN)-1) << AW87XXX_PID_60_PAVG_ADJ_START_BIT))
+
+#define AW87XXX_PID_60_PAVG_ADJ_0P94	(0)
+#define AW87XXX_PID_60_PAVG_ADJ_0P94_VALUE	\
+	(AW87XXX_PID_60_PAVG_ADJ_0P94 << AW87XXX_PID_60_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_PAVG_ADJ_0P97	(1)
+#define AW87XXX_PID_60_PAVG_ADJ_0P97_VALUE	\
+	(AW87XXX_PID_60_PAVG_ADJ_0P97 << AW87XXX_PID_60_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_PAVG_ADJ_1P0		(2)
+#define AW87XXX_PID_60_PAVG_ADJ_1P0_VALUE	\
+	(AW87XXX_PID_60_PAVG_ADJ_1P0 << AW87XXX_PID_60_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_PAVG_ADJ_1P03	(3)
+#define AW87XXX_PID_60_PAVG_ADJ_1P03_VALUE	\
+	(AW87XXX_PID_60_PAVG_ADJ_1P03 << AW87XXX_PID_60_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_PAVG_ADJ_1P06	(4)
+#define AW87XXX_PID_60_PAVG_ADJ_1P06_VALUE	\
+	(AW87XXX_PID_60_PAVG_ADJ_1P06 << AW87XXX_PID_60_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_PAVG_ADJ_1P09	(5)
+#define AW87XXX_PID_60_PAVG_ADJ_1P09_VALUE	\
+	(AW87XXX_PID_60_PAVG_ADJ_1P09 << AW87XXX_PID_60_PAVG_ADJ_START_BIT)
+
+#define AW87XXX_PID_60_PAVG_ADJ_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_PAVG_ADJ_DEFAULT	\
+	(AW87XXX_PID_60_PAVG_ADJ_DEFAULT_VALUE << AW87XXX_PID_60_PAVG_ADJ_START_BIT)
+
+/* NG_GAIN bit 1:0 (NG 0x74) */
+#define AW87XXX_PID_60_NG_GAIN_START_BIT	(0)
+#define AW87XXX_PID_60_NG_GAIN_BITS_LEN	(2)
+#define AW87XXX_PID_60_NG_GAIN_MASK		\
+	(~(((1<<AW87XXX_PID_60_NG_GAIN_BITS_LEN)-1) << AW87XXX_PID_60_NG_GAIN_START_BIT))
+
+#define AW87XXX_PID_60_NG_GAIN_MINUS6DB	(0)
+#define AW87XXX_PID_60_NG_GAIN_MINUS6DB_VALUE	\
+	(AW87XXX_PID_60_NG_GAIN_MINUS6DB << AW87XXX_PID_60_NG_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_NG_GAIN_MINUS7P5DB	(1)
+#define AW87XXX_PID_60_NG_GAIN_MINUS7P5DB_VALUE	\
+	(AW87XXX_PID_60_NG_GAIN_MINUS7P5DB << AW87XXX_PID_60_NG_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_NG_GAIN_MINUS9DB	(2)
+#define AW87XXX_PID_60_NG_GAIN_MINUS9DB_VALUE	\
+	(AW87XXX_PID_60_NG_GAIN_MINUS9DB << AW87XXX_PID_60_NG_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_NG_GAIN_MINUS12DB	(3)
+#define AW87XXX_PID_60_NG_GAIN_MINUS12DB_VALUE	\
+	(AW87XXX_PID_60_NG_GAIN_MINUS12DB << AW87XXX_PID_60_NG_GAIN_START_BIT)
+
+#define AW87XXX_PID_60_NG_GAIN_DEFAULT_VALUE	(0x3)
+#define AW87XXX_PID_60_NG_GAIN_DEFAULT	\
+	(AW87XXX_PID_60_NG_GAIN_DEFAULT_VALUE << AW87XXX_PID_60_NG_GAIN_START_BIT)
+
+/* default value of NG (0x74) */
+/* #define AW87XXX_PID_60_NG_DEFAULT		(0x27) */
+
+/* NG2 (0x75) detail */
+/* NG_CLK0_SEL bit 0 (NG2 0x75) */
+#define AW87XXX_PID_60_NG_CLK0_SEL_START_BIT	(0)
+#define AW87XXX_PID_60_NG_CLK0_SEL_BITS_LEN	(1)
+#define AW87XXX_PID_60_NG_CLK0_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_NG_CLK0_SEL_BITS_LEN)-1) << AW87XXX_PID_60_NG_CLK0_SEL_START_BIT))
+
+#define AW87XXX_PID_60_NG_CLK0_SEL_5US	(0)
+#define AW87XXX_PID_60_NG_CLK0_SEL_5US_VALUE	\
+	(AW87XXX_PID_60_NG_CLK0_SEL_5US << AW87XXX_PID_60_NG_CLK0_SEL_START_BIT)
+
+#define AW87XXX_PID_60_NG_CLK0_SEL_10US	(1)
+#define AW87XXX_PID_60_NG_CLK0_SEL_10US_VALUE	\
+	(AW87XXX_PID_60_NG_CLK0_SEL_10US << AW87XXX_PID_60_NG_CLK0_SEL_START_BIT)
+
+#define AW87XXX_PID_60_NG_CLK0_SEL_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_NG_CLK0_SEL_DEFAULT	\
+	(AW87XXX_PID_60_NG_CLK0_SEL_DEFAULT_VALUE << AW87XXX_PID_60_NG_CLK0_SEL_START_BIT)
+
+/* NG_DEGLITCH_CTRL bit 7 (NG2 0x75) */
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_START_BIT	(7)
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_BITS_LEN	(1)
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_MASK	\
+	(~(((1<<AW87XXX_PID_60_NG_DEGLITCH_CTRL_BITS_LEN)-1) << AW87XXX_PID_60_NG_DEGLITCH_CTRL_START_BIT))
+
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_8P6N	(0)
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_8P6N_VALUE	\
+	(AW87XXX_PID_60_NG_DEGLITCH_CTRL_8P6N << AW87XXX_PID_60_NG_DEGLITCH_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_14P2N	(1)
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_14P2N_VALUE	\
+	(AW87XXX_PID_60_NG_DEGLITCH_CTRL_14P2N << AW87XXX_PID_60_NG_DEGLITCH_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_NG_DEGLITCH_CTRL_DEFAULT	\
+	(AW87XXX_PID_60_NG_DEGLITCH_CTRL_DEFAULT_VALUE << AW87XXX_PID_60_NG_DEGLITCH_CTRL_START_BIT)
+
+/* NGVTH bit 6:4 (NG2 0x75) */
+#define AW87XXX_PID_60_NGVTH_START_BIT	(4)
+#define AW87XXX_PID_60_NGVTH_BITS_LEN	(3)
+#define AW87XXX_PID_60_NGVTH_MASK		\
+	(~(((1<<AW87XXX_PID_60_NGVTH_BITS_LEN)-1) << AW87XXX_PID_60_NGVTH_START_BIT))
+
+#define AW87XXX_PID_60_NGVTH_5MV		(0)
+#define AW87XXX_PID_60_NGVTH_5MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_5MV << AW87XXX_PID_60_NGVTH_START_BIT)
+
+#define AW87XXX_PID_60_NGVTH_8MV		(1)
+#define AW87XXX_PID_60_NGVTH_8MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_8MV << AW87XXX_PID_60_NGVTH_START_BIT)
+
+#define AW87XXX_PID_60_NGVTH_10MV		(2)
+#define AW87XXX_PID_60_NGVTH_10MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_10MV << AW87XXX_PID_60_NGVTH_START_BIT)
+
+#define AW87XXX_PID_60_NGVTH_12MV		(3)
+#define AW87XXX_PID_60_NGVTH_12MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_12MV << AW87XXX_PID_60_NGVTH_START_BIT)
+
+#define AW87XXX_PID_60_NGVTH_14MV		(4)
+#define AW87XXX_PID_60_NGVTH_14MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_14MV << AW87XXX_PID_60_NGVTH_START_BIT)
+
+#define AW87XXX_PID_60_NGVTH_16MV		(5)
+#define AW87XXX_PID_60_NGVTH_16MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_16MV << AW87XXX_PID_60_NGVTH_START_BIT)
+
+/*
+#define AW87XXX_PID_60_NGVTH_16MV		(6)
+#define AW87XXX_PID_60_NGVTH_16MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_16MV << AW87XXX_PID_60_NGVTH_START_BIT)
+*/
+
+#define AW87XXX_PID_60_NGVTH_25MV		(7)
+#define AW87XXX_PID_60_NGVTH_25MV_VALUE	\
+	(AW87XXX_PID_60_NGVTH_25MV << AW87XXX_PID_60_NGVTH_START_BIT)
+
+#define AW87XXX_PID_60_NGVTH_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_NGVTH_DEFAULT	\
+	(AW87XXX_PID_60_NGVTH_DEFAULT_VALUE << AW87XXX_PID_60_NGVTH_START_BIT)
+
+/* NG_ACK bit 3:1 (NG2 0x75) */
+#define AW87XXX_PID_60_NG_ACK_START_BIT	(1)
+#define AW87XXX_PID_60_NG_ACK_BITS_LEN	(3)
+#define AW87XXX_PID_60_NG_ACK_MASK		\
+	(~(((1<<AW87XXX_PID_60_NG_ACK_BITS_LEN)-1) << AW87XXX_PID_60_NG_ACK_START_BIT))
+
+#define AW87XXX_PID_60_NG_ACK_10MS		(0)
+#define AW87XXX_PID_60_NG_ACK_10MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_10MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_20MS		(1)
+#define AW87XXX_PID_60_NG_ACK_20MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_20MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_40MS		(2)
+#define AW87XXX_PID_60_NG_ACK_40MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_40MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_80MS		(3)
+#define AW87XXX_PID_60_NG_ACK_80MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_80MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_160MS		(4)
+#define AW87XXX_PID_60_NG_ACK_160MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_160MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_320MS		(5)
+#define AW87XXX_PID_60_NG_ACK_320MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_320MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_640MS		(6)
+#define AW87XXX_PID_60_NG_ACK_640MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_640MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_1280MS	(7)
+#define AW87XXX_PID_60_NG_ACK_1280MS_VALUE	\
+	(AW87XXX_PID_60_NG_ACK_1280MS << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+#define AW87XXX_PID_60_NG_ACK_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_60_NG_ACK_DEFAULT	\
+	(AW87XXX_PID_60_NG_ACK_DEFAULT_VALUE << AW87XXX_PID_60_NG_ACK_START_BIT)
+
+/* default value of NG2 (0x75) */
+/* #define AW87XXX_PID_60_NG2_DEFAULT		(0xA9) */
+
+/* NG3 (0x76) detail */
+/* NG_RCK bit 7:5 (NG3 0x76) */
+#define AW87XXX_PID_60_NG_RCK_START_BIT	(5)
+#define AW87XXX_PID_60_NG_RCK_BITS_LEN	(3)
+#define AW87XXX_PID_60_NG_RCK_MASK		\
+	(~(((1<<AW87XXX_PID_60_NG_RCK_BITS_LEN)-1) << AW87XXX_PID_60_NG_RCK_START_BIT))
+
+#define AW87XXX_PID_60_NG_RCK_90US		(0)
+#define AW87XXX_PID_60_NG_RCK_90US_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_90US << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_180US		(1)
+#define AW87XXX_PID_60_NG_RCK_180US_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_180US << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_360US		(2)
+#define AW87XXX_PID_60_NG_RCK_360US_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_360US << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_720US		(3)
+#define AW87XXX_PID_60_NG_RCK_720US_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_720US << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_1P44M		(4)
+#define AW87XXX_PID_60_NG_RCK_1P44M_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_1P44M << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_2P88MS	(5)
+#define AW87XXX_PID_60_NG_RCK_2P88MS_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_2P88MS << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_5P76MS	(6)
+#define AW87XXX_PID_60_NG_RCK_5P76MS_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_5P76MS << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_11P52MS	(7)
+#define AW87XXX_PID_60_NG_RCK_11P52MS_VALUE	\
+	(AW87XXX_PID_60_NG_RCK_11P52MS << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_RCK_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_60_NG_RCK_DEFAULT	\
+	(AW87XXX_PID_60_NG_RCK_DEFAULT_VALUE << AW87XXX_PID_60_NG_RCK_START_BIT)
+
+/* NG_WCK bit 4:2 (NG3 0x76) */
+#define AW87XXX_PID_60_NG_WCK_START_BIT	(2)
+#define AW87XXX_PID_60_NG_WCK_BITS_LEN	(3)
+#define AW87XXX_PID_60_NG_WCK_MASK		\
+	(~(((1<<AW87XXX_PID_60_NG_WCK_BITS_LEN)-1) << AW87XXX_PID_60_NG_WCK_START_BIT))
+
+#define AW87XXX_PID_60_NG_WCK_20MS		(0)
+#define AW87XXX_PID_60_NG_WCK_20MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_20MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_40MS		(1)
+#define AW87XXX_PID_60_NG_WCK_40MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_40MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_80MS		(2)
+#define AW87XXX_PID_60_NG_WCK_80MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_80MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_160MS		(3)
+#define AW87XXX_PID_60_NG_WCK_160MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_160MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_320MS		(4)
+#define AW87XXX_PID_60_NG_WCK_320MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_320MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_640MS		(5)
+#define AW87XXX_PID_60_NG_WCK_640MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_640MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_1280MS	(6)
+#define AW87XXX_PID_60_NG_WCK_1280MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_1280MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_2556MS	(7)
+#define AW87XXX_PID_60_NG_WCK_2556MS_VALUE	\
+	(AW87XXX_PID_60_NG_WCK_2556MS << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+#define AW87XXX_PID_60_NG_WCK_DEFAULT_VALUE	(0x4)
+#define AW87XXX_PID_60_NG_WCK_DEFAULT	\
+	(AW87XXX_PID_60_NG_WCK_DEFAULT_VALUE << AW87XXX_PID_60_NG_WCK_START_BIT)
+
+/* CP_VOS bit 1:0 (NG3 0x76) */
+#define AW87XXX_PID_60_CP_VOS_START_BIT	(0)
+#define AW87XXX_PID_60_CP_VOS_BITS_LEN	(2)
+#define AW87XXX_PID_60_CP_VOS_MASK		\
+	(~(((1<<AW87XXX_PID_60_CP_VOS_BITS_LEN)-1) << AW87XXX_PID_60_CP_VOS_START_BIT))
+
+#define AW87XXX_PID_60_CP_VOS_0MV		(0)
+#define AW87XXX_PID_60_CP_VOS_0MV_VALUE	\
+	(AW87XXX_PID_60_CP_VOS_0MV << AW87XXX_PID_60_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_60_CP_VOS_50MV		(1)
+#define AW87XXX_PID_60_CP_VOS_50MV_VALUE	\
+	(AW87XXX_PID_60_CP_VOS_50MV << AW87XXX_PID_60_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_60_CP_VOS_100MV		(2)
+#define AW87XXX_PID_60_CP_VOS_100MV_VALUE	\
+	(AW87XXX_PID_60_CP_VOS_100MV << AW87XXX_PID_60_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_60_CP_VOS_150MV		(3)
+#define AW87XXX_PID_60_CP_VOS_150MV_VALUE	\
+	(AW87XXX_PID_60_CP_VOS_150MV << AW87XXX_PID_60_CP_VOS_START_BIT)
+
+#define AW87XXX_PID_60_CP_VOS_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_CP_VOS_DEFAULT	\
+	(AW87XXX_PID_60_CP_VOS_DEFAULT_VALUE << AW87XXX_PID_60_CP_VOS_START_BIT)
+
+/* default value of NG3 (0x76) */
+/* #define AW87XXX_PID_60_NG3_DEFAULT		(0x91) */
+
+/* CP (0x77) detail */
+/* CP_DDT bit 0 (CP 0x77) */
+#define AW87XXX_PID_60_CP_DDT_START_BIT	(0)
+#define AW87XXX_PID_60_CP_DDT_BITS_LEN	(1)
+#define AW87XXX_PID_60_CP_DDT_MASK		\
+	(~(((1<<AW87XXX_PID_60_CP_DDT_BITS_LEN)-1) << AW87XXX_PID_60_CP_DDT_START_BIT))
+
+#define AW87XXX_PID_60_CP_DDT_0NS		(0)
+#define AW87XXX_PID_60_CP_DDT_0NS_VALUE	\
+	(AW87XXX_PID_60_CP_DDT_0NS << AW87XXX_PID_60_CP_DDT_START_BIT)
+
+#define AW87XXX_PID_60_CP_DDT_10NS		(1)
+#define AW87XXX_PID_60_CP_DDT_10NS_VALUE	\
+	(AW87XXX_PID_60_CP_DDT_10NS << AW87XXX_PID_60_CP_DDT_START_BIT)
+
+#define AW87XXX_PID_60_CP_DDT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_CP_DDT_DEFAULT	\
+	(AW87XXX_PID_60_CP_DDT_DEFAULT_VALUE << AW87XXX_PID_60_CP_DDT_START_BIT)
+
+/* CPOK_TM bit 1 (CP 0x77) */
+#define AW87XXX_PID_60_CPOK_TM_START_BIT	(1)
+#define AW87XXX_PID_60_CPOK_TM_BITS_LEN	(1)
+#define AW87XXX_PID_60_CPOK_TM_MASK		\
+	(~(((1<<AW87XXX_PID_60_CPOK_TM_BITS_LEN)-1) << AW87XXX_PID_60_CPOK_TM_START_BIT))
+
+#define AW87XXX_PID_60_CPOK_TM_0P6MS	(0)
+#define AW87XXX_PID_60_CPOK_TM_0P6MS_VALUE	\
+	(AW87XXX_PID_60_CPOK_TM_0P6MS << AW87XXX_PID_60_CPOK_TM_START_BIT)
+
+#define AW87XXX_PID_60_CPOK_TM_1MS		(1)
+#define AW87XXX_PID_60_CPOK_TM_1MS_VALUE	\
+	(AW87XXX_PID_60_CPOK_TM_1MS << AW87XXX_PID_60_CPOK_TM_START_BIT)
+
+#define AW87XXX_PID_60_CPOK_TM_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_CPOK_TM_DEFAULT	\
+	(AW87XXX_PID_60_CPOK_TM_DEFAULT_VALUE << AW87XXX_PID_60_CPOK_TM_START_BIT)
+
+/* UVLO_DT bit 4 (CP 0x77) */
+#define AW87XXX_PID_60_UVLO_DT_START_BIT	(4)
+#define AW87XXX_PID_60_UVLO_DT_BITS_LEN	(1)
+#define AW87XXX_PID_60_UVLO_DT_MASK		\
+	(~(((1<<AW87XXX_PID_60_UVLO_DT_BITS_LEN)-1) << AW87XXX_PID_60_UVLO_DT_START_BIT))
+
+#define AW87XXX_PID_60_UVLO_DT_3US		(0)
+#define AW87XXX_PID_60_UVLO_DT_3US_VALUE	\
+	(AW87XXX_PID_60_UVLO_DT_3US << AW87XXX_PID_60_UVLO_DT_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_DT_10US		(1)
+#define AW87XXX_PID_60_UVLO_DT_10US_VALUE	\
+	(AW87XXX_PID_60_UVLO_DT_10US << AW87XXX_PID_60_UVLO_DT_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_DT_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_UVLO_DT_DEFAULT	\
+	(AW87XXX_PID_60_UVLO_DT_DEFAULT_VALUE << AW87XXX_PID_60_UVLO_DT_START_BIT)
+
+/* PD_UVLO bit 7 (CP 0x77) */
+#define AW87XXX_PID_60_PD_UVLO_START_BIT	(7)
+#define AW87XXX_PID_60_PD_UVLO_BITS_LEN	(1)
+#define AW87XXX_PID_60_PD_UVLO_MASK		\
+	(~(((1<<AW87XXX_PID_60_PD_UVLO_BITS_LEN)-1) << AW87XXX_PID_60_PD_UVLO_START_BIT))
+
+#define AW87XXX_PID_60_PD_UVLO_ENABLE	(0)
+#define AW87XXX_PID_60_PD_UVLO_ENABLE_VALUE	\
+	(AW87XXX_PID_60_PD_UVLO_ENABLE << AW87XXX_PID_60_PD_UVLO_START_BIT)
+
+#define AW87XXX_PID_60_PD_UVLO_DISABLE	(1)
+#define AW87XXX_PID_60_PD_UVLO_DISABLE_VALUE	\
+	(AW87XXX_PID_60_PD_UVLO_DISABLE << AW87XXX_PID_60_PD_UVLO_START_BIT)
+
+#define AW87XXX_PID_60_PD_UVLO_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PD_UVLO_DEFAULT	\
+	(AW87XXX_PID_60_PD_UVLO_DEFAULT_VALUE << AW87XXX_PID_60_PD_UVLO_START_BIT)
+
+/* UVLO_VTH bit 6:5 (CP 0x77) */
+#define AW87XXX_PID_60_UVLO_VTH_START_BIT	(5)
+#define AW87XXX_PID_60_UVLO_VTH_BITS_LEN	(2)
+#define AW87XXX_PID_60_UVLO_VTH_MASK	\
+	(~(((1<<AW87XXX_PID_60_UVLO_VTH_BITS_LEN)-1) << AW87XXX_PID_60_UVLO_VTH_START_BIT))
+
+#define AW87XXX_PID_60_UVLO_VTH_VH2P6V_VL2P5V	(0)
+#define AW87XXX_PID_60_UVLO_VTH_VH2P6V_VL2P5V_VALUE	\
+	(AW87XXX_PID_60_UVLO_VTH_VH2P6V_VL2P5V << AW87XXX_PID_60_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_VTH_VH2P7V_VL2P6V	(1)
+#define AW87XXX_PID_60_UVLO_VTH_VH2P7V_VL2P6V_VALUE	\
+	(AW87XXX_PID_60_UVLO_VTH_VH2P7V_VL2P6V << AW87XXX_PID_60_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_VTH_VH2P8V_VL2P7V	(2)
+#define AW87XXX_PID_60_UVLO_VTH_VH2P8V_VL2P7V_VALUE	\
+	(AW87XXX_PID_60_UVLO_VTH_VH2P8V_VL2P7V << AW87XXX_PID_60_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_VTH_VH2P9V_VL2P8V	(3)
+#define AW87XXX_PID_60_UVLO_VTH_VH2P9V_VL2P8V_VALUE	\
+	(AW87XXX_PID_60_UVLO_VTH_VH2P9V_VL2P8V << AW87XXX_PID_60_UVLO_VTH_START_BIT)
+
+#define AW87XXX_PID_60_UVLO_VTH_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_UVLO_VTH_DEFAULT	\
+	(AW87XXX_PID_60_UVLO_VTH_DEFAULT_VALUE << AW87XXX_PID_60_UVLO_VTH_START_BIT)
+
+/* CP_LDO bit 3:2 (CP 0x77) */
+#define AW87XXX_PID_60_CP_LDO_START_BIT	(2)
+#define AW87XXX_PID_60_CP_LDO_BITS_LEN	(2)
+#define AW87XXX_PID_60_CP_LDO_MASK		\
+	(~(((1<<AW87XXX_PID_60_CP_LDO_BITS_LEN)-1) << AW87XXX_PID_60_CP_LDO_START_BIT))
+
+#define AW87XXX_PID_60_CP_LDO_4P75V		(0)
+#define AW87XXX_PID_60_CP_LDO_4P75V_VALUE	\
+	(AW87XXX_PID_60_CP_LDO_4P75V << AW87XXX_PID_60_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_60_CP_LDO_5V		(1)
+#define AW87XXX_PID_60_CP_LDO_5V_VALUE	\
+	(AW87XXX_PID_60_CP_LDO_5V << AW87XXX_PID_60_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_60_CP_LDO_5P25V		(2)
+#define AW87XXX_PID_60_CP_LDO_5P25V_VALUE	\
+	(AW87XXX_PID_60_CP_LDO_5P25V << AW87XXX_PID_60_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_60_CP_LDO_5P5V		(3)
+#define AW87XXX_PID_60_CP_LDO_5P5V_VALUE	\
+	(AW87XXX_PID_60_CP_LDO_5P5V << AW87XXX_PID_60_CP_LDO_START_BIT)
+
+#define AW87XXX_PID_60_CP_LDO_DEFAULT_VALUE	(0x2)
+#define AW87XXX_PID_60_CP_LDO_DEFAULT	\
+	(AW87XXX_PID_60_CP_LDO_DEFAULT_VALUE << AW87XXX_PID_60_CP_LDO_START_BIT)
+
+/* default value of CP (0x77) */
+/* #define AW87XXX_PID_60_CP_DEFAULT		(0x5A) */
+
+/* TEST_GTDR (0x78) detail */
+/* TEST_OC bit 0 (TEST_GTDR 0x78) */
+#define AW87XXX_PID_60_TEST_OC_START_BIT	(0)
+#define AW87XXX_PID_60_TEST_OC_BITS_LEN	(1)
+#define AW87XXX_PID_60_TEST_OC_MASK		\
+	(~(((1<<AW87XXX_PID_60_TEST_OC_BITS_LEN)-1) << AW87XXX_PID_60_TEST_OC_START_BIT))
+
+#define AW87XXX_PID_60_TEST_OC_DISABLE	(0)
+#define AW87XXX_PID_60_TEST_OC_DISABLE_VALUE	\
+	(AW87XXX_PID_60_TEST_OC_DISABLE << AW87XXX_PID_60_TEST_OC_START_BIT)
+
+#define AW87XXX_PID_60_TEST_OC_ENABLE	(1)
+#define AW87XXX_PID_60_TEST_OC_ENABLE_VALUE	\
+	(AW87XXX_PID_60_TEST_OC_ENABLE << AW87XXX_PID_60_TEST_OC_START_BIT)
+
+#define AW87XXX_PID_60_TEST_OC_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_TEST_OC_DEFAULT	\
+	(AW87XXX_PID_60_TEST_OC_DEFAULT_VALUE << AW87XXX_PID_60_TEST_OC_START_BIT)
+
+/* OC_DEBUG_EN bit 1 (TEST_GTDR 0x78) */
+#define AW87XXX_PID_60_OC_DEBUG_EN_START_BIT	(1)
+#define AW87XXX_PID_60_OC_DEBUG_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_OC_DEBUG_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_OC_DEBUG_EN_BITS_LEN)-1) << AW87XXX_PID_60_OC_DEBUG_EN_START_BIT))
+
+#define AW87XXX_PID_60_OC_DEBUG_EN_DISABLE	(0)
+#define AW87XXX_PID_60_OC_DEBUG_EN_DISABLE_VALUE	\
+	(AW87XXX_PID_60_OC_DEBUG_EN_DISABLE << AW87XXX_PID_60_OC_DEBUG_EN_START_BIT)
+
+#define AW87XXX_PID_60_OC_DEBUG_EN_ENABLE	(1)
+#define AW87XXX_PID_60_OC_DEBUG_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_OC_DEBUG_EN_ENABLE << AW87XXX_PID_60_OC_DEBUG_EN_START_BIT)
+
+#define AW87XXX_PID_60_OC_DEBUG_EN_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_OC_DEBUG_EN_DEFAULT	\
+	(AW87XXX_PID_60_OC_DEBUG_EN_DEFAULT_VALUE << AW87XXX_PID_60_OC_DEBUG_EN_START_BIT)
+
+/* LN_RON_T_N bit 4 (TEST_GTDR 0x78) */
+#define AW87XXX_PID_60_LN_RON_T_N_START_BIT	(4)
+#define AW87XXX_PID_60_LN_RON_T_N_BITS_LEN	(1)
+#define AW87XXX_PID_60_LN_RON_T_N_MASK	\
+	(~(((1<<AW87XXX_PID_60_LN_RON_T_N_BITS_LEN)-1) << AW87XXX_PID_60_LN_RON_T_N_START_BIT))
+
+#define AW87XXX_PID_60_LN_RON_T_N_DISABLE	(0)
+#define AW87XXX_PID_60_LN_RON_T_N_DISABLE_VALUE	\
+	(AW87XXX_PID_60_LN_RON_T_N_DISABLE << AW87XXX_PID_60_LN_RON_T_N_START_BIT)
+
+#define AW87XXX_PID_60_LN_RON_T_N_ENALBE	(1)
+#define AW87XXX_PID_60_LN_RON_T_N_ENALBE_VALUE	\
+	(AW87XXX_PID_60_LN_RON_T_N_ENALBE << AW87XXX_PID_60_LN_RON_T_N_START_BIT)
+
+#define AW87XXX_PID_60_LN_RON_T_N_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_LN_RON_T_N_DEFAULT	\
+	(AW87XXX_PID_60_LN_RON_T_N_DEFAULT_VALUE << AW87XXX_PID_60_LN_RON_T_N_START_BIT)
+
+/* LN_RON_T_P bit 5 (TEST_GTDR 0x78) */
+#define AW87XXX_PID_60_LN_RON_T_P_START_BIT	(5)
+#define AW87XXX_PID_60_LN_RON_T_P_BITS_LEN	(1)
+#define AW87XXX_PID_60_LN_RON_T_P_MASK	\
+	(~(((1<<AW87XXX_PID_60_LN_RON_T_P_BITS_LEN)-1) << AW87XXX_PID_60_LN_RON_T_P_START_BIT))
+
+#define AW87XXX_PID_60_LN_RON_T_P_DISABLE	(0)
+#define AW87XXX_PID_60_LN_RON_T_P_DISABLE_VALUE	\
+	(AW87XXX_PID_60_LN_RON_T_P_DISABLE << AW87XXX_PID_60_LN_RON_T_P_START_BIT)
+
+#define AW87XXX_PID_60_LN_RON_T_P_ENALBE	(1)
+#define AW87XXX_PID_60_LN_RON_T_P_ENALBE_VALUE	\
+	(AW87XXX_PID_60_LN_RON_T_P_ENALBE << AW87XXX_PID_60_LN_RON_T_P_START_BIT)
+
+#define AW87XXX_PID_60_LN_RON_T_P_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_LN_RON_T_P_DEFAULT	\
+	(AW87XXX_PID_60_LN_RON_T_P_DEFAULT_VALUE << AW87XXX_PID_60_LN_RON_T_P_START_BIT)
+
+/* HN_RON_T_N bit 6 (TEST_GTDR 0x78) */
+#define AW87XXX_PID_60_HN_RON_T_N_START_BIT	(6)
+#define AW87XXX_PID_60_HN_RON_T_N_BITS_LEN	(1)
+#define AW87XXX_PID_60_HN_RON_T_N_MASK	\
+	(~(((1<<AW87XXX_PID_60_HN_RON_T_N_BITS_LEN)-1) << AW87XXX_PID_60_HN_RON_T_N_START_BIT))
+
+#define AW87XXX_PID_60_HN_RON_T_N_DISABLE	(0)
+#define AW87XXX_PID_60_HN_RON_T_N_DISABLE_VALUE	\
+	(AW87XXX_PID_60_HN_RON_T_N_DISABLE << AW87XXX_PID_60_HN_RON_T_N_START_BIT)
+
+#define AW87XXX_PID_60_HN_RON_T_N_ENALBE	(1)
+#define AW87XXX_PID_60_HN_RON_T_N_ENALBE_VALUE	\
+	(AW87XXX_PID_60_HN_RON_T_N_ENALBE << AW87XXX_PID_60_HN_RON_T_N_START_BIT)
+
+#define AW87XXX_PID_60_HN_RON_T_N_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_HN_RON_T_N_DEFAULT	\
+	(AW87XXX_PID_60_HN_RON_T_N_DEFAULT_VALUE << AW87XXX_PID_60_HN_RON_T_N_START_BIT)
+
+/* HN_RON_T_P bit 7 (TEST_GTDR 0x78) */
+#define AW87XXX_PID_60_HN_RON_T_P_START_BIT	(7)
+#define AW87XXX_PID_60_HN_RON_T_P_BITS_LEN	(1)
+#define AW87XXX_PID_60_HN_RON_T_P_MASK	\
+	(~(((1<<AW87XXX_PID_60_HN_RON_T_P_BITS_LEN)-1) << AW87XXX_PID_60_HN_RON_T_P_START_BIT))
+
+#define AW87XXX_PID_60_HN_RON_T_P_DISABLE	(0)
+#define AW87XXX_PID_60_HN_RON_T_P_DISABLE_VALUE	\
+	(AW87XXX_PID_60_HN_RON_T_P_DISABLE << AW87XXX_PID_60_HN_RON_T_P_START_BIT)
+
+#define AW87XXX_PID_60_HN_RON_T_P_ENALBE	(1)
+#define AW87XXX_PID_60_HN_RON_T_P_ENALBE_VALUE	\
+	(AW87XXX_PID_60_HN_RON_T_P_ENALBE << AW87XXX_PID_60_HN_RON_T_P_START_BIT)
+
+#define AW87XXX_PID_60_HN_RON_T_P_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_HN_RON_T_P_DEFAULT	\
+	(AW87XXX_PID_60_HN_RON_T_P_DEFAULT_VALUE << AW87XXX_PID_60_HN_RON_T_P_START_BIT)
+
+/* OC_DEBUG_SEL bit 3:2 (TEST_GTDR 0x78) */
+#define AW87XXX_PID_60_OC_DEBUG_SEL_START_BIT	(2)
+#define AW87XXX_PID_60_OC_DEBUG_SEL_BITS_LEN	(2)
+#define AW87XXX_PID_60_OC_DEBUG_SEL_MASK	\
+	(~(((1<<AW87XXX_PID_60_OC_DEBUG_SEL_BITS_LEN)-1) << AW87XXX_PID_60_OC_DEBUG_SEL_START_BIT))
+
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VOPP	(0)
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VOPP_VALUE	\
+	(AW87XXX_PID_60_OC_DEBUG_SEL_VOPP << AW87XXX_PID_60_OC_DEBUG_SEL_START_BIT)
+
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VOPN	(1)
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VOPN_VALUE	\
+	(AW87XXX_PID_60_OC_DEBUG_SEL_VOPN << AW87XXX_PID_60_OC_DEBUG_SEL_START_BIT)
+
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VONP	(2)
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VONP_VALUE	\
+	(AW87XXX_PID_60_OC_DEBUG_SEL_VONP << AW87XXX_PID_60_OC_DEBUG_SEL_START_BIT)
+
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VONN	(3)
+#define AW87XXX_PID_60_OC_DEBUG_SEL_VONN_VALUE	\
+	(AW87XXX_PID_60_OC_DEBUG_SEL_VONN << AW87XXX_PID_60_OC_DEBUG_SEL_START_BIT)
+
+#define AW87XXX_PID_60_OC_DEBUG_SEL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_OC_DEBUG_SEL_DEFAULT	\
+	(AW87XXX_PID_60_OC_DEBUG_SEL_DEFAULT_VALUE << AW87XXX_PID_60_OC_DEBUG_SEL_START_BIT)
+
+/* default value of TEST_GTDR (0x78) */
+/* #define AW87XXX_PID_60_TEST_GTDR_DEFAULT		(0x00) */
+
+/* TEST_BST (0x79) detail */
+/* BST_LNMOS_TEST bit 0 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_BST_LNMOS_TEST_START_BIT	(0)
+#define AW87XXX_PID_60_BST_LNMOS_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_LNMOS_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_LNMOS_TEST_BITS_LEN)-1) << AW87XXX_PID_60_BST_LNMOS_TEST_START_BIT))
+
+#define AW87XXX_PID_60_BST_LNMOS_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_BST_LNMOS_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_BST_LNMOS_TEST_DIABLE << AW87XXX_PID_60_BST_LNMOS_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_LNMOS_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_LNMOS_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_LNMOS_TEST_ENABLE << AW87XXX_PID_60_BST_LNMOS_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_LNMOS_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_LNMOS_TEST_DEFAULT	\
+	(AW87XXX_PID_60_BST_LNMOS_TEST_DEFAULT_VALUE << AW87XXX_PID_60_BST_LNMOS_TEST_START_BIT)
+
+/* BST_HNMOS_TEST bit 1 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_BST_HNMOS_TEST_START_BIT	(1)
+#define AW87XXX_PID_60_BST_HNMOS_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_HNMOS_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_HNMOS_TEST_BITS_LEN)-1) << AW87XXX_PID_60_BST_HNMOS_TEST_START_BIT))
+
+#define AW87XXX_PID_60_BST_HNMOS_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_BST_HNMOS_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_BST_HNMOS_TEST_DIABLE << AW87XXX_PID_60_BST_HNMOS_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_HNMOS_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_HNMOS_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_HNMOS_TEST_ENABLE << AW87XXX_PID_60_BST_HNMOS_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_HNMOS_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_HNMOS_TEST_DEFAULT	\
+	(AW87XXX_PID_60_BST_HNMOS_TEST_DEFAULT_VALUE << AW87XXX_PID_60_BST_HNMOS_TEST_START_BIT)
+
+/* BST_SCP_TEST bit 2 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_BST_SCP_TEST_START_BIT	(2)
+#define AW87XXX_PID_60_BST_SCP_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_SCP_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_SCP_TEST_BITS_LEN)-1) << AW87XXX_PID_60_BST_SCP_TEST_START_BIT))
+
+#define AW87XXX_PID_60_BST_SCP_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_BST_SCP_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_TEST_DIABLE << AW87XXX_PID_60_BST_SCP_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_SCP_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_SCP_TEST_ENABLE << AW87XXX_PID_60_BST_SCP_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_SCP_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_SCP_TEST_DEFAULT	\
+	(AW87XXX_PID_60_BST_SCP_TEST_DEFAULT_VALUE << AW87XXX_PID_60_BST_SCP_TEST_START_BIT)
+
+/* ANTIR_TEST bit 3 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_ANTIR_TEST_START_BIT	(3)
+#define AW87XXX_PID_60_ANTIR_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_ANTIR_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_ANTIR_TEST_BITS_LEN)-1) << AW87XXX_PID_60_ANTIR_TEST_START_BIT))
+
+#define AW87XXX_PID_60_ANTIR_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_ANTIR_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_ANTIR_TEST_DIABLE << AW87XXX_PID_60_ANTIR_TEST_START_BIT)
+
+#define AW87XXX_PID_60_ANTIR_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_ANTIR_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_ANTIR_TEST_ENABLE << AW87XXX_PID_60_ANTIR_TEST_START_BIT)
+
+#define AW87XXX_PID_60_ANTIR_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_ANTIR_TEST_DEFAULT	\
+	(AW87XXX_PID_60_ANTIR_TEST_DEFAULT_VALUE << AW87XXX_PID_60_ANTIR_TEST_START_BIT)
+
+/* BST_PEAK_TEST bit 4 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_BST_PEAK_TEST_START_BIT	(4)
+#define AW87XXX_PID_60_BST_PEAK_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_PEAK_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_PEAK_TEST_BITS_LEN)-1) << AW87XXX_PID_60_BST_PEAK_TEST_START_BIT))
+
+#define AW87XXX_PID_60_BST_PEAK_TEST_DISABLE	(0)
+#define AW87XXX_PID_60_BST_PEAK_TEST_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_PEAK_TEST_DISABLE << AW87XXX_PID_60_BST_PEAK_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_PEAK_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_PEAK_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_PEAK_TEST_ENABLE << AW87XXX_PID_60_BST_PEAK_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_PEAK_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_PEAK_TEST_DEFAULT	\
+	(AW87XXX_PID_60_BST_PEAK_TEST_DEFAULT_VALUE << AW87XXX_PID_60_BST_PEAK_TEST_START_BIT)
+
+/* BST_OVP_TEST bit 5 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_BST_OVP_TEST_START_BIT	(5)
+#define AW87XXX_PID_60_BST_OVP_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_OVP_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_OVP_TEST_BITS_LEN)-1) << AW87XXX_PID_60_BST_OVP_TEST_START_BIT))
+
+#define AW87XXX_PID_60_BST_OVP_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_BST_OVP_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_TEST_DIABLE << AW87XXX_PID_60_BST_OVP_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_OVP_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_OVP_TEST_ENABLE << AW87XXX_PID_60_BST_OVP_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_OVP_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_OVP_TEST_DEFAULT	\
+	(AW87XXX_PID_60_BST_OVP_TEST_DEFAULT_VALUE << AW87XXX_PID_60_BST_OVP_TEST_START_BIT)
+
+/* BST_TEST_EN bit 6 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_BST_TEST_EN_START_BIT	(6)
+#define AW87XXX_PID_60_BST_TEST_EN_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_TEST_EN_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_TEST_EN_BITS_LEN)-1) << AW87XXX_PID_60_BST_TEST_EN_START_BIT))
+
+#define AW87XXX_PID_60_BST_TEST_EN_DIABLE	(0)
+#define AW87XXX_PID_60_BST_TEST_EN_DIABLE_VALUE	\
+	(AW87XXX_PID_60_BST_TEST_EN_DIABLE << AW87XXX_PID_60_BST_TEST_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_TEST_EN_ENABLE	(1)
+#define AW87XXX_PID_60_BST_TEST_EN_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_TEST_EN_ENABLE << AW87XXX_PID_60_BST_TEST_EN_START_BIT)
+
+#define AW87XXX_PID_60_BST_TEST_EN_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_TEST_EN_DEFAULT	\
+	(AW87XXX_PID_60_BST_TEST_EN_DEFAULT_VALUE << AW87XXX_PID_60_BST_TEST_EN_START_BIT)
+
+/* OC_DISABLE bit 7 (TEST_BST 0x79) */
+#define AW87XXX_PID_60_OC_DISABLE_START_BIT	(7)
+#define AW87XXX_PID_60_OC_DISABLE_BITS_LEN	(1)
+#define AW87XXX_PID_60_OC_DISABLE_MASK	\
+	(~(((1<<AW87XXX_PID_60_OC_DISABLE_BITS_LEN)-1) << AW87XXX_PID_60_OC_DISABLE_START_BIT))
+
+#define AW87XXX_PID_60_OC_DISABLE_ENABLE	(0)
+#define AW87XXX_PID_60_OC_DISABLE_ENABLE_VALUE	\
+	(AW87XXX_PID_60_OC_DISABLE_ENABLE << AW87XXX_PID_60_OC_DISABLE_START_BIT)
+
+#define AW87XXX_PID_60_OC_DISABLE_SHUTDOWN	(1)
+#define AW87XXX_PID_60_OC_DISABLE_SHUTDOWN_VALUE	\
+	(AW87XXX_PID_60_OC_DISABLE_SHUTDOWN << AW87XXX_PID_60_OC_DISABLE_START_BIT)
+
+#define AW87XXX_PID_60_OC_DISABLE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_OC_DISABLE_DEFAULT	\
+	(AW87XXX_PID_60_OC_DISABLE_DEFAULT_VALUE << AW87XXX_PID_60_OC_DISABLE_START_BIT)
+
+/* default value of TEST_BST (0x79) */
+/* #define AW87XXX_PID_60_TEST_BST_DEFAULT		(0x00) */
+
+/* TEST_MODE (0x7A) detail */
+/* PA_TEST_FORCE bit 3 (TEST_MODE 0x7A) */
+#define AW87XXX_PID_60_PA_TEST_FORCE_START_BIT	(3)
+#define AW87XXX_PID_60_PA_TEST_FORCE_BITS_LEN	(1)
+#define AW87XXX_PID_60_PA_TEST_FORCE_MASK	\
+	(~(((1<<AW87XXX_PID_60_PA_TEST_FORCE_BITS_LEN)-1) << AW87XXX_PID_60_PA_TEST_FORCE_START_BIT))
+
+#define AW87XXX_PID_60_PA_TEST_FORCE_DISABLE	(0)
+#define AW87XXX_PID_60_PA_TEST_FORCE_DISABLE_VALUE	\
+	(AW87XXX_PID_60_PA_TEST_FORCE_DISABLE << AW87XXX_PID_60_PA_TEST_FORCE_START_BIT)
+
+#define AW87XXX_PID_60_PA_TEST_FORCE_ENABLE	(1)
+#define AW87XXX_PID_60_PA_TEST_FORCE_ENABLE_VALUE	\
+	(AW87XXX_PID_60_PA_TEST_FORCE_ENABLE << AW87XXX_PID_60_PA_TEST_FORCE_START_BIT)
+
+#define AW87XXX_PID_60_PA_TEST_FORCE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_PA_TEST_FORCE_DEFAULT	\
+	(AW87XXX_PID_60_PA_TEST_FORCE_DEFAULT_VALUE << AW87XXX_PID_60_PA_TEST_FORCE_START_BIT)
+
+/* BST_TEST_FORCE bit 4 (TEST_MODE 0x7A) */
+#define AW87XXX_PID_60_BST_TEST_FORCE_START_BIT	(4)
+#define AW87XXX_PID_60_BST_TEST_FORCE_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_TEST_FORCE_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_TEST_FORCE_BITS_LEN)-1) << AW87XXX_PID_60_BST_TEST_FORCE_START_BIT))
+
+#define AW87XXX_PID_60_BST_TEST_FORCE_DISABLE	(0)
+#define AW87XXX_PID_60_BST_TEST_FORCE_DISABLE_VALUE	\
+	(AW87XXX_PID_60_BST_TEST_FORCE_DISABLE << AW87XXX_PID_60_BST_TEST_FORCE_START_BIT)
+
+#define AW87XXX_PID_60_BST_TEST_FORCE_ENABLE	(1)
+#define AW87XXX_PID_60_BST_TEST_FORCE_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_TEST_FORCE_ENABLE << AW87XXX_PID_60_BST_TEST_FORCE_START_BIT)
+
+#define AW87XXX_PID_60_BST_TEST_FORCE_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_BST_TEST_FORCE_DEFAULT	\
+	(AW87XXX_PID_60_BST_TEST_FORCE_DEFAULT_VALUE << AW87XXX_PID_60_BST_TEST_FORCE_START_BIT)
+
+/* EN_OT_TEST bit 5 (TEST_MODE 0x7A) */
+#define AW87XXX_PID_60_EN_OT_TEST_START_BIT	(5)
+#define AW87XXX_PID_60_EN_OT_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_OT_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_OT_TEST_BITS_LEN)-1) << AW87XXX_PID_60_EN_OT_TEST_START_BIT))
+
+#define AW87XXX_PID_60_EN_OT_TEST_DISABLE	(0)
+#define AW87XXX_PID_60_EN_OT_TEST_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_OT_TEST_DISABLE << AW87XXX_PID_60_EN_OT_TEST_START_BIT)
+
+#define AW87XXX_PID_60_EN_OT_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_EN_OT_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_OT_TEST_ENABLE << AW87XXX_PID_60_EN_OT_TEST_START_BIT)
+
+#define AW87XXX_PID_60_EN_OT_TEST_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_OT_TEST_DEFAULT	\
+	(AW87XXX_PID_60_EN_OT_TEST_DEFAULT_VALUE << AW87XXX_PID_60_EN_OT_TEST_START_BIT)
+
+/* EN_TEST bit 6 (TEST_MODE 0x7A) */
+#define AW87XXX_PID_60_EN_TEST_START_BIT	(6)
+#define AW87XXX_PID_60_EN_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_TEST_MASK		\
+	(~(((1<<AW87XXX_PID_60_EN_TEST_BITS_LEN)-1) << AW87XXX_PID_60_EN_TEST_START_BIT))
+
+#define AW87XXX_PID_60_EN_TEST_DISABLE	(0)
+#define AW87XXX_PID_60_EN_TEST_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_TEST_DISABLE << AW87XXX_PID_60_EN_TEST_START_BIT)
+
+#define AW87XXX_PID_60_EN_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_EN_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_TEST_ENABLE << AW87XXX_PID_60_EN_TEST_START_BIT)
+
+#define AW87XXX_PID_60_EN_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_EN_TEST_DEFAULT	\
+	(AW87XXX_PID_60_EN_TEST_DEFAULT_VALUE << AW87XXX_PID_60_EN_TEST_START_BIT)
+
+/* BST_BURST_TEST bit 7 (TEST_MODE 0x7A) */
+#define AW87XXX_PID_60_BST_BURST_TEST_START_BIT	(7)
+#define AW87XXX_PID_60_BST_BURST_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_BST_BURST_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_BST_BURST_TEST_BITS_LEN)-1) << AW87XXX_PID_60_BST_BURST_TEST_START_BIT))
+
+#define AW87XXX_PID_60_BST_BURST_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_BST_BURST_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_TEST_DIABLE << AW87XXX_PID_60_BST_BURST_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_BST_BURST_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_BST_BURST_TEST_ENABLE << AW87XXX_PID_60_BST_BURST_TEST_START_BIT)
+
+#define AW87XXX_PID_60_BST_BURST_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_BST_BURST_TEST_DEFAULT	\
+	(AW87XXX_PID_60_BST_BURST_TEST_DEFAULT_VALUE << AW87XXX_PID_60_BST_BURST_TEST_START_BIT)
+
+/* TEST_ANALOG_CTRL bit 2:0 (TEST_MODE 0x7A) */
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT	(0)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_BITS_LEN	(3)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_MASK	\
+	(~(((1<<AW87XXX_PID_60_TEST_ANALOG_CTRL_BITS_LEN)-1) << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT))
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VBG	(0)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VBG_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_VBG << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_COMPT	(1)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_COMPT_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_COMPT << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_HALFVDD	(2)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_HALFVDD_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_HALFVDD << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VCSEL	(3)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VCSEL_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_VCSEL << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VLSEL	(4)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VLSEL_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_VLSEL << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_MSBMVTH1T	(5)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_MSBMVTH1T_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_MSBMVTH1T << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_MSBMVTH4T	(6)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_MSBMVTH4T_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_MSBMVTH4T << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VCM2T	(7)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_VCM2T_VALUE	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_VCM2T << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_TEST_ANALOG_CTRL_DEFAULT	\
+	(AW87XXX_PID_60_TEST_ANALOG_CTRL_DEFAULT_VALUE << AW87XXX_PID_60_TEST_ANALOG_CTRL_START_BIT)
+
+/* default value of TEST_MODE (0x7A) */
+/* #define AW87XXX_PID_60_TEST_MODE_DEFAULT		(0x00) */
+
+/* TEST_CON (0x7B) detail */
+/* COMP_TEST bit 5 (TEST_CON 0x7B) */
+#define AW87XXX_PID_60_COMP_TEST_START_BIT	(5)
+#define AW87XXX_PID_60_COMP_TEST_BITS_LEN	(1)
+#define AW87XXX_PID_60_COMP_TEST_MASK	\
+	(~(((1<<AW87XXX_PID_60_COMP_TEST_BITS_LEN)-1) << AW87XXX_PID_60_COMP_TEST_START_BIT))
+
+#define AW87XXX_PID_60_COMP_TEST_DIABLE	(0)
+#define AW87XXX_PID_60_COMP_TEST_DIABLE_VALUE	\
+	(AW87XXX_PID_60_COMP_TEST_DIABLE << AW87XXX_PID_60_COMP_TEST_START_BIT)
+
+#define AW87XXX_PID_60_COMP_TEST_ENABLE	(1)
+#define AW87XXX_PID_60_COMP_TEST_ENABLE_VALUE	\
+	(AW87XXX_PID_60_COMP_TEST_ENABLE << AW87XXX_PID_60_COMP_TEST_START_BIT)
+
+#define AW87XXX_PID_60_COMP_TEST_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_COMP_TEST_DEFAULT	\
+	(AW87XXX_PID_60_COMP_TEST_DEFAULT_VALUE << AW87XXX_PID_60_COMP_TEST_START_BIT)
+
+/* REG_EN_ESD bit 6 (TEST_CON 0x7B) */
+#define AW87XXX_PID_60_REG_EN_ESD_START_BIT	(6)
+#define AW87XXX_PID_60_REG_EN_ESD_BITS_LEN	(1)
+#define AW87XXX_PID_60_REG_EN_ESD_MASK	\
+	(~(((1<<AW87XXX_PID_60_REG_EN_ESD_BITS_LEN)-1) << AW87XXX_PID_60_REG_EN_ESD_START_BIT))
+
+#define AW87XXX_PID_60_REG_EN_ESD_DISABLE	(0)
+#define AW87XXX_PID_60_REG_EN_ESD_DISABLE_VALUE	\
+	(AW87XXX_PID_60_REG_EN_ESD_DISABLE << AW87XXX_PID_60_REG_EN_ESD_START_BIT)
+
+#define AW87XXX_PID_60_REG_EN_ESD_ENABLE	(1)
+#define AW87XXX_PID_60_REG_EN_ESD_ENABLE_VALUE	\
+	(AW87XXX_PID_60_REG_EN_ESD_ENABLE << AW87XXX_PID_60_REG_EN_ESD_START_BIT)
+
+#define AW87XXX_PID_60_REG_EN_ESD_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_60_REG_EN_ESD_DEFAULT	\
+	(AW87XXX_PID_60_REG_EN_ESD_DEFAULT_VALUE << AW87XXX_PID_60_REG_EN_ESD_START_BIT)
+
+/* EN_POWER_MT bit 7 (TEST_CON 0x7B) */
+#define AW87XXX_PID_60_EN_POWER_MT_START_BIT	(7)
+#define AW87XXX_PID_60_EN_POWER_MT_BITS_LEN	(1)
+#define AW87XXX_PID_60_EN_POWER_MT_MASK	\
+	(~(((1<<AW87XXX_PID_60_EN_POWER_MT_BITS_LEN)-1) << AW87XXX_PID_60_EN_POWER_MT_START_BIT))
+
+#define AW87XXX_PID_60_EN_POWER_MT_DISABLE	(0)
+#define AW87XXX_PID_60_EN_POWER_MT_DISABLE_VALUE	\
+	(AW87XXX_PID_60_EN_POWER_MT_DISABLE << AW87XXX_PID_60_EN_POWER_MT_START_BIT)
+
+#define AW87XXX_PID_60_EN_POWER_MT_ENABLE	(1)
+#define AW87XXX_PID_60_EN_POWER_MT_ENABLE_VALUE	\
+	(AW87XXX_PID_60_EN_POWER_MT_ENABLE << AW87XXX_PID_60_EN_POWER_MT_START_BIT)
+
+#define AW87XXX_PID_60_EN_POWER_MT_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_EN_POWER_MT_DEFAULT	\
+	(AW87XXX_PID_60_EN_POWER_MT_DEFAULT_VALUE << AW87XXX_PID_60_EN_POWER_MT_START_BIT)
+
+/* TEST_DIGITAL_CTRL bit 4:0 (TEST_CON 0x7B) */
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT	(0)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BITS_LEN	(5)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_MASK	\
+	(~(((1<<AW87XXX_PID_60_TEST_DIGITAL_CTRL_BITS_LEN)-1) << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT))
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTOVPS	(0)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTOVPS_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTOVPS << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTOVP2	(1)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTOVP2_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTOVP2 << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTHNMOS	(2)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTHNMOS_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTHNMOS << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTLNMOS	(3)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTLNMOS_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTLNMOS << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTPEAK	(4)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTPEAK_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTPEAK << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTRSQ	(5)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTRSQ_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTRSQ << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTBURST	(6)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTBURST_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTBURST << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTSSFINISH	(7)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTSSFINISH_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTSSFINISH << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTSCP	(8)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTSCP_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTSCP << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTENCLAMP	(9)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTENCLAMP_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTENCLAMP << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTNCD	(10)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTNCD_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_BSTNCD << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_ENSYNC	(11)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_ENSYNC_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_ENSYNC << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_ENOTA	(12)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_ENOTA_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_ENOTA << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_NGDET	(13)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_NGDET_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_NGDET << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_HNGTN	(14)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_HNGTN_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_HNGTN << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_LNGTN	(15)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_LNGTN_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_LNGTN << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_GATESENSEN	(16)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_GATESENSEN_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_GATESENSEN << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_HNGTP	(17)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_HNGTP_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_HNGTP << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_LNGTP	(18)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_LNGTP_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_LNGTP << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_GATESENSEP	(19)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_GATESENSEP_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_GATESENSEP << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_ADPBOOST	(20)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_ADPBOOST_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_ADPBOOST << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_POVTHBELOW0ABOVE	(21)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_POVTHBELOW0ABOVE_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_POVTHBELOW0ABOVE << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_POVTHBELOW3ABOVE	(22)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_POVTHBELOW3ABOVE_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_POVTHBELOW3ABOVE << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_AMPOC	(23)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_AMPOC_VALUE	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_AMPOC << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_TEST_DIGITAL_CTRL_DEFAULT	\
+	(AW87XXX_PID_60_TEST_DIGITAL_CTRL_DEFAULT_VALUE << AW87XXX_PID_60_TEST_DIGITAL_CTRL_START_BIT)
+
+/* default value of TEST_CON (0x7B) */
+/* #define AW87XXX_PID_60_TEST_CON_DEFAULT		(0x00) */
+
+/* ENCR (0x7C) detail */
+/* TEST_REG_ENCRY bit 7:6 (ENCR 0x7C) */
+#define AW87XXX_PID_60_TEST_REG_ENCRY_START_BIT	(6)
+#define AW87XXX_PID_60_TEST_REG_ENCRY_BITS_LEN	(2)
+#define AW87XXX_PID_60_TEST_REG_ENCRY_MASK	\
+	(~(((1<<AW87XXX_PID_60_TEST_REG_ENCRY_BITS_LEN)-1) << AW87XXX_PID_60_TEST_REG_ENCRY_START_BIT))
+
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE0	(0)
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE0_VALUE	\
+	(AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE0 << AW87XXX_PID_60_TEST_REG_ENCRY_START_BIT)
+
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE1	(1)
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE1_VALUE	\
+	(AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE1 << AW87XXX_PID_60_TEST_REG_ENCRY_START_BIT)
+
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE2	(2)
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE2_VALUE	\
+	(AW87XXX_PID_60_TEST_REG_ENCRY_CANNOT_WRITE2 << AW87XXX_PID_60_TEST_REG_ENCRY_START_BIT)
+
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CAN_BE_WRITE	(3)
+#define AW87XXX_PID_60_TEST_REG_ENCRY_CAN_BE_WRITE_VALUE	\
+	(AW87XXX_PID_60_TEST_REG_ENCRY_CAN_BE_WRITE << AW87XXX_PID_60_TEST_REG_ENCRY_START_BIT)
+/*
+Fix me here:
+reg_addr:0x7C, reg_name:ENCR, field_name:TEST_REG_ENCRY, content:Encryption bits for test registers(78h~7Ch) , when0x7D=7a & test_Reg_Encry=11
+maybe need to fix manually
+*/
+#define AW87XXX_PID_60_TEST_REG_ENCRY_DEFAULT_VALUE	(0x0)
+#define AW87XXX_PID_60_TEST_REG_ENCRY_DEFAULT	\
+	(AW87XXX_PID_60_TEST_REG_ENCRY_DEFAULT_VALUE << AW87XXX_PID_60_TEST_REG_ENCRY_START_BIT)
+
+/* PRODUCT_REG_ENCRY bit 5:0 (ENCR 0x7C) */
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_START_BIT	(0)
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_BITS_LEN	(6)
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_MASK	\
+	(~(((1<<AW87XXX_PID_60_PRODUCT_REG_ENCRY_BITS_LEN)-1) << AW87XXX_PID_60_PRODUCT_REG_ENCRY_START_BIT))
+
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87560	(1)
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87560_VALUE	\
+	(AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87560 << AW87XXX_PID_60_PRODUCT_REG_ENCRY_START_BIT)
+
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87561	(2)
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87561_VALUE	\
+	(AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87561 << AW87XXX_PID_60_PRODUCT_REG_ENCRY_START_BIT)
+
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87562	(4)
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87562_VALUE	\
+	(AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87562 << AW87XXX_PID_60_PRODUCT_REG_ENCRY_START_BIT)
+
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87501	(8)
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87501_VALUE	\
+	(AW87XXX_PID_60_PRODUCT_REG_ENCRY_AW87501 << AW87XXX_PID_60_PRODUCT_REG_ENCRY_START_BIT)
+/*
+Fix me here:
+reg_addr:0x7C, reg_name:ENCR, field_name:PRODUCT_REG_ENCRY, content:Production encryption  register (when0x7D=7a & test_Reg_Encry=11 ��
+maybe need to fix manually
+*/
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_DEFAULT_VALUE	(0x1)
+#define AW87XXX_PID_60_PRODUCT_REG_ENCRY_DEFAULT	\
+	(AW87XXX_PID_60_PRODUCT_REG_ENCRY_DEFAULT_VALUE << AW87XXX_PID_60_PRODUCT_REG_ENCRY_START_BIT)
+
+/* default value of ENCR (0x7C) */
+/* #define AW87XXX_PID_60_ENCR_DEFAULT		(0x01) */
+
+/* detail information of registers end */
+
+#endif  /* #ifndef  __AW87XXX_PID_60_REG_H__ */
\ No newline at end of file
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_76_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_76_reg.h
new file mode 100644
index 000000000000..13280cdb5b2e
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_76_reg.h
@@ -0,0 +1,1205 @@
+#ifndef __AW87XXX_PID_76_REG_H__
+#define __AW87XXX_PID_76_REG_H__
+
+/* registers list */
+#define AW87XXX_PID_76_ID_REG			(0x00)
+#define AW87XXX_PID_76_SYSCTRL_REG		(0x01)
+#define AW87XXX_PID_76_MDCTRL_REG		(0x02)
+#define AW87XXX_PID_76_CPOVP_REG		(0x03)
+#define AW87XXX_PID_76_CPP_REG			(0x04)
+#define AW87XXX_PID_76_PAG_REG			(0x05)
+#define AW87XXX_PID_76_AGC3P_REG		(0x06)
+#define AW87XXX_PID_76_AGC3PA_REG		(0x07)
+#define AW87XXX_PID_76_AGC2P_REG		(0x08)
+#define AW87XXX_PID_76_AGC2PA_REG		(0x09)
+#define AW87XXX_PID_76_AGC1PA_REG		(0x0A)
+#define AW87XXX_PID_76_SYSST_REG		(0x59)
+#define AW87XXX_PID_76_SYSINT_REG		(0x60)
+#define AW87XXX_PID_76_DFT_SYSCTRL_REG		(0x61)
+#define AW87XXX_PID_76_DFT_MDCTRL_REG		(0x62)
+#define AW87XXX_PID_76_DFT_CPADP_REG		(0x63)
+#define AW87XXX_PID_76_DFT_AGCPA_REG		(0x64)
+#define AW87XXX_PID_76_DFT_POFR_REG		(0x65)
+#define AW87XXX_PID_76_DFT_OC_REG		(0x66)
+#define AW87XXX_PID_76_DFT_ADP1_REG		(0x67)
+#define AW87XXX_PID_76_DFT_REF_REG		(0x68)
+#define AW87XXX_PID_76_DFT_LDO_REG		(0x69)
+#define AW87XXX_PID_76_ADP1_REG			(0x70)
+#define AW87XXX_PID_76_ADP2_REG			(0x71)
+#define AW87XXX_PID_76_NG1_REG			(0x72)
+#define AW87XXX_PID_76_NG2_REG			(0x73)
+#define AW87XXX_PID_76_NG3_REG			(0x74)
+#define AW87XXX_PID_76_CP_REG			(0x75)
+#define AW87XXX_PID_76_AB_REG			(0x76)
+#define AW87XXX_PID_76_TEST_REG			(0x77)
+#define AW87XXX_PID_76_ENCR_REG			(0x78)
+#define AW87XXX_PID_76_DFT_ADP1_CHECK		(0x04)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_76_softrst_access[2] = {0x00, 0xaa};
+
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_76_REG_MAX				(0x79)
+
+#define REG_NONE_ACCESS					(0)
+#define REG_RD_ACCESS					(1 << 0)
+#define REG_WR_ACCESS					(1 << 1)
+
+const unsigned char aw87xxx_pid_76_reg_access[AW87XXX_PID_76_REG_MAX] = {
+	[AW87XXX_PID_76_ID_REG]		= (REG_RD_ACCESS),
+	[AW87XXX_PID_76_SYSCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_MDCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_CPOVP_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_CPP_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_PAG_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_AGC3P_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_AGC3PA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_AGC2P_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_AGC2PA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_AGC1PA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_SYSST_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_76_SYSINT_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_76_DFT_SYSCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_MDCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_CPADP_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_AGCPA_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_POFR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_OC_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_ADP1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_REF_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_DFT_LDO_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_ADP1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_ADP2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_NG1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_NG2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_NG3_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_CP_REG]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_AB_REG]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_TEST_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_76_ENCR_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+};
+
+/* detail information of registers begin */
+/* ID (0x00) detail */
+/* IDCODE bit 7:0 (ID 0x00) */
+#define AW87XXX_PID_76_IDCODE_START_BIT	(0)
+#define AW87XXX_PID_76_IDCODE_BITS_LEN	(8)
+#define AW87XXX_PID_76_IDCODE_MASK		\
+	(~(((1<<AW87XXX_PID_76_IDCODE_BITS_LEN)-1) << AW87XXX_PID_76_IDCODE_START_BIT))
+
+#define AW87XXX_PID_76_IDCODE_DEFAULT_VALUE	(0x76)
+#define AW87XXX_PID_76_IDCODE_DEFAULT	\
+	(AW87XXX_PID_76_IDCODE_DEFAULT_VALUE << AW87XXX_PID_76_IDCODE_START_BIT)
+
+/* default value of ID (0x00) */
+/* #define AW87XXX_PID_76_ID_DEFAULT		(0x76) */
+
+/* SYSCTRL (0x01) detail */
+/* EN_PA bit 2 (SYSCTRL 0x01) */
+#define AW87XXX_PID_76_EN_PA_START_BIT	(2)
+#define AW87XXX_PID_76_EN_PA_BITS_LEN	(1)
+#define AW87XXX_PID_76_EN_PA_MASK		\
+	(~(((1<<AW87XXX_PID_76_EN_PA_BITS_LEN)-1) << AW87XXX_PID_76_EN_PA_START_BIT))
+
+#define AW87XXX_PID_76_EN_PA_DISABLE	(0)
+#define AW87XXX_PID_76_EN_PA_DISABLE_VALUE	\
+	(AW87XXX_PID_76_EN_PA_DISABLE << AW87XXX_PID_76_EN_PA_START_BIT)
+
+#define AW87XXX_PID_76_EN_PA_ENABLE_DEPENDS_ON_EN_AB	(1)
+#define AW87XXX_PID_76_EN_PA_ENABLE_DEPENDS_ON_EN_AB_VALUE	\
+	(AW87XXX_PID_76_EN_PA_ENABLE_DEPENDS_ON_EN_AB << AW87XXX_PID_76_EN_PA_START_BIT)
+
+#define AW87XXX_PID_76_EN_PA_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_EN_PA_DEFAULT	\
+	(AW87XXX_PID_76_EN_PA_DEFAULT_VALUE << AW87XXX_PID_76_EN_PA_START_BIT)
+
+/* EN_CP bit 1 (SYSCTRL 0x01) */
+#define AW87XXX_PID_76_EN_CP_START_BIT	(1)
+#define AW87XXX_PID_76_EN_CP_BITS_LEN	(1)
+#define AW87XXX_PID_76_EN_CP_MASK		\
+	(~(((1<<AW87XXX_PID_76_EN_CP_BITS_LEN)-1) << AW87XXX_PID_76_EN_CP_START_BIT))
+
+#define AW87XXX_PID_76_EN_CP_DISABLE_PVDD0	(0)
+#define AW87XXX_PID_76_EN_CP_DISABLE_PVDD0_VALUE	\
+	(AW87XXX_PID_76_EN_CP_DISABLE_PVDD0 << AW87XXX_PID_76_EN_CP_START_BIT)
+
+#define AW87XXX_PID_76_EN_CP_ENABLE_DEPENDS_ON_EN_2X	(1)
+#define AW87XXX_PID_76_EN_CP_ENABLE_DEPENDS_ON_EN_2X_VALUE	\
+	(AW87XXX_PID_76_EN_CP_ENABLE_DEPENDS_ON_EN_2X << AW87XXX_PID_76_EN_CP_START_BIT)
+
+#define AW87XXX_PID_76_EN_CP_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_EN_CP_DEFAULT	\
+	(AW87XXX_PID_76_EN_CP_DEFAULT_VALUE << AW87XXX_PID_76_EN_CP_START_BIT)
+
+/* PU_CPPA bit 0 (SYSCTRL 0x01) */
+#define AW87XXX_PID_76_PU_CPPA_START_BIT	(0)
+#define AW87XXX_PID_76_PU_CPPA_BITS_LEN	(1)
+#define AW87XXX_PID_76_PU_CPPA_MASK		\
+	(~(((1<<AW87XXX_PID_76_PU_CPPA_BITS_LEN)-1) << AW87XXX_PID_76_PU_CPPA_START_BIT))
+
+#define AW87XXX_PID_76_PU_CPPA_POWERMINUS_DOWN	(0)
+#define AW87XXX_PID_76_PU_CPPA_POWERMINUS_DOWN_VALUE	\
+	(AW87XXX_PID_76_PU_CPPA_POWERMINUS_DOWN << AW87XXX_PID_76_PU_CPPA_START_BIT)
+
+#define AW87XXX_PID_76_PU_CPPA_POWERMINUS_UP	(1)
+#define AW87XXX_PID_76_PU_CPPA_POWERMINUS_UP_VALUE	\
+	(AW87XXX_PID_76_PU_CPPA_POWERMINUS_UP << AW87XXX_PID_76_PU_CPPA_START_BIT)
+
+#define AW87XXX_PID_76_PU_CPPA_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_PU_CPPA_DEFAULT	\
+	(AW87XXX_PID_76_PU_CPPA_DEFAULT_VALUE << AW87XXX_PID_76_PU_CPPA_START_BIT)
+
+/* default value of SYSCTRL (0x01) */
+/* #define AW87XXX_PID_76_SYSCTRL_DEFAULT		(0x06) */
+
+/* MDCTRL (0x02) detail */
+/* EN_ADAP bit 4 (MDCTRL 0x02) */
+#define AW87XXX_PID_76_EN_ADAP_START_BIT	(4)
+#define AW87XXX_PID_76_EN_ADAP_BITS_LEN	(1)
+#define AW87XXX_PID_76_EN_ADAP_MASK		\
+	(~(((1<<AW87XXX_PID_76_EN_ADAP_BITS_LEN)-1) << AW87XXX_PID_76_EN_ADAP_START_BIT))
+
+#define AW87XXX_PID_76_EN_ADAP_DISABLEDEFAULT	(0)
+#define AW87XXX_PID_76_EN_ADAP_DISABLEDEFAULT_VALUE	\
+	(AW87XXX_PID_76_EN_ADAP_DISABLEDEFAULT << AW87XXX_PID_76_EN_ADAP_START_BIT)
+
+#define AW87XXX_PID_76_EN_ADAP_ENABLE	(1)
+#define AW87XXX_PID_76_EN_ADAP_ENABLE_VALUE	\
+	(AW87XXX_PID_76_EN_ADAP_ENABLE << AW87XXX_PID_76_EN_ADAP_START_BIT)
+
+#define AW87XXX_PID_76_EN_ADAP_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_EN_ADAP_DEFAULT	\
+	(AW87XXX_PID_76_EN_ADAP_DEFAULT_VALUE << AW87XXX_PID_76_EN_ADAP_START_BIT)
+
+/* EN_2X bit 3 (MDCTRL 0x02) */
+#define AW87XXX_PID_76_EN_2X_START_BIT	(3)
+#define AW87XXX_PID_76_EN_2X_BITS_LEN	(1)
+#define AW87XXX_PID_76_EN_2X_MASK		\
+	(~(((1<<AW87XXX_PID_76_EN_2X_BITS_LEN)-1) << AW87XXX_PID_76_EN_2X_START_BIT))
+
+#define AW87XXX_PID_76_EN_2X_DISABLE	(0)
+#define AW87XXX_PID_76_EN_2X_DISABLE_VALUE	\
+	(AW87XXX_PID_76_EN_2X_DISABLE << AW87XXX_PID_76_EN_2X_START_BIT)
+
+#define AW87XXX_PID_76_EN_2X_ENABLE		(1)
+#define AW87XXX_PID_76_EN_2X_ENABLE_VALUE	\
+	(AW87XXX_PID_76_EN_2X_ENABLE << AW87XXX_PID_76_EN_2X_START_BIT)
+
+#define AW87XXX_PID_76_EN_2X_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_EN_2X_DEFAULT	\
+	(AW87XXX_PID_76_EN_2X_DEFAULT_VALUE << AW87XXX_PID_76_EN_2X_START_BIT)
+
+/* EN_SPK bit 2 (MDCTRL 0x02) */
+#define AW87XXX_PID_76_EN_SPK_START_BIT	(2)
+#define AW87XXX_PID_76_EN_SPK_BITS_LEN	(1)
+#define AW87XXX_PID_76_EN_SPK_MASK		\
+	(~(((1<<AW87XXX_PID_76_EN_SPK_BITS_LEN)-1) << AW87XXX_PID_76_EN_SPK_START_BIT))
+
+#define AW87XXX_PID_76_EN_SPK_DISABLE	(0)
+#define AW87XXX_PID_76_EN_SPK_DISABLE_VALUE	\
+	(AW87XXX_PID_76_EN_SPK_DISABLE << AW87XXX_PID_76_EN_SPK_START_BIT)
+
+#define AW87XXX_PID_76_EN_SPK_ENABLE	(1)
+#define AW87XXX_PID_76_EN_SPK_ENABLE_VALUE	\
+	(AW87XXX_PID_76_EN_SPK_ENABLE << AW87XXX_PID_76_EN_SPK_START_BIT)
+
+#define AW87XXX_PID_76_EN_SPK_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_EN_SPK_DEFAULT	\
+	(AW87XXX_PID_76_EN_SPK_DEFAULT_VALUE << AW87XXX_PID_76_EN_SPK_START_BIT)
+
+/* EN_LG bit 1 (MDCTRL 0x02) */
+#define AW87XXX_PID_76_EN_LG_START_BIT	(1)
+#define AW87XXX_PID_76_EN_LG_BITS_LEN	(1)
+#define AW87XXX_PID_76_EN_LG_MASK		\
+	(~(((1<<AW87XXX_PID_76_EN_LG_BITS_LEN)-1) << AW87XXX_PID_76_EN_LG_START_BIT))
+
+#define AW87XXX_PID_76_EN_LG_DISABLE	(0)
+#define AW87XXX_PID_76_EN_LG_DISABLE_VALUE	\
+	(AW87XXX_PID_76_EN_LG_DISABLE << AW87XXX_PID_76_EN_LG_START_BIT)
+
+#define AW87XXX_PID_76_EN_LG_ENABLE		(1)
+#define AW87XXX_PID_76_EN_LG_ENABLE_VALUE	\
+	(AW87XXX_PID_76_EN_LG_ENABLE << AW87XXX_PID_76_EN_LG_START_BIT)
+
+#define AW87XXX_PID_76_EN_LG_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_EN_LG_DEFAULT	\
+	(AW87XXX_PID_76_EN_LG_DEFAULT_VALUE << AW87XXX_PID_76_EN_LG_START_BIT)
+
+/* EN_AB bit 0 (MDCTRL 0x02) */
+#define AW87XXX_PID_76_EN_AB_START_BIT	(0)
+#define AW87XXX_PID_76_EN_AB_BITS_LEN	(1)
+#define AW87XXX_PID_76_EN_AB_MASK		\
+	(~(((1<<AW87XXX_PID_76_EN_AB_BITS_LEN)-1) << AW87XXX_PID_76_EN_AB_START_BIT))
+
+#define AW87XXX_PID_76_EN_AB_DISABLE	(0)
+#define AW87XXX_PID_76_EN_AB_DISABLE_VALUE	\
+	(AW87XXX_PID_76_EN_AB_DISABLE << AW87XXX_PID_76_EN_AB_START_BIT)
+
+#define AW87XXX_PID_76_EN_AB_ENABLE		(1)
+#define AW87XXX_PID_76_EN_AB_ENABLE_VALUE	\
+	(AW87XXX_PID_76_EN_AB_ENABLE << AW87XXX_PID_76_EN_AB_START_BIT)
+
+#define AW87XXX_PID_76_EN_AB_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_EN_AB_DEFAULT	\
+	(AW87XXX_PID_76_EN_AB_DEFAULT_VALUE << AW87XXX_PID_76_EN_AB_START_BIT)
+
+/* default value of MDCTRL (0x02) */
+/* #define AW87XXX_PID_76_MDCTRL_DEFAULT		(0x0C) */
+
+/* CPOVP (0x03) detail */
+/* CP_OVP1 bit 3:0 (CPOVP 0x03) */
+#define AW87XXX_PID_76_CP_OVP1_START_BIT	(0)
+#define AW87XXX_PID_76_CP_OVP1_BITS_LEN	(4)
+#define AW87XXX_PID_76_CP_OVP1_MASK		\
+	(~(((1<<AW87XXX_PID_76_CP_OVP1_BITS_LEN)-1) << AW87XXX_PID_76_CP_OVP1_START_BIT))
+
+#define AW87XXX_PID_76_CP_OVP1_6P0V		(0)
+#define AW87XXX_PID_76_CP_OVP1_6P0V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_6P0V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_6P25V	(1)
+#define AW87XXX_PID_76_CP_OVP1_6P25V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_6P25V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_6P5V		(2)
+#define AW87XXX_PID_76_CP_OVP1_6P5V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_6P5V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_6P75V	(3)
+#define AW87XXX_PID_76_CP_OVP1_6P75V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_6P75V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_7V		(4)
+#define AW87XXX_PID_76_CP_OVP1_7V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_7V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_7P25V	(5)
+#define AW87XXX_PID_76_CP_OVP1_7P25V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_7P25V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_7P5V		(6)
+#define AW87XXX_PID_76_CP_OVP1_7P5V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_7P5V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_7P75V	(7)
+#define AW87XXX_PID_76_CP_OVP1_7P75V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_7P75V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_8V		(8)
+#define AW87XXX_PID_76_CP_OVP1_8V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_8V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_8P25V	(9)
+#define AW87XXX_PID_76_CP_OVP1_8P25V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_8P25V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_8P5V		(10)
+#define AW87XXX_PID_76_CP_OVP1_8P5V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_8P5V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_8P75V	(11)
+#define AW87XXX_PID_76_CP_OVP1_8P75V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_8P75V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_9V		(12)
+#define AW87XXX_PID_76_CP_OVP1_9V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_9V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_9P25V	(13)
+#define AW87XXX_PID_76_CP_OVP1_9P25V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_9P25V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_9P5V		(14)
+#define AW87XXX_PID_76_CP_OVP1_9P5V_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_9P5V << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_RESERVEDP_IF_SET_TURNS_TO_DEFAULTP	(15)
+#define AW87XXX_PID_76_CP_OVP1_RESERVEDP_IF_SET_TURNS_TO_DEFAULTP_VALUE	\
+	(AW87XXX_PID_76_CP_OVP1_RESERVEDP_IF_SET_TURNS_TO_DEFAULTP << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+#define AW87XXX_PID_76_CP_OVP1_DEFAULT_VALUE	(8)
+#define AW87XXX_PID_76_CP_OVP1_DEFAULT	\
+	(AW87XXX_PID_76_CP_OVP1_DEFAULT_VALUE << AW87XXX_PID_76_CP_OVP1_START_BIT)
+
+/* default value of CPOVP (0x03) */
+/* #define AW87XXX_PID_76_CPOVP_DEFAULT		(0x08) */
+
+/* CPP (0x04) detail */
+/* CP_PEAK_CUR bit 4:2 (CPP 0x04) */
+#define AW87XXX_PID_76_CP_PEAK_CUR_START_BIT	(2)
+#define AW87XXX_PID_76_CP_PEAK_CUR_BITS_LEN	(3)
+#define AW87XXX_PID_76_CP_PEAK_CUR_MASK	\
+	(~(((1<<AW87XXX_PID_76_CP_PEAK_CUR_BITS_LEN)-1) << AW87XXX_PID_76_CP_PEAK_CUR_START_BIT))
+
+#define AW87XXX_PID_76_CP_PEAK_CUR_2A	(0)
+#define AW87XXX_PID_76_CP_PEAK_CUR_2A_VALUE	\
+	(AW87XXX_PID_76_CP_PEAK_CUR_2A << AW87XXX_PID_76_CP_PEAK_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_PEAK_CUR_2P5A	(1)
+#define AW87XXX_PID_76_CP_PEAK_CUR_2P5A_VALUE	\
+	(AW87XXX_PID_76_CP_PEAK_CUR_2P5A << AW87XXX_PID_76_CP_PEAK_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_PEAK_CUR_3A	(2)
+#define AW87XXX_PID_76_CP_PEAK_CUR_3A_VALUE	\
+	(AW87XXX_PID_76_CP_PEAK_CUR_3A << AW87XXX_PID_76_CP_PEAK_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_PEAK_CUR_3P5A	(3)
+#define AW87XXX_PID_76_CP_PEAK_CUR_3P5A_VALUE	\
+	(AW87XXX_PID_76_CP_PEAK_CUR_3P5A << AW87XXX_PID_76_CP_PEAK_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_PEAK_CUR_4A	(4)
+#define AW87XXX_PID_76_CP_PEAK_CUR_4A_VALUE	\
+	(AW87XXX_PID_76_CP_PEAK_CUR_4A << AW87XXX_PID_76_CP_PEAK_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_PEAK_CUR_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_CP_PEAK_CUR_DEFAULT	\
+	(AW87XXX_PID_76_CP_PEAK_CUR_DEFAULT_VALUE << AW87XXX_PID_76_CP_PEAK_CUR_START_BIT)
+
+/* CP_SOFT_CUR bit 1:0 (CPP 0x04) */
+#define AW87XXX_PID_76_CP_SOFT_CUR_START_BIT	(0)
+#define AW87XXX_PID_76_CP_SOFT_CUR_BITS_LEN	(2)
+#define AW87XXX_PID_76_CP_SOFT_CUR_MASK	\
+	(~(((1<<AW87XXX_PID_76_CP_SOFT_CUR_BITS_LEN)-1) << AW87XXX_PID_76_CP_SOFT_CUR_START_BIT))
+
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P2A	(0)
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P2A_VALUE	\
+	(AW87XXX_PID_76_CP_SOFT_CUR_0P2A << AW87XXX_PID_76_CP_SOFT_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P3A	(1)
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P3A_VALUE	\
+	(AW87XXX_PID_76_CP_SOFT_CUR_0P3A << AW87XXX_PID_76_CP_SOFT_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P4A	(2)
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P4A_VALUE	\
+	(AW87XXX_PID_76_CP_SOFT_CUR_0P4A << AW87XXX_PID_76_CP_SOFT_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P5A	(3)
+#define AW87XXX_PID_76_CP_SOFT_CUR_0P5A_VALUE	\
+	(AW87XXX_PID_76_CP_SOFT_CUR_0P5A << AW87XXX_PID_76_CP_SOFT_CUR_START_BIT)
+
+#define AW87XXX_PID_76_CP_SOFT_CUR_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_CP_SOFT_CUR_DEFAULT	\
+	(AW87XXX_PID_76_CP_SOFT_CUR_DEFAULT_VALUE << AW87XXX_PID_76_CP_SOFT_CUR_START_BIT)
+
+/* default value of CPP (0x04) */
+/* #define AW87XXX_PID_76_CPP_DEFAULT		(0x05) */
+
+/* PAG (0x05) detail */
+/* GAIN bit 4:0 (PAG 0x05) */
+#define AW87XXX_PID_76_GAIN_START_BIT	(0)
+#define AW87XXX_PID_76_GAIN_BITS_LEN	(5)
+#define AW87XXX_PID_76_GAIN_MASK		\
+	(~(((1<<AW87XXX_PID_76_GAIN_BITS_LEN)-1) << AW87XXX_PID_76_GAIN_START_BIT))
+
+#define AW87XXX_PID_76_GAIN_0DB			(0)
+#define AW87XXX_PID_76_GAIN_0DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_0DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_1P5DB		(1)
+#define AW87XXX_PID_76_GAIN_1P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_1P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_3DB			(2)
+#define AW87XXX_PID_76_GAIN_3DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_3DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_4P5DB		(3)
+#define AW87XXX_PID_76_GAIN_4P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_4P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_6DB			(4)
+#define AW87XXX_PID_76_GAIN_6DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_6DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_7P5DB		(5)
+#define AW87XXX_PID_76_GAIN_7P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_7P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_9DB			(6)
+#define AW87XXX_PID_76_GAIN_9DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_9DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_10P5DB		(7)
+#define AW87XXX_PID_76_GAIN_10P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_10P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_12DB		(8)
+#define AW87XXX_PID_76_GAIN_12DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_12DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_13P5DB		(9)
+#define AW87XXX_PID_76_GAIN_13P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_13P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_15DB		(10)
+#define AW87XXX_PID_76_GAIN_15DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_15DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_16P5DB		(11)
+#define AW87XXX_PID_76_GAIN_16P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_16P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_18DB		(12)
+#define AW87XXX_PID_76_GAIN_18DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_18DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_19P5DB		(13)
+#define AW87XXX_PID_76_GAIN_19P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_19P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_21DB		(14)
+#define AW87XXX_PID_76_GAIN_21DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_21DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_22P5DB		(15)
+#define AW87XXX_PID_76_GAIN_22P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_22P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_24DB		(16)
+#define AW87XXX_PID_76_GAIN_24DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_24DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_25P5DB		(17)
+#define AW87XXX_PID_76_GAIN_25P5DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_25P5DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_27DB		(18)
+#define AW87XXX_PID_76_GAIN_27DB_VALUE	\
+	(AW87XXX_PID_76_GAIN_27DB << AW87XXX_PID_76_GAIN_START_BIT)
+
+#define AW87XXX_PID_76_GAIN_DEFAULT_VALUE	(12)
+#define AW87XXX_PID_76_GAIN_DEFAULT		\
+	(AW87XXX_PID_76_GAIN_DEFAULT_VALUE << AW87XXX_PID_76_GAIN_START_BIT)
+
+/* default value of PAG (0x05) */
+/* #define AW87XXX_PID_76_PAG_DEFAULT		(0x0C) */
+
+/* AGC3P (0x06) detail */
+/* AGC3PO bit 3:0 (AGC3P 0x06) */
+#define AW87XXX_PID_76_AGC3PO_START_BIT	(0)
+#define AW87XXX_PID_76_AGC3PO_BITS_LEN	(4)
+#define AW87XXX_PID_76_AGC3PO_MASK		\
+	(~(((1<<AW87XXX_PID_76_AGC3PO_BITS_LEN)-1) << AW87XXX_PID_76_AGC3PO_START_BIT))
+
+#define AW87XXX_PID_76_AGC3PO_0P2W4		(0)
+#define AW87XXX_PID_76_AGC3PO_0P2W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_0P2W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_0P4W4		(1)
+#define AW87XXX_PID_76_AGC3PO_0P4W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_0P4W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_0P6W4		(2)
+#define AW87XXX_PID_76_AGC3PO_0P6W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_0P6W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_0P8W4		(3)
+#define AW87XXX_PID_76_AGC3PO_0P8W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_0P8W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_1P0W4		(4)
+#define AW87XXX_PID_76_AGC3PO_1P0W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_1P0W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_1P2W4		(5)
+#define AW87XXX_PID_76_AGC3PO_1P2W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_1P2W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_1P4W4		(6)
+#define AW87XXX_PID_76_AGC3PO_1P4W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_1P4W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_1P6W4		(7)
+#define AW87XXX_PID_76_AGC3PO_1P6W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_1P6W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_1P8W4		(8)
+#define AW87XXX_PID_76_AGC3PO_1P8W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_1P8W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_2P0W4		(9)
+#define AW87XXX_PID_76_AGC3PO_2P0W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_2P0W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_2P2W4		(10)
+#define AW87XXX_PID_76_AGC3PO_2P2W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_2P2W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_2P4W4		(11)
+#define AW87XXX_PID_76_AGC3PO_2P4W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_2P4W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_2P6W4		(12)
+#define AW87XXX_PID_76_AGC3PO_2P6W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_2P6W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_2P8W4		(13)
+#define AW87XXX_PID_76_AGC3PO_2P8W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_2P8W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_3P0W4		(14)
+#define AW87XXX_PID_76_AGC3PO_3P0W4_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_3P0W4 << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_AGC3_OFF	(15)
+#define AW87XXX_PID_76_AGC3PO_AGC3_OFF_VALUE	\
+	(AW87XXX_PID_76_AGC3PO_AGC3_OFF << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC3PO_DEFAULT_VALUE	(7)
+#define AW87XXX_PID_76_AGC3PO_DEFAULT	\
+	(AW87XXX_PID_76_AGC3PO_DEFAULT_VALUE << AW87XXX_PID_76_AGC3PO_START_BIT)
+
+/* default value of AGC3P (0x06) */
+/* #define AW87XXX_PID_76_AGC3P_DEFAULT		(0x07) */
+
+/* AGC3PA (0x07) detail */
+/* AGC3RT bit 7:5 (AGC3PA 0x07) */
+#define AW87XXX_PID_76_AGC3RT_START_BIT	(5)
+#define AW87XXX_PID_76_AGC3RT_BITS_LEN	(3)
+#define AW87XXX_PID_76_AGC3RT_MASK		\
+	(~(((1<<AW87XXX_PID_76_AGC3RT_BITS_LEN)-1) << AW87XXX_PID_76_AGC3RT_START_BIT))
+
+#define AW87XXX_PID_76_AGC3RT_5P12MSDB	(0)
+#define AW87XXX_PID_76_AGC3RT_5P12MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_5P12MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_10P24MSDB	(1)
+#define AW87XXX_PID_76_AGC3RT_10P24MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_10P24MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_20P48MSDB	(2)
+#define AW87XXX_PID_76_AGC3RT_20P48MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_20P48MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_41MSDB	(3)
+#define AW87XXX_PID_76_AGC3RT_41MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_41MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_82MSDB	(4)
+#define AW87XXX_PID_76_AGC3RT_82MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_82MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_164MSDB	(5)
+#define AW87XXX_PID_76_AGC3RT_164MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_164MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_328MSDB	(6)
+#define AW87XXX_PID_76_AGC3RT_328MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_328MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_656MSDB	(7)
+#define AW87XXX_PID_76_AGC3RT_656MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3RT_656MSDB << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3RT_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_76_AGC3RT_DEFAULT	\
+	(AW87XXX_PID_76_AGC3RT_DEFAULT_VALUE << AW87XXX_PID_76_AGC3RT_START_BIT)
+
+/* AGC3AT bit 4:2 (AGC3PA 0x07) */
+#define AW87XXX_PID_76_AGC3AT_START_BIT	(2)
+#define AW87XXX_PID_76_AGC3AT_BITS_LEN	(3)
+#define AW87XXX_PID_76_AGC3AT_MASK		\
+	(~(((1<<AW87XXX_PID_76_AGC3AT_BITS_LEN)-1) << AW87XXX_PID_76_AGC3AT_START_BIT))
+
+#define AW87XXX_PID_76_AGC3AT_1P28MSDB	(0)
+#define AW87XXX_PID_76_AGC3AT_1P28MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_1P28MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_2P56MSDB	(1)
+#define AW87XXX_PID_76_AGC3AT_2P56MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_2P56MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_10P24MSDB	(2)
+#define AW87XXX_PID_76_AGC3AT_10P24MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_10P24MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_41MSDB	(3)
+#define AW87XXX_PID_76_AGC3AT_41MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_41MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_82MSDB	(4)
+#define AW87XXX_PID_76_AGC3AT_82MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_82MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_164MSDB	(5)
+#define AW87XXX_PID_76_AGC3AT_164MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_164MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_328MSDB	(6)
+#define AW87XXX_PID_76_AGC3AT_328MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_328MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_656MSDB	(7)
+#define AW87XXX_PID_76_AGC3AT_656MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3AT_656MSDB << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3AT_DEFAULT_VALUE	(3)
+#define AW87XXX_PID_76_AGC3AT_DEFAULT	\
+	(AW87XXX_PID_76_AGC3AT_DEFAULT_VALUE << AW87XXX_PID_76_AGC3AT_START_BIT)
+
+/* AGC3FSAT bit 1:0 (AGC3PA 0x07) */
+#define AW87XXX_PID_76_AGC3FSAT_START_BIT	(0)
+#define AW87XXX_PID_76_AGC3FSAT_BITS_LEN	(2)
+#define AW87XXX_PID_76_AGC3FSAT_MASK	\
+	(~(((1<<AW87XXX_PID_76_AGC3FSAT_BITS_LEN)-1) << AW87XXX_PID_76_AGC3FSAT_START_BIT))
+
+#define AW87XXX_PID_76_AGC3FSAT_10P24MSDB	(0)
+#define AW87XXX_PID_76_AGC3FSAT_10P24MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3FSAT_10P24MSDB << AW87XXX_PID_76_AGC3FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3FSAT_20P48MSDB	(1)
+#define AW87XXX_PID_76_AGC3FSAT_20P48MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3FSAT_20P48MSDB << AW87XXX_PID_76_AGC3FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3FSAT_41MSDB	(2)
+#define AW87XXX_PID_76_AGC3FSAT_41MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3FSAT_41MSDB << AW87XXX_PID_76_AGC3FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3FSAT_82MSDB	(3)
+#define AW87XXX_PID_76_AGC3FSAT_82MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC3FSAT_82MSDB << AW87XXX_PID_76_AGC3FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC3FSAT_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_76_AGC3FSAT_DEFAULT	\
+	(AW87XXX_PID_76_AGC3FSAT_DEFAULT_VALUE << AW87XXX_PID_76_AGC3FSAT_START_BIT)
+
+/* default value of AGC3PA (0x07) */
+/* #define AW87XXX_PID_76_AGC3PA_DEFAULT		(0x4E) */
+
+/* AGC2P (0x08) detail */
+/* AGC2PO bit 3:0 (AGC2P 0x08) */
+#define AW87XXX_PID_76_AGC2PO_START_BIT	(0)
+#define AW87XXX_PID_76_AGC2PO_BITS_LEN	(4)
+#define AW87XXX_PID_76_AGC2PO_MASK		\
+	(~(((1<<AW87XXX_PID_76_AGC2PO_BITS_LEN)-1) << AW87XXX_PID_76_AGC2PO_START_BIT))
+
+#define AW87XXX_PID_76_AGC2PO_0P8W4		(0)
+#define AW87XXX_PID_76_AGC2PO_0P8W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_0P8W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_1P2W4		(1)
+#define AW87XXX_PID_76_AGC2PO_1P2W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_1P2W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_1P6W4		(2)
+#define AW87XXX_PID_76_AGC2PO_1P6W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_1P6W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_2P0W4		(3)
+#define AW87XXX_PID_76_AGC2PO_2P0W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_2P0W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_2P4W4		(4)
+#define AW87XXX_PID_76_AGC2PO_2P4W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_2P4W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_2P8W4		(5)
+#define AW87XXX_PID_76_AGC2PO_2P8W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_2P8W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_3P2W4		(6)
+#define AW87XXX_PID_76_AGC2PO_3P2W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_3P2W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_3P6W4		(7)
+#define AW87XXX_PID_76_AGC2PO_3P6W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_3P6W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_4P0W4		(8)
+#define AW87XXX_PID_76_AGC2PO_4P0W4_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_4P0W4 << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_AGC2_OFF	(9)
+#define AW87XXX_PID_76_AGC2PO_AGC2_OFF_VALUE	\
+	(AW87XXX_PID_76_AGC2PO_AGC2_OFF << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+#define AW87XXX_PID_76_AGC2PO_DEFAULT_VALUE	(6)
+#define AW87XXX_PID_76_AGC2PO_DEFAULT	\
+	(AW87XXX_PID_76_AGC2PO_DEFAULT_VALUE << AW87XXX_PID_76_AGC2PO_START_BIT)
+
+/* default value of AGC2P (0x08) */
+/* #define AW87XXX_PID_76_AGC2P_DEFAULT		(0x06) */
+
+/* AGC2PA (0x09) detail */
+/* AGC2AT bit 4:2 (AGC2PA 0x09) */
+#define AW87XXX_PID_76_AGC2AT_START_BIT	(2)
+#define AW87XXX_PID_76_AGC2AT_BITS_LEN	(3)
+#define AW87XXX_PID_76_AGC2AT_MASK		\
+	(~(((1<<AW87XXX_PID_76_AGC2AT_BITS_LEN)-1) << AW87XXX_PID_76_AGC2AT_START_BIT))
+
+#define AW87XXX_PID_76_AGC2AT_0P16MSDB	(0)
+#define AW87XXX_PID_76_AGC2AT_0P16MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_0P16MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_0P32MSDB	(1)
+#define AW87XXX_PID_76_AGC2AT_0P32MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_0P32MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_0P64MSDB	(2)
+#define AW87XXX_PID_76_AGC2AT_0P64MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_0P64MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_2P56MSDB	(3)
+#define AW87XXX_PID_76_AGC2AT_2P56MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_2P56MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_10P24MSDB	(4)
+#define AW87XXX_PID_76_AGC2AT_10P24MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_10P24MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_41MSDB	(5)
+#define AW87XXX_PID_76_AGC2AT_41MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_41MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_82MSDB	(6)
+#define AW87XXX_PID_76_AGC2AT_82MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_82MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_164MSDB	(7)
+#define AW87XXX_PID_76_AGC2AT_164MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2AT_164MSDB << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2AT_DEFAULT_VALUE	(2)
+#define AW87XXX_PID_76_AGC2AT_DEFAULT	\
+	(AW87XXX_PID_76_AGC2AT_DEFAULT_VALUE << AW87XXX_PID_76_AGC2AT_START_BIT)
+
+/* AGC2FSAT bit 1:0 (AGC2PA 0x09) */
+#define AW87XXX_PID_76_AGC2FSAT_START_BIT	(0)
+#define AW87XXX_PID_76_AGC2FSAT_BITS_LEN	(2)
+#define AW87XXX_PID_76_AGC2FSAT_MASK	\
+	(~(((1<<AW87XXX_PID_76_AGC2FSAT_BITS_LEN)-1) << AW87XXX_PID_76_AGC2FSAT_START_BIT))
+
+#define AW87XXX_PID_76_AGC2FSAT_0P16MSDB	(0)
+#define AW87XXX_PID_76_AGC2FSAT_0P16MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2FSAT_0P16MSDB << AW87XXX_PID_76_AGC2FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2FSAT_0P64MSDB	(1)
+#define AW87XXX_PID_76_AGC2FSAT_0P64MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2FSAT_0P64MSDB << AW87XXX_PID_76_AGC2FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2FSAT_2P56MSDB	(2)
+#define AW87XXX_PID_76_AGC2FSAT_2P56MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2FSAT_2P56MSDB << AW87XXX_PID_76_AGC2FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2FSAT_10P24MSDB	(3)
+#define AW87XXX_PID_76_AGC2FSAT_10P24MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC2FSAT_10P24MSDB << AW87XXX_PID_76_AGC2FSAT_START_BIT)
+
+#define AW87XXX_PID_76_AGC2FSAT_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_AGC2FSAT_DEFAULT	\
+	(AW87XXX_PID_76_AGC2FSAT_DEFAULT_VALUE << AW87XXX_PID_76_AGC2FSAT_START_BIT)
+
+/* default value of AGC2PA (0x09) */
+/* #define AW87XXX_PID_76_AGC2PA_DEFAULT		(0x08) */
+
+/* AGC1PA (0x0A) detail */
+/* AGC1THVTH bit 6:3 (AGC1PA 0x0A) */
+#define AW87XXX_PID_76_AGC1THVTH_START_BIT	(3)
+#define AW87XXX_PID_76_AGC1THVTH_BITS_LEN	(4)
+#define AW87XXX_PID_76_AGC1THVTH_MASK	\
+	(~(((1<<AW87XXX_PID_76_AGC1THVTH_BITS_LEN)-1) << AW87XXX_PID_76_AGC1THVTH_START_BIT))
+
+#define AW87XXX_PID_76_AGC1THVTH_5V		(0)
+#define AW87XXX_PID_76_AGC1THVTH_5V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_5V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_5P2V	(1)
+#define AW87XXX_PID_76_AGC1THVTH_5P2V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_5P2V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_5P4V	(2)
+#define AW87XXX_PID_76_AGC1THVTH_5P4V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_5P4V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_5P6V	(3)
+#define AW87XXX_PID_76_AGC1THVTH_5P6V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_5P6V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_5P8V	(4)
+#define AW87XXX_PID_76_AGC1THVTH_5P8V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_5P8V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_6P0V	(5)
+#define AW87XXX_PID_76_AGC1THVTH_6P0V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_6P0V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_6P2V	(6)
+#define AW87XXX_PID_76_AGC1THVTH_6P2V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_6P2V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_6P4V	(7)
+#define AW87XXX_PID_76_AGC1THVTH_6P4V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_6P4V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_6P6V	(8)
+#define AW87XXX_PID_76_AGC1THVTH_6P6V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_6P6V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_6P8V	(9)
+#define AW87XXX_PID_76_AGC1THVTH_6P8V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_6P8V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_7V		(10)
+#define AW87XXX_PID_76_AGC1THVTH_7V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_7V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_7P2V	(11)
+#define AW87XXX_PID_76_AGC1THVTH_7P2V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_7P2V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_7P4V	(12)
+#define AW87XXX_PID_76_AGC1THVTH_7P4V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_7P4V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_7P6V	(13)
+#define AW87XXX_PID_76_AGC1THVTH_7P6V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_7P6V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_7P8V	(14)
+#define AW87XXX_PID_76_AGC1THVTH_7P8V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_7P8V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_8V		(15)
+#define AW87XXX_PID_76_AGC1THVTH_8V_VALUE	\
+	(AW87XXX_PID_76_AGC1THVTH_8V << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+#define AW87XXX_PID_76_AGC1THVTH_DEFAULT_VALUE	(9)
+#define AW87XXX_PID_76_AGC1THVTH_DEFAULT	\
+	(AW87XXX_PID_76_AGC1THVTH_DEFAULT_VALUE << AW87XXX_PID_76_AGC1THVTH_START_BIT)
+
+/* AGC1AT bit 2:1 (AGC1PA 0x0A) */
+#define AW87XXX_PID_76_AGC1AT_START_BIT	(1)
+#define AW87XXX_PID_76_AGC1AT_BITS_LEN	(2)
+#define AW87XXX_PID_76_AGC1AT_MASK		\
+	(~(((1<<AW87XXX_PID_76_AGC1AT_BITS_LEN)-1) << AW87XXX_PID_76_AGC1AT_START_BIT))
+
+#define AW87XXX_PID_76_AGC1AT_0P04MSDB	(0)
+#define AW87XXX_PID_76_AGC1AT_0P04MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC1AT_0P04MSDB << AW87XXX_PID_76_AGC1AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC1AT_0P08MSDB	(1)
+#define AW87XXX_PID_76_AGC1AT_0P08MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC1AT_0P08MSDB << AW87XXX_PID_76_AGC1AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC1AT_0P16MSDB	(2)
+#define AW87XXX_PID_76_AGC1AT_0P16MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC1AT_0P16MSDB << AW87XXX_PID_76_AGC1AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC1AT_0P32MSDB	(3)
+#define AW87XXX_PID_76_AGC1AT_0P32MSDB_VALUE	\
+	(AW87XXX_PID_76_AGC1AT_0P32MSDB << AW87XXX_PID_76_AGC1AT_START_BIT)
+
+#define AW87XXX_PID_76_AGC1AT_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_AGC1AT_DEFAULT	\
+	(AW87XXX_PID_76_AGC1AT_DEFAULT_VALUE << AW87XXX_PID_76_AGC1AT_START_BIT)
+
+/* PD_AGC1 bit 0 (AGC1PA 0x0A) */
+#define AW87XXX_PID_76_PD_AGC1_START_BIT	(0)
+#define AW87XXX_PID_76_PD_AGC1_BITS_LEN	(1)
+#define AW87XXX_PID_76_PD_AGC1_MASK		\
+	(~(((1<<AW87XXX_PID_76_PD_AGC1_BITS_LEN)-1) << AW87XXX_PID_76_PD_AGC1_START_BIT))
+
+#define AW87XXX_PID_76_PD_AGC1_AGC1_FUNCTION_POWERMINUS_UP	(0)
+#define AW87XXX_PID_76_PD_AGC1_AGC1_FUNCTION_POWERMINUS_UP_VALUE	\
+	(AW87XXX_PID_76_PD_AGC1_AGC1_FUNCTION_POWERMINUS_UP << AW87XXX_PID_76_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_76_PD_AGC1_AGC1_FUNCTION_POWERMINUS_DOWN	(1)
+#define AW87XXX_PID_76_PD_AGC1_AGC1_FUNCTION_POWERMINUS_DOWN_VALUE	\
+	(AW87XXX_PID_76_PD_AGC1_AGC1_FUNCTION_POWERMINUS_DOWN << AW87XXX_PID_76_PD_AGC1_START_BIT)
+
+#define AW87XXX_PID_76_PD_AGC1_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_PD_AGC1_DEFAULT	\
+	(AW87XXX_PID_76_PD_AGC1_DEFAULT_VALUE << AW87XXX_PID_76_PD_AGC1_START_BIT)
+
+/* default value of AGC1PA (0x0A) */
+/* #define AW87XXX_PID_76_AGC1PA_DEFAULT		(0x4A) */
+
+/* SYSST (0x59) detail */
+/* UVLOS bit 7 (SYSST 0x59) */
+#define AW87XXX_PID_76_UVLOS_START_BIT	(7)
+#define AW87XXX_PID_76_UVLOS_BITS_LEN	(1)
+#define AW87XXX_PID_76_UVLOS_MASK		\
+	(~(((1<<AW87XXX_PID_76_UVLOS_BITS_LEN)-1) << AW87XXX_PID_76_UVLOS_START_BIT))
+
+#define AW87XXX_PID_76_UVLOS_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_76_UVLOS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_76_UVLOS_NORMAL_OPERATION << AW87XXX_PID_76_UVLOS_START_BIT)
+
+#define AW87XXX_PID_76_UVLOS_VBAT_UNDER_VOLTAGE	(1)
+#define AW87XXX_PID_76_UVLOS_VBAT_UNDER_VOLTAGE_VALUE	\
+	(AW87XXX_PID_76_UVLOS_VBAT_UNDER_VOLTAGE << AW87XXX_PID_76_UVLOS_START_BIT)
+
+#define AW87XXX_PID_76_UVLOS_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_UVLOS_DEFAULT	\
+	(AW87XXX_PID_76_UVLOS_DEFAULT_VALUE << AW87XXX_PID_76_UVLOS_START_BIT)
+
+/* OTNS bit 6 (SYSST 0x59) */
+#define AW87XXX_PID_76_OTNS_START_BIT	(6)
+#define AW87XXX_PID_76_OTNS_BITS_LEN	(1)
+#define AW87XXX_PID_76_OTNS_MASK		\
+	(~(((1<<AW87XXX_PID_76_OTNS_BITS_LEN)-1) << AW87XXX_PID_76_OTNS_START_BIT))
+
+#define AW87XXX_PID_76_OTNS_DETECTED	(0)
+#define AW87XXX_PID_76_OTNS_DETECTED_VALUE	\
+	(AW87XXX_PID_76_OTNS_DETECTED << AW87XXX_PID_76_OTNS_START_BIT)
+
+#define AW87XXX_PID_76_OTNS_NORMAL_OPERATION	(1)
+#define AW87XXX_PID_76_OTNS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_76_OTNS_NORMAL_OPERATION << AW87XXX_PID_76_OTNS_START_BIT)
+
+#define AW87XXX_PID_76_OTNS_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_OTNS_DEFAULT		\
+	(AW87XXX_PID_76_OTNS_DEFAULT_VALUE << AW87XXX_PID_76_OTNS_START_BIT)
+
+/* OC_FLAGS bit 5 (SYSST 0x59) */
+#define AW87XXX_PID_76_OC_FLAGS_START_BIT	(5)
+#define AW87XXX_PID_76_OC_FLAGS_BITS_LEN	(1)
+#define AW87XXX_PID_76_OC_FLAGS_MASK	\
+	(~(((1<<AW87XXX_PID_76_OC_FLAGS_BITS_LEN)-1) << AW87XXX_PID_76_OC_FLAGS_START_BIT))
+
+#define AW87XXX_PID_76_OC_FLAGS_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_76_OC_FLAGS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_76_OC_FLAGS_NORMAL_OPERATION << AW87XXX_PID_76_OC_FLAGS_START_BIT)
+
+#define AW87XXX_PID_76_OC_FLAGS_DETECTED	(1)
+#define AW87XXX_PID_76_OC_FLAGS_DETECTED_VALUE	\
+	(AW87XXX_PID_76_OC_FLAGS_DETECTED << AW87XXX_PID_76_OC_FLAGS_START_BIT)
+
+#define AW87XXX_PID_76_OC_FLAGS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_OC_FLAGS_DEFAULT	\
+	(AW87XXX_PID_76_OC_FLAGS_DEFAULT_VALUE << AW87XXX_PID_76_OC_FLAGS_START_BIT)
+
+/* ADAP_CPS bit 4 (SYSST 0x59) */
+#define AW87XXX_PID_76_ADAP_CPS_START_BIT	(4)
+#define AW87XXX_PID_76_ADAP_CPS_BITS_LEN	(1)
+#define AW87XXX_PID_76_ADAP_CPS_MASK	\
+	(~(((1<<AW87XXX_PID_76_ADAP_CPS_BITS_LEN)-1) << AW87XXX_PID_76_ADAP_CPS_START_BIT))
+
+#define AW87XXX_PID_76_ADAP_CPS_1X_MODE	(0)
+#define AW87XXX_PID_76_ADAP_CPS_1X_MODE_VALUE	\
+	(AW87XXX_PID_76_ADAP_CPS_1X_MODE << AW87XXX_PID_76_ADAP_CPS_START_BIT)
+
+#define AW87XXX_PID_76_ADAP_CPS_2X_MODE	(1)
+#define AW87XXX_PID_76_ADAP_CPS_2X_MODE_VALUE	\
+	(AW87XXX_PID_76_ADAP_CPS_2X_MODE << AW87XXX_PID_76_ADAP_CPS_START_BIT)
+
+#define AW87XXX_PID_76_ADAP_CPS_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_76_ADAP_CPS_DEFAULT	\
+	(AW87XXX_PID_76_ADAP_CPS_DEFAULT_VALUE << AW87XXX_PID_76_ADAP_CPS_START_BIT)
+
+/* STARTOKS bit 3 (SYSST 0x59) */
+#define AW87XXX_PID_76_STARTOKS_START_BIT	(3)
+#define AW87XXX_PID_76_STARTOKS_BITS_LEN	(1)
+#define AW87XXX_PID_76_STARTOKS_MASK	\
+	(~(((1<<AW87XXX_PID_76_STARTOKS_BITS_LEN)-1) << AW87XXX_PID_76_STARTOKS_START_BIT))
+
+#define AW87XXX_PID_76_STARTOKS_CP_START_FAIL_DECTECTED	(0)
+#define AW87XXX_PID_76_STARTOKS_CP_START_FAIL_DECTECTED_VALUE	\
+	(AW87XXX_PID_76_STARTOKS_CP_START_FAIL_DECTECTED << AW87XXX_PID_76_STARTOKS_START_BIT)
+
+#define AW87XXX_PID_76_STARTOKS_NORMAL_OPERATION	(1)
+#define AW87XXX_PID_76_STARTOKS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_76_STARTOKS_NORMAL_OPERATION << AW87XXX_PID_76_STARTOKS_START_BIT)
+
+#define AW87XXX_PID_76_STARTOKS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_STARTOKS_DEFAULT	\
+	(AW87XXX_PID_76_STARTOKS_DEFAULT_VALUE << AW87XXX_PID_76_STARTOKS_START_BIT)
+
+/* OVP1S bit 2 (SYSST 0x59) */
+#define AW87XXX_PID_76_OVP1S_START_BIT	(2)
+#define AW87XXX_PID_76_OVP1S_BITS_LEN	(1)
+#define AW87XXX_PID_76_OVP1S_MASK		\
+	(~(((1<<AW87XXX_PID_76_OVP1S_BITS_LEN)-1) << AW87XXX_PID_76_OVP1S_START_BIT))
+
+#define AW87XXX_PID_76_OVP1S_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_76_OVP1S_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_76_OVP1S_NORMAL_OPERATION << AW87XXX_PID_76_OVP1S_START_BIT)
+
+#define AW87XXX_PID_76_OVP1S_CP_OVP_DETECTED	(1)
+#define AW87XXX_PID_76_OVP1S_CP_OVP_DETECTED_VALUE	\
+	(AW87XXX_PID_76_OVP1S_CP_OVP_DETECTED << AW87XXX_PID_76_OVP1S_START_BIT)
+
+#define AW87XXX_PID_76_OVP1S_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_OVP1S_DEFAULT	\
+	(AW87XXX_PID_76_OVP1S_DEFAULT_VALUE << AW87XXX_PID_76_OVP1S_START_BIT)
+
+/* PORNS bit 1 (SYSST 0x59) */
+#define AW87XXX_PID_76_PORNS_START_BIT	(1)
+#define AW87XXX_PID_76_PORNS_BITS_LEN	(1)
+#define AW87XXX_PID_76_PORNS_MASK		\
+	(~(((1<<AW87XXX_PID_76_PORNS_BITS_LEN)-1) << AW87XXX_PID_76_PORNS_START_BIT))
+
+#define AW87XXX_PID_76_PORNS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_PORNS_DEFAULT	\
+	(AW87XXX_PID_76_PORNS_DEFAULT_VALUE << AW87XXX_PID_76_PORNS_START_BIT)
+
+/* CP_SHORTS bit 0 (SYSST 0x59) */
+#define AW87XXX_PID_76_CP_SHORTS_START_BIT	(0)
+#define AW87XXX_PID_76_CP_SHORTS_BITS_LEN	(1)
+#define AW87XXX_PID_76_CP_SHORTS_MASK	\
+	(~(((1<<AW87XXX_PID_76_CP_SHORTS_BITS_LEN)-1) << AW87XXX_PID_76_CP_SHORTS_START_BIT))
+
+#define AW87XXX_PID_76_CP_SHORTS_NORMAL_OPERATION	(0)
+#define AW87XXX_PID_76_CP_SHORTS_NORMAL_OPERATION_VALUE	\
+	(AW87XXX_PID_76_CP_SHORTS_NORMAL_OPERATION << AW87XXX_PID_76_CP_SHORTS_START_BIT)
+
+#define AW87XXX_PID_76_CP_SHORTS_CHARGE_PUMP_SHORT_DECTECTED	(1)
+#define AW87XXX_PID_76_CP_SHORTS_CHARGE_PUMP_SHORT_DECTECTED_VALUE	\
+	(AW87XXX_PID_76_CP_SHORTS_CHARGE_PUMP_SHORT_DECTECTED << AW87XXX_PID_76_CP_SHORTS_START_BIT)
+
+#define AW87XXX_PID_76_CP_SHORTS_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_CP_SHORTS_DEFAULT	\
+	(AW87XXX_PID_76_CP_SHORTS_DEFAULT_VALUE << AW87XXX_PID_76_CP_SHORTS_START_BIT)
+
+/* default value of SYSST (0x59) */
+/* #define AW87XXX_PID_76_SYSST_DEFAULT		(0xD0) */
+
+/* SYSINT (0x60) detail */
+/* UVLOI bit 7 (SYSINT 0x60) */
+#define AW87XXX_PID_76_UVLOI_START_BIT	(7)
+#define AW87XXX_PID_76_UVLOI_BITS_LEN	(1)
+#define AW87XXX_PID_76_UVLOI_MASK		\
+	(~(((1<<AW87XXX_PID_76_UVLOI_BITS_LEN)-1) << AW87XXX_PID_76_UVLOI_START_BIT))
+
+#define AW87XXX_PID_76_UVLOI_NOT_CHANGE	(0)
+#define AW87XXX_PID_76_UVLOI_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_76_UVLOI_NOT_CHANGE << AW87XXX_PID_76_UVLOI_START_BIT)
+
+#define AW87XXX_PID_76_UVLOI_DETECTED	(1)
+#define AW87XXX_PID_76_UVLOI_DETECTED_VALUE	\
+	(AW87XXX_PID_76_UVLOI_DETECTED << AW87XXX_PID_76_UVLOI_START_BIT)
+
+#define AW87XXX_PID_76_UVLOI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_UVLOI_DEFAULT	\
+	(AW87XXX_PID_76_UVLOI_DEFAULT_VALUE << AW87XXX_PID_76_UVLOI_START_BIT)
+
+/* OTNI bit 6 (SYSINT 0x60) */
+#define AW87XXX_PID_76_OTNI_START_BIT	(6)
+#define AW87XXX_PID_76_OTNI_BITS_LEN	(1)
+#define AW87XXX_PID_76_OTNI_MASK		\
+	(~(((1<<AW87XXX_PID_76_OTNI_BITS_LEN)-1) << AW87XXX_PID_76_OTNI_START_BIT))
+
+#define AW87XXX_PID_76_OTNI_NOT_CHANGE	(0)
+#define AW87XXX_PID_76_OTNI_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_76_OTNI_NOT_CHANGE << AW87XXX_PID_76_OTNI_START_BIT)
+
+#define AW87XXX_PID_76_OTNI_DETECTED	(1)
+#define AW87XXX_PID_76_OTNI_DETECTED_VALUE	\
+	(AW87XXX_PID_76_OTNI_DETECTED << AW87XXX_PID_76_OTNI_START_BIT)
+
+#define AW87XXX_PID_76_OTNI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_OTNI_DEFAULT		\
+	(AW87XXX_PID_76_OTNI_DEFAULT_VALUE << AW87XXX_PID_76_OTNI_START_BIT)
+
+/* OC_FLAGI bit 5 (SYSINT 0x60) */
+#define AW87XXX_PID_76_OC_FLAGI_START_BIT	(5)
+#define AW87XXX_PID_76_OC_FLAGI_BITS_LEN	(1)
+#define AW87XXX_PID_76_OC_FLAGI_MASK	\
+	(~(((1<<AW87XXX_PID_76_OC_FLAGI_BITS_LEN)-1) << AW87XXX_PID_76_OC_FLAGI_START_BIT))
+
+#define AW87XXX_PID_76_OC_FLAGI_NOT_CHANGE	(0)
+#define AW87XXX_PID_76_OC_FLAGI_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_76_OC_FLAGI_NOT_CHANGE << AW87XXX_PID_76_OC_FLAGI_START_BIT)
+
+#define AW87XXX_PID_76_OC_FLAGI_DETECTED	(1)
+#define AW87XXX_PID_76_OC_FLAGI_DETECTED_VALUE	\
+	(AW87XXX_PID_76_OC_FLAGI_DETECTED << AW87XXX_PID_76_OC_FLAGI_START_BIT)
+
+#define AW87XXX_PID_76_OC_FLAGI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_OC_FLAGI_DEFAULT	\
+	(AW87XXX_PID_76_OC_FLAGI_DEFAULT_VALUE << AW87XXX_PID_76_OC_FLAGI_START_BIT)
+
+/* ADAP_CPI bit 4 (SYSINT 0x60) */
+#define AW87XXX_PID_76_ADAP_CPI_START_BIT	(4)
+#define AW87XXX_PID_76_ADAP_CPI_BITS_LEN	(1)
+#define AW87XXX_PID_76_ADAP_CPI_MASK	\
+	(~(((1<<AW87XXX_PID_76_ADAP_CPI_BITS_LEN)-1) << AW87XXX_PID_76_ADAP_CPI_START_BIT))
+
+#define AW87XXX_PID_76_ADAP_CPI_1X_MODE	(0)
+#define AW87XXX_PID_76_ADAP_CPI_1X_MODE_VALUE	\
+	(AW87XXX_PID_76_ADAP_CPI_1X_MODE << AW87XXX_PID_76_ADAP_CPI_START_BIT)
+
+#define AW87XXX_PID_76_ADAP_CPI_2X_MODE	(1)
+#define AW87XXX_PID_76_ADAP_CPI_2X_MODE_VALUE	\
+	(AW87XXX_PID_76_ADAP_CPI_2X_MODE << AW87XXX_PID_76_ADAP_CPI_START_BIT)
+
+#define AW87XXX_PID_76_ADAP_CPI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_ADAP_CPI_DEFAULT	\
+	(AW87XXX_PID_76_ADAP_CPI_DEFAULT_VALUE << AW87XXX_PID_76_ADAP_CPI_START_BIT)
+
+/* STARTOKI bit 3 (SYSINT 0x60) */
+#define AW87XXX_PID_76_STARTOKI_START_BIT	(3)
+#define AW87XXX_PID_76_STARTOKI_BITS_LEN	(1)
+#define AW87XXX_PID_76_STARTOKI_MASK	\
+	(~(((1<<AW87XXX_PID_76_STARTOKI_BITS_LEN)-1) << AW87XXX_PID_76_STARTOKI_START_BIT))
+
+#define AW87XXX_PID_76_STARTOKI_NOT_CHANGE	(0)
+#define AW87XXX_PID_76_STARTOKI_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_76_STARTOKI_NOT_CHANGE << AW87XXX_PID_76_STARTOKI_START_BIT)
+
+#define AW87XXX_PID_76_STARTOKI_DECTECTED	(1)
+#define AW87XXX_PID_76_STARTOKI_DECTECTED_VALUE	\
+	(AW87XXX_PID_76_STARTOKI_DECTECTED << AW87XXX_PID_76_STARTOKI_START_BIT)
+
+#define AW87XXX_PID_76_STARTOKI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_STARTOKI_DEFAULT	\
+	(AW87XXX_PID_76_STARTOKI_DEFAULT_VALUE << AW87XXX_PID_76_STARTOKI_START_BIT)
+
+/* OVP1I bit 2 (SYSINT 0x60) */
+#define AW87XXX_PID_76_OVP1I_START_BIT	(2)
+#define AW87XXX_PID_76_OVP1I_BITS_LEN	(1)
+#define AW87XXX_PID_76_OVP1I_MASK		\
+	(~(((1<<AW87XXX_PID_76_OVP1I_BITS_LEN)-1) << AW87XXX_PID_76_OVP1I_START_BIT))
+
+#define AW87XXX_PID_76_OVP1I_NOT_CHANGE	(0)
+#define AW87XXX_PID_76_OVP1I_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_76_OVP1I_NOT_CHANGE << AW87XXX_PID_76_OVP1I_START_BIT)
+
+#define AW87XXX_PID_76_OVP1I_DETECTED	(1)
+#define AW87XXX_PID_76_OVP1I_DETECTED_VALUE	\
+	(AW87XXX_PID_76_OVP1I_DETECTED << AW87XXX_PID_76_OVP1I_START_BIT)
+
+#define AW87XXX_PID_76_OVP1I_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_OVP1I_DEFAULT	\
+	(AW87XXX_PID_76_OVP1I_DEFAULT_VALUE << AW87XXX_PID_76_OVP1I_START_BIT)
+
+/* PORNI bit 1 (SYSINT 0x60) */
+#define AW87XXX_PID_76_PORNI_START_BIT	(1)
+#define AW87XXX_PID_76_PORNI_BITS_LEN	(1)
+#define AW87XXX_PID_76_PORNI_MASK		\
+	(~(((1<<AW87XXX_PID_76_PORNI_BITS_LEN)-1) << AW87XXX_PID_76_PORNI_START_BIT))
+
+#define AW87XXX_PID_76_PORNI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_PORNI_DEFAULT	\
+	(AW87XXX_PID_76_PORNI_DEFAULT_VALUE << AW87XXX_PID_76_PORNI_START_BIT)
+
+/* CP_SHORTI bit 0 (SYSINT 0x60) */
+#define AW87XXX_PID_76_CP_SHORTI_START_BIT	(0)
+#define AW87XXX_PID_76_CP_SHORTI_BITS_LEN	(1)
+#define AW87XXX_PID_76_CP_SHORTI_MASK	\
+	(~(((1<<AW87XXX_PID_76_CP_SHORTI_BITS_LEN)-1) << AW87XXX_PID_76_CP_SHORTI_START_BIT))
+
+#define AW87XXX_PID_76_CP_SHORTI_NOT_CHANGE	(0)
+#define AW87XXX_PID_76_CP_SHORTI_NOT_CHANGE_VALUE	\
+	(AW87XXX_PID_76_CP_SHORTI_NOT_CHANGE << AW87XXX_PID_76_CP_SHORTI_START_BIT)
+
+#define AW87XXX_PID_76_CP_SHORTI_SHORT_DECTECTED	(1)
+#define AW87XXX_PID_76_CP_SHORTI_SHORT_DECTECTED_VALUE	\
+	(AW87XXX_PID_76_CP_SHORTI_SHORT_DECTECTED << AW87XXX_PID_76_CP_SHORTI_START_BIT)
+
+#define AW87XXX_PID_76_CP_SHORTI_DEFAULT_VALUE	(0)
+#define AW87XXX_PID_76_CP_SHORTI_DEFAULT	\
+	(AW87XXX_PID_76_CP_SHORTI_DEFAULT_VALUE << AW87XXX_PID_76_CP_SHORTI_START_BIT)
+
+/* default value of SYSINT (0x60) */
+/* #define AW87XXX_PID_76_SYSINT_DEFAULT		(0x00) */
+
+/* detail information of registers end */
+
+#endif  /* #ifndef  __AW87XXX_PID_76_REG_H__ */
\ No newline at end of file
diff --git a/sound/soc/codecs/aw87xxx/aw87xxx_pid_9b_reg.h b/sound/soc/codecs/aw87xxx/aw87xxx_pid_9b_reg.h
new file mode 100644
index 000000000000..27baeb96ce7c
--- /dev/null
+++ b/sound/soc/codecs/aw87xxx/aw87xxx_pid_9b_reg.h
@@ -0,0 +1,81 @@
+#ifndef __AW87XXX_PID_9B_REG_H__
+#define __AW87XXX_PID_9B_REG_H__
+
+#define AW87XXX_PID_9B_CHIPID_REG		(0x00)
+#define AW87XXX_PID_9B_SYSCTRL_REG		(0x01)
+#define AW87XXX_PID_9B_BATSAFE_REG		(0x02)
+#define AW87XXX_PID_9B_BOV_REG			(0x03)
+#define AW87XXX_PID_9B_BP_REG			(0x04)
+#define AW87XXX_PID_9B_GAIN_REG			(0x05)
+#define AW87XXX_PID_9B_AGC3_PO_REG		(0x06)
+#define AW87XXX_PID_9B_AGC3_REG			(0x07)
+#define AW87XXX_PID_9B_AGC2_REG			(0x08)
+#define AW87XXX_PID_9B_AGC1_REG			(0x09)
+
+#define AW87XXX_PID_9B_SYSCTRL_DEFAULT		(0x03)
+
+/********************************************
+ * soft control info
+ * If you need to update this file, add this information manually
+ *******************************************/
+unsigned char aw87xxx_pid_9b_softrst_access[2] = {0x00, 0xaa};
+
+/********************************************
+ * Register Access
+ *******************************************/
+#define AW87XXX_PID_9B_REG_MAX			(0x64)
+
+#define REG_NONE_ACCESS		(0)
+#define REG_RD_ACCESS		(1 << 0)
+#define REG_WR_ACCESS		(1 << 1)
+
+const unsigned char aw87xxx_pid_9b_reg_access[AW87XXX_PID_9B_REG_MAX] = {
+	[AW87XXX_PID_9B_CHIPID_REG]	= (REG_RD_ACCESS),
+	[AW87XXX_PID_9B_SYSCTRL_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_BATSAFE_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_BOV_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_BP_REG]		= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_GAIN_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_AGC3_PO_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_AGC3_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_AGC2_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+	[AW87XXX_PID_9B_AGC1_REG]	= (REG_RD_ACCESS | REG_WR_ACCESS),
+};
+
+
+#define AW87XXX_PID_9B_ENCRYPTION_REG		(0x64)
+#define AW87XXX_PID_9B_ENCRYPTION_BOOST_OUTPUT_SET	(0x2C)
+
+/* REG_EN_SW bit 2 (SYSCTRL 0x01) */
+#define AW87XXX_PID_9B_REG_EN_SW_START_BIT	(2)
+#define AW87XXX_PID_9B_REG_EN_SW_BITS_LEN	(1)
+#define AW87XXX_PID_9B_REG_EN_SW_MASK	\
+	(~(((1<<AW87XXX_PID_9B_REG_EN_SW_BITS_LEN)-1) << AW87XXX_PID_9B_REG_EN_SW_START_BIT))
+
+#define AW87XXX_PID_9B_REG_EN_SW_DISABLE	(0)
+#define AW87XXX_PID_9B_REG_EN_SW_DISABLE_VALUE	\
+	(AW87XXX_PID_9B_REG_EN_SW_DISABLE << AW87XXX_PID_9B_REG_EN_SW_START_BIT)
+
+#define AW87XXX_PID_9B_REG_EN_SW_ENABLE		(1)
+#define AW87XXX_PID_9B_REG_EN_SW_ENABLE_VALUE	\
+	(AW87XXX_PID_9B_REG_EN_SW_ENABLE << AW87XXX_PID_9B_REG_EN_SW_START_BIT)
+
+#define AW87XXX_PID_9B_REG_EN_SW_DEFAULT_VALUE	(1)
+#define AW87XXX_PID_9B_REG_EN_SW_DEFAULT	\
+	(AW87XXX_PID_9B_REG_EN_SW_DEFAULT_VALUE << AW87XXX_PID_9B_REG_EN_SW_START_BIT)
+
+/* SPK_MODE bit 0 (SYSCTRL 0x01) */
+#define AW87XXX_PID_9B_SPK_MODE_START_BIT	(0)
+#define AW87XXX_PID_9B_SPK_MODE_BITS_LEN	(1)
+#define AW87XXX_PID_9B_SPK_MODE_MASK	\
+	(~(((1<<AW87XXX_PID_9B_SPK_MODE_BITS_LEN)-1) << AW87XXX_PID_9B_SPK_MODE_START_BIT))
+
+#define AW87XXX_PID_9B_SPK_MODE_DISABLE	(0)
+#define AW87XXX_PID_9B_SPK_MODE_DISABLE_VALUE	\
+	(AW87XXX_PID_9B_SPK_MODE_DISABLE << AW87XXX_PID_9B_SPK_MODE_START_BIT)
+
+#define AW87XXX_PID_9B_SPK_MODE_ENABLE	(1)
+#define AW87XXX_PID_9B_SPK_MODE_ENABLE_VALUE	\
+	(AW87XXX_PID_9B_SPK_MODE_ENABLE << AW87XXX_PID_9B_SPK_MODE_START_BIT)
+
+#endif
-- 
2.47.0


From 3ecb7ac355a56222194cde8a5e2f393babfd9838 Mon Sep 17 00:00:00 2001
From: fewtarius <fewtarius@steamfork.org>
Date: Fri, 26 Jul 2024 22:28:04 +0200
Subject: [PATCH v1.4 063/120] ALSA: hda/realtek: Add Ayaneo Air 1S audio quirk

Co-developed-by: linh1987
---
 sound/pci/hda/patch_realtek.c | 10 ++++++++++
 1 file changed, 10 insertions(+)

diff --git a/sound/pci/hda/patch_realtek.c b/sound/pci/hda/patch_realtek.c
index c95a056ed764..c5582788bd7a 100644
--- a/sound/pci/hda/patch_realtek.c
+++ b/sound/pci/hda/patch_realtek.c
@@ -7529,6 +7529,7 @@ enum {
 	ALC269VB_FIXUP_ASUS_ZENBOOK,
 	ALC269VB_FIXUP_ASUS_ZENBOOK_UX31A,
 	ALC269VB_FIXUP_ASUS_MIC_NO_PRESENCE,
+	ALC269VB_FIXUP_AYANEO_SPKR_PIN_FIX,
 	ALC269_FIXUP_LIMIT_INT_MIC_BOOST_MUTE_LED,
 	ALC269VB_FIXUP_ORDISSIMO_EVE2,
 	ALC283_FIXUP_CHROME_BOOK,
@@ -8279,6 +8280,13 @@ static const struct hda_fixup alc269_fixups[] = {
 		.chained = true,
 		.chain_id = ALC269_FIXUP_HEADSET_MIC
 	},
+	[ALC269VB_FIXUP_AYANEO_SPKR_PIN_FIX] = {
+		.type = HDA_FIXUP_PINS,
+		.v.pins = (const struct hda_pintbl[]) {
+			{ 0x1a, 0x90170110 },
+			{ }
+		},
+	},
 	[ALC269_FIXUP_LIMIT_INT_MIC_BOOST_MUTE_LED] = {
 		.type = HDA_FIXUP_FUNC,
 		.v.func = alc269_fixup_limit_int_mic_boost,
@@ -10967,6 +10975,7 @@ static const struct snd_pci_quirk alc269_fixup_tbl[] = {
 	SND_PCI_QUIRK(0x2782, 0x0214, "VAIO VJFE-CL", ALC269_FIXUP_LIMIT_INT_MIC_BOOST),
 	SND_PCI_QUIRK(0x2782, 0x0232, "CHUWI CoreBook XPro", ALC269VB_FIXUP_CHUWI_COREBOOK_XPRO),
 	SND_PCI_QUIRK(0x2782, 0x1707, "Vaio VJFE-ADL", ALC298_FIXUP_SPK_VOLUME),
+	SND_PCI_QUIRK(0x1f66, 0x0103, "AYANEO AIR 1S", ALC269VB_FIXUP_AYANEO_SPKR_PIN_FIX),
 	SND_PCI_QUIRK(0x8086, 0x2074, "Intel NUC 8", ALC233_FIXUP_INTEL_NUC8_DMIC),
 	SND_PCI_QUIRK(0x8086, 0x2080, "Intel NUC 8 Rugged", ALC256_FIXUP_INTEL_NUC8_RUGGED),
 	SND_PCI_QUIRK(0x8086, 0x2081, "Intel NUC 10", ALC256_FIXUP_INTEL_NUC10),
@@ -11089,6 +11098,7 @@ static const struct hda_model_fixup alc269_fixup_models[] = {
 	{.id = ALC269VB_FIXUP_ASUS_ZENBOOK, .name = "asus-zenbook"},
 	{.id = ALC269VB_FIXUP_ASUS_ZENBOOK_UX31A, .name = "asus-zenbook-ux31a"},
 	{.id = ALC269VB_FIXUP_ORDISSIMO_EVE2, .name = "ordissimo"},
+	{.id = ALC269VB_FIXUP_AYANEO_SPKR_PIN_FIX, .name = "ayaneo-speaker-pin-fix"},
 	{.id = ALC282_FIXUP_ASUS_TX300, .name = "asus-tx300"},
 	{.id = ALC283_FIXUP_INT_MIC, .name = "alc283-int-mic"},
 	{.id = ALC290_FIXUP_MONO_SPEAKERS_HSJACK, .name = "mono-speakers"},
-- 
2.47.0


From bf930295973407cab91ebf761d58376275acefb7 Mon Sep 17 00:00:00 2001
From: CVMagic <546352+CVMagic@users.noreply.github.com>
Date: Fri, 22 Sep 2023 21:53:06 +0200
Subject: [PATCH v1.4 064/120] ALSA: hda/realtek: Use DMI matching for Ayaneo
 Geek, Ayaneo 2, and Ayn MiniPro

Co-developed-by: Bouke Sybren Haarsma <boukehaarsma23@gmail.com>
---
 sound/pci/hda/patch_realtek.c | 49 +++++++++++++++++++++++++++++++++++
 1 file changed, 49 insertions(+)

diff --git a/sound/pci/hda/patch_realtek.c b/sound/pci/hda/patch_realtek.c
index c5582788bd7a..1751e1deeebe 100644
--- a/sound/pci/hda/patch_realtek.c
+++ b/sound/pci/hda/patch_realtek.c
@@ -6723,6 +6723,20 @@ static void alc294_gx502_toggle_output(struct hda_codec *codec,
 		alc_write_coef_idx(codec, 0x10, 0x0a20);
 }
 
+static void alc269_fixup_headphone_volume(struct hda_codec *codec,
+					const struct hda_fixup *fix, int action)
+{
+	/* Pin 0x21: Some devices share 0x14 for headphones and speakers.
+	 * This will fix ensure these devices have volume controls. */
+	if (!is_jack_detectable(codec, 0x21))
+		return;
+
+	if (action == HDA_FIXUP_ACT_PRE_PROBE) {
+		static const hda_nid_t conn1[] = { 0x02 };
+		snd_hda_override_conn_list(codec, 0x14, ARRAY_SIZE(conn1), conn1);
+	}
+}
+
 static void alc294_fixup_gx502_hp(struct hda_codec *codec,
 					const struct hda_fixup *fix, int action)
 {
@@ -7516,6 +7530,8 @@ enum {
 	ALC269_FIXUP_DELL4_MIC_NO_PRESENCE,
 	ALC269_FIXUP_DELL4_MIC_NO_PRESENCE_QUIET,
 	ALC269_FIXUP_HEADSET_MODE,
+	ALC269_FIXUP_DMI_MATCH,
+	ALC269_FIXUP_AYA_HEADSET_VOLUME,
 	ALC269_FIXUP_HEADSET_MODE_NO_HP_MIC,
 	ALC269_FIXUP_ASPIRE_HEADSET_MIC,
 	ALC269_FIXUP_ASUS_X101_FUNC,
@@ -7772,6 +7788,30 @@ enum {
 	ALC245_FIXUP_CLEVO_NOISY_MIC,
 };
 
+/* A special fixup for AYN and AYANEO handhelds as both
+*  have the same PCI SSID as well as the same codec, but
+*  require different quirks, falling back to DMI matching.
+*/
+static void alc269_fixup_match_via_dmi(struct hda_codec *codec,
+                                        const struct hda_fixup *fix, int action)
+{
+	int alc269_fix_id;
+	const char *board_name = dmi_get_system_info(DMI_BOARD_NAME);
+
+	if (dmi_name_in_vendors("AYANEO") || dmi_name_in_vendors("AYADEVICE") || dmi_name_in_vendors("AYA DEVICE")) {
+		if (board_name && (strcmp(board_name, "AYANEO 2") || strcmp(board_name, "AYANEO 2S") || strcmp(board_name, "GEEK") || strcmp(board_name, "GEEK 1S"))) {
+			alc269_fix_id = ALC269_FIXUP_AYA_HEADSET_VOLUME;
+		} else {
+			return;
+		}
+	} else if (dmi_name_in_vendors("ayn") && strcmp(board_name, "Loki MiniPro")) {
+		alc269_fix_id = ALC269VB_FIXUP_AYANEO_SPKR_PIN_FIX;
+	} else {
+		return;
+	}
+	__snd_hda_apply_fixup(codec, alc269_fix_id, action, 0);
+}
+
 /* A special fixup for Lenovo C940 and Yoga Duet 7;
  * both have the very same PCI SSID, and we need to apply different fixups
  * depending on the codec ID
@@ -9060,6 +9100,14 @@ static const struct hda_fixup alc269_fixups[] = {
 		.chained = true,
 		.chain_id = ALC256_FIXUP_ASUS_HEADSET_MODE
 	},
+	[ALC269_FIXUP_DMI_MATCH] = {
+		.type = HDA_FIXUP_FUNC,
+		.v.func = alc269_fixup_match_via_dmi,
+	},
+	[ALC269_FIXUP_AYA_HEADSET_VOLUME] = {
+		.type = HDA_FIXUP_FUNC,
+		.v.func = alc269_fixup_headphone_volume,
+	},
 	[ALC299_FIXUP_PREDATOR_SPK] = {
 		.type = HDA_FIXUP_PINS,
 		.v.pins = (const struct hda_pintbl[]) {
@@ -10975,6 +11023,7 @@ static const struct snd_pci_quirk alc269_fixup_tbl[] = {
 	SND_PCI_QUIRK(0x2782, 0x0214, "VAIO VJFE-CL", ALC269_FIXUP_LIMIT_INT_MIC_BOOST),
 	SND_PCI_QUIRK(0x2782, 0x0232, "CHUWI CoreBook XPro", ALC269VB_FIXUP_CHUWI_COREBOOK_XPRO),
 	SND_PCI_QUIRK(0x2782, 0x1707, "Vaio VJFE-ADL", ALC298_FIXUP_SPK_VOLUME),
+	SND_PCI_QUIRK(0x1f66, 0x0101, "AYANEO 2/GEEK/Ayn MiniPro", ALC269_FIXUP_DMI_MATCH),
 	SND_PCI_QUIRK(0x1f66, 0x0103, "AYANEO AIR 1S", ALC269VB_FIXUP_AYANEO_SPKR_PIN_FIX),
 	SND_PCI_QUIRK(0x8086, 0x2074, "Intel NUC 8", ALC233_FIXUP_INTEL_NUC8_DMIC),
 	SND_PCI_QUIRK(0x8086, 0x2080, "Intel NUC 8 Rugged", ALC256_FIXUP_INTEL_NUC8_RUGGED),
-- 
2.47.0


From b1d96c91190bf02db005f31674983ee823caea6b Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:33:24 +0200
Subject: [PATCH v1.4 065/120] [BEGIN] Steam Deck fixes

-- 
2.47.0


From 1f785479f77584b211da4d82e62cdc179d45d290 Mon Sep 17 00:00:00 2001
From: Andrey Smirnov <andrew.smirnov@gmail.com>
Date: Sat, 19 Feb 2022 16:08:36 -0800
Subject: [PATCH v1.4 066/120] mfd: Add MFD core driver for Steam Deck

Add MFD core driver for Steam Deck. Doesn't really do much so far
besides instantiating a number of MFD cells that implement all the
interesting functionality.

(cherry picked from commit 5f534c2d6ebdefccb9c024eb0f013bc1c0c622d9)
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/mfd/Kconfig     |  11 ++++
 drivers/mfd/Makefile    |   2 +
 drivers/mfd/steamdeck.c | 127 ++++++++++++++++++++++++++++++++++++++++
 3 files changed, 140 insertions(+)
 create mode 100644 drivers/mfd/steamdeck.c

diff --git a/drivers/mfd/Kconfig b/drivers/mfd/Kconfig
index bc8be2e593b6..f450294fbd25 100644
--- a/drivers/mfd/Kconfig
+++ b/drivers/mfd/Kconfig
@@ -2390,5 +2390,16 @@ config MFD_RSMU_SPI
 	  Additional drivers must be enabled in order to use the functionality
 	  of the device.
 
+config MFD_STEAMDECK
+	tristate "Valve Steam Deck"
+	select MFD_CORE
+	depends on ACPI
+	depends on X86_64 || COMPILE_TEST
+	help
+	  This driver registers various MFD cells that expose aspects
+	  of Steam Deck specific ACPI functionality.
+
+	  Say N here, unless you are running on Steam Deck hardware.
+
 endmenu
 endif
diff --git a/drivers/mfd/Makefile b/drivers/mfd/Makefile
index 02b651cd7535..71d992afbadd 100644
--- a/drivers/mfd/Makefile
+++ b/drivers/mfd/Makefile
@@ -288,3 +288,5 @@ obj-$(CONFIG_MFD_ATC260X_I2C)	+= atc260x-i2c.o
 
 obj-$(CONFIG_MFD_RSMU_I2C)	+= rsmu_i2c.o rsmu_core.o
 obj-$(CONFIG_MFD_RSMU_SPI)	+= rsmu_spi.o rsmu_core.o
+
+obj-$(CONFIG_MFD_STEAMDECK)	+= steamdeck.o
diff --git a/drivers/mfd/steamdeck.c b/drivers/mfd/steamdeck.c
new file mode 100644
index 000000000000..0e504b3c2796
--- /dev/null
+++ b/drivers/mfd/steamdeck.c
@@ -0,0 +1,127 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+/*
+ * Steam Deck EC MFD core driver
+ *
+ * Copyright (C) 2021-2022 Valve Corporation
+ *
+ */
+
+#include <linux/acpi.h>
+#include <linux/platform_device.h>
+#include <linux/mfd/core.h>
+
+#define STEAMDECK_STA_OK			\
+	(ACPI_STA_DEVICE_ENABLED |		\
+	 ACPI_STA_DEVICE_PRESENT |		\
+	 ACPI_STA_DEVICE_FUNCTIONING)
+
+struct steamdeck {
+	struct acpi_device *adev;
+	struct device *dev;
+};
+
+#define STEAMDECK_ATTR_RO(_name, _method)				\
+	static ssize_t _name##_show(struct device *dev,			\
+				    struct device_attribute *attr,	\
+				    char *buf)				\
+	{								\
+		struct steamdeck *sd = dev_get_drvdata(dev);		\
+		unsigned long long val;					\
+									\
+		if (ACPI_FAILURE(acpi_evaluate_integer(			\
+					 sd->adev->handle,		\
+					 _method, NULL, &val)))		\
+			return -EIO;					\
+									\
+		return sysfs_emit(buf, "%llu\n", val);			\
+	}								\
+	static DEVICE_ATTR_RO(_name)
+
+STEAMDECK_ATTR_RO(firmware_version, "PDFW");
+STEAMDECK_ATTR_RO(board_id, "BOID");
+
+static struct attribute *steamdeck_attrs[] = {
+	&dev_attr_firmware_version.attr,
+	&dev_attr_board_id.attr,
+	NULL
+};
+
+ATTRIBUTE_GROUPS(steamdeck);
+
+static const struct mfd_cell steamdeck_cells[] = {
+	{ .name = "steamdeck-hwmon"  },
+	{ .name = "steamdeck-leds"   },
+	{ .name = "steamdeck-extcon" },
+};
+
+static void steamdeck_remove_sysfs_groups(void *data)
+{
+	struct steamdeck *sd = data;
+
+	sysfs_remove_groups(&sd->dev->kobj, steamdeck_groups);
+}
+
+static int steamdeck_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	unsigned long long sta;
+	struct steamdeck *sd;
+	acpi_status status;
+	int ret;
+
+	sd = devm_kzalloc(dev, sizeof(*sd), GFP_KERNEL);
+	if (!sd)
+		return -ENOMEM;
+	sd->adev = ACPI_COMPANION(dev);
+	sd->dev = dev;
+	platform_set_drvdata(pdev, sd);
+
+	status = acpi_evaluate_integer(sd->adev->handle, "_STA",
+				       NULL, &sta);
+	if (ACPI_FAILURE(status)) {
+		dev_err(dev, "Status check failed (0x%x)\n", status);
+		return -EINVAL;
+	}
+
+	if ((sta & STEAMDECK_STA_OK) != STEAMDECK_STA_OK) {
+		dev_err(dev, "Device is not ready\n");
+		return -EINVAL;
+	}
+
+	ret = sysfs_create_groups(&dev->kobj, steamdeck_groups);
+	if (ret) {
+		dev_err(dev, "Failed to create sysfs group\n");
+		return ret;
+	}
+
+	ret = devm_add_action_or_reset(dev, steamdeck_remove_sysfs_groups,
+				       sd);
+	if (ret) {
+		dev_err(dev, "Failed to register devres action\n");
+		return ret;
+	}
+
+	return devm_mfd_add_devices(dev, PLATFORM_DEVID_NONE,
+				    steamdeck_cells, ARRAY_SIZE(steamdeck_cells),
+				    NULL, 0, NULL);
+}
+
+static const struct acpi_device_id steamdeck_device_ids[] = {
+	{ "VLV0100", 0 },
+	{ "", 0 },
+};
+MODULE_DEVICE_TABLE(acpi, steamdeck_device_ids);
+
+static struct platform_driver steamdeck_driver = {
+	.probe = steamdeck_probe,
+	.driver = {
+		.name = "steamdeck",
+		.acpi_match_table = steamdeck_device_ids,
+	},
+};
+module_platform_driver(steamdeck_driver);
+
+MODULE_AUTHOR("Andrey Smirnov <andrew.smirnov@gmail.com>");
+MODULE_DESCRIPTION("Steam Deck EC MFD core driver");
+MODULE_LICENSE("GPL");
-- 
2.47.0


From 46f5a4ef4a4f136c61753b4428fd349fdeabb82b Mon Sep 17 00:00:00 2001
From: Andrey Smirnov <andrew.smirnov@gmail.com>
Date: Sat, 19 Feb 2022 16:09:45 -0800
Subject: [PATCH v1.4 067/120] hwmon: Add driver for Steam Deck's EC sensors

Add driver for sensors exposed by EC firmware on Steam Deck hardware.

(cherry picked from commit 6917aac77bee6185ae3920b936cdbe7876118c0b)
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/hwmon/Kconfig           |  11 ++
 drivers/hwmon/Makefile          |   1 +
 drivers/hwmon/steamdeck-hwmon.c | 224 ++++++++++++++++++++++++++++++++
 3 files changed, 236 insertions(+)
 create mode 100644 drivers/hwmon/steamdeck-hwmon.c

diff --git a/drivers/hwmon/Kconfig b/drivers/hwmon/Kconfig
index b8d65292db83..f8dd3390c91b 100644
--- a/drivers/hwmon/Kconfig
+++ b/drivers/hwmon/Kconfig
@@ -2050,6 +2050,17 @@ config SENSORS_SCH5636
 	  This driver can also be built as a module. If so, the module
 	  will be called sch5636.
 
+config SENSORS_STEAMDECK
+	tristate "Steam Deck EC sensors"
+	depends on MFD_STEAMDECK
+	help
+	  If you say yes here you get support for the hardware
+	  monitoring features exposed by EC firmware on Steam Deck
+	  devices
+
+	  This driver can also be built as a module. If so, the module
+	  will be called steamdeck-hwmon.
+
 config SENSORS_STTS751
 	tristate "ST Microelectronics STTS751"
 	depends on I2C
diff --git a/drivers/hwmon/Makefile b/drivers/hwmon/Makefile
index 3ce8d6a9202e..0ab2876917a5 100644
--- a/drivers/hwmon/Makefile
+++ b/drivers/hwmon/Makefile
@@ -207,6 +207,7 @@ obj-$(CONFIG_SENSORS_SMSC47M1)	+= smsc47m1.o
 obj-$(CONFIG_SENSORS_SMSC47M192)+= smsc47m192.o
 obj-$(CONFIG_SENSORS_SPARX5)	+= sparx5-temp.o
 obj-$(CONFIG_SENSORS_SPD5118)	+= spd5118.o
+obj-$(CONFIG_SENSORS_STEAMDECK) += steamdeck-hwmon.o
 obj-$(CONFIG_SENSORS_STTS751)	+= stts751.o
 obj-$(CONFIG_SENSORS_SURFACE_FAN)+= surface_fan.o
 obj-$(CONFIG_SENSORS_SURFACE_TEMP)+= surface_temp.o
diff --git a/drivers/hwmon/steamdeck-hwmon.c b/drivers/hwmon/steamdeck-hwmon.c
new file mode 100644
index 000000000000..fab9e9460bd4
--- /dev/null
+++ b/drivers/hwmon/steamdeck-hwmon.c
@@ -0,0 +1,224 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Steam Deck EC sensors driver
+ *
+ * Copyright (C) 2021-2022 Valve Corporation
+ */
+
+#include <linux/acpi.h>
+#include <linux/hwmon.h>
+#include <linux/platform_device.h>
+
+#define STEAMDECK_HWMON_NAME	"steamdeck-hwmon"
+
+struct steamdeck_hwmon {
+	struct acpi_device *adev;
+};
+
+static long
+steamdeck_hwmon_get(struct steamdeck_hwmon *sd, const char *method)
+{
+	unsigned long long val;
+	if (ACPI_FAILURE(acpi_evaluate_integer(sd->adev->handle,
+					       (char *)method, NULL, &val)))
+		return -EIO;
+
+	return val;
+}
+
+static int
+steamdeck_hwmon_read(struct device *dev, enum hwmon_sensor_types type,
+		     u32 attr, int channel, long *out)
+{
+	struct steamdeck_hwmon *sd = dev_get_drvdata(dev);
+
+	switch (type) {
+	case hwmon_curr:
+		if (attr != hwmon_curr_input)
+			return -EOPNOTSUPP;
+
+		*out = steamdeck_hwmon_get(sd, "PDAM");
+		if (*out < 0)
+			return *out;
+		break;
+	case hwmon_in:
+		if (attr != hwmon_in_input)
+			return -EOPNOTSUPP;
+
+		*out = steamdeck_hwmon_get(sd, "PDVL");
+		if (*out < 0)
+			return *out;
+		break;
+	case hwmon_temp:
+		if (attr != hwmon_temp_input)
+			return -EOPNOTSUPP;
+
+		*out = steamdeck_hwmon_get(sd, "BATT");
+		if (*out < 0)
+			return *out;
+		/*
+		 * Assuming BATT returns deg C we need to mutiply it
+		 * by 1000 to convert to mC
+		 */
+		*out *= 1000;
+		break;
+	case hwmon_fan:
+		switch (attr) {
+		case hwmon_fan_input:
+			*out = steamdeck_hwmon_get(sd, "FANR");
+			if (*out < 0)
+				return *out;
+			break;
+		case hwmon_fan_target:
+			*out = steamdeck_hwmon_get(sd, "FSSR");
+			if (*out < 0)
+				return *out;
+			break;
+		case hwmon_fan_fault:
+			*out = steamdeck_hwmon_get(sd, "FANC");
+			if (*out < 0)
+				return *out;
+			/*
+			 * FANC (Fan check):
+			 * 0: Abnormal
+			 * 1: Normal
+			 */
+			*out = !*out;
+			break;
+		default:
+			return -EOPNOTSUPP;
+		}
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+static int
+steamdeck_hwmon_read_string(struct device *dev, enum hwmon_sensor_types type,
+			    u32 attr, int channel, const char **str)
+{
+	switch (type) {
+		/*
+		 * These two aren't, strictly speaking, measured. EC
+		 * firmware just reports what PD negotiation resulted
+		 * in.
+		 */
+	case hwmon_curr:
+		*str = "PD Contract Current";
+		break;
+	case hwmon_in:
+		*str = "PD Contract Voltage";
+		break;
+	case hwmon_temp:
+		*str = "Battery Temp";
+		break;
+	case hwmon_fan:
+		*str = "System Fan";
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+static int
+steamdeck_hwmon_write(struct device *dev, enum hwmon_sensor_types type,
+		      u32 attr, int channel, long val)
+{
+	struct steamdeck_hwmon *sd = dev_get_drvdata(dev);
+
+	if (type != hwmon_fan ||
+	    attr != hwmon_fan_target)
+		return -EOPNOTSUPP;
+
+	val = clamp_val(val, 0, 7300);
+
+	if (ACPI_FAILURE(acpi_execute_simple_method(sd->adev->handle,
+						    "FANS", val)))
+		return -EIO;
+
+	return 0;
+}
+
+static umode_t
+steamdeck_hwmon_is_visible(const void *data, enum hwmon_sensor_types type,
+			   u32 attr, int channel)
+{
+	if (type == hwmon_fan &&
+	    attr == hwmon_fan_target)
+		return 0644;
+
+	return 0444;
+}
+
+static const struct hwmon_channel_info *steamdeck_hwmon_info[] = {
+	HWMON_CHANNEL_INFO(in,
+			   HWMON_I_INPUT | HWMON_I_LABEL),
+	HWMON_CHANNEL_INFO(curr,
+			   HWMON_C_INPUT | HWMON_C_LABEL),
+	HWMON_CHANNEL_INFO(temp,
+			   HWMON_T_INPUT | HWMON_T_LABEL),
+	HWMON_CHANNEL_INFO(fan,
+			   HWMON_F_INPUT | HWMON_F_LABEL |
+			   HWMON_F_TARGET | HWMON_F_FAULT),
+	NULL
+};
+
+static const struct hwmon_ops steamdeck_hwmon_ops = {
+	.is_visible = steamdeck_hwmon_is_visible,
+	.read = steamdeck_hwmon_read,
+	.read_string = steamdeck_hwmon_read_string,
+	.write = steamdeck_hwmon_write,
+};
+
+static const struct hwmon_chip_info steamdeck_hwmon_chip_info = {
+	.ops = &steamdeck_hwmon_ops,
+	.info = steamdeck_hwmon_info,
+};
+
+static int steamdeck_hwmon_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct steamdeck_hwmon *sd;
+	struct device *hwmon;
+
+	sd = devm_kzalloc(dev, sizeof(*sd), GFP_KERNEL);
+	if (!sd)
+		return -ENOMEM;
+
+	sd->adev = ACPI_COMPANION(dev->parent);
+	hwmon = devm_hwmon_device_register_with_info(dev,
+						     "steamdeck_hwmon",
+						     sd,
+						     &steamdeck_hwmon_chip_info,
+						     NULL);
+	if (IS_ERR(hwmon)) {
+		dev_err(dev, "Failed to register HWMON device");
+		return PTR_ERR(hwmon);
+	}
+
+	return 0;
+}
+
+static const struct platform_device_id steamdeck_hwmon_id_table[] = {
+	{ .name = STEAMDECK_HWMON_NAME },
+	{}
+};
+MODULE_DEVICE_TABLE(platform, steamdeck_hwmon_id_table);
+
+static struct platform_driver steamdeck_hwmon_driver = {
+	.probe = steamdeck_hwmon_probe,
+	.driver = {
+		.name = STEAMDECK_HWMON_NAME,
+	},
+	.id_table = steamdeck_hwmon_id_table,
+};
+module_platform_driver(steamdeck_hwmon_driver);
+
+MODULE_AUTHOR("Andrey Smirnov <andrew.smirnov@gmail.com>");
+MODULE_DESCRIPTION("Steam Deck EC sensors driver");
+MODULE_LICENSE("GPL");
-- 
2.47.0


From 9f3a5214e4e7b5020d50f82ca12826f74c4bb6a0 Mon Sep 17 00:00:00 2001
From: Andrey Smirnov <andrew.smirnov@gmail.com>
Date: Sun, 27 Feb 2022 12:58:05 -0800
Subject: [PATCH v1.4 068/120] leds: steamdeck: Add support for Steam Deck LED

(cherry picked from commit 85a86d19aa7022ff0555023d53aef78323a42d0c)
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/leds/Kconfig          |  7 ++++
 drivers/leds/Makefile         |  1 +
 drivers/leds/leds-steamdeck.c | 74 +++++++++++++++++++++++++++++++++++
 3 files changed, 82 insertions(+)
 create mode 100644 drivers/leds/leds-steamdeck.c

diff --git a/drivers/leds/Kconfig b/drivers/leds/Kconfig
index d8597897aa83..1e23fdabf09f 100644
--- a/drivers/leds/Kconfig
+++ b/drivers/leds/Kconfig
@@ -963,6 +963,13 @@ config LEDS_ACER_A500
 	  This option enables support for the Power Button LED of
 	  Acer Iconia Tab A500.
 
+config LEDS_STEAMDECK
+	tristate "LED support for Steam Deck"
+	depends on LEDS_CLASS && MFD_STEAMDECK
+	help
+	  This option enabled support for the status LED (next to the
+	  power button) on Steam Deck
+
 source "drivers/leds/blink/Kconfig"
 
 comment "Flash and Torch LED drivers"
diff --git a/drivers/leds/Makefile b/drivers/leds/Makefile
index a1d16c0af82d..7d037c477ecb 100644
--- a/drivers/leds/Makefile
+++ b/drivers/leds/Makefile
@@ -82,6 +82,7 @@ obj-$(CONFIG_LEDS_PWM)			+= leds-pwm.o
 obj-$(CONFIG_LEDS_REGULATOR)		+= leds-regulator.o
 obj-$(CONFIG_LEDS_SC27XX_BLTC)		+= leds-sc27xx-bltc.o
 obj-$(CONFIG_LEDS_SUN50I_A100)		+= leds-sun50i-a100.o
+obj-$(CONFIG_LEDS_STEAMDECK)		+= leds-steamdeck.o
 obj-$(CONFIG_LEDS_SUNFIRE)		+= leds-sunfire.o
 obj-$(CONFIG_LEDS_SYSCON)		+= leds-syscon.o
 obj-$(CONFIG_LEDS_TCA6507)		+= leds-tca6507.o
diff --git a/drivers/leds/leds-steamdeck.c b/drivers/leds/leds-steamdeck.c
new file mode 100644
index 000000000000..686500b8de73
--- /dev/null
+++ b/drivers/leds/leds-steamdeck.c
@@ -0,0 +1,74 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+/*
+ * Steam Deck EC MFD LED cell driver
+ *
+ * Copyright (C) 2021-2022 Valve Corporation
+ *
+ */
+
+#include <linux/acpi.h>
+#include <linux/leds.h>
+#include <linux/platform_device.h>
+
+struct steamdeck_led {
+	struct acpi_device *adev;
+	struct led_classdev cdev;
+};
+
+static int steamdeck_leds_brightness_set(struct led_classdev *cdev,
+					 enum led_brightness value)
+{
+	struct steamdeck_led *sd = container_of(cdev, struct steamdeck_led,
+						cdev);
+
+	if (ACPI_FAILURE(acpi_execute_simple_method(sd->adev->handle,
+						    "CHBV", value)))
+		return -EIO;
+
+	return 0;
+}
+
+static int steamdeck_leds_probe(struct platform_device *pdev)
+{
+  	struct device *dev = &pdev->dev;
+	struct steamdeck_led *sd;
+	int ret;
+
+	sd = devm_kzalloc(dev, sizeof(*sd), GFP_KERNEL);
+	if (!sd)
+		return -ENOMEM;
+
+	sd->adev = ACPI_COMPANION(dev->parent);
+
+	sd->cdev.name = "status:white";
+	sd->cdev.brightness_set_blocking = steamdeck_leds_brightness_set;
+	sd->cdev.max_brightness = 100;
+
+	ret = devm_led_classdev_register(dev, &sd->cdev);
+	if (ret) {
+		dev_err(dev, "Failed to register LEDs device: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct platform_device_id steamdeck_leds_id_table[] = {
+	{ .name = "steamdeck-leds" },
+	{}
+};
+MODULE_DEVICE_TABLE(platform, steamdeck_leds_id_table);
+
+static struct platform_driver steamdeck_leds_driver = {
+	.probe = steamdeck_leds_probe,
+	.driver = {
+		.name = "steamdeck-leds",
+	},
+	.id_table = steamdeck_leds_id_table,
+};
+module_platform_driver(steamdeck_leds_driver);
+
+MODULE_AUTHOR("Andrey Smirnov <andrew.smirnov@gmail.com>");
+MODULE_DESCRIPTION("Steam Deck LEDs driver");
+MODULE_LICENSE("GPL");
-- 
2.47.0


From aadd82a5d08b9aaf38d7efabb806787e159f7b49 Mon Sep 17 00:00:00 2001
From: Andrey Smirnov <andrew.smirnov@gmail.com>
Date: Sun, 27 Feb 2022 14:46:08 -0800
Subject: [PATCH v1.4 069/120] extcon: Add driver for Steam Deck

(cherry picked from commit f9f2eddae582ae39d5f89c1218448fc259b90aa8)
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/extcon/Kconfig            |   8 ++
 drivers/extcon/Makefile           |   1 +
 drivers/extcon/extcon-steamdeck.c | 180 ++++++++++++++++++++++++++++++
 3 files changed, 189 insertions(+)
 create mode 100644 drivers/extcon/extcon-steamdeck.c

diff --git a/drivers/extcon/Kconfig b/drivers/extcon/Kconfig
index 3da94b382292..3a6e104d6ef6 100644
--- a/drivers/extcon/Kconfig
+++ b/drivers/extcon/Kconfig
@@ -203,4 +203,12 @@ config EXTCON_RTK_TYPE_C
 	  The DHC (Digital Home Hub) RTD series SoC contains a type c module.
 	  This driver will detect the status of the type-c port.
 
+config EXTCON_STEAMDECK
+	tristate "Steam Deck extcon support"
+	depends on MFD_STEAMDECK
+	help
+	  Say Y here to enable support of USB Type C cable detection extcon
+	  support on Steam Deck devices
+
+
 endif
diff --git a/drivers/extcon/Makefile b/drivers/extcon/Makefile
index f779adb5e4c7..6e0569b21d2f 100644
--- a/drivers/extcon/Makefile
+++ b/drivers/extcon/Makefile
@@ -26,3 +26,4 @@ obj-$(CONFIG_EXTCON_USB_GPIO)	+= extcon-usb-gpio.o
 obj-$(CONFIG_EXTCON_USBC_CROS_EC) += extcon-usbc-cros-ec.o
 obj-$(CONFIG_EXTCON_USBC_TUSB320) += extcon-usbc-tusb320.o
 obj-$(CONFIG_EXTCON_RTK_TYPE_C) += extcon-rtk-type-c.o
+obj-$(CONFIG_EXTCON_STEAMDECK)  += extcon-steamdeck.o
diff --git a/drivers/extcon/extcon-steamdeck.c b/drivers/extcon/extcon-steamdeck.c
new file mode 100644
index 000000000000..74f190adc8ea
--- /dev/null
+++ b/drivers/extcon/extcon-steamdeck.c
@@ -0,0 +1,180 @@
+
+#include <linux/acpi.h>
+#include <linux/platform_device.h>
+#include <linux/extcon-provider.h>
+
+#define ACPI_STEAMDECK_NOTIFY_STATUS	0x80
+
+/* 0 - port connected, 1 -port disconnected */
+#define ACPI_STEAMDECK_PORT_CONNECT	BIT(0)
+/* 0 - Upstream Facing Port, 1 - Downdstream Facing Port */
+#define ACPI_STEAMDECK_CUR_DATA_ROLE	BIT(3)
+/*
+ * Debouncing delay to allow negotiation process to settle. 2s value
+ * was arrived at via trial and error.
+ */
+#define STEAMDECK_ROLE_SWITCH_DELAY	(msecs_to_jiffies(2000))
+
+struct steamdeck_extcon {
+	struct acpi_device *adev;
+	struct delayed_work role_work;
+	struct extcon_dev *edev;
+	struct device *dev;
+};
+
+static int steamdeck_read_pdcs(struct steamdeck_extcon *sd, unsigned long long *pdcs)
+{
+	acpi_status status;
+
+	status = acpi_evaluate_integer(sd->adev->handle, "PDCS", NULL, pdcs);
+	if (ACPI_FAILURE(status)) {
+		dev_err(sd->dev, "PDCS evaluation failed: %s\n",
+			acpi_format_exception(status));
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static void steamdeck_usb_role_work(struct work_struct *work)
+{
+	struct steamdeck_extcon *sd =
+		container_of(work, struct steamdeck_extcon, role_work.work);
+	unsigned long long pdcs;
+	bool usb_host;
+
+	if (steamdeck_read_pdcs(sd, &pdcs))
+		return;
+
+	/*
+	 * We only care about these two
+	 */
+	pdcs &= ACPI_STEAMDECK_PORT_CONNECT | ACPI_STEAMDECK_CUR_DATA_ROLE;
+
+	/*
+	 * For "connect" events our role is determined by a bit in
+	 * PDCS, for "disconnect" we switch to being a gadget
+	 * unconditionally. The thinking for the latter is we don't
+	 * want to start acting as a USB host until we get
+	 * confirmation from the firmware that we are a USB host
+	 */
+	usb_host = (pdcs & ACPI_STEAMDECK_PORT_CONNECT) ?
+		pdcs & ACPI_STEAMDECK_CUR_DATA_ROLE : false;
+
+	dev_dbg(sd->dev, "USB role is %s\n", usb_host ? "host" : "device");
+	WARN_ON(extcon_set_state_sync(sd->edev, EXTCON_USB_HOST,
+				      usb_host));
+
+}
+
+static void steamdeck_notify(acpi_handle handle, u32 event, void *context)
+{
+	struct device *dev = context;
+	struct steamdeck_extcon *sd = dev_get_drvdata(dev);
+	unsigned long long pdcs;
+	unsigned long delay;
+
+	switch (event) {
+	case ACPI_STEAMDECK_NOTIFY_STATUS:
+		if (steamdeck_read_pdcs(sd, &pdcs))
+			return;
+		/*
+		 * We process "disconnect" events immediately and
+		 * "connect" events with a delay to give the HW time
+		 * to settle. For example attaching USB hub (at least
+		 * for HW used for testing) will generate intermediary
+		 * event with "host" bit not set, followed by the one
+		 * that does have it set.
+		 */
+		delay = (pdcs & ACPI_STEAMDECK_PORT_CONNECT) ?
+			STEAMDECK_ROLE_SWITCH_DELAY : 0;
+
+		queue_delayed_work(system_long_wq, &sd->role_work, delay);
+		break;
+	default:
+		dev_warn(dev, "Unsupported event [0x%x]\n", event);
+	}
+}
+
+static void steamdeck_remove_notify_handler(void *data)
+{
+	struct steamdeck_extcon *sd = data;
+
+	acpi_remove_notify_handler(sd->adev->handle, ACPI_DEVICE_NOTIFY,
+				   steamdeck_notify);
+	cancel_delayed_work_sync(&sd->role_work);
+}
+
+static const unsigned int steamdeck_extcon_cable[] = {
+	EXTCON_USB,
+	EXTCON_USB_HOST,
+	EXTCON_CHG_USB_SDP,
+	EXTCON_CHG_USB_CDP,
+	EXTCON_CHG_USB_DCP,
+	EXTCON_CHG_USB_ACA,
+	EXTCON_NONE,
+};
+
+static int steamdeck_extcon_probe(struct platform_device *pdev)
+{
+  	struct device *dev = &pdev->dev;
+	struct steamdeck_extcon *sd;
+	acpi_status status;
+	int ret;
+
+	sd = devm_kzalloc(dev, sizeof(*sd), GFP_KERNEL);
+	if (!sd)
+		return -ENOMEM;
+
+	INIT_DELAYED_WORK(&sd->role_work, steamdeck_usb_role_work);
+	platform_set_drvdata(pdev, sd);
+	sd->adev = ACPI_COMPANION(dev->parent);
+	sd->dev  = dev;
+	sd->edev = devm_extcon_dev_allocate(dev, steamdeck_extcon_cable);
+	if (IS_ERR(sd->edev))
+		return PTR_ERR(sd->edev);
+
+	ret = devm_extcon_dev_register(dev, sd->edev);
+	if (ret < 0) {
+		dev_err(dev, "Failed to register extcon device: %d\n", ret);
+		return ret;
+	}
+
+	/*
+	 * Set initial role value
+	 */
+	queue_delayed_work(system_long_wq, &sd->role_work, 0);
+	flush_delayed_work(&sd->role_work);
+
+	status = acpi_install_notify_handler(sd->adev->handle,
+					     ACPI_DEVICE_NOTIFY,
+					     steamdeck_notify,
+					     dev);
+	if (ACPI_FAILURE(status)) {
+		dev_err(dev, "Error installing ACPI notify handler\n");
+		return -EIO;
+	}
+
+	ret = devm_add_action_or_reset(dev, steamdeck_remove_notify_handler,
+				       sd);
+	return ret;
+}
+
+static const struct platform_device_id steamdeck_extcon_id_table[] = {
+	{ .name = "steamdeck-extcon" },
+	{}
+};
+MODULE_DEVICE_TABLE(platform, steamdeck_extcon_id_table);
+
+static struct platform_driver steamdeck_extcon_driver = {
+	.probe = steamdeck_extcon_probe,
+	.driver = {
+		.name = "steamdeck-extcon",
+	},
+	.id_table = steamdeck_extcon_id_table,
+};
+module_platform_driver(steamdeck_extcon_driver);
+
+MODULE_AUTHOR("Andrey Smirnov <andrew.smirnov@gmail.com>");
+MODULE_DESCRIPTION("Steam Deck extcon driver");
+MODULE_LICENSE("GPL");
-- 
2.47.0


From f327da40726b964dc59c28d9b37ba82e2185e1db Mon Sep 17 00:00:00 2001
From: Andrey Smirnov <andrew.smirnov@gmail.com>
Date: Sat, 15 Jul 2023 12:58:54 -0700
Subject: [PATCH v1.4 070/120] hwmon: steamdeck-hwmon: Add support for max
 battery level/rate

Add support for max battery level/charge rate attributes.

Signed-off-by: Andrey Smirnov <andrew.smirnov@gmail.com>
(cherry picked from commit 50af83e8fd75dc52221edd3fb6fd7a7f70c4d8a4)
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/hwmon/steamdeck-hwmon.c | 72 ++++++++++++++++++++++++++++++++-
 1 file changed, 71 insertions(+), 1 deletion(-)

diff --git a/drivers/hwmon/steamdeck-hwmon.c b/drivers/hwmon/steamdeck-hwmon.c
index fab9e9460bd4..9d0a5471b181 100644
--- a/drivers/hwmon/steamdeck-hwmon.c
+++ b/drivers/hwmon/steamdeck-hwmon.c
@@ -180,6 +180,76 @@ static const struct hwmon_chip_info steamdeck_hwmon_chip_info = {
 	.info = steamdeck_hwmon_info,
 };
 
+
+static ssize_t
+steamdeck_hwmon_simple_store(struct device *dev, const char *buf, size_t count,
+			     const char *method,
+			     unsigned long upper_limit)
+{
+	struct steamdeck_hwmon *sd = dev_get_drvdata(dev);
+	unsigned long value;
+
+	if (kstrtoul(buf, 10, &value) || value >= upper_limit)
+		return -EINVAL;
+
+	if (ACPI_FAILURE(acpi_execute_simple_method(sd->adev->handle,
+						    (char *)method, value)))
+		return -EIO;
+
+	return count;
+}
+
+static ssize_t
+steamdeck_hwmon_simple_show(struct device *dev, char *buf,
+			    const char *method)
+{
+	struct steamdeck_hwmon *sd = dev_get_drvdata(dev);
+	unsigned long value;
+
+	value = steamdeck_hwmon_get(sd, method);
+	if (value < 0)
+		return value;
+
+	return sprintf(buf, "%ld\n", value);
+}
+
+#define STEAMDECK_HWMON_ATTR_RW(_name, _set_method, _get_method,	\
+				_upper_limit)				\
+	static ssize_t _name##_show(struct device *dev,			\
+				    struct device_attribute *attr,	\
+				    char *buf)				\
+	{								\
+		return steamdeck_hwmon_simple_show(dev, buf,		\
+						   _get_method);	\
+	}								\
+	static ssize_t _name##_store(struct device *dev,		\
+				     struct device_attribute *attr,	\
+				     const char *buf, size_t count)	\
+	{								\
+		return steamdeck_hwmon_simple_store(dev, buf, count,	\
+						    _set_method,	\
+						    _upper_limit);	\
+	}								\
+	static DEVICE_ATTR_RW(_name)
+
+STEAMDECK_HWMON_ATTR_RW(max_battery_charge_level, "FCBL", "SFBL", 101);
+STEAMDECK_HWMON_ATTR_RW(max_battery_charge_rate,  "CHGR", "GCHR", 101);
+
+static struct attribute *steamdeck_hwmon_attributes[] = {
+	&dev_attr_max_battery_charge_level.attr,
+	&dev_attr_max_battery_charge_rate.attr,
+	NULL
+};
+
+static const struct attribute_group steamdeck_hwmon_group = {
+	.attrs = steamdeck_hwmon_attributes,
+};
+
+static const struct attribute_group *steamdeck_hwmon_groups[] = {
+	&steamdeck_hwmon_group,
+	NULL
+};
+
 static int steamdeck_hwmon_probe(struct platform_device *pdev)
 {
 	struct device *dev = &pdev->dev;
@@ -195,7 +265,7 @@ static int steamdeck_hwmon_probe(struct platform_device *pdev)
 						     "steamdeck_hwmon",
 						     sd,
 						     &steamdeck_hwmon_chip_info,
-						     NULL);
+						     steamdeck_hwmon_groups);
 	if (IS_ERR(hwmon)) {
 		dev_err(dev, "Failed to register HWMON device");
 		return PTR_ERR(hwmon);
-- 
2.47.0


From 52cb231a1e802232216237f63cdb6f52bf9643fd Mon Sep 17 00:00:00 2001
From: Andrey Smirnov <andrew.smirnov@gmail.com>
Date: Sun, 24 Sep 2023 15:02:33 -0700
Subject: [PATCH v1.4 071/120] mfd: steamdeck: Expose controller board power in
 sysfs

As of version 118 Deck's BIOS implements "SCBP" method that allows
gating power of the controller board (VBUS). Add a basic WO method to
our root MFD device to allow toggling that.

Signed-off-by: Andrey Smirnov <andrew.smirnov@gmail.com>
(cherry picked from commit f97f32718acc10cbb51fef925842392e80904d74)
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/leds/leds-steamdeck.c | 49 +++++++++++++++++++++++++++++++++++
 drivers/mfd/steamdeck.c       | 20 ++++++++++++++
 2 files changed, 69 insertions(+)

diff --git a/drivers/leds/leds-steamdeck.c b/drivers/leds/leds-steamdeck.c
index 686500b8de73..ada9fffc0a42 100644
--- a/drivers/leds/leds-steamdeck.c
+++ b/drivers/leds/leds-steamdeck.c
@@ -16,6 +16,54 @@ struct steamdeck_led {
 	struct led_classdev cdev;
 };
 
+static ssize_t led_brightness_multiplier_show(struct device *dev,
+					      struct device_attribute *attr,
+					      char *buf)
+{
+	struct led_classdev *cdev = dev_get_drvdata(dev);
+	struct steamdeck_led *sd = container_of(cdev, struct steamdeck_led,
+						cdev);
+	unsigned long long led_brightness_multiplier;
+
+	if (ACPI_FAILURE(acpi_evaluate_integer(sd->adev->handle,
+					       "GLDM",
+					       NULL,
+					       &led_brightness_multiplier)))
+		return -EIO;
+
+
+	return sprintf(buf, "%llu", led_brightness_multiplier);
+}
+
+static ssize_t led_brightness_multiplier_store(struct device *dev,
+					       struct device_attribute *attr,
+					       const char *buf, size_t count)
+{
+	struct led_classdev *cdev = dev_get_drvdata(dev);
+	struct steamdeck_led *sd = container_of(cdev, struct steamdeck_led,
+						cdev);
+	unsigned long value;
+
+	if (kstrtoul(buf, 10, &value) || value > 100)
+		return -EINVAL;
+
+
+	if (ACPI_FAILURE(acpi_execute_simple_method(sd->adev->handle,
+						    "SLDM", value)))
+		return -EIO;
+
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(led_brightness_multiplier);
+
+static struct attribute *steamdeck_led_attrs[] = {
+	&dev_attr_led_brightness_multiplier.attr,
+	NULL
+};
+ATTRIBUTE_GROUPS(steamdeck_led);
+
 static int steamdeck_leds_brightness_set(struct led_classdev *cdev,
 					 enum led_brightness value)
 {
@@ -44,6 +92,7 @@ static int steamdeck_leds_probe(struct platform_device *pdev)
 	sd->cdev.name = "status:white";
 	sd->cdev.brightness_set_blocking = steamdeck_leds_brightness_set;
 	sd->cdev.max_brightness = 100;
+	sd->cdev.groups = steamdeck_led_groups;
 
 	ret = devm_led_classdev_register(dev, &sd->cdev);
 	if (ret) {
diff --git a/drivers/mfd/steamdeck.c b/drivers/mfd/steamdeck.c
index 0e504b3c2796..a60fa7db9141 100644
--- a/drivers/mfd/steamdeck.c
+++ b/drivers/mfd/steamdeck.c
@@ -41,9 +41,29 @@ struct steamdeck {
 STEAMDECK_ATTR_RO(firmware_version, "PDFW");
 STEAMDECK_ATTR_RO(board_id, "BOID");
 
+static ssize_t controller_board_power_store(struct device *dev,
+					    struct device_attribute *attr,
+					    const char *buf, size_t count)
+{
+	struct steamdeck *sd = dev_get_drvdata(dev);
+	bool enabled;
+	ssize_t ret = kstrtobool(buf, &enabled);
+
+	if (ret)
+		return ret;
+
+	if (ACPI_FAILURE(acpi_execute_simple_method(sd->adev->handle,
+						    "SCBP", enabled)))
+		return -EIO;
+
+	return count;
+}
+static DEVICE_ATTR_WO(controller_board_power);
+
 static struct attribute *steamdeck_attrs[] = {
 	&dev_attr_firmware_version.attr,
 	&dev_attr_board_id.attr,
+	&dev_attr_controller_board_power.attr,
 	NULL
 };
 
-- 
2.47.0


From 9fb09bf4659e804c2ffd69816a2feb81c05b87f9 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:38:52 +0200
Subject: [PATCH v1.4 072/120] [BEGIN] Steam Deck OLED fixes

-- 
2.47.0


From 0a38928630ce5cef2f4e02285cb6b0c1ee6f1706 Mon Sep 17 00:00:00 2001
From: GloriousEggroll <gloriouseggroll@gmail.com>
Date: Thu, 4 Apr 2024 12:43:12 -0600
Subject: [PATCH v1.4 073/120] Steam Deck OLED WIFI fixes

---
 drivers/net/wireless/ath/ath11k/hw.c  |  1 +
 drivers/net/wireless/ath/ath11k/hw.h  |  1 +
 drivers/net/wireless/ath/ath11k/mac.c |  3 +-
 drivers/net/wireless/ath/ath11k/pci.c |  7 ++-
 drivers/net/wireless/ath/ath11k/wmi.c | 27 +++++++++
 drivers/net/wireless/ath/ath11k/wmi.h | 87 +++++++++++++++++++++++++++
 6 files changed, 124 insertions(+), 2 deletions(-)

diff --git a/drivers/net/wireless/ath/ath11k/hw.c b/drivers/net/wireless/ath/ath11k/hw.c
index caa6dc12a790..516181319f9c 100644
--- a/drivers/net/wireless/ath/ath11k/hw.c
+++ b/drivers/net/wireless/ath/ath11k/hw.c
@@ -100,6 +100,7 @@ static void ath11k_init_wmi_config_qca6390(struct ath11k_base *ab,
 	config->num_wow_filters = 0x16;
 	config->num_keep_alive_pattern = 0;
 	config->flag1 |= WMI_RSRC_CFG_FLAG1_BSS_CHANNEL_INFO_64;
+	config->host_service_flags |= WMI_RSRC_CFG_HOST_SERVICE_FLAG_NAN_IFACE_SUPPORT;
 }
 
 static void ath11k_hw_ipq8074_reo_setup(struct ath11k_base *ab)
diff --git a/drivers/net/wireless/ath/ath11k/hw.h b/drivers/net/wireless/ath/ath11k/hw.h
index 300322535766..8f82171a73c2 100644
--- a/drivers/net/wireless/ath/ath11k/hw.h
+++ b/drivers/net/wireless/ath/ath11k/hw.h
@@ -206,6 +206,7 @@ struct ath11k_hw_params {
 	bool fw_wmi_diag_event;
 	bool current_cc_support;
 	bool dbr_debug_support;
+	bool coex_isolation;
 	bool global_reset;
 	const struct cfg80211_sar_capa *bios_sar_capa;
 	bool m3_fw_support;
diff --git a/drivers/net/wireless/ath/ath11k/mac.c b/drivers/net/wireless/ath/ath11k/mac.c
index f8068d2e848c..6c343a8914c8 100644
--- a/drivers/net/wireless/ath/ath11k/mac.c
+++ b/drivers/net/wireless/ath/ath11k/mac.c
@@ -2551,7 +2551,7 @@ static void ath11k_peer_assoc_h_he(struct ath11k *ar,
 		    he_mcs_mask[i])
 			max_nss = i + 1;
 	}
-	arg->peer_nss = min(sta->deflink.rx_nss, max_nss);
+	arg->peer_nss = min(arg->peer_nss, (u32)max_nss);
 
 	if (arg->peer_phymode == MODE_11AX_HE160 ||
 	    arg->peer_phymode == MODE_11AX_HE80_80) {
@@ -4803,6 +4803,7 @@ static void ath11k_sta_rc_update_wk(struct work_struct *wk)
 
 	nss = max_t(u32, 1, nss);
 	nss = min(nss, ath11k_mac_max_nss(ht_mcs_mask, vht_mcs_mask, he_mcs_mask));
+	nss = min(nss, (u32)ar->num_tx_chains);
 
 	if (changed & IEEE80211_RC_BW_CHANGED) {
 		/* Get the peer phymode */
diff --git a/drivers/net/wireless/ath/ath11k/pci.c b/drivers/net/wireless/ath/ath11k/pci.c
index be9d2c69cc41..e5a3389941c8 100644
--- a/drivers/net/wireless/ath/ath11k/pci.c
+++ b/drivers/net/wireless/ath/ath11k/pci.c
@@ -111,7 +111,12 @@ static u32 ath11k_pci_window_read32(struct ath11k_base *ab, u32 offset)
 	struct ath11k_pci *ab_pci = ath11k_pci_priv(ab);
 	u32 window_start, val;
 
-	window_start = ath11k_pci_get_window_start(ab, offset);
+	if (ab->hw_params.static_window_map)
+        	window_start = ath11k_pci_get_window_start(ab, offset);
+      	else
+        	window_start = ATH11K_PCI_WINDOW_START;
+
+	//window_start = ath11k_pci_get_window_start(ab, offset);
 
 	if (window_start == ATH11K_PCI_WINDOW_START) {
 		spin_lock_bh(&ab_pci->window_lock);
diff --git a/drivers/net/wireless/ath/ath11k/wmi.c b/drivers/net/wireless/ath/ath11k/wmi.c
index 2662092ee00a..d62248ba40b9 100644
--- a/drivers/net/wireless/ath/ath11k/wmi.c
+++ b/drivers/net/wireless/ath/ath11k/wmi.c
@@ -4177,6 +4177,7 @@ ath11k_wmi_copy_resource_config(struct wmi_resource_config *wmi_cfg,
 	wmi_cfg->sched_params = tg_cfg->sched_params;
 	wmi_cfg->twt_ap_pdev_count = tg_cfg->twt_ap_pdev_count;
 	wmi_cfg->twt_ap_sta_count = tg_cfg->twt_ap_sta_count;
+	wmi_cfg->host_service_flags = tg_cfg->host_service_flags;
 	wmi_cfg->host_service_flags &=
 		~(1 << WMI_CFG_HOST_SERVICE_FLAG_REG_CC_EXT);
 	wmi_cfg->host_service_flags |= (tg_cfg->is_reg_cc_ext_event_supported <<
@@ -9860,3 +9861,29 @@ bool ath11k_wmi_supports_6ghz_cc_ext(struct ath11k *ar)
 	return test_bit(WMI_TLV_SERVICE_REG_CC_EXT_EVENT_SUPPORT,
 			ar->ab->wmi_ab.svc_map) && ar->supports_6ghz;
 }
+
+int ath11k_wmi_send_coex_config(struct ath11k *ar,
+				struct wmi_coex_config_params *param)
+{
+	struct ath11k_pdev_wmi *wmi = ar->wmi;
+	struct wmi_coex_config_cmd *cmd;
+	struct sk_buff *skb;
+
+	skb = ath11k_wmi_alloc_skb(wmi->wmi_ab, sizeof(*cmd));
+	if (!skb)
+		return -ENOMEM;
+
+	cmd = (struct wmi_coex_config_cmd *)skb->data;
+	cmd->tlv_header = FIELD_PREP(WMI_TLV_TAG, WMI_TAG_COEX_CONFIG_CMD) |
+			  FIELD_PREP(WMI_TLV_LEN, sizeof(*cmd) - TLV_HDR_SIZE);
+	cmd->vdev_id = param->vdev_id;
+	cmd->config_type = param->config_type;
+	cmd->config_arg1 = param->config_arg1;
+	cmd->config_arg2 = param->config_arg2;
+	cmd->config_arg3 = param->config_arg3;
+	cmd->config_arg4 = param->config_arg4;
+	cmd->config_arg5 = param->config_arg5;
+	cmd->config_arg6 = param->config_arg6;
+
+	return ath11k_wmi_cmd_send(wmi, skb, WMI_COEX_CONFIG_CMDID);
+}
diff --git a/drivers/net/wireless/ath/ath11k/wmi.h b/drivers/net/wireless/ath/ath11k/wmi.h
index 8982b909c821..5810959a72fb 100644
--- a/drivers/net/wireless/ath/ath11k/wmi.h
+++ b/drivers/net/wireless/ath/ath11k/wmi.h
@@ -2348,6 +2348,7 @@ struct wmi_init_cmd {
 #define WMI_RSRC_CFG_FLAG1_BSS_CHANNEL_INFO_64 BIT(5)
 #define WMI_RSRC_CFG_FLAG2_CALC_NEXT_DTIM_COUNT_SET BIT(9)
 #define WMI_RSRC_CFG_FLAG1_ACK_RSSI BIT(18)
+#define WMI_RSRC_CFG_HOST_SERVICE_FLAG_NAN_IFACE_SUPPORT       BIT(0)
 
 #define WMI_CFG_HOST_SERVICE_FLAG_REG_CC_EXT 4
 
@@ -5729,6 +5730,15 @@ struct target_resource_config {
 	u8 is_reg_cc_ext_event_supported;
 	u32 ema_max_vap_cnt;
 	u32 ema_max_profile_period;
+	u32 max_nlo_ssids;
+	u32 num_packet_filters;
+	u32 num_max_sta_vdevs;
+	u32 max_bssid_indicator;
+	u32 ul_resp_config;
+	u32 msdu_flow_override_config0;
+	u32 msdu_flow_override_config1;
+	u32 flags2;
+	u32 host_service_flags;
 };
 
 enum wmi_debug_log_param {
@@ -6348,6 +6358,82 @@ enum wmi_sta_keepalive_method {
 
 const void **ath11k_wmi_tlv_parse_alloc(struct ath11k_base *ab,
 					struct sk_buff *skb, gfp_t gfp);
+enum wmi_coex_config_type {
+    WMI_COEX_CONFIG_PAGE_P2P_TDM        =  1,
+    WMI_COEX_CONFIG_PAGE_STA_TDM        =  2,
+    WMI_COEX_CONFIG_PAGE_SAP_TDM        =  3,
+    WMI_COEX_CONFIG_DURING_WLAN_CONN    =  4,
+    WMI_COEX_CONFIG_BTC_ENABLE          =  5,
+    WMI_COEX_CONFIG_COEX_DBG            =  6,
+    WMI_COEX_CONFIG_PAGE_P2P_STA_TDM    =  7,
+    WMI_COEX_CONFIG_INQUIRY_P2P_TDM     =  8,
+    WMI_COEX_CONFIG_INQUIRY_STA_TDM     =  9,
+    WMI_COEX_CONFIG_INQUIRY_SAP_TDM     = 10,
+    WMI_COEX_CONFIG_INQUIRY_P2P_STA_TDM = 11,
+    WMI_COEX_CONFIG_TX_POWER            = 12,
+    WMI_COEX_CONFIG_PTA_CONFIG          = 13,
+    WMI_COEX_CONFIG_AP_TDM              = 14,
+    WMI_COEX_CONFIG_WLAN_SCAN_PRIORITY  = 15,
+    WMI_COEX_CONFIG_WLAN_PKT_PRIORITY   = 16,
+    WMI_COEX_CONFIG_PTA_INTERFACE       = 17,
+    WMI_COEX_CONFIG_BTC_DUTYCYCLE       = 18,
+    WMI_COEX_CONFIG_HANDOVER_RSSI       = 19,
+    WMI_COEX_CONFIG_PTA_BT_INFO         = 20,
+    WMI_COEX_CONFIG_SINK_WLAN_TDM       = 21,
+    WMI_COEX_CONFIG_COEX_ENABLE_MCC_TDM = 22,
+    WMI_COEX_CONFIG_LOWRSSI_A2DPOPP_TDM = 23,
+    WMI_COEX_CONFIG_BTC_MODE            = 24,
+    WMI_COEX_CONFIG_ANTENNA_ISOLATION   = 25,
+    WMI_COEX_CONFIG_BT_LOW_RSSI_THRESHOLD = 26,
+    WMI_COEX_CONFIG_BT_INTERFERENCE_LEVEL = 27,
+    WMI_COEX_CONFIG_WLAN_OVER_ZBLOW        = 28,
+    WMI_COEX_CONFIG_WLAN_MGMT_OVER_BT_A2DP = 29,
+    WMI_COEX_CONFIG_WLAN_CONN_OVER_LE      = 30,
+    WMI_COEX_CONFIG_LE_OVER_WLAN_TRAFFIC   = 31,
+    WMI_COEX_CONFIG_THREE_WAY_COEX_RESET   = 32,
+    WMI_COEX_CONFIG_THREE_WAY_DELAY_PARA   = 33,
+    WMI_COEX_CONFIG_THREE_WAY_COEX_START   = 34,
+    WMI_COEX_CONFIG_MPTA_HELPER_ENABLE     = 35,
+    WMI_COEX_CONFIG_MPTA_HELPER_ZIGBEE_STATE = 36,
+    WMI_COEX_CONFIG_MPTA_HELPER_INT_OCS_PARAMS = 37,
+    WMI_COEX_CONFIG_MPTA_HELPER_MON_OCS_PARAMS   = 38,
+    WMI_COEX_CONFIG_MPTA_HELPER_INT_MON_DURATION = 39,
+    WMI_COEX_CONFIG_MPTA_HELPER_ZIGBEE_CHANNEL   = 40,
+    WMI_COEX_CONFIG_MPTA_HELPER_WLAN_MUTE_DURATION   = 41,
+    WMI_COEX_CONFIG_BT_SCO_ALLOW_WLAN_2G_SCAN   = 42,
+    WMI_COEX_CONFIG_ENABLE_2ND_HARMONIC_WAR     = 43,
+    WMI_COEX_CONFIG_BTCOEX_SEPARATE_CHAIN_MODE  = 44,
+    WMI_COEX_CONFIG_ENABLE_TPUT_SHAPING = 45,
+    WMI_COEX_CONFIG_ENABLE_TXBF = 46,
+    WMI_COEX_CONFIG_FORCED_ALGO = 47,
+    WMI_COEX_CONFIG_LE_SCAN_POLICY = 48,
+};
+
+struct wmi_coex_config_params {
+	u32 vdev_id;
+	u32 config_type;
+	u32 config_arg1;
+	u32 config_arg2;
+	u32 config_arg3;
+	u32 config_arg4;
+	u32 config_arg5;
+	u32 config_arg6;
+};
+
+struct wmi_coex_config_cmd {
+	u32 tlv_header;
+	u32 vdev_id;
+	u32 config_type;
+	u32 config_arg1;
+	u32 config_arg2;
+	u32 config_arg3;
+	u32 config_arg4;
+	u32 config_arg5;
+	u32 config_arg6;
+} __packed;
+
+#define WMI_COEX_ISOLATION_ARG1_DEFAUT     30
+
 int ath11k_wmi_cmd_send(struct ath11k_pdev_wmi *wmi, struct sk_buff *skb,
 			u32 cmd_id);
 struct sk_buff *ath11k_wmi_alloc_skb(struct ath11k_wmi_base *wmi_sc, u32 len);
@@ -6511,6 +6597,7 @@ int ath11k_wmi_scan_prob_req_oui(struct ath11k *ar,
 				 const u8 mac_addr[ETH_ALEN]);
 int ath11k_wmi_fw_dbglog_cfg(struct ath11k *ar, u32 *module_id_bitmap,
 			     struct ath11k_fw_dbglog *dbglog);
+int ath11k_wmi_send_coex_config(struct ath11k *ar, struct wmi_coex_config_params *param);
 int ath11k_wmi_wow_config_pno(struct ath11k *ar, u32 vdev_id,
 			      struct wmi_pno_scan_req  *pno_scan);
 int ath11k_wmi_wow_del_pattern(struct ath11k *ar, u32 vdev_id, u32 pattern_id);
-- 
2.47.0


From 6cc822ab12e9f8e8af0705c63c3d2402cde3ac64 Mon Sep 17 00:00:00 2001
From: Ethan Geller <ethang@valvesoftware.com>
Date: Thu, 16 Nov 2023 21:31:48 +0000
Subject: [PATCH v1.4 074/120] Fix for Max98388 issue where speakers would not
 be powered on when we resume from S3.

The issue was that when we flush the regmap to the amp on resume, we were also flushing a value of 1 to the SW_RESET pin.

Theoretically, this is fixed by marking the SW_RESET register volatile. However, we did not observe a fix after marking the register volatile, so we opted for a more complete fix of ensuring SW_RESET is zero in our regmap after we have waited for the reset loop to be complete.
---
 sound/soc/amd/acp/acp-mach-common.c | 104 ++++++++++++++++++++++++++++
 sound/soc/amd/acp/acp-mach.h        |   3 +-
 sound/soc/codecs/max98388.c         |  24 ++++++-
 3 files changed, 128 insertions(+), 3 deletions(-)

diff --git a/sound/soc/amd/acp/acp-mach-common.c b/sound/soc/amd/acp/acp-mach-common.c
index a36300a4ed8a..6f7604a70e1f 100644
--- a/sound/soc/amd/acp/acp-mach-common.c
+++ b/sound/soc/amd/acp/acp-mach-common.c
@@ -26,6 +26,7 @@
 #include "../../codecs/rt5682s.h"
 #include "../../codecs/nau8825.h"
 #include "../../codecs/nau8821.h"
+#include "../../codecs/cs35l41.h"
 #include "acp-mach.h"
 
 #define PCO_PLAT_CLK 48000000
@@ -1282,6 +1283,78 @@ SND_SOC_DAILINK_DEF(nau8821,
 		    DAILINK_COMP_ARRAY(COMP_CODEC("i2c-NVTN2020:00",
 						  NAU8821_CODEC_DAI)));
 
+static int acp_cs35l41_init(struct snd_soc_pcm_runtime *rtd)
+{
+	return 0;
+}
+
+static int acp_cs35l41_startup(struct snd_pcm_substream *substream)
+{
+	struct snd_pcm_runtime *runtime = substream->runtime;
+
+	runtime->hw.channels_max = DUAL_CHANNEL;
+	snd_pcm_hw_constraint_list(runtime, 0, SNDRV_PCM_HW_PARAM_CHANNELS,
+				   &constraints_channels);
+	snd_pcm_hw_constraint_list(runtime, 0, SNDRV_PCM_HW_PARAM_RATE,
+				   &constraints_rates);
+	return 0;
+}
+
+static int acp_cs35l41_hw_params(struct snd_pcm_substream *substream,
+				   struct snd_pcm_hw_params *params)
+{
+	struct snd_soc_pcm_runtime *rtd = snd_soc_substream_to_rtd(substream);
+	struct snd_soc_card *card = rtd->card;
+	struct snd_soc_dai *codec_dai;
+	int ret, i;
+	unsigned int num_codecs = rtd->dai_link->num_codecs;
+	unsigned int bclk_val;
+
+	ret = 0;
+	for (i = 0; i < num_codecs; i++) {
+		codec_dai = snd_soc_rtd_to_codec(rtd, i);
+		if (strcmp(codec_dai->name, "cs35l41-pcm") == 0) {
+			switch (params_rate(params)) {
+			case 48000:
+				bclk_val = 1536000;
+				break;
+			default:
+				dev_err(card->dev, "Invalid Samplerate:0x%x\n",
+					params_rate(params));
+				return -EINVAL;
+			}
+			ret = snd_soc_component_set_sysclk(codec_dai->component,
+							   0, 0, bclk_val, SND_SOC_CLOCK_IN);
+			if (ret < 0) {
+				dev_err(card->dev, "failed to set sysclk for CS35l41 dai\n");
+				return ret;
+			}
+		}
+	}
+
+	return ret;
+}
+
+static struct snd_soc_codec_conf cs35l41_conf[] = {
+	{
+		.dlc = COMP_CODEC_CONF("spi-VLV1776:00"),
+		.name_prefix = "Left",
+	},
+	{
+		.dlc = COMP_CODEC_CONF("spi-VLV1776:01"),
+		.name_prefix = "Right",
+	},
+};
+
+static const struct snd_soc_ops acp_cs35l41_ops = {
+	.startup = acp_cs35l41_startup,
+	.hw_params = acp_cs35l41_hw_params,
+};
+
+SND_SOC_DAILINK_DEF(cs35l41,
+		    DAILINK_COMP_ARRAY(COMP_CODEC("spi-VLV1776:00", "cs35l41-pcm"),
+				       COMP_CODEC("spi-VLV1776:01", "cs35l41-pcm")));
+
 /* Declare DMIC codec components */
 SND_SOC_DAILINK_DEF(dmic_codec,
 		DAILINK_COMP_ARRAY(COMP_CODEC("dmic-codec", "dmic-hifi")));
@@ -1481,6 +1554,7 @@ int acp_sofdsp_dai_links_create(struct snd_soc_card *card)
 		links[i].platforms = sof_component;
 		links[i].num_platforms = ARRAY_SIZE(sof_component);
 		links[i].dpcm_playback = 1;
+		links[i].dpcm_capture = 1;
 		links[i].nonatomic = true;
 		links[i].no_pcm = 1;
 		if (!drv_data->amp_codec_id) {
@@ -1513,6 +1587,7 @@ int acp_sofdsp_dai_links_create(struct snd_soc_card *card)
 		links[i].platforms = sof_component;
 		links[i].num_platforms = ARRAY_SIZE(sof_component);
 		links[i].dpcm_playback = 1;
+		links[i].dpcm_capture = 1;
 		links[i].nonatomic = true;
 		links[i].no_pcm = 1;
 		if (!drv_data->amp_codec_id) {
@@ -1597,6 +1672,8 @@ int acp_legacy_dai_links_create(struct snd_soc_card *card)
 
 	if (drv_data->hs_cpu_id)
 		num_links++;
+	if (drv_data->bt_cpu_id)
+		num_links++;
 	if (drv_data->amp_cpu_id)
 		num_links++;
 	if (drv_data->dmic_cpu_id)
@@ -1744,6 +1821,33 @@ int acp_legacy_dai_links_create(struct snd_soc_card *card)
 			card->codec_conf = rt1019_conf;
 			card->num_configs = ARRAY_SIZE(rt1019_conf);
 		}
+		if (drv_data->amp_codec_id == CS35L41) {
+			links[i].codecs = cs35l41;
+			links[i].num_codecs = ARRAY_SIZE(cs35l41);
+			links[i].init = acp_cs35l41_init;
+			card->codec_conf = cs35l41_conf;
+			card->num_configs = ARRAY_SIZE(cs35l41_conf);
+			links[i].ops = &acp_cs35l41_ops;
+		}
+		i++;
+	}
+
+	if (drv_data->bt_cpu_id == I2S_BT) {
+		links[i].name = "acp-bt-codec";
+		links[i].id = BT_BE_ID;
+		links[i].cpus = sof_bt;
+		links[i].num_cpus = ARRAY_SIZE(sof_bt);
+		links[i].platforms = sof_component;
+		links[i].num_platforms = ARRAY_SIZE(sof_component);
+		links[i].dpcm_playback = 1;
+		links[i].dpcm_capture = 1;
+		links[i].nonatomic = true;
+		links[i].no_pcm = 1;
+		if (!drv_data->bt_codec_id) {
+			/* Use dummy codec if codec id not specified */
+			links[i].codecs = &snd_soc_dummy_dlc;
+			links[i].num_codecs = 1;
+		}
 		i++;
 	}
 
diff --git a/sound/soc/amd/acp/acp-mach.h b/sound/soc/amd/acp/acp-mach.h
index a48546d8d407..1d38025307b5 100644
--- a/sound/soc/amd/acp/acp-mach.h
+++ b/sound/soc/amd/acp/acp-mach.h
@@ -27,8 +27,8 @@
 enum be_id {
 	HEADSET_BE_ID = 0,
 	AMP_BE_ID,
-	DMIC_BE_ID,
 	BT_BE_ID,
+	DMIC_BE_ID,
 };
 
 enum cpu_endpoints {
@@ -46,6 +46,7 @@ enum codec_endpoints {
 	MAX98360A,
 	RT5682S,
 	NAU8825,
+	CS35L41,
 	NAU8821,
 	MAX98388,
 	ES83XX,
diff --git a/sound/soc/codecs/max98388.c b/sound/soc/codecs/max98388.c
index b847d7c59ec0..bb03fb0c62b4 100644
--- a/sound/soc/codecs/max98388.c
+++ b/sound/soc/codecs/max98388.c
@@ -390,27 +390,43 @@ static void max98388_reset(struct max98388_priv *max98388, struct device *dev)
 {
 	int ret, reg, count;
 
+
 	/* Software Reset */
 	ret = regmap_update_bits(max98388->regmap,
 				 MAX98388_R2000_SW_RESET,
 				 MAX98388_SOFT_RESET,
 				 MAX98388_SOFT_RESET);
-	if (ret)
+
+	if (ret) {
 		dev_err(dev, "Reset command failed. (ret:%d)\n", ret);
+		goto exit;
+	}
+
 
 	count = 0;
 	while (count < 3) {
 		usleep_range(10000, 11000);
+
 		/* Software Reset Verification */
 		ret = regmap_read(max98388->regmap,
 				  MAX98388_R22FF_REV_ID, &reg);
+
 		if (!ret) {
 			dev_info(dev, "Reset completed (retry:%d)\n", count);
-			return;
+			goto exit;
 		}
 		count++;
 	}
+
 	dev_err(dev, "Reset failed. (ret:%d)\n", ret);
+
+
+exit:
+	regcache_cache_only(max98388->regmap, true);
+	ret = regmap_update_bits(max98388->regmap,
+				 MAX98388_R2000_SW_RESET,
+				 MAX98388_SOFT_RESET, 0);
+	regcache_cache_only(max98388->regmap, false);
 }
 
 static int max98388_probe(struct snd_soc_component *component)
@@ -419,6 +435,7 @@ static int max98388_probe(struct snd_soc_component *component)
 
 	/* Software Reset */
 	max98388_reset(max98388, component->dev);
+	usleep_range(400, 1000);
 
 	/* General channel source configuration */
 	regmap_write(max98388->regmap,
@@ -811,6 +828,7 @@ static bool max98388_readable_register(struct device *dev,
 	case MAX98388_R210E_AUTO_RESTART:
 	case MAX98388_R210F_GLOBAL_EN:
 	case MAX98388_R22FF_REV_ID:
+	case MAX98388_R2000_SW_RESET:
 		return true;
 	default:
 		return false;
@@ -823,6 +841,7 @@ static bool max98388_volatile_reg(struct device *dev, unsigned int reg)
 	case MAX98388_R2001_INT_RAW1 ... MAX98388_R2005_INT_STATE2:
 	case MAX98388_R210F_GLOBAL_EN:
 	case MAX98388_R22FF_REV_ID:
+	case MAX98388_R2000_SW_RESET:
 		return true;
 	default:
 		return false;
@@ -866,6 +885,7 @@ static int max98388_resume(struct device *dev)
 
 	regcache_cache_only(max98388->regmap, false);
 	max98388_reset(max98388, dev);
+	usleep_range(400, 1000);
 	regcache_sync(max98388->regmap);
 
 	return 0;
-- 
2.47.0


From 6ace1f975d4587895f1e93196cabac927aec151b Mon Sep 17 00:00:00 2001
From: John Schoenick <johns@valvesoftware.com>
Date: Tue, 11 Jul 2023 15:27:33 -0700
Subject: [PATCH v1.4 075/120] drm: panel-orientation-quirks: Add quirk for
 Valve Galileo

---
 drivers/gpu/drm/drm_panel_orientation_quirks.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/drivers/gpu/drm/drm_panel_orientation_quirks.c b/drivers/gpu/drm/drm_panel_orientation_quirks.c
index 1d61e718dfd6..7df8f6b94ff0 100644
--- a/drivers/gpu/drm/drm_panel_orientation_quirks.c
+++ b/drivers/gpu/drm/drm_panel_orientation_quirks.c
@@ -476,6 +476,13 @@ static const struct dmi_system_id orientation_data[] = {
 		  DMI_EXACT_MATCH(DMI_PRODUCT_VERSION, "1"),
 		},
 		.driver_data = (void *)&lcd800x1280_rightside_up,
+	}, {	/* Valve Steam Deck */
+		.matches = {
+		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "Valve"),
+		  DMI_EXACT_MATCH(DMI_PRODUCT_NAME, "Galileo"),
+		  DMI_EXACT_MATCH(DMI_PRODUCT_VERSION, "1"),
+		},
+		.driver_data = (void *)&lcd800x1280_rightside_up,
 	}, {	/* VIOS LTH17 */
 		.matches = {
 		  DMI_EXACT_MATCH(DMI_SYS_VENDOR, "VIOS"),
-- 
2.47.0


From 09ace97e2b62bb7edbbd456c016abbefd47a6cf3 Mon Sep 17 00:00:00 2001
From: Swapnil Patel <swapatel@amd.com>
Date: Thu, 2 Nov 2023 16:16:49 -0400
Subject: [PATCH v1.4 076/120] Disable modes with >1200 MHz Pixel clocks when
 connected via dock

(cherry picked from commit 36301114e8a32e7f13985cbbeff7282d4c599aed)
---
 drivers/gpu/drm/amd/display/dc/link/link_validation.c | 11 +++++++++++
 1 file changed, 11 insertions(+)

diff --git a/drivers/gpu/drm/amd/display/dc/link/link_validation.c b/drivers/gpu/drm/amd/display/dc/link/link_validation.c
index 1aed55b0ab6a..27af7505caca 100644
--- a/drivers/gpu/drm/amd/display/dc/link/link_validation.c
+++ b/drivers/gpu/drm/amd/display/dc/link/link_validation.c
@@ -35,6 +35,8 @@
 
 #define DC_LOGGER_INIT(logger)
 
+static const uint8_t DP_SINK_BRANCH_DEV_NAME_KT50X0[] = "KT50X0!";
+
 static uint32_t get_tmds_output_pixel_clock_100hz(const struct dc_crtc_timing *timing)
 {
 
@@ -276,6 +278,15 @@ static bool dp_validate_mode_timing(
 		timing->v_addressable == (uint32_t) 480)
 		return true;
 
+	if (link->ctx->dce_version == DCN_VERSION_3_01 &&
+	    link->dpcd_caps.sink_dev_id == DP_BRANCH_DEVICE_ID_0060AD &&
+	    memcmp(&link->dpcd_caps.branch_dev_name,
+		   DP_SINK_BRANCH_DEV_NAME_KT50X0,
+		   sizeof(link->dpcd_caps.branch_dev_name)) == 0) {
+		if (timing->pix_clk_100hz / 10 >= (uint32_t) 1200000)
+			return false; /* KT50X0 does not support Pxl clock >= 1200MHz */
+	}
+
 	link_setting = dp_get_verified_link_cap(link);
 
 	/* TODO: DYNAMIC_VALIDATION needs to be implemented */
-- 
2.47.0


From 036ecb6c26d9796008180ed18df9689bbb918d40 Mon Sep 17 00:00:00 2001
From: Jeremy Selan <jeremys@valvesoftware.com>
Date: Wed, 28 Apr 2021 14:33:36 -0700
Subject: [PATCH v1.4 077/120] drm/amd: bump backlight brightness precision
 from 8 -> 16-bits

Signed-off-by: Gabriel Krisman Bertazi <krisman@collabora.com>
[Fwd-ported to DC_VER 3.2.237]
Signed-off-by: Cristian Ciocaltea <cristian.ciocaltea@collabora.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h          |  2 --
 drivers/gpu/drm/amd/amdgpu/atombios_encoders.c    | 10 +++++++---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c |  3 +++
 3 files changed, 10 insertions(+), 5 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h
index d002b845d8ac..78a8aac99f24 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mode.h
@@ -435,8 +435,6 @@ struct amdgpu_mode_info {
 	struct drm_property *regamma_tf_property;
 };
 
-#define AMDGPU_MAX_BL_LEVEL 0xFF
-
 struct amdgpu_backlight_privdata {
 	struct amdgpu_encoder *encoder;
 	uint8_t negative;
diff --git a/drivers/gpu/drm/amd/amdgpu/atombios_encoders.c b/drivers/gpu/drm/amd/amdgpu/atombios_encoders.c
index ebf83fee43bb..f0b759e8683d 100644
--- a/drivers/gpu/drm/amd/amdgpu/atombios_encoders.c
+++ b/drivers/gpu/drm/amd/amdgpu/atombios_encoders.c
@@ -39,6 +39,10 @@
 #include <linux/backlight.h>
 #include "bif/bif_4_1_d.h"
 
+
+/* Maximum backlight level. */
+#define AMDGPU_ATOM_MAX_BL_LEVEL 0xFF
+
 u8
 amdgpu_atombios_encoder_get_backlight_level_from_reg(struct amdgpu_device *adev)
 {
@@ -127,8 +131,8 @@ static u8 amdgpu_atombios_encoder_backlight_level(struct backlight_device *bd)
 	/* Convert brightness to hardware level */
 	if (bd->props.brightness < 0)
 		level = 0;
-	else if (bd->props.brightness > AMDGPU_MAX_BL_LEVEL)
-		level = AMDGPU_MAX_BL_LEVEL;
+	else if (bd->props.brightness > AMDGPU_ATOM_MAX_BL_LEVEL)
+		level = AMDGPU_ATOM_MAX_BL_LEVEL;
 	else
 		level = bd->props.brightness;
 
@@ -198,7 +202,7 @@ void amdgpu_atombios_encoder_init_backlight(struct amdgpu_encoder *amdgpu_encode
 	}
 
 	memset(&props, 0, sizeof(props));
-	props.max_brightness = AMDGPU_MAX_BL_LEVEL;
+	props.max_brightness = AMDGPU_ATOM_MAX_BL_LEVEL;
 	props.type = BACKLIGHT_RAW;
 	snprintf(bl_name, sizeof(bl_name),
 		 "amdgpu_bl%d", dev->primary->index);
diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index 5266424ce15b..b9fefe4e6d75 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -162,6 +162,9 @@ MODULE_FIRMWARE(FIRMWARE_DCN_401_DMUB);
 /* Number of bytes in PSP footer for firmware. */
 #define PSP_FOOTER_BYTES 0x100
 
+/* Maximum backlight level. */
+#define AMDGPU_MAX_BL_LEVEL 0xFFFF
+
 /**
  * DOC: overview
  *
-- 
2.47.0


From 9bd33a111f58c6b91653d099d35bd0d2d24e06bb Mon Sep 17 00:00:00 2001
From: Jeremy Selan <jeremys@valvesoftware.com>
Date: Fri, 12 Nov 2021 10:03:20 -0800
Subject: [PATCH v1.4 078/120] amd/drm: override backlight min value from 12 ->
 0

This overrides backlight handling to the FULL range of the device {0,max}
with no additional interpretation / rescaling. This places the burden
of selecting appropriate device-specific minimum ranges fully on
userspace.

Device defaults provided by ACPI/ATIF are logged, but ignored.

[Merge, add line breaks]
Signed-off-by: Gabriel Krisman Bertazi <krisman@collabora.com>
---
 .../gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c | 25 ++++++++++++++++---
 1 file changed, 22 insertions(+), 3 deletions(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index b9fefe4e6d75..872a17ff89e1 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -4466,7 +4466,7 @@ static int amdgpu_dm_mode_config_init(struct amdgpu_device *adev)
 	return 0;
 }
 
-#define AMDGPU_DM_DEFAULT_MIN_BACKLIGHT 12
+#define AMDGPU_DM_DEFAULT_MIN_BACKLIGHT 0
 #define AMDGPU_DM_DEFAULT_MAX_BACKLIGHT 255
 #define AMDGPU_DM_MIN_SPREAD ((AMDGPU_DM_DEFAULT_MAX_BACKLIGHT - AMDGPU_DM_DEFAULT_MIN_BACKLIGHT) / 2)
 #define AUX_BL_DEFAULT_TRANSITION_TIME_MS 50
@@ -4500,11 +4500,27 @@ static void amdgpu_dm_update_backlight_caps(struct amdgpu_display_manager *dm,
 
 	if (caps.caps_valid) {
 		dm->backlight_caps[bl_idx].caps_valid = true;
+
+		printk(KERN_NOTICE"VLV Successfully queried backlight range over ACPI: %d %d\n",
+		       (int) caps.min_input_signal, (int) caps.max_input_signal);
+
+		if ( caps.min_input_signal != AMDGPU_DM_DEFAULT_MIN_BACKLIGHT ||
+			caps.max_input_signal != AMDGPU_DM_DEFAULT_MAX_BACKLIGHT )
+		{
+			caps.min_input_signal = AMDGPU_DM_DEFAULT_MIN_BACKLIGHT;
+			caps.max_input_signal = AMDGPU_DM_DEFAULT_MAX_BACKLIGHT;
+
+			printk(KERN_NOTICE"VLV OVERRIDE backlight range: %d %d\n",
+			       (int) caps.min_input_signal, (int) caps.max_input_signal);
+		}
+
 		if (caps.aux_support)
 			return;
 		dm->backlight_caps[bl_idx].min_input_signal = caps.min_input_signal;
 		dm->backlight_caps[bl_idx].max_input_signal = caps.max_input_signal;
 	} else {
+		printk(KERN_NOTICE"VLV ACPI does not provide backlight range, using defaults: %d %d\n",
+		       AMDGPU_DM_DEFAULT_MIN_BACKLIGHT, AMDGPU_DM_DEFAULT_MAX_BACKLIGHT);
 		dm->backlight_caps[bl_idx].min_input_signal =
 				AMDGPU_DM_DEFAULT_MIN_BACKLIGHT;
 		dm->backlight_caps[bl_idx].max_input_signal =
@@ -4514,6 +4530,9 @@ static void amdgpu_dm_update_backlight_caps(struct amdgpu_display_manager *dm,
 	if (dm->backlight_caps[bl_idx].aux_support)
 		return;
 
+	printk(KERN_NOTICE"VLV Kernel built without ACPI. using backlight range defaults: %d %d\n",
+	       AMDGPU_DM_DEFAULT_MIN_BACKLIGHT, AMDGPU_DM_DEFAULT_MAX_BACKLIGHT);
+
 	dm->backlight_caps[bl_idx].min_input_signal = AMDGPU_DM_DEFAULT_MIN_BACKLIGHT;
 	dm->backlight_caps[bl_idx].max_input_signal = AMDGPU_DM_DEFAULT_MAX_BACKLIGHT;
 #endif
@@ -4545,7 +4564,7 @@ static u32 convert_brightness_from_user(const struct amdgpu_dm_backlight_caps *c
 	if (!get_brightness_range(caps, &min, &max))
 		return brightness;
 
-	// Rescale 0..255 to min..max
+	// Rescale 0..AMDGPU_MAX_BL_LEVEL to min..max
 	return min + DIV_ROUND_CLOSEST((max - min) * brightness,
 				       AMDGPU_MAX_BL_LEVEL);
 }
@@ -4560,7 +4579,7 @@ static u32 convert_brightness_to_user(const struct amdgpu_dm_backlight_caps *cap
 
 	if (brightness < min)
 		return 0;
-	// Rescale min..max to 0..255
+	// Rescale min..max to 0..AMDGPU_MAX_BL_LEVEL
 	return DIV_ROUND_CLOSEST(AMDGPU_MAX_BL_LEVEL * (brightness - min),
 				 max - min);
 }
-- 
2.47.0


From 7748ba66517b5097b7f9043670682b597d4d1650 Mon Sep 17 00:00:00 2001
From: "Pierre-Loup A. Griffais" <pgriffais@valvesoftware.com>
Date: Wed, 8 Nov 2023 19:45:52 -0800
Subject: [PATCH v1.4 079/120] amdgpu: fix Galileo desktop brightness
 overflowing

500k uNits * 65k max brightness range overflows in the conversion code.
Scale back brightness range to 12bit max.
---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index 872a17ff89e1..ae5516c40d70 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -163,7 +163,7 @@ MODULE_FIRMWARE(FIRMWARE_DCN_401_DMUB);
 #define PSP_FOOTER_BYTES 0x100
 
 /* Maximum backlight level. */
-#define AMDGPU_MAX_BL_LEVEL 0xFFFF
+#define AMDGPU_MAX_BL_LEVEL 0xFFF
 
 /**
  * DOC: overview
-- 
2.47.0


From bf375ad9a8e45ab797769b2341f1990c1c1df7b5 Mon Sep 17 00:00:00 2001
From: Andres Rodriguez <andresr@valvesoftware.com>
Date: Wed, 22 Nov 2023 11:28:35 -0800
Subject: [PATCH v1.4 080/120] Revert "PCI: Prevent xHCI driver from claiming
 AMD VanGogh USB3 DRD device"

This reverts commit a4904c47fcd7fc9152b6b04409feac1130e2033d.

This results in some USB devices to fail to enumerate. Revert pending
further investigation.
---
 drivers/pci/quirks.c | 8 +++-----
 1 file changed, 3 insertions(+), 5 deletions(-)

diff --git a/drivers/pci/quirks.c b/drivers/pci/quirks.c
index 69449d13ff21..ac32ff9b866a 100644
--- a/drivers/pci/quirks.c
+++ b/drivers/pci/quirks.c
@@ -710,7 +710,7 @@ DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_ATI,	PCI_DEVICE_ID_ATI_RS100,   quirk_ati_
 /*
  * In the AMD NL platform, this device ([1022:7912]) has a class code of
  * PCI_CLASS_SERIAL_USB_XHCI (0x0c0330), which means the xhci driver will
- * claim it. The same applies on the VanGogh platform device ([1022:163a]).
+ * claim it.
  *
  * But the dwc3 driver is a more specific driver for this device, and we'd
  * prefer to use it instead of xhci. To prevent xhci from claiming the
@@ -718,7 +718,7 @@ DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_ATI,	PCI_DEVICE_ID_ATI_RS100,   quirk_ati_
  * defines as "USB device (not host controller)". The dwc3 driver can then
  * claim it based on its Vendor and Device ID.
  */
-static void quirk_amd_dwc_class(struct pci_dev *pdev)
+static void quirk_amd_nl_class(struct pci_dev *pdev)
 {
 	u32 class = pdev->class;
 
@@ -731,9 +731,7 @@ static void quirk_amd_dwc_class(struct pci_dev *pdev)
 	}
 }
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_NL_USB,
-		quirk_amd_dwc_class);
-DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_VANGOGH_USB,
-		quirk_amd_dwc_class);
+		quirk_amd_nl_class);
 
 /*
  * Synopsys USB 3.x host HAPS platform has a class code of
-- 
2.47.0


From 69abc5ffd48afb57880bfea880b9fd9740cb88d3 Mon Sep 17 00:00:00 2001
From: Joshua Ashton <joshua@froggi.es>
Date: Wed, 6 Sep 2023 22:00:26 +0100
Subject: [PATCH v1.4 081/120] drm/amd/display: Don't consider vblank passed if
 currently in vertical front porch time

Changing refresh rates on OLED displays works differently to typical
LCD panels in that instead of changing the clock, the vertical porch
is extended significantly for lower rates.

This can mean that the vertical porch can be incredibly large for
non-base refresh rates eg. 60Hz on a 90Hz display.

This isn't an issue for X11/typical compositors as their present slop
is 1/2th of the refresh interval so the issue never manifests.

However in Gamescope, the present slop very small and tuned to be
optimal in real-time to try and reduce display latency significantly.
This results in us queueing up the atomic commit inside the vertical
porch region which, due to legacy X11/sync control reasons, means that
AMDGPU must target the next vblank.

This patch changes that behaviour to make FRR displays match what occurs
on VRR/Freesync displays where the vertical porch time is not included
in determining what vblank to target and solves the issue.

This means that smarter compositors can get large input latency
reductions when using OLED displays at lower than base refresh rates.

For upstreaming this patch, it will need to be considered what the best
solution is to enable this behaviour from the userspace side.
Obviously the X11/legacy stuff probably cannot change here -- so we
either need to enable this new behaviour globally for all DRM atomic
clients (ie. basically Wayland compositors) or have a
new DRM_MODE_ATOMIC flag.

Signed-off-by: Joshua Ashton <joshua@froggi.es>
---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index ae5516c40d70..7617bf23bcd2 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -470,7 +470,7 @@ static void dm_pflip_high_irq(void *interrupt_params)
 
 	WARN_ON(!e);
 
-	vrr_active = amdgpu_dm_crtc_vrr_active_irq(amdgpu_crtc);
+	vrr_active = true;//amdgpu_dm_crtc_vrr_active_irq(amdgpu_crtc);
 
 	/* Fixed refresh rate, or VRR scanout position outside front-porch? */
 	if (!vrr_active ||
@@ -567,11 +567,11 @@ static void dm_vupdate_high_irq(void *interrupt_params)
 		 * page-flip completion events that have been queued to us
 		 * if a pageflip happened inside front-porch.
 		 */
-		if (vrr_active) {
+		if (true) {
 			amdgpu_dm_crtc_handle_vblank(acrtc);
 
 			/* BTR processing for pre-DCE12 ASICs */
-			if (acrtc->dm_irq_params.stream &&
+			if (vrr_active && acrtc->dm_irq_params.stream &&
 			    adev->family < AMDGPU_FAMILY_AI) {
 				spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags);
 				mod_freesync_handle_v_update(
@@ -8871,7 +8871,7 @@ static void amdgpu_dm_commit_planes(struct drm_atomic_state *state,
 	int planes_count = 0, vpos, hpos;
 	unsigned long flags;
 	u32 target_vblank, last_flip_vblank;
-	bool vrr_active = amdgpu_dm_crtc_vrr_active(acrtc_state);
+	bool vrr_active = true;//amdgpu_dm_crtc_vrr_active(acrtc_state);
 	bool cursor_update = false;
 	bool pflip_present = false;
 	bool dirty_rects_changed = false;
-- 
2.47.0


From a26341bf14a747ca94fa62f0102479fd4caed618 Mon Sep 17 00:00:00 2001
From: Joshua Ashton <joshua@froggi.es>
Date: Sun, 3 Dec 2023 11:35:06 +0000
Subject: [PATCH v1.4 082/120] drm/amd/display: Revert some of the vrr always
 on hack

Fixes frame timings on some non)-VRR external displays going all whacky.

This makes us not use the late vblank irq handler (backporch line 0) and instead send the vblank event immediately on page flip when we know where the vblank is going to be.

Should also improve latency/stutter on internal display potentially too.
---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index 7617bf23bcd2..9ca72b3ae912 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -470,7 +470,7 @@ static void dm_pflip_high_irq(void *interrupt_params)
 
 	WARN_ON(!e);
 
-	vrr_active = true;//amdgpu_dm_crtc_vrr_active_irq(amdgpu_crtc);
+	vrr_active = amdgpu_dm_crtc_vrr_active_irq(amdgpu_crtc);
 
 	/* Fixed refresh rate, or VRR scanout position outside front-porch? */
 	if (!vrr_active ||
@@ -567,11 +567,11 @@ static void dm_vupdate_high_irq(void *interrupt_params)
 		 * page-flip completion events that have been queued to us
 		 * if a pageflip happened inside front-porch.
 		 */
-		if (true) {
+		if (vrr_active) {
 			amdgpu_dm_crtc_handle_vblank(acrtc);
 
 			/* BTR processing for pre-DCE12 ASICs */
-			if (vrr_active && acrtc->dm_irq_params.stream &&
+			if (acrtc->dm_irq_params.stream &&
 			    adev->family < AMDGPU_FAMILY_AI) {
 				spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags);
 				mod_freesync_handle_v_update(
-- 
2.47.0


From 5661a54399d14ec7f4b0aff534a5d64162eed6d5 Mon Sep 17 00:00:00 2001
From: Matthew Schwartz <njtransit215@gmail.com>
Date: Thu, 18 Apr 2024 11:36:01 -0700
Subject: [PATCH v1.4 083/120] revert series of drivers/bluetooth/hci commits
 to fix Galileo/OLED Deck + Legion Go failing to suspend on 6.8 kernels

Revert series of commits that causes a bluetooth regression where toggling Bluetooth on the Galileo/OLED Deck and Legion Go can cause the device to fail to suspend properly. Instead, the display turns dims for 5-10s before waking up automatically again.

Logs show:

"Failed to add UUID: Failed (0x03)"
"Failed to set mode: Authentication Failed (0x05)"

hci0: Retry BT power ON:0
hci0: command tx timeout
hci0: Reading QCA version information failed (-110)

`sudo rmmod hci_uart` was a workaround, while this patch removes the need for that until it is sorted out upstream.

Matthew Schwartz (7):
  Revert "Bluetooth: qca: fix device-address endianness"
  Reapply "Bluetooth: hci_qca: Set BDA quirk bit if fwnode exists in DT"
  Revert "Bluetooth: btrtl: fix out of bounds memory access"
  Revert "Bluetooth: hci_h5: Add ability to allocate memory for private
    data"
  Revert "Bluetooth: hci_qca: don't use IS_ERR_OR_NULL() with
    gpiod_get_optional()"
  Revert "Bluetooth: qca: Fix triggering coredump implementation"
  Revert "Bluetooth: hci_qca: Set BDA quirk bit if fwnode exists in DT"

 drivers/bluetooth/btqca.c       |  8 ++------
 drivers/bluetooth/hci_h5.c      |  5 +----
 drivers/bluetooth/hci_qca.c     | 25 ++++++++-----------------
 drivers/bluetooth/hci_serdev.c  |  9 ++++-----
 drivers/bluetooth/hci_uart.h    | 12 +-----------
 5 files changed, 16 insertions(+), 43 deletions(-)

--
2.44.0
-- 
2.47.0


From 1616c1d07fadc341e3b1295057422405e4b3eb1c Mon Sep 17 00:00:00 2001
From: Matthew Schwartz <njtransit215@gmail.com>
Date: Thu, 18 Apr 2024 11:30:27 -0700
Subject: [PATCH v1.4 084/120] Revert "Bluetooth: qca: fix device-address
 endianness"

This reverts commit a5425a30739fb1eab3c39ac8a8a9e6795c681fe8.
---
 drivers/bluetooth/btqca.c   | 8 ++------
 drivers/bluetooth/hci_qca.c | 9 ---------
 2 files changed, 2 insertions(+), 15 deletions(-)

diff --git a/drivers/bluetooth/btqca.c b/drivers/bluetooth/btqca.c
index dfbbac92242a..f3a5e5f82b90 100644
--- a/drivers/bluetooth/btqca.c
+++ b/drivers/bluetooth/btqca.c
@@ -930,15 +930,11 @@ EXPORT_SYMBOL_GPL(qca_uart_setup);
 
 int qca_set_bdaddr(struct hci_dev *hdev, const bdaddr_t *bdaddr)
 {
-	bdaddr_t bdaddr_swapped;
 	struct sk_buff *skb;
 	int err;
 
-	baswap(&bdaddr_swapped, bdaddr);
-
-	skb = __hci_cmd_sync_ev(hdev, EDL_WRITE_BD_ADDR_OPCODE, 6,
-				&bdaddr_swapped, HCI_EV_VENDOR,
-				HCI_INIT_TIMEOUT);
+	skb = __hci_cmd_sync_ev(hdev, EDL_WRITE_BD_ADDR_OPCODE, 6, bdaddr,
+				HCI_EV_VENDOR, HCI_INIT_TIMEOUT);
 	if (IS_ERR(skb)) {
 		err = PTR_ERR(skb);
 		bt_dev_err(hdev, "QCA Change address cmd failed (%d)", err);
diff --git a/drivers/bluetooth/hci_qca.c b/drivers/bluetooth/hci_qca.c
index 678f150229e7..e47b07ae1a63 100644
--- a/drivers/bluetooth/hci_qca.c
+++ b/drivers/bluetooth/hci_qca.c
@@ -227,7 +227,6 @@ struct qca_serdev {
 	struct qca_power *bt_power;
 	u32 init_speed;
 	u32 oper_speed;
-	bool bdaddr_property_broken;
 	const char *firmware_name;
 };
 
@@ -1857,7 +1856,6 @@ static int qca_setup(struct hci_uart *hu)
 	const char *firmware_name = qca_get_firmware_name(hu);
 	int ret;
 	struct qca_btsoc_version ver;
-	struct qca_serdev *qcadev;
 	const char *soc_name;
 
 	ret = qca_check_speeds(hu);
@@ -1919,10 +1917,6 @@ static int qca_setup(struct hci_uart *hu)
 	case QCA_WCN6750:
 	case QCA_WCN6855:
 	case QCA_WCN7850:
-		qcadev = serdev_device_get_drvdata(hu->serdev);
-		if (qcadev->bdaddr_property_broken)
-			set_bit(HCI_QUIRK_BDADDR_PROPERTY_BROKEN, &hdev->quirks);
-
 		hci_set_aosp_capable(hdev);
 
 		ret = qca_read_soc_version(hdev, &ver, soc_type);
@@ -2323,9 +2317,6 @@ static int qca_serdev_probe(struct serdev_device *serdev)
 	if (!qcadev->oper_speed)
 		BT_DBG("UART will pick default operating speed");
 
-	qcadev->bdaddr_property_broken = device_property_read_bool(&serdev->dev,
-			"qcom,local-bd-address-broken");
-
 	if (data)
 		qcadev->btsoc_type = data->soc_type;
 	else
-- 
2.47.0


From be4bf38bdc27b17d33b4384a107d581a4518b901 Mon Sep 17 00:00:00 2001
From: Matthew Schwartz <njtransit215@gmail.com>
Date: Thu, 18 Apr 2024 11:30:39 -0700
Subject: [PATCH v1.4 085/120] Revert "Bluetooth: btrtl: fix out of bounds
 memory access"

This reverts commit 0c657e641df1e77d6087688190f632cad9c0439b.
---
 drivers/bluetooth/hci_h5.c | 1 -
 1 file changed, 1 deletion(-)

diff --git a/drivers/bluetooth/hci_h5.c b/drivers/bluetooth/hci_h5.c
index c0436881a533..b66136348bd6 100644
--- a/drivers/bluetooth/hci_h5.c
+++ b/drivers/bluetooth/hci_h5.c
@@ -1072,7 +1072,6 @@ static struct h5_vnd rtl_vnd = {
 	.suspend	= h5_btrtl_suspend,
 	.resume		= h5_btrtl_resume,
 	.acpi_gpio_map	= acpi_btrtl_gpios,
-	.sizeof_priv    = sizeof(struct btrealtek_data),
 };
 
 static const struct h5_device_data h5_data_rtl8822cs = {
-- 
2.47.0


From bb3c6028f382290f150f25d6aa87e0716c6eabdf Mon Sep 17 00:00:00 2001
From: Matthew Schwartz <njtransit215@gmail.com>
Date: Thu, 18 Apr 2024 11:30:43 -0700
Subject: [PATCH v1.4 086/120] Revert "Bluetooth: hci_h5: Add ability to
 allocate memory for private data"

This reverts commit 46a7d9aa6e456c161746744cc08961bb83dc97b3.
---
 drivers/bluetooth/hci_h5.c     |  4 +---
 drivers/bluetooth/hci_serdev.c |  9 ++++-----
 drivers/bluetooth/hci_uart.h   | 12 +-----------
 3 files changed, 6 insertions(+), 19 deletions(-)

diff --git a/drivers/bluetooth/hci_h5.c b/drivers/bluetooth/hci_h5.c
index b66136348bd6..71e748a9477e 100644
--- a/drivers/bluetooth/hci_h5.c
+++ b/drivers/bluetooth/hci_h5.c
@@ -113,7 +113,6 @@ struct h5_vnd {
 	int (*suspend)(struct h5 *h5);
 	int (*resume)(struct h5 *h5);
 	const struct acpi_gpio_mapping *acpi_gpio_map;
-	int sizeof_priv;
 };
 
 struct h5_device_data {
@@ -864,8 +863,7 @@ static int h5_serdev_probe(struct serdev_device *serdev)
 	if (IS_ERR(h5->device_wake_gpio))
 		return PTR_ERR(h5->device_wake_gpio);
 
-	return hci_uart_register_device_priv(&h5->serdev_hu, &h5p,
-					     h5->vnd->sizeof_priv);
+	return hci_uart_register_device(&h5->serdev_hu, &h5p);
 }
 
 static void h5_serdev_remove(struct serdev_device *serdev)
diff --git a/drivers/bluetooth/hci_serdev.c b/drivers/bluetooth/hci_serdev.c
index 89a22e9b3253..2a6e01dc0cf8 100644
--- a/drivers/bluetooth/hci_serdev.c
+++ b/drivers/bluetooth/hci_serdev.c
@@ -300,9 +300,8 @@ static const struct serdev_device_ops hci_serdev_client_ops = {
 	.write_wakeup = hci_uart_write_wakeup,
 };
 
-int hci_uart_register_device_priv(struct hci_uart *hu,
-			     const struct hci_uart_proto *p,
-			     int sizeof_priv)
+int hci_uart_register_device(struct hci_uart *hu,
+			     const struct hci_uart_proto *p)
 {
 	int err;
 	struct hci_dev *hdev;
@@ -326,7 +325,7 @@ int hci_uart_register_device_priv(struct hci_uart *hu,
 	set_bit(HCI_UART_PROTO_READY, &hu->flags);
 
 	/* Initialize and register HCI device */
-	hdev = hci_alloc_dev_priv(sizeof_priv);
+	hdev = hci_alloc_dev();
 	if (!hdev) {
 		BT_ERR("Can't allocate HCI device");
 		err = -ENOMEM;
@@ -390,7 +389,7 @@ int hci_uart_register_device_priv(struct hci_uart *hu,
 	percpu_free_rwsem(&hu->proto_lock);
 	return err;
 }
-EXPORT_SYMBOL_GPL(hci_uart_register_device_priv);
+EXPORT_SYMBOL_GPL(hci_uart_register_device);
 
 void hci_uart_unregister_device(struct hci_uart *hu)
 {
diff --git a/drivers/bluetooth/hci_uart.h b/drivers/bluetooth/hci_uart.h
index 00bf7ae82c5b..308e4d8de54e 100644
--- a/drivers/bluetooth/hci_uart.h
+++ b/drivers/bluetooth/hci_uart.h
@@ -96,17 +96,7 @@ struct hci_uart {
 
 int hci_uart_register_proto(const struct hci_uart_proto *p);
 int hci_uart_unregister_proto(const struct hci_uart_proto *p);
-
-int hci_uart_register_device_priv(struct hci_uart *hu,
-				  const struct hci_uart_proto *p,
-				  int sizeof_priv);
-
-static inline int hci_uart_register_device(struct hci_uart *hu,
-					   const struct hci_uart_proto *p)
-{
-	return hci_uart_register_device_priv(hu, p, 0);
-}
-
+int hci_uart_register_device(struct hci_uart *hu, const struct hci_uart_proto *p);
 void hci_uart_unregister_device(struct hci_uart *hu);
 
 int hci_uart_tx_wakeup(struct hci_uart *hu);
-- 
2.47.0


From c324da3fb8c0f8855759568d70d29d7ec1ee3572 Mon Sep 17 00:00:00 2001
From: Matthew Schwartz <njtransit215@gmail.com>
Date: Thu, 18 Apr 2024 11:30:50 -0700
Subject: [PATCH v1.4 087/120] Revert "Bluetooth: hci_qca: don't use
 IS_ERR_OR_NULL() with gpiod_get_optional()"

This reverts commit d8c7785e8104359f139cdfa99e2511575c4d4823.
---
 drivers/bluetooth/hci_qca.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/bluetooth/hci_qca.c b/drivers/bluetooth/hci_qca.c
index e47b07ae1a63..78fe4d2a2e08 100644
--- a/drivers/bluetooth/hci_qca.c
+++ b/drivers/bluetooth/hci_qca.c
@@ -2388,7 +2388,7 @@ static int qca_serdev_probe(struct serdev_device *serdev)
 
 		qcadev->sw_ctrl = devm_gpiod_get_optional(&serdev->dev, "swctrl",
 					       GPIOD_IN);
-		if (IS_ERR(qcadev->sw_ctrl) &&
+		if (IS_ERR_OR_NULL(qcadev->sw_ctrl) &&
 		    (data->soc_type == QCA_WCN6750 ||
 		     data->soc_type == QCA_WCN6855 ||
 		     data->soc_type == QCA_WCN7850)) {
-- 
2.47.0


From caadd278fe286ef281df02f11eca5efe58349d44 Mon Sep 17 00:00:00 2001
From: Matthew Schwartz <njtransit215@gmail.com>
Date: Thu, 18 Apr 2024 11:30:54 -0700
Subject: [PATCH v1.4 088/120] Revert "Bluetooth: qca: Fix triggering coredump
 implementation"

This reverts commit 6abf9dd26bb1699c17d601b9a292577d01827c0e.
---
 drivers/bluetooth/hci_qca.c | 9 +++++----
 1 file changed, 5 insertions(+), 4 deletions(-)

diff --git a/drivers/bluetooth/hci_qca.c b/drivers/bluetooth/hci_qca.c
index 78fe4d2a2e08..d5f209d53b28 100644
--- a/drivers/bluetooth/hci_qca.c
+++ b/drivers/bluetooth/hci_qca.c
@@ -1820,12 +1820,13 @@ static int qca_power_on(struct hci_dev *hdev)
 
 static void hci_coredump_qca(struct hci_dev *hdev)
 {
-	int err;
 	static const u8 param[] = { 0x26 };
+	struct sk_buff *skb;
 
-	err = __hci_cmd_send(hdev, 0xfc0c, 1, param);
-	if (err < 0)
-		bt_dev_err(hdev, "%s: trigger crash failed (%d)", __func__, err);
+	skb = __hci_cmd_sync(hdev, 0xfc0c, 1, param, HCI_CMD_TIMEOUT);
+	if (IS_ERR(skb))
+		bt_dev_err(hdev, "%s: trigger crash failed (%ld)", __func__, PTR_ERR(skb));
+	kfree_skb(skb);
 }
 
 static int qca_get_data_path_id(struct hci_dev *hdev, __u8 *data_path_id)
-- 
2.47.0


From 807d6cb142fda689b356d4fbab0db34244b6e719 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:14:10 +0200
Subject: [PATCH v1.4 089/120] [BEGIN] Modern Suspend patchset

-- 
2.47.0


From a77b8dbd726d62136a112a72269eaf2441004224 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Wed, 18 Sep 2024 23:53:53 +0200
Subject: [PATCH v1.4 090/120] acpi/x86: s2idle: add support for Display Off
 and Display On callbacks

The Display Off and Display On firmware notifications are meant to signify
the system entering a state where the user is not actively interacting
with it (i.e., in Windows this state is called "Screen Off" and the
system enters it once it turns the screen off e.g., due to inactivity).

Currently, these functions are called within the suspend sequence, which
causes issues when these notifications interact with e.g., a USB device
and makes them unable to be called as part of the screen turning off.

This patch adds a set of callbacks to allow calling the Display On/Off
notifications outside of the suspend/resume path.

Co-developed-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 include/linux/suspend.h |  5 +++++
 kernel/power/suspend.c  | 12 ++++++++++++
 2 files changed, 17 insertions(+)

diff --git a/include/linux/suspend.h b/include/linux/suspend.h
index da6ebca3ff77..8f33249cc067 100644
--- a/include/linux/suspend.h
+++ b/include/linux/suspend.h
@@ -132,6 +132,7 @@ struct platform_suspend_ops {
 };
 
 struct platform_s2idle_ops {
+	int (*display_off)(void);
 	int (*begin)(void);
 	int (*prepare)(void);
 	int (*prepare_late)(void);
@@ -140,6 +141,7 @@ struct platform_s2idle_ops {
 	void (*restore_early)(void);
 	void (*restore)(void);
 	void (*end)(void);
+	int (*display_on)(void);
 };
 
 #ifdef CONFIG_SUSPEND
@@ -160,6 +162,9 @@ extern unsigned int pm_suspend_global_flags;
 #define PM_SUSPEND_FLAG_FW_RESUME	BIT(1)
 #define PM_SUSPEND_FLAG_NO_PLATFORM	BIT(2)
 
+int platform_suspend_display_off(void);
+int platform_suspend_display_on(void);
+
 static inline void pm_suspend_clear_flags(void)
 {
 	pm_suspend_global_flags = 0;
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 09f8397bae15..c527dc0ae5ae 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -254,6 +254,18 @@ static bool sleep_state_supported(suspend_state_t state)
 	       (valid_state(state) && !cxl_mem_active());
 }
 
+int platform_suspend_display_off(void)
+{
+	return s2idle_ops && s2idle_ops->display_off ? s2idle_ops->display_off() : 0;
+}
+EXPORT_SYMBOL_GPL(platform_suspend_display_off);
+
+int platform_suspend_display_on(void)
+{
+	return s2idle_ops && s2idle_ops->display_on ? s2idle_ops->display_on() : 0;
+}
+EXPORT_SYMBOL_GPL(platform_suspend_display_on);
+
 static int platform_suspend_prepare(suspend_state_t state)
 {
 	return state != PM_SUSPEND_TO_IDLE && suspend_ops->prepare ?
-- 
2.47.0


From beb7642eaa8d1399eb3428164c1055c23935b5d9 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Thu, 19 Sep 2024 00:02:32 +0200
Subject: [PATCH v1.4 091/120] acpi/x86: s2idle: handle Display On/Off calls
 outside of suspend sequence

Currently, the Display On/Off calls are handled within the suspend
sequence, which is a deviation from Windows. This causes issues with
certain devices, where the notification interacts with a USB device
that expects the kernel to be fully awake.

This patch calls the Display On/Off callbacks before entering the suspend
sequence, which fixes this issue. In addition, it opens the possibility
of modelling a state such as "Screen Off" that mirrors Windows, as the
callbacks will be accessible and validated to work outside of the
suspend sequence.

Suggested-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 kernel/power/suspend.c | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index c527dc0ae5ae..610f8ecaeebd 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -589,6 +589,13 @@ static int enter_state(suspend_state_t state)
 	if (state == PM_SUSPEND_TO_IDLE)
 		s2idle_begin();
 
+	/*
+	 * Linux does not have the concept of a "Screen Off" state, so call
+	 * the platform functions for Display On/Off prior to the suspend
+	 * sequence, mirroring Windows which calls them outside of it as well.
+	 */
+	platform_suspend_display_off();
+
 	if (sync_on_suspend_enabled) {
 		trace_suspend_resume(TPS("sync_filesystems"), 0, true);
 		ksys_sync_helper();
@@ -616,6 +623,8 @@ static int enter_state(suspend_state_t state)
 	suspend_finish();
  Unlock:
 	mutex_unlock(&system_transition_mutex);
+
+	platform_suspend_display_on();
 	return error;
 }
 
-- 
2.47.0


From 04d94b173d83ba3d6c409625f6907b98bd3f1199 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Sun, 22 Sep 2024 11:47:29 +0200
Subject: [PATCH v1.4 092/120] acpi/x86: s2idle: add quirk table for modern
 standby delays

Unfortunately, some modern standby systems, including the ROG Ally, rely
on a delay between modern standby transitions. Add a quirk table for
introducing delays between modern standby transitions, and quirk the
ROG Ally on "Display Off", which needs a bit of time to turn off its
controllers prior to suspending (i.e., entering DRIPS).

Reported-by: Denis Benato <benato.denis96@gmail.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 include/linux/suspend.h |  5 +++++
 kernel/power/suspend.c  | 41 +++++++++++++++++++++++++++++++++++++++++
 2 files changed, 46 insertions(+)

diff --git a/include/linux/suspend.h b/include/linux/suspend.h
index 8f33249cc067..d7e2a4d8ab0c 100644
--- a/include/linux/suspend.h
+++ b/include/linux/suspend.h
@@ -144,6 +144,11 @@ struct platform_s2idle_ops {
 	int (*display_on)(void);
 };
 
+struct platform_s2idle_quirks {
+	int delay_display_off;
+	int delay_display_on;
+};
+
 #ifdef CONFIG_SUSPEND
 extern suspend_state_t pm_suspend_target_state;
 extern suspend_state_t mem_sleep_current;
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 610f8ecaeebd..af2abdd2f8c3 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -11,6 +11,7 @@
 
 #include <linux/string.h>
 #include <linux/delay.h>
+#include <linux/dmi.h>
 #include <linux/errno.h>
 #include <linux/init.h>
 #include <linux/console.h>
@@ -61,6 +62,30 @@ static DECLARE_SWAIT_QUEUE_HEAD(s2idle_wait_head);
 enum s2idle_states __read_mostly s2idle_state;
 static DEFINE_RAW_SPINLOCK(s2idle_lock);
 
+// The ROG Ally series disconnects its controllers on Display Off, without
+// holding a lock, introducing a race condition. Add a delay to allow the
+// controller to disconnect cleanly prior to suspend.
+static const struct platform_s2idle_quirks rog_ally_quirks = {
+	.delay_display_off = 500,
+};
+
+static const struct dmi_system_id platform_s2idle_quirks[] = {
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_NAME, "RC71L"),
+		},
+		.driver_data = (void *)&rog_ally_quirks
+	},
+	{
+		.matches = {
+			DMI_MATCH(DMI_BOARD_NAME, "RC72L"),
+		},
+		.driver_data = (void *)&rog_ally_quirks
+	},
+	{}
+};
+
+
 /**
  * pm_suspend_default_s2idle - Check if suspend-to-idle is the default suspend.
  *
@@ -589,12 +614,26 @@ static int enter_state(suspend_state_t state)
 	if (state == PM_SUSPEND_TO_IDLE)
 		s2idle_begin();
 
+	/*
+	 * Windows transitions between Modern Standby states slowly, over multiple
+	 * seconds. Certain manufacturers may rely on this, introducing race
+	 * conditions. Until Linux can support modern standby, add the relevant
+	 * delays between transitions here.
+	 */
+	const struct dmi_system_id *s2idle_sysid = dmi_first_match(
+		platform_s2idle_quirks
+	);
+	const struct platform_s2idle_quirks *s2idle_quirks = s2idle_sysid ?
+		s2idle_sysid->driver_data : NULL;
+
 	/*
 	 * Linux does not have the concept of a "Screen Off" state, so call
 	 * the platform functions for Display On/Off prior to the suspend
 	 * sequence, mirroring Windows which calls them outside of it as well.
 	 */
 	platform_suspend_display_off();
+	if (s2idle_quirks && s2idle_quirks->delay_display_off)
+		msleep(s2idle_quirks->delay_display_off);
 
 	if (sync_on_suspend_enabled) {
 		trace_suspend_resume(TPS("sync_filesystems"), 0, true);
@@ -624,6 +663,8 @@ static int enter_state(suspend_state_t state)
  Unlock:
 	mutex_unlock(&system_transition_mutex);
 
+	if (s2idle_quirks && s2idle_quirks->delay_display_on)
+		msleep(s2idle_quirks->delay_display_on);
 	platform_suspend_display_on();
 	return error;
 }
-- 
2.47.0


From 7c0ef744690f6ca6b7889bfc7461d3d864b004eb Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Thu, 19 Sep 2024 00:22:03 +0200
Subject: [PATCH v1.4 093/120] acpi/x86: s2idle: call Display On/Off as part of
 callbacks and rename

Move the Display On/Off notifications into dedicated callbacks that gate
the ACPI mutex, so they can be called outside of the suspend path.
This fixes issues on certain devices that expect kernel drivers to be
fully active during the calls, and allows for the flexibility of calling
them as part of a more elaborate userspace suspend sequence (such as
with "Screen Off" in Windows Modern Standby).

In addition, rename the notifications from "screen_" to "display_", as
there is no documentation referring to them as screen, either by
Intel or Microsoft.

Co-developed-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 drivers/acpi/x86/s2idle.c | 89 +++++++++++++++++++++++++++------------
 1 file changed, 62 insertions(+), 27 deletions(-)

diff --git a/drivers/acpi/x86/s2idle.c b/drivers/acpi/x86/s2idle.c
index dd0b40b9bbe8..a17e28b91326 100644
--- a/drivers/acpi/x86/s2idle.c
+++ b/drivers/acpi/x86/s2idle.c
@@ -39,8 +39,8 @@ static const struct acpi_device_id lps0_device_ids[] = {
 #define ACPI_LPS0_DSM_UUID	"c4eb40a0-6cd2-11e2-bcfd-0800200c9a66"
 
 #define ACPI_LPS0_GET_DEVICE_CONSTRAINTS	1
-#define ACPI_LPS0_SCREEN_OFF	3
-#define ACPI_LPS0_SCREEN_ON	4
+#define ACPI_LPS0_DISPLAY_OFF	3
+#define ACPI_LPS0_DISPLAY_ON	4
 #define ACPI_LPS0_ENTRY		5
 #define ACPI_LPS0_EXIT		6
 #define ACPI_LPS0_MS_ENTRY      7
@@ -50,8 +50,8 @@ static const struct acpi_device_id lps0_device_ids[] = {
 #define ACPI_LPS0_DSM_UUID_AMD      "e3f32452-febc-43ce-9039-932122d37721"
 #define ACPI_LPS0_ENTRY_AMD         2
 #define ACPI_LPS0_EXIT_AMD          3
-#define ACPI_LPS0_SCREEN_OFF_AMD    4
-#define ACPI_LPS0_SCREEN_ON_AMD     5
+#define ACPI_LPS0_DISPLAY_OFF_AMD   4
+#define ACPI_LPS0_DISPLAY_ON_AMD    5
 
 static acpi_handle lps0_device_handle;
 static guid_t lps0_dsm_guid;
@@ -60,6 +60,7 @@ static int lps0_dsm_func_mask;
 static guid_t lps0_dsm_guid_microsoft;
 static int lps0_dsm_func_mask_microsoft;
 static int lps0_dsm_state;
+static bool lsp0_dsm_in_display_off;
 
 /* Device constraint entry structure */
 struct lpi_device_info {
@@ -361,9 +362,9 @@ static const char *acpi_sleep_dsm_state_to_str(unsigned int state)
 {
 	if (lps0_dsm_func_mask_microsoft || !acpi_s2idle_vendor_amd()) {
 		switch (state) {
-		case ACPI_LPS0_SCREEN_OFF:
+		case ACPI_LPS0_DISPLAY_OFF:
 			return "screen off";
-		case ACPI_LPS0_SCREEN_ON:
+		case ACPI_LPS0_DISPLAY_ON:
 			return "screen on";
 		case ACPI_LPS0_ENTRY:
 			return "lps0 entry";
@@ -376,9 +377,9 @@ static const char *acpi_sleep_dsm_state_to_str(unsigned int state)
 		}
 	} else {
 		switch (state) {
-		case ACPI_LPS0_SCREEN_ON_AMD:
+		case ACPI_LPS0_DISPLAY_ON_AMD:
 			return "screen on";
-		case ACPI_LPS0_SCREEN_OFF_AMD:
+		case ACPI_LPS0_DISPLAY_OFF_AMD:
 			return "screen off";
 		case ACPI_LPS0_ENTRY_AMD:
 			return "lps0 entry";
@@ -539,27 +540,69 @@ static struct acpi_scan_handler lps0_handler = {
 	.attach = lps0_device_attach,
 };
 
-int acpi_s2idle_prepare_late(void)
+static int acpi_s2idle_display_off(void)
 {
-	struct acpi_s2idle_dev_ops *handler;
-
 	if (!lps0_device_handle || sleep_no_lps0)
 		return 0;
 
-	if (pm_debug_messages_on)
-		lpi_check_constraints();
+	if (unlikely(WARN_ON(lsp0_dsm_in_display_off)))
+		return -EINVAL;
+
+	lsp0_dsm_in_display_off = true;
+	acpi_scan_lock_acquire();
 
-	/* Screen off */
+	/* Display off */
 	if (lps0_dsm_func_mask > 0)
 		acpi_sleep_run_lps0_dsm(acpi_s2idle_vendor_amd() ?
-					ACPI_LPS0_SCREEN_OFF_AMD :
-					ACPI_LPS0_SCREEN_OFF,
+					ACPI_LPS0_DISPLAY_OFF_AMD :
+					ACPI_LPS0_DISPLAY_OFF,
 					lps0_dsm_func_mask, lps0_dsm_guid);
 
 	if (lps0_dsm_func_mask_microsoft > 0)
-		acpi_sleep_run_lps0_dsm(ACPI_LPS0_SCREEN_OFF,
+		acpi_sleep_run_lps0_dsm(ACPI_LPS0_DISPLAY_OFF,
 				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
 
+	acpi_scan_lock_release();
+
+	return 0;
+}
+
+static int acpi_s2idle_display_on(void)
+{
+	if (!lps0_device_handle || sleep_no_lps0)
+		return 0;
+
+	if (unlikely(WARN_ON(!lsp0_dsm_in_display_off)))
+		return -EINVAL;
+
+	lsp0_dsm_in_display_off = false;
+	acpi_scan_lock_acquire();
+
+	/* Display on */
+	if (lps0_dsm_func_mask_microsoft > 0)
+		acpi_sleep_run_lps0_dsm(ACPI_LPS0_DISPLAY_ON,
+				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
+	if (lps0_dsm_func_mask > 0)
+		acpi_sleep_run_lps0_dsm(acpi_s2idle_vendor_amd() ?
+					ACPI_LPS0_DISPLAY_ON_AMD :
+					ACPI_LPS0_DISPLAY_ON,
+					lps0_dsm_func_mask, lps0_dsm_guid);
+
+	acpi_scan_lock_release();
+
+	return 0;
+}
+
+int acpi_s2idle_prepare_late(void)
+{
+	struct acpi_s2idle_dev_ops *handler;
+
+	if (!lps0_device_handle || sleep_no_lps0)
+		return 0;
+
+	if (pm_debug_messages_on)
+		lpi_check_constraints();
+
 	/* LPS0 entry */
 	if (lps0_dsm_func_mask > 0 && acpi_s2idle_vendor_amd())
 		acpi_sleep_run_lps0_dsm(ACPI_LPS0_ENTRY_AMD,
@@ -623,19 +666,10 @@ void acpi_s2idle_restore_early(void)
 		acpi_sleep_run_lps0_dsm(ACPI_LPS0_MS_EXIT,
 				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
 	}
-
-	/* Screen on */
-	if (lps0_dsm_func_mask_microsoft > 0)
-		acpi_sleep_run_lps0_dsm(ACPI_LPS0_SCREEN_ON,
-				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
-	if (lps0_dsm_func_mask > 0)
-		acpi_sleep_run_lps0_dsm(acpi_s2idle_vendor_amd() ?
-					ACPI_LPS0_SCREEN_ON_AMD :
-					ACPI_LPS0_SCREEN_ON,
-					lps0_dsm_func_mask, lps0_dsm_guid);
 }
 
 static const struct platform_s2idle_ops acpi_s2idle_ops_lps0 = {
+	.display_off = acpi_s2idle_display_off,
 	.begin = acpi_s2idle_begin,
 	.prepare = acpi_s2idle_prepare,
 	.prepare_late = acpi_s2idle_prepare_late,
@@ -644,6 +678,7 @@ static const struct platform_s2idle_ops acpi_s2idle_ops_lps0 = {
 	.restore_early = acpi_s2idle_restore_early,
 	.restore = acpi_s2idle_restore,
 	.end = acpi_s2idle_end,
+	.display_on = acpi_s2idle_display_on,
 };
 
 void __init acpi_s2idle_setup(void)
-- 
2.47.0


From 2ea5bd79b6084112a3434b9a758b72a8e2c48b4d Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Thu, 19 Sep 2024 00:29:59 +0200
Subject: [PATCH v1.4 094/120] platform/x86: asus-wmi: remove Ally (1st gen)
 and Ally X suspend quirk

By moving the Display On/Off calls outside of the suspend sequence and
introducing a slight delay after Display Off, the ROG Ally controller
functions exactly as it does in Windows.

Therefore, remove the quirk that fixed the controller only when the
mcu_powersave attribute was disabled, while adding a large amount of
delay to the suspend and wake process.

Reviewed-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 drivers/platform/x86/asus-wmi.c | 54 ---------------------------------
 1 file changed, 54 deletions(-)

diff --git a/drivers/platform/x86/asus-wmi.c b/drivers/platform/x86/asus-wmi.c
index 321a658e4554..0a3202bfaf1a 100644
--- a/drivers/platform/x86/asus-wmi.c
+++ b/drivers/platform/x86/asus-wmi.c
@@ -137,29 +137,10 @@ module_param(fnlock_default, bool, 0444);
 #define ASUS_MINI_LED_2024_STRONG	0x01
 #define ASUS_MINI_LED_2024_OFF		0x02
 
-/* Controls the power state of the USB0 hub on ROG Ally which input is on */
-#define ASUS_USB0_PWR_EC0_CSEE "\\_SB.PCI0.SBRG.EC0.CSEE"
-/* 300ms so far seems to produce a reliable result on AC and battery */
-#define ASUS_USB0_PWR_EC0_CSEE_WAIT 1500
-
 static const char * const ashs_ids[] = { "ATK4001", "ATK4002", NULL };
 
 static int throttle_thermal_policy_write(struct asus_wmi *);
 
-static const struct dmi_system_id asus_ally_mcu_quirk[] = {
-	{
-		.matches = {
-			DMI_MATCH(DMI_BOARD_NAME, "RC71L"),
-		},
-	},
-	{
-		.matches = {
-			DMI_MATCH(DMI_BOARD_NAME, "RC72L"),
-		},
-	},
-	{ },
-};
-
 static bool ashs_present(void)
 {
 	int i = 0;
@@ -269,9 +250,6 @@ struct asus_wmi {
 	u32 tablet_switch_dev_id;
 	bool tablet_switch_inverted;
 
-	/* The ROG Ally device requires the MCU USB device be disconnected before suspend */
-	bool ally_mcu_usb_switch;
-
 	enum fan_type fan_type;
 	enum fan_type gpu_fan_type;
 	enum fan_type mid_fan_type;
@@ -4708,8 +4686,6 @@ static int asus_wmi_add(struct platform_device *pdev)
 	asus->egpu_enable_available = asus_wmi_dev_is_present(asus, ASUS_WMI_DEVID_EGPU);
 	asus->dgpu_disable_available = asus_wmi_dev_is_present(asus, ASUS_WMI_DEVID_DGPU);
 	asus->kbd_rgb_state_available = asus_wmi_dev_is_present(asus, ASUS_WMI_DEVID_TUF_RGB_STATE);
-	asus->ally_mcu_usb_switch = acpi_has_method(NULL, ASUS_USB0_PWR_EC0_CSEE)
-						&& dmi_check_system(asus_ally_mcu_quirk);
 
 	if (asus_wmi_dev_is_present(asus, ASUS_WMI_DEVID_MINI_LED_MODE))
 		asus->mini_led_dev_id = ASUS_WMI_DEVID_MINI_LED_MODE;
@@ -4902,34 +4878,6 @@ static int asus_hotk_resume(struct device *device)
 	return 0;
 }
 
-static int asus_hotk_resume_early(struct device *device)
-{
-	struct asus_wmi *asus = dev_get_drvdata(device);
-
-	if (asus->ally_mcu_usb_switch) {
-		/* sleep required to prevent USB0 being yanked then reappearing rapidly */
-		if (ACPI_FAILURE(acpi_execute_simple_method(NULL, ASUS_USB0_PWR_EC0_CSEE, 0xB8)))
-			dev_err(device, "ROG Ally MCU failed to connect USB dev\n");
-		else
-			msleep(ASUS_USB0_PWR_EC0_CSEE_WAIT);
-	}
-	return 0;
-}
-
-static int asus_hotk_prepare(struct device *device)
-{
-	struct asus_wmi *asus = dev_get_drvdata(device);
-
-	if (asus->ally_mcu_usb_switch) {
-		/* sleep required to ensure USB0 is disabled before sleep continues */
-		if (ACPI_FAILURE(acpi_execute_simple_method(NULL, ASUS_USB0_PWR_EC0_CSEE, 0xB7)))
-			dev_err(device, "ROG Ally MCU failed to disconnect USB dev\n");
-		else
-			msleep(ASUS_USB0_PWR_EC0_CSEE_WAIT);
-	}
-	return 0;
-}
-
 static int asus_hotk_restore(struct device *device)
 {
 	struct asus_wmi *asus = dev_get_drvdata(device);
@@ -4974,8 +4922,6 @@ static const struct dev_pm_ops asus_pm_ops = {
 	.thaw = asus_hotk_thaw,
 	.restore = asus_hotk_restore,
 	.resume = asus_hotk_resume,
-	.resume_early = asus_hotk_resume_early,
-	.prepare = asus_hotk_prepare,
 };
 
 /* Registration ***************************************************************/
-- 
2.47.0


From e20435f9ecbd38d67d3b92b0c9f4c2c813ff45b1 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Wed, 25 Sep 2024 14:10:11 +0200
Subject: [PATCH v1.4 095/120] acpi/x86: s2idle: add support for Sleep Entry
 and Sleep Exit callbacks

The Sleep Entry and Sleep Exit firmware notifications allow the platform
to enter Modern Standby. In this state, if supported, the platform turns
off auxiliary USB devices (e.g., the controllers of the Legion Go),
makes the power light of the device flash, and lowers the power envelope
to a minimum that still allows for software activity without affecting
battery life.

Allow for entering this state prior to initiating the suspend sequence.
This fixes issues where the EC or the USB of the device need time to
power down before entering the suspend sequence, and allows for entering
this power state without suspending the device.

Suggested-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 include/linux/suspend.h |  4 ++++
 kernel/power/suspend.c  | 12 ++++++++++++
 2 files changed, 16 insertions(+)

diff --git a/include/linux/suspend.h b/include/linux/suspend.h
index d7e2a4d8ab0c..66c5b434334d 100644
--- a/include/linux/suspend.h
+++ b/include/linux/suspend.h
@@ -133,6 +133,7 @@ struct platform_suspend_ops {
 
 struct platform_s2idle_ops {
 	int (*display_off)(void);
+	int (*sleep_entry)(void);
 	int (*begin)(void);
 	int (*prepare)(void);
 	int (*prepare_late)(void);
@@ -141,6 +142,7 @@ struct platform_s2idle_ops {
 	void (*restore_early)(void);
 	void (*restore)(void);
 	void (*end)(void);
+	int (*sleep_exit)(void);
 	int (*display_on)(void);
 };
 
@@ -168,6 +170,8 @@ extern unsigned int pm_suspend_global_flags;
 #define PM_SUSPEND_FLAG_NO_PLATFORM	BIT(2)
 
 int platform_suspend_display_off(void);
+int platform_suspend_sleep_entry(void);
+int platform_suspend_sleep_exit(void);
 int platform_suspend_display_on(void);
 
 static inline void pm_suspend_clear_flags(void)
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index af2abdd2f8c3..dab299e28195 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -285,6 +285,18 @@ int platform_suspend_display_off(void)
 }
 EXPORT_SYMBOL_GPL(platform_suspend_display_off);
 
+int platform_suspend_sleep_entry(void)
+{
+	return s2idle_ops && s2idle_ops->sleep_entry ? s2idle_ops->sleep_entry() : 0;
+}
+EXPORT_SYMBOL_GPL(platform_suspend_sleep_entry);
+
+int platform_suspend_sleep_exit(void)
+{
+	return s2idle_ops && s2idle_ops->sleep_exit ? s2idle_ops->sleep_exit() : 0;
+}
+EXPORT_SYMBOL_GPL(platform_suspend_sleep_exit);
+
 int platform_suspend_display_on(void)
 {
 	return s2idle_ops && s2idle_ops->display_on ? s2idle_ops->display_on() : 0;
-- 
2.47.0


From fcba05813312eed8548478dbcfe4386eee9601da Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Wed, 25 Sep 2024 14:19:00 +0200
Subject: [PATCH v1.4 096/120] acpi/x86: s2idle: handle Sleep Entry/Exit calls
 outside of suspend sequence

As with Display On/Off, these calls should be made outside the suspend
sequence, to allow the EC and USB devices that are affected to complete
their power off sequence before the kernel suspends their power rails
and interrupts.

Suggested-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 kernel/power/suspend.c | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index dab299e28195..9dcdd5273318 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -547,6 +547,13 @@ int suspend_devices_and_enter(suspend_state_t state)
 	if (state == PM_SUSPEND_TO_IDLE)
 		pm_set_suspend_no_platform();
 
+	/*
+	 * Linux does not have the concept of a "Sleep" state. As with Display
+	 * On/Off, call the platform functions for Sleep Entry/Exit prior to the
+	 * suspend sequence.
+	 */
+	platform_suspend_sleep_entry();
+
 	error = platform_suspend_begin(state);
 	if (error)
 		goto Close;
@@ -577,6 +584,8 @@ int suspend_devices_and_enter(suspend_state_t state)
  Close:
 	platform_resume_end(state);
 	pm_suspend_target_state = PM_SUSPEND_ON;
+
+	platform_suspend_sleep_exit();
 	return error;
 
  Recover_platform:
-- 
2.47.0


From b8fdd822e92725439c61701233bb0a5b2f01e696 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Wed, 25 Sep 2024 14:29:42 +0200
Subject: [PATCH v1.4 097/120] acpi/x86: s2idle: update quirk table for Sleep
 Entry/Exit

Add delays between the Sleep Entry and Sleep Exit calls, to avoid issues
in devices that rely on them that need time to power off.

Especially for the ROG Ally, this should allow its EC to suspend gracefully,
avoiding issues where it is stuck in its suspend state. Since the delays
are additive, steal some of the delay from Display On/Off.

Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 include/linux/suspend.h |  2 ++
 kernel/power/suspend.c  | 21 +++++++++++++++++++--
 2 files changed, 21 insertions(+), 2 deletions(-)

diff --git a/include/linux/suspend.h b/include/linux/suspend.h
index 66c5b434334d..5b4d4d9ef65a 100644
--- a/include/linux/suspend.h
+++ b/include/linux/suspend.h
@@ -148,6 +148,8 @@ struct platform_s2idle_ops {
 
 struct platform_s2idle_quirks {
 	int delay_display_off;
+	int delay_sleep_entry;
+	int delay_sleep_exit;
 	int delay_display_on;
 };
 
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 9dcdd5273318..1352c4066822 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -65,8 +65,11 @@ static DEFINE_RAW_SPINLOCK(s2idle_lock);
 // The ROG Ally series disconnects its controllers on Display Off, without
 // holding a lock, introducing a race condition. Add a delay to allow the
 // controller to disconnect cleanly prior to suspend.
+// In addition, the EC of the device rarely (1/20 attempts) may get stuck
+// after suspend in an invalid state, where it mirros Sleep behavior.
 static const struct platform_s2idle_quirks rog_ally_quirks = {
-	.delay_display_off = 500,
+	.delay_display_off = 200,
+	.delay_sleep_entry = 300,
 };
 
 static const struct dmi_system_id platform_s2idle_quirks[] = {
@@ -548,11 +551,23 @@ int suspend_devices_and_enter(suspend_state_t state)
 		pm_set_suspend_no_platform();
 
 	/*
-	 * Linux does not have the concept of a "Sleep" state. As with Display
+	 * Windows transitions between Modern Standby states slowly, as with
+	 * Display On/Off, query the appropriate delays here for Sleep Entry/Exit.
+	 */
+	const struct dmi_system_id *s2idle_sysid = dmi_first_match(
+		platform_s2idle_quirks
+	);
+	const struct platform_s2idle_quirks *s2idle_quirks = s2idle_sysid ?
+		s2idle_sysid->driver_data : NULL;
+
+	/*
+	 * Linux does not have the concept of a "Sleep" state. As done with Display
 	 * On/Off, call the platform functions for Sleep Entry/Exit prior to the
 	 * suspend sequence.
 	 */
 	platform_suspend_sleep_entry();
+	if (s2idle_quirks && s2idle_quirks->delay_sleep_entry)
+		msleep(s2idle_quirks->delay_sleep_entry);
 
 	error = platform_suspend_begin(state);
 	if (error)
@@ -585,6 +600,8 @@ int suspend_devices_and_enter(suspend_state_t state)
 	platform_resume_end(state);
 	pm_suspend_target_state = PM_SUSPEND_ON;
 
+	if (s2idle_quirks && s2idle_quirks->delay_sleep_exit)
+		msleep(s2idle_quirks->delay_sleep_exit);
 	platform_suspend_sleep_exit();
 	return error;
 
-- 
2.47.0


From 5d9ec68f0f01e3c7dfac47695527836a361dfac8 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <lkml@antheas.dev>
Date: Wed, 25 Sep 2024 14:41:03 +0200
Subject: [PATCH v1.4 098/120] acpi/x86: s2idle: call Sleep Entry/Exit as part
 of callbacks.

Move the Sleep Entry/Exit notifications outside the suspend sequence,
with their own ACPI lock, as was done for Display On/Off.

Suggested-by: Mario Limonciello <mario.limonciello@amd.com>
Signed-off-by: Antheas Kapenekakis <lkml@antheas.dev>
---
 drivers/acpi/x86/s2idle.c | 69 ++++++++++++++++++++++++++++++---------
 1 file changed, 53 insertions(+), 16 deletions(-)

diff --git a/drivers/acpi/x86/s2idle.c b/drivers/acpi/x86/s2idle.c
index a17e28b91326..6ff5e34c016e 100644
--- a/drivers/acpi/x86/s2idle.c
+++ b/drivers/acpi/x86/s2idle.c
@@ -43,8 +43,8 @@ static const struct acpi_device_id lps0_device_ids[] = {
 #define ACPI_LPS0_DISPLAY_ON	4
 #define ACPI_LPS0_ENTRY		5
 #define ACPI_LPS0_EXIT		6
-#define ACPI_LPS0_MS_ENTRY      7
-#define ACPI_LPS0_MS_EXIT       8
+#define ACPI_LPS0_SLEEP_ENTRY      7
+#define ACPI_LPS0_SLEEP_EXIT       8
 
 /* AMD */
 #define ACPI_LPS0_DSM_UUID_AMD      "e3f32452-febc-43ce-9039-932122d37721"
@@ -61,6 +61,7 @@ static guid_t lps0_dsm_guid_microsoft;
 static int lps0_dsm_func_mask_microsoft;
 static int lps0_dsm_state;
 static bool lsp0_dsm_in_display_off;
+static bool lsp0_dsm_in_sleep;
 
 /* Device constraint entry structure */
 struct lpi_device_info {
@@ -370,10 +371,10 @@ static const char *acpi_sleep_dsm_state_to_str(unsigned int state)
 			return "lps0 entry";
 		case ACPI_LPS0_EXIT:
 			return "lps0 exit";
-		case ACPI_LPS0_MS_ENTRY:
-			return "lps0 ms entry";
-		case ACPI_LPS0_MS_EXIT:
-			return "lps0 ms exit";
+		case ACPI_LPS0_SLEEP_ENTRY:
+			return "lps0 sleep entry";
+		case ACPI_LPS0_SLEEP_EXIT:
+			return "lps0 sleep exit";
 		}
 	} else {
 		switch (state) {
@@ -567,6 +568,48 @@ static int acpi_s2idle_display_off(void)
 	return 0;
 }
 
+static int acpi_s2idle_sleep_entry(void)
+{
+	if (!lps0_device_handle || sleep_no_lps0 || lps0_dsm_func_mask_microsoft <= 0)
+		return 0;
+
+	if (WARN_ON(lsp0_dsm_in_sleep))
+		return -EINVAL;
+
+	lsp0_dsm_in_sleep = true;
+	acpi_scan_lock_acquire();
+
+	/* Modern Standby Sleep Entry */
+	if (lps0_dsm_func_mask_microsoft > 0)
+		acpi_sleep_run_lps0_dsm(ACPI_LPS0_SLEEP_ENTRY,
+				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
+
+	acpi_scan_lock_release();
+
+	return 0;
+}
+
+static int acpi_s2idle_sleep_exit(void)
+{
+	if (!lps0_device_handle || sleep_no_lps0 || lps0_dsm_func_mask_microsoft <= 0)
+		return 0;
+
+	if (WARN_ON(!lsp0_dsm_in_sleep))
+		return -EINVAL;
+
+	lsp0_dsm_in_sleep = false;
+	acpi_scan_lock_acquire();
+
+	/* Modern Standby Sleep Exit */
+	if (lps0_dsm_func_mask_microsoft > 0)
+		acpi_sleep_run_lps0_dsm(ACPI_LPS0_SLEEP_EXIT,
+				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
+
+	acpi_scan_lock_release();
+
+	return 0;
+}
+
 static int acpi_s2idle_display_on(void)
 {
 	if (!lps0_device_handle || sleep_no_lps0)
@@ -608,13 +651,9 @@ int acpi_s2idle_prepare_late(void)
 		acpi_sleep_run_lps0_dsm(ACPI_LPS0_ENTRY_AMD,
 					lps0_dsm_func_mask, lps0_dsm_guid);
 
-	if (lps0_dsm_func_mask_microsoft > 0) {
-		/* Modern Standby entry */
-		acpi_sleep_run_lps0_dsm(ACPI_LPS0_MS_ENTRY,
-				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
+	if (lps0_dsm_func_mask_microsoft > 0)
 		acpi_sleep_run_lps0_dsm(ACPI_LPS0_ENTRY,
 				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
-	}
 
 	if (lps0_dsm_func_mask > 0 && !acpi_s2idle_vendor_amd())
 		acpi_sleep_run_lps0_dsm(ACPI_LPS0_ENTRY,
@@ -659,17 +698,14 @@ void acpi_s2idle_restore_early(void)
 					ACPI_LPS0_EXIT,
 					lps0_dsm_func_mask, lps0_dsm_guid);
 
-	if (lps0_dsm_func_mask_microsoft > 0) {
+	if (lps0_dsm_func_mask_microsoft > 0)
 		acpi_sleep_run_lps0_dsm(ACPI_LPS0_EXIT,
 				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
-		/* Modern Standby exit */
-		acpi_sleep_run_lps0_dsm(ACPI_LPS0_MS_EXIT,
-				lps0_dsm_func_mask_microsoft, lps0_dsm_guid_microsoft);
-	}
 }
 
 static const struct platform_s2idle_ops acpi_s2idle_ops_lps0 = {
 	.display_off = acpi_s2idle_display_off,
+	.sleep_entry = acpi_s2idle_sleep_entry,
 	.begin = acpi_s2idle_begin,
 	.prepare = acpi_s2idle_prepare,
 	.prepare_late = acpi_s2idle_prepare_late,
@@ -678,6 +714,7 @@ static const struct platform_s2idle_ops acpi_s2idle_ops_lps0 = {
 	.restore_early = acpi_s2idle_restore_early,
 	.restore = acpi_s2idle_restore,
 	.end = acpi_s2idle_end,
+	.sleep_exit = acpi_s2idle_sleep_exit,
 	.display_on = acpi_s2idle_display_on,
 };
 
-- 
2.47.0


From 1a183c7d73473e3f05dbab5a68dce3c0d304625f Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 20:11:00 +0200
Subject: [PATCH v1.4 099/120] [BEGIN] TKG, BORE, Sched_ext and HDR patches

TKG is from [1] and BORE and sched_ext are from [2].

Link: https://github.com/Frogging-Family/linux-tkg/blob/master/linux-tkg-patches/6.11/0012-misc-additions.patch [1]
Link: https://github.com/CachyOS/kernel-patches [2]
-- 
2.47.0


From 964f98b8220a9a7f4194968dc3434607b76d7775 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Felix=20H=C3=A4dicke?= <felixhaedicke@web.de>
Date: Thu, 19 Nov 2020 09:22:32 +0100
Subject: [PATCH v1.4 100/120] HID: quirks: Add Apple Magic Trackpad 2 to
 hid_have_special_driver list
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

The Apple Magic Trackpad 2 is handled by the magicmouse driver. And
there were severe stability issues when both drivers (hid-generic and
hid-magicmouse) were loaded for this device.

Fixes: https://bugzilla.kernel.org/show_bug.cgi?id=210241

Signed-off-by: Felix Hädicke <felixhaedicke@web.de>
---
 drivers/hid/hid-quirks.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/drivers/hid/hid-quirks.c b/drivers/hid/hid-quirks.c
index e0bbf0c6345d..5d229e2781c7 100644
--- a/drivers/hid/hid-quirks.c
+++ b/drivers/hid/hid-quirks.c
@@ -510,6 +510,8 @@ static const struct hid_device_id hid_have_special_driver[] = {
 #if IS_ENABLED(CONFIG_HID_MAGICMOUSE)
 	{ HID_BLUETOOTH_DEVICE(USB_VENDOR_ID_APPLE, USB_DEVICE_ID_APPLE_MAGICMOUSE) },
 	{ HID_BLUETOOTH_DEVICE(USB_VENDOR_ID_APPLE, USB_DEVICE_ID_APPLE_MAGICTRACKPAD) },
+	{ HID_BLUETOOTH_DEVICE(BT_VENDOR_ID_APPLE, USB_DEVICE_ID_APPLE_MAGICTRACKPAD2) },
+	{ HID_USB_DEVICE(USB_VENDOR_ID_APPLE, USB_DEVICE_ID_APPLE_MAGICTRACKPAD2) },
 #endif
 #if IS_ENABLED(CONFIG_HID_MAYFLASH)
 	{ HID_USB_DEVICE(USB_VENDOR_ID_DRAGONRISE, USB_DEVICE_ID_DRAGONRISE_PS3) },
-- 
2.47.0


From a48c884cc097a9c818e300ebe9aa270f7f62fa07 Mon Sep 17 00:00:00 2001
From: Tk-Glitch <ti3nou@gmail.com>
Date: Wed, 3 Feb 2021 11:20:12 +0200
Subject: [PATCH v1.4 101/120] Revert "cpufreq: Avoid configuring old governors
 as default with intel_pstate"

This is an undesirable behavior for us since our aggressive ondemand performs
better than schedutil for gaming when using intel_pstate in passive mode.
Also it interferes with the option to select the desired default governor we have.
---
 drivers/cpufreq/Kconfig | 2 --
 1 file changed, 2 deletions(-)

diff --git a/drivers/cpufreq/Kconfig b/drivers/cpufreq/Kconfig
index 10cda6f2fe1d..76a8e4741163 100644
--- a/drivers/cpufreq/Kconfig
+++ b/drivers/cpufreq/Kconfig
@@ -71,7 +71,6 @@ config CPU_FREQ_DEFAULT_GOV_USERSPACE
 
 config CPU_FREQ_DEFAULT_GOV_ONDEMAND
 	bool "ondemand"
-	depends on !(X86_INTEL_PSTATE && SMP)
 	select CPU_FREQ_GOV_ONDEMAND
 	select CPU_FREQ_GOV_PERFORMANCE
 	help
@@ -84,7 +83,6 @@ config CPU_FREQ_DEFAULT_GOV_ONDEMAND
 
 config CPU_FREQ_DEFAULT_GOV_CONSERVATIVE
 	bool "conservative"
-	depends on !(X86_INTEL_PSTATE && SMP)
 	select CPU_FREQ_GOV_CONSERVATIVE
 	select CPU_FREQ_GOV_PERFORMANCE
 	help
-- 
2.47.0


From 1e18433ceda75f60bdf15e6cafbf266a0fd95fc9 Mon Sep 17 00:00:00 2001
From: "Jan Alexander Steffens (heftig)" <heftig@archlinux.org>
Date: Sat, 13 Jan 2024 15:29:25 +0100
Subject: [PATCH v1.4 102/120] arch/Kconfig: Default to maximum amount of ASLR
 bits

To mitigate https://zolutal.github.io/aslrnt/; do this with a patch to
avoid having to enable `CONFIG_EXPERT`.
---
 arch/Kconfig | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/arch/Kconfig b/arch/Kconfig
index 975dd22a2dbd..de69b8f5b5be 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -1050,7 +1050,7 @@ config ARCH_MMAP_RND_BITS
 	int "Number of bits to use for ASLR of mmap base address" if EXPERT
 	range ARCH_MMAP_RND_BITS_MIN ARCH_MMAP_RND_BITS_MAX
 	default ARCH_MMAP_RND_BITS_DEFAULT if ARCH_MMAP_RND_BITS_DEFAULT
-	default ARCH_MMAP_RND_BITS_MIN
+	default ARCH_MMAP_RND_BITS_MAX
 	depends on HAVE_ARCH_MMAP_RND_BITS
 	help
 	  This value can be used to select the number of bits to use to
@@ -1084,7 +1084,7 @@ config ARCH_MMAP_RND_COMPAT_BITS
 	int "Number of bits to use for ASLR of mmap base address for compatible applications" if EXPERT
 	range ARCH_MMAP_RND_COMPAT_BITS_MIN ARCH_MMAP_RND_COMPAT_BITS_MAX
 	default ARCH_MMAP_RND_COMPAT_BITS_DEFAULT if ARCH_MMAP_RND_COMPAT_BITS_DEFAULT
-	default ARCH_MMAP_RND_COMPAT_BITS_MIN
+	default ARCH_MMAP_RND_COMPAT_BITS_MAX
 	depends on HAVE_ARCH_MMAP_RND_COMPAT_BITS
 	help
 	  This value can be used to select the number of bits to use to
-- 
2.47.0


From 5501f3f96df5b6b1446bb9bd43f7adcf98ded2cf Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Thu, 21 Mar 2024 19:00:50 +0100
Subject: [PATCH v1.4 103/120] cachy: move AMD_PRIVATE_COLOR to Kconfig

Co-authored-by: PedroHLC <root@pedrohlc.com>
Signed-off-by: Peter Jung <admin@ptr1337.dev>
---
 drivers/gpu/drm/amd/display/Kconfig                     | 6 ++++++
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c       | 2 +-
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_color.c | 2 +-
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c  | 6 +++---
 drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c | 6 +++---
 5 files changed, 14 insertions(+), 8 deletions(-)

diff --git a/drivers/gpu/drm/amd/display/Kconfig b/drivers/gpu/drm/amd/display/Kconfig
index df17e79c45c7..e454488c1a31 100644
--- a/drivers/gpu/drm/amd/display/Kconfig
+++ b/drivers/gpu/drm/amd/display/Kconfig
@@ -53,4 +53,10 @@ config DRM_AMD_SECURE_DISPLAY
 	  This option enables the calculation of crc of specific region via
 	  debugfs. Cooperate with specific DMCU FW.
 
+config AMD_PRIVATE_COLOR
+	bool "Enable KMS color management by AMD for AMD"
+	default n
+	help
+	  This option extends the KMS color management API with AMD driver-specific properties to enhance the color management support on AMD Steam Deck.
+
 endmenu
diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
index 9ca72b3ae912..a853625955bb 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c
@@ -4448,7 +4448,7 @@ static int amdgpu_dm_mode_config_init(struct amdgpu_device *adev)
 		return r;
 	}
 
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 	if (amdgpu_dm_create_color_properties(adev)) {
 		dc_state_release(state->context);
 		kfree(state);
diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_color.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_color.c
index ebabfe3a512f..4d3ebcaacca1 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_color.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_color.c
@@ -97,7 +97,7 @@ static inline struct fixed31_32 amdgpu_dm_fixpt_from_s3132(__u64 x)
 	return val;
 }
 
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 /* Pre-defined Transfer Functions (TF)
  *
  * AMD driver supports pre-defined mathematical functions for transferring
diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c
index 1726bd1f3de4..d5a3e1293bf5 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c
@@ -426,7 +426,7 @@ static int amdgpu_dm_crtc_late_register(struct drm_crtc *crtc)
 }
 #endif
 
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 /**
  * dm_crtc_additional_color_mgmt - enable additional color properties
  * @crtc: DRM CRTC
@@ -508,7 +508,7 @@ static const struct drm_crtc_funcs amdgpu_dm_crtc_funcs = {
 #if defined(CONFIG_DEBUG_FS)
 	.late_register = amdgpu_dm_crtc_late_register,
 #endif
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 	.atomic_set_property = amdgpu_dm_atomic_crtc_set_property,
 	.atomic_get_property = amdgpu_dm_atomic_crtc_get_property,
 #endif
@@ -675,7 +675,7 @@ int amdgpu_dm_crtc_init(struct amdgpu_display_manager *dm,
 
 	drm_mode_crtc_set_gamma_size(&acrtc->base, MAX_COLOR_LEGACY_LUT_ENTRIES);
 
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 	dm_crtc_additional_color_mgmt(&acrtc->base);
 #endif
 	return 0;
diff --git a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c
index a573a6639898..52e0e42e26a5 100644
--- a/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c
+++ b/drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c
@@ -1569,7 +1569,7 @@ static void amdgpu_dm_plane_drm_plane_destroy_state(struct drm_plane *plane,
 	drm_atomic_helper_plane_destroy_state(plane, state);
 }
 
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 static void
 dm_atomic_plane_attach_color_mgmt_properties(struct amdgpu_display_manager *dm,
 					     struct drm_plane *plane)
@@ -1760,7 +1760,7 @@ static const struct drm_plane_funcs dm_plane_funcs = {
 	.atomic_duplicate_state = amdgpu_dm_plane_drm_plane_duplicate_state,
 	.atomic_destroy_state = amdgpu_dm_plane_drm_plane_destroy_state,
 	.format_mod_supported = amdgpu_dm_plane_format_mod_supported,
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 	.atomic_set_property = dm_atomic_plane_set_property,
 	.atomic_get_property = dm_atomic_plane_get_property,
 #endif
@@ -1853,7 +1853,7 @@ int amdgpu_dm_plane_init(struct amdgpu_display_manager *dm,
 
 	drm_plane_helper_add(plane, &dm_plane_helper_funcs);
 
-#ifdef AMD_PRIVATE_COLOR
+#ifdef CONFIG_AMD_PRIVATE_COLOR
 	dm_atomic_plane_attach_color_mgmt_properties(dm, plane);
 #endif
 	/* Create (reset) the plane state */
-- 
2.47.0


From 0f272e788659e838ecb973cb94480d744b27eaa6 Mon Sep 17 00:00:00 2001
From: Tk-Glitch <ti3nou@gmail.com>
Date: Sat, 13 Apr 2024 18:25:35 +0530
Subject: [PATCH v1.4 104/120] revert "drm/amd/pm: Fetch current power limit
 from FW"

Fetching FW value prevents power limit modification by the user.
The "out-of-band ways" are unclear.
This fixes 7700/7800 XT custom powercap.
---
 drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c | 1 -
 1 file changed, 1 deletion(-)

diff --git a/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c b/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c
index 87672ca714de..d3ab0f9b5a97 100644
--- a/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c
+++ b/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c
@@ -2740,7 +2740,6 @@ int smu_get_power_limit(void *handle,
 		case SMU_PPT_LIMIT_CURRENT:
 			switch (amdgpu_ip_version(adev, MP1_HWIP, 0)) {
 			case IP_VERSION(13, 0, 2):
-			case IP_VERSION(13, 0, 6):
 			case IP_VERSION(13, 0, 14):
 			case IP_VERSION(11, 0, 7):
 			case IP_VERSION(11, 0, 11):
-- 
2.47.0


From 53a0aa73aa6de5f9e75ae51780a567f716099adf Mon Sep 17 00:00:00 2001
From: Javier Martinez Canillas <javierm@redhat.com>
Date: Thu, 19 May 2022 14:40:07 +0200
Subject: [PATCH v1.4 105/120] drivers/firmware: skip simpledrm if
 nvidia-drm.modeset=1 is set

The Nvidia proprietary driver has some bugs that leads to issues if used
with the simpledrm driver. The most noticeable is that does not register
an emulated fbdev device.

It just relies on a fbdev to be registered by another driver, that could
be that could be attached to the framebuffer console. On UEFI machines,
this is the efifb driver.

This means that disabling the efifb driver will cause virtual consoles to
not be present in the system when using the Nvidia driver. Legacy BIOS is
not affected just because fbcon is not used there, but instead vgacon.

Unless a VGA mode is specified using the vga= kernel command line option,
in that case the vesafb driver is used instead and its fbdev attached to
the fbcon.

This is a problem because with CONFIG_SYSFB_SIMPLEFB=y, the sysfb platform
code attempts to register a "simple-framebuffer" platform device (that is
matched against simpledrm) and only registers either an "efi-framebuffer"
or "vesa-framebuffer" if this fails to be registered due the video modes
not being compatible.

The Nvidia driver relying on another driver to register the fbdev is quite
fragile, since it can't really assume those will stick around. For example
there are patches posted to remove the EFI and VESA platform devices once
a real DRM or fbdev driver probes.

But in any case, moving to a simpledrm + emulated fbdev only breaks this
assumption and causes users to not have VT if the Nvidia driver is used.

So to prevent this, let's add a workaround and make the sysfb to skip the
"simple-framebuffer" registration when nvidia-drm.modeset=1 option is set.

This is quite horrible, but honestly I can't think of any other approach.

For this to work, the CONFIG_FB_EFI and CONFIG_FB_VESA config options must
be enabled besides CONFIG_DRM_SIMPLEDRM.

Signed-off-by: Javier Martinez Canillas <javierm@redhat.com>
Cherry-picked-for: https://bugs.archlinux.org/task/73720
---
 drivers/firmware/sysfb.c | 18 +++++++++++++++++-
 1 file changed, 17 insertions(+), 1 deletion(-)

diff --git a/drivers/firmware/sysfb.c b/drivers/firmware/sysfb.c
index a3df782fa687..940d8f514341 100644
--- a/drivers/firmware/sysfb.c
+++ b/drivers/firmware/sysfb.c
@@ -35,6 +35,22 @@
 #include <linux/screen_info.h>
 #include <linux/sysfb.h>
 
+static int skip_simpledrm;
+
+static int __init simpledrm_disable(char *opt)
+{
+	if (!opt)
+                return -EINVAL;
+
+	get_option(&opt, &skip_simpledrm);
+
+	if (skip_simpledrm)
+		pr_info("The simpledrm driver will not be probed\n");
+
+	return 0;
+}
+early_param("nvidia-drm.modeset", simpledrm_disable);
+
 static struct platform_device *pd;
 static DEFINE_MUTEX(disable_lock);
 static bool disabled;
@@ -145,7 +161,7 @@ static __init int sysfb_init(void)
 
 	/* try to create a simple-framebuffer device */
 	compatible = sysfb_parse_mode(si, &mode);
-	if (compatible) {
+	if (compatible && !skip_simpledrm) {
 		pd = sysfb_create_simplefb(si, &mode, parent);
 		if (!IS_ERR(pd))
 			goto put_device;
-- 
2.47.0


From a28b8a3eac3c8f5d955023786a7af7f101de78f3 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Thu, 10 Oct 2024 12:47:12 +0200
Subject: [PATCH v1.4 106/120] add sched-ext support

Signed-off-by: Peter Jung <admin@ptr1337.dev>
---
 Documentation/scheduler/index.rst             |    1 +
 Documentation/scheduler/sched-ext.rst         |  326 +
 MAINTAINERS                                   |   13 +
 drivers/tty/sysrq.c                           |    1 +
 include/asm-generic/vmlinux.lds.h             |    1 +
 include/linux/cgroup.h                        |    4 +-
 include/linux/sched.h                         |    5 +
 include/linux/sched/ext.h                     |  216 +
 include/linux/sched/task.h                    |    8 +-
 include/trace/events/sched_ext.h              |   32 +
 include/uapi/linux/sched.h                    |    1 +
 init/Kconfig                                  |   10 +
 init/init_task.c                              |   12 +
 kernel/Kconfig.preempt                        |   27 +-
 kernel/fork.c                                 |   17 +-
 kernel/sched/build_policy.c                   |   11 +
 kernel/sched/core.c                           |  296 +-
 kernel/sched/cpufreq_schedutil.c              |   50 +-
 kernel/sched/debug.c                          |    3 +
 kernel/sched/ext.c                            | 7281 +++++++++++++++++
 kernel/sched/ext.h                            |   91 +
 kernel/sched/fair.c                           |   21 +-
 kernel/sched/idle.c                           |    2 +
 kernel/sched/sched.h                          |  203 +-
 kernel/sched/syscalls.c                       |   26 +
 lib/dump_stack.c                              |    1 +
 tools/Makefile                                |   10 +-
 tools/sched_ext/.gitignore                    |    2 +
 tools/sched_ext/Makefile                      |  246 +
 tools/sched_ext/README.md                     |  270 +
 .../sched_ext/include/bpf-compat/gnu/stubs.h  |   11 +
 tools/sched_ext/include/scx/common.bpf.h      |  427 +
 tools/sched_ext/include/scx/common.h          |   75 +
 tools/sched_ext/include/scx/compat.bpf.h      |   47 +
 tools/sched_ext/include/scx/compat.h          |  186 +
 tools/sched_ext/include/scx/user_exit_info.h  |  115 +
 tools/sched_ext/scx_central.bpf.c             |  361 +
 tools/sched_ext/scx_central.c                 |  135 +
 tools/sched_ext/scx_flatcg.bpf.c              |  957 +++
 tools/sched_ext/scx_flatcg.c                  |  233 +
 tools/sched_ext/scx_flatcg.h                  |   51 +
 tools/sched_ext/scx_qmap.bpf.c                |  813 ++
 tools/sched_ext/scx_qmap.c                    |  153 +
 tools/sched_ext/scx_show_state.py             |   40 +
 tools/sched_ext/scx_simple.bpf.c              |  156 +
 tools/sched_ext/scx_simple.c                  |  107 +
 tools/testing/selftests/sched_ext/.gitignore  |    6 +
 tools/testing/selftests/sched_ext/Makefile    |  218 +
 tools/testing/selftests/sched_ext/config      |    9 +
 .../selftests/sched_ext/create_dsq.bpf.c      |   58 +
 .../testing/selftests/sched_ext/create_dsq.c  |   57 +
 .../sched_ext/ddsp_bogus_dsq_fail.bpf.c       |   42 +
 .../selftests/sched_ext/ddsp_bogus_dsq_fail.c |   57 +
 .../sched_ext/ddsp_vtimelocal_fail.bpf.c      |   39 +
 .../sched_ext/ddsp_vtimelocal_fail.c          |   56 +
 .../selftests/sched_ext/dsp_local_on.bpf.c    |   65 +
 .../selftests/sched_ext/dsp_local_on.c        |   58 +
 .../sched_ext/enq_last_no_enq_fails.bpf.c     |   21 +
 .../sched_ext/enq_last_no_enq_fails.c         |   60 +
 .../sched_ext/enq_select_cpu_fails.bpf.c      |   43 +
 .../sched_ext/enq_select_cpu_fails.c          |   61 +
 tools/testing/selftests/sched_ext/exit.bpf.c  |   84 +
 tools/testing/selftests/sched_ext/exit.c      |   55 +
 tools/testing/selftests/sched_ext/exit_test.h |   20 +
 .../testing/selftests/sched_ext/hotplug.bpf.c |   61 +
 tools/testing/selftests/sched_ext/hotplug.c   |  168 +
 .../selftests/sched_ext/hotplug_test.h        |   15 +
 .../sched_ext/init_enable_count.bpf.c         |   53 +
 .../selftests/sched_ext/init_enable_count.c   |  166 +
 .../testing/selftests/sched_ext/maximal.bpf.c |  164 +
 tools/testing/selftests/sched_ext/maximal.c   |   51 +
 .../selftests/sched_ext/maybe_null.bpf.c      |   36 +
 .../testing/selftests/sched_ext/maybe_null.c  |   49 +
 .../sched_ext/maybe_null_fail_dsp.bpf.c       |   25 +
 .../sched_ext/maybe_null_fail_yld.bpf.c       |   28 +
 .../testing/selftests/sched_ext/minimal.bpf.c |   21 +
 tools/testing/selftests/sched_ext/minimal.c   |   58 +
 .../selftests/sched_ext/prog_run.bpf.c        |   33 +
 tools/testing/selftests/sched_ext/prog_run.c  |   78 +
 .../testing/selftests/sched_ext/reload_loop.c |   75 +
 tools/testing/selftests/sched_ext/runner.c    |  201 +
 tools/testing/selftests/sched_ext/scx_test.h  |  131 +
 .../selftests/sched_ext/select_cpu_dfl.bpf.c  |   40 +
 .../selftests/sched_ext/select_cpu_dfl.c      |   72 +
 .../sched_ext/select_cpu_dfl_nodispatch.bpf.c |   89 +
 .../sched_ext/select_cpu_dfl_nodispatch.c     |   72 +
 .../sched_ext/select_cpu_dispatch.bpf.c       |   41 +
 .../selftests/sched_ext/select_cpu_dispatch.c |   70 +
 .../select_cpu_dispatch_bad_dsq.bpf.c         |   37 +
 .../sched_ext/select_cpu_dispatch_bad_dsq.c   |   56 +
 .../select_cpu_dispatch_dbl_dsp.bpf.c         |   38 +
 .../sched_ext/select_cpu_dispatch_dbl_dsp.c   |   56 +
 .../sched_ext/select_cpu_vtime.bpf.c          |   92 +
 .../selftests/sched_ext/select_cpu_vtime.c    |   59 +
 .../selftests/sched_ext/test_example.c        |   49 +
 tools/testing/selftests/sched_ext/util.c      |   71 +
 tools/testing/selftests/sched_ext/util.h      |   13 +
 97 files changed, 16201 insertions(+), 130 deletions(-)
 create mode 100644 Documentation/scheduler/sched-ext.rst
 create mode 100644 include/linux/sched/ext.h
 create mode 100644 include/trace/events/sched_ext.h
 create mode 100644 kernel/sched/ext.c
 create mode 100644 kernel/sched/ext.h
 create mode 100644 tools/sched_ext/.gitignore
 create mode 100644 tools/sched_ext/Makefile
 create mode 100644 tools/sched_ext/README.md
 create mode 100644 tools/sched_ext/include/bpf-compat/gnu/stubs.h
 create mode 100644 tools/sched_ext/include/scx/common.bpf.h
 create mode 100644 tools/sched_ext/include/scx/common.h
 create mode 100644 tools/sched_ext/include/scx/compat.bpf.h
 create mode 100644 tools/sched_ext/include/scx/compat.h
 create mode 100644 tools/sched_ext/include/scx/user_exit_info.h
 create mode 100644 tools/sched_ext/scx_central.bpf.c
 create mode 100644 tools/sched_ext/scx_central.c
 create mode 100644 tools/sched_ext/scx_flatcg.bpf.c
 create mode 100644 tools/sched_ext/scx_flatcg.c
 create mode 100644 tools/sched_ext/scx_flatcg.h
 create mode 100644 tools/sched_ext/scx_qmap.bpf.c
 create mode 100644 tools/sched_ext/scx_qmap.c
 create mode 100644 tools/sched_ext/scx_show_state.py
 create mode 100644 tools/sched_ext/scx_simple.bpf.c
 create mode 100644 tools/sched_ext/scx_simple.c
 create mode 100644 tools/testing/selftests/sched_ext/.gitignore
 create mode 100644 tools/testing/selftests/sched_ext/Makefile
 create mode 100644 tools/testing/selftests/sched_ext/config
 create mode 100644 tools/testing/selftests/sched_ext/create_dsq.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/create_dsq.c
 create mode 100644 tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.c
 create mode 100644 tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.c
 create mode 100644 tools/testing/selftests/sched_ext/dsp_local_on.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/dsp_local_on.c
 create mode 100644 tools/testing/selftests/sched_ext/enq_last_no_enq_fails.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/enq_last_no_enq_fails.c
 create mode 100644 tools/testing/selftests/sched_ext/enq_select_cpu_fails.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/enq_select_cpu_fails.c
 create mode 100644 tools/testing/selftests/sched_ext/exit.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/exit.c
 create mode 100644 tools/testing/selftests/sched_ext/exit_test.h
 create mode 100644 tools/testing/selftests/sched_ext/hotplug.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/hotplug.c
 create mode 100644 tools/testing/selftests/sched_ext/hotplug_test.h
 create mode 100644 tools/testing/selftests/sched_ext/init_enable_count.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/init_enable_count.c
 create mode 100644 tools/testing/selftests/sched_ext/maximal.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/maximal.c
 create mode 100644 tools/testing/selftests/sched_ext/maybe_null.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/maybe_null.c
 create mode 100644 tools/testing/selftests/sched_ext/maybe_null_fail_dsp.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/maybe_null_fail_yld.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/minimal.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/minimal.c
 create mode 100644 tools/testing/selftests/sched_ext/prog_run.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/prog_run.c
 create mode 100644 tools/testing/selftests/sched_ext/reload_loop.c
 create mode 100644 tools/testing/selftests/sched_ext/runner.c
 create mode 100644 tools/testing/selftests/sched_ext/scx_test.h
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dfl.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dfl.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dispatch.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dispatch.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_vtime.bpf.c
 create mode 100644 tools/testing/selftests/sched_ext/select_cpu_vtime.c
 create mode 100644 tools/testing/selftests/sched_ext/test_example.c
 create mode 100644 tools/testing/selftests/sched_ext/util.c
 create mode 100644 tools/testing/selftests/sched_ext/util.h

diff --git a/Documentation/scheduler/index.rst b/Documentation/scheduler/index.rst
index 43bd8a145b7a..0611dc3dda8e 100644
--- a/Documentation/scheduler/index.rst
+++ b/Documentation/scheduler/index.rst
@@ -20,6 +20,7 @@ Scheduler
     sched-nice-design
     sched-rt-group
     sched-stats
+    sched-ext
     sched-debug
 
     text_files
diff --git a/Documentation/scheduler/sched-ext.rst b/Documentation/scheduler/sched-ext.rst
new file mode 100644
index 000000000000..6c0d70e2e27d
--- /dev/null
+++ b/Documentation/scheduler/sched-ext.rst
@@ -0,0 +1,326 @@
+==========================
+Extensible Scheduler Class
+==========================
+
+sched_ext is a scheduler class whose behavior can be defined by a set of BPF
+programs - the BPF scheduler.
+
+* sched_ext exports a full scheduling interface so that any scheduling
+  algorithm can be implemented on top.
+
+* The BPF scheduler can group CPUs however it sees fit and schedule them
+  together, as tasks aren't tied to specific CPUs at the time of wakeup.
+
+* The BPF scheduler can be turned on and off dynamically anytime.
+
+* The system integrity is maintained no matter what the BPF scheduler does.
+  The default scheduling behavior is restored anytime an error is detected,
+  a runnable task stalls, or on invoking the SysRq key sequence
+  :kbd:`SysRq-S`.
+
+* When the BPF scheduler triggers an error, debug information is dumped to
+  aid debugging. The debug dump is passed to and printed out by the
+  scheduler binary. The debug dump can also be accessed through the
+  `sched_ext_dump` tracepoint. The SysRq key sequence :kbd:`SysRq-D`
+  triggers a debug dump. This doesn't terminate the BPF scheduler and can
+  only be read through the tracepoint.
+
+Switching to and from sched_ext
+===============================
+
+``CONFIG_SCHED_CLASS_EXT`` is the config option to enable sched_ext and
+``tools/sched_ext`` contains the example schedulers. The following config
+options should be enabled to use sched_ext:
+
+.. code-block:: none
+
+    CONFIG_BPF=y
+    CONFIG_SCHED_CLASS_EXT=y
+    CONFIG_BPF_SYSCALL=y
+    CONFIG_BPF_JIT=y
+    CONFIG_DEBUG_INFO_BTF=y
+    CONFIG_BPF_JIT_ALWAYS_ON=y
+    CONFIG_BPF_JIT_DEFAULT_ON=y
+    CONFIG_PAHOLE_HAS_SPLIT_BTF=y
+    CONFIG_PAHOLE_HAS_BTF_TAG=y
+
+sched_ext is used only when the BPF scheduler is loaded and running.
+
+If a task explicitly sets its scheduling policy to ``SCHED_EXT``, it will be
+treated as ``SCHED_NORMAL`` and scheduled by CFS until the BPF scheduler is
+loaded.
+
+When the BPF scheduler is loaded and ``SCX_OPS_SWITCH_PARTIAL`` is not set
+in ``ops->flags``, all ``SCHED_NORMAL``, ``SCHED_BATCH``, ``SCHED_IDLE``, and
+``SCHED_EXT`` tasks are scheduled by sched_ext.
+
+However, when the BPF scheduler is loaded and ``SCX_OPS_SWITCH_PARTIAL`` is
+set in ``ops->flags``, only tasks with the ``SCHED_EXT`` policy are scheduled
+by sched_ext, while tasks with ``SCHED_NORMAL``, ``SCHED_BATCH`` and
+``SCHED_IDLE`` policies are scheduled by CFS.
+
+Terminating the sched_ext scheduler program, triggering :kbd:`SysRq-S`, or
+detection of any internal error including stalled runnable tasks aborts the
+BPF scheduler and reverts all tasks back to CFS.
+
+.. code-block:: none
+
+    # make -j16 -C tools/sched_ext
+    # tools/sched_ext/scx_simple
+    local=0 global=3
+    local=5 global=24
+    local=9 global=44
+    local=13 global=56
+    local=17 global=72
+    ^CEXIT: BPF scheduler unregistered
+
+The current status of the BPF scheduler can be determined as follows:
+
+.. code-block:: none
+
+    # cat /sys/kernel/sched_ext/state
+    enabled
+    # cat /sys/kernel/sched_ext/root/ops
+    simple
+
+You can check if any BPF scheduler has ever been loaded since boot by examining
+this monotonically incrementing counter (a value of zero indicates that no BPF
+scheduler has been loaded):
+
+.. code-block:: none
+
+    # cat /sys/kernel/sched_ext/enable_seq
+    1
+
+``tools/sched_ext/scx_show_state.py`` is a drgn script which shows more
+detailed information:
+
+.. code-block:: none
+
+    # tools/sched_ext/scx_show_state.py
+    ops           : simple
+    enabled       : 1
+    switching_all : 1
+    switched_all  : 1
+    enable_state  : enabled (2)
+    bypass_depth  : 0
+    nr_rejected   : 0
+    enable_seq    : 1
+
+If ``CONFIG_SCHED_DEBUG`` is set, whether a given task is on sched_ext can
+be determined as follows:
+
+.. code-block:: none
+
+    # grep ext /proc/self/sched
+    ext.enabled                                  :                    1
+
+The Basics
+==========
+
+Userspace can implement an arbitrary BPF scheduler by loading a set of BPF
+programs that implement ``struct sched_ext_ops``. The only mandatory field
+is ``ops.name`` which must be a valid BPF object name. All operations are
+optional. The following modified excerpt is from
+``tools/sched_ext/scx_simple.bpf.c`` showing a minimal global FIFO scheduler.
+
+.. code-block:: c
+
+    /*
+     * Decide which CPU a task should be migrated to before being
+     * enqueued (either at wakeup, fork time, or exec time). If an
+     * idle core is found by the default ops.select_cpu() implementation,
+     * then dispatch the task directly to SCX_DSQ_LOCAL and skip the
+     * ops.enqueue() callback.
+     *
+     * Note that this implementation has exactly the same behavior as the
+     * default ops.select_cpu implementation. The behavior of the scheduler
+     * would be exactly same if the implementation just didn't define the
+     * simple_select_cpu() struct_ops prog.
+     */
+    s32 BPF_STRUCT_OPS(simple_select_cpu, struct task_struct *p,
+                       s32 prev_cpu, u64 wake_flags)
+    {
+            s32 cpu;
+            /* Need to initialize or the BPF verifier will reject the program */
+            bool direct = false;
+
+            cpu = scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags, &direct);
+
+            if (direct)
+                    scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, 0);
+
+            return cpu;
+    }
+
+    /*
+     * Do a direct dispatch of a task to the global DSQ. This ops.enqueue()
+     * callback will only be invoked if we failed to find a core to dispatch
+     * to in ops.select_cpu() above.
+     *
+     * Note that this implementation has exactly the same behavior as the
+     * default ops.enqueue implementation, which just dispatches the task
+     * to SCX_DSQ_GLOBAL. The behavior of the scheduler would be exactly same
+     * if the implementation just didn't define the simple_enqueue struct_ops
+     * prog.
+     */
+    void BPF_STRUCT_OPS(simple_enqueue, struct task_struct *p, u64 enq_flags)
+    {
+            scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
+    }
+
+    s32 BPF_STRUCT_OPS_SLEEPABLE(simple_init)
+    {
+            /*
+             * By default, all SCHED_EXT, SCHED_OTHER, SCHED_IDLE, and
+             * SCHED_BATCH tasks should use sched_ext.
+             */
+            return 0;
+    }
+
+    void BPF_STRUCT_OPS(simple_exit, struct scx_exit_info *ei)
+    {
+            exit_type = ei->type;
+    }
+
+    SEC(".struct_ops")
+    struct sched_ext_ops simple_ops = {
+            .select_cpu             = (void *)simple_select_cpu,
+            .enqueue                = (void *)simple_enqueue,
+            .init                   = (void *)simple_init,
+            .exit                   = (void *)simple_exit,
+            .name                   = "simple",
+    };
+
+Dispatch Queues
+---------------
+
+To match the impedance between the scheduler core and the BPF scheduler,
+sched_ext uses DSQs (dispatch queues) which can operate as both a FIFO and a
+priority queue. By default, there is one global FIFO (``SCX_DSQ_GLOBAL``),
+and one local dsq per CPU (``SCX_DSQ_LOCAL``). The BPF scheduler can manage
+an arbitrary number of dsq's using ``scx_bpf_create_dsq()`` and
+``scx_bpf_destroy_dsq()``.
+
+A CPU always executes a task from its local DSQ. A task is "dispatched" to a
+DSQ. A non-local DSQ is "consumed" to transfer a task to the consuming CPU's
+local DSQ.
+
+When a CPU is looking for the next task to run, if the local DSQ is not
+empty, the first task is picked. Otherwise, the CPU tries to consume the
+global DSQ. If that doesn't yield a runnable task either, ``ops.dispatch()``
+is invoked.
+
+Scheduling Cycle
+----------------
+
+The following briefly shows how a waking task is scheduled and executed.
+
+1. When a task is waking up, ``ops.select_cpu()`` is the first operation
+   invoked. This serves two purposes. First, CPU selection optimization
+   hint. Second, waking up the selected CPU if idle.
+
+   The CPU selected by ``ops.select_cpu()`` is an optimization hint and not
+   binding. The actual decision is made at the last step of scheduling.
+   However, there is a small performance gain if the CPU
+   ``ops.select_cpu()`` returns matches the CPU the task eventually runs on.
+
+   A side-effect of selecting a CPU is waking it up from idle. While a BPF
+   scheduler can wake up any cpu using the ``scx_bpf_kick_cpu()`` helper,
+   using ``ops.select_cpu()`` judiciously can be simpler and more efficient.
+
+   A task can be immediately dispatched to a DSQ from ``ops.select_cpu()`` by
+   calling ``scx_bpf_dispatch()``. If the task is dispatched to
+   ``SCX_DSQ_LOCAL`` from ``ops.select_cpu()``, it will be dispatched to the
+   local DSQ of whichever CPU is returned from ``ops.select_cpu()``.
+   Additionally, dispatching directly from ``ops.select_cpu()`` will cause the
+   ``ops.enqueue()`` callback to be skipped.
+
+   Note that the scheduler core will ignore an invalid CPU selection, for
+   example, if it's outside the allowed cpumask of the task.
+
+2. Once the target CPU is selected, ``ops.enqueue()`` is invoked (unless the
+   task was dispatched directly from ``ops.select_cpu()``). ``ops.enqueue()``
+   can make one of the following decisions:
+
+   * Immediately dispatch the task to either the global or local DSQ by
+     calling ``scx_bpf_dispatch()`` with ``SCX_DSQ_GLOBAL`` or
+     ``SCX_DSQ_LOCAL``, respectively.
+
+   * Immediately dispatch the task to a custom DSQ by calling
+     ``scx_bpf_dispatch()`` with a DSQ ID which is smaller than 2^63.
+
+   * Queue the task on the BPF side.
+
+3. When a CPU is ready to schedule, it first looks at its local DSQ. If
+   empty, it then looks at the global DSQ. If there still isn't a task to
+   run, ``ops.dispatch()`` is invoked which can use the following two
+   functions to populate the local DSQ.
+
+   * ``scx_bpf_dispatch()`` dispatches a task to a DSQ. Any target DSQ can
+     be used - ``SCX_DSQ_LOCAL``, ``SCX_DSQ_LOCAL_ON | cpu``,
+     ``SCX_DSQ_GLOBAL`` or a custom DSQ. While ``scx_bpf_dispatch()``
+     currently can't be called with BPF locks held, this is being worked on
+     and will be supported. ``scx_bpf_dispatch()`` schedules dispatching
+     rather than performing them immediately. There can be up to
+     ``ops.dispatch_max_batch`` pending tasks.
+
+   * ``scx_bpf_consume()`` tranfers a task from the specified non-local DSQ
+     to the dispatching DSQ. This function cannot be called with any BPF
+     locks held. ``scx_bpf_consume()`` flushes the pending dispatched tasks
+     before trying to consume the specified DSQ.
+
+4. After ``ops.dispatch()`` returns, if there are tasks in the local DSQ,
+   the CPU runs the first one. If empty, the following steps are taken:
+
+   * Try to consume the global DSQ. If successful, run the task.
+
+   * If ``ops.dispatch()`` has dispatched any tasks, retry #3.
+
+   * If the previous task is an SCX task and still runnable, keep executing
+     it (see ``SCX_OPS_ENQ_LAST``).
+
+   * Go idle.
+
+Note that the BPF scheduler can always choose to dispatch tasks immediately
+in ``ops.enqueue()`` as illustrated in the above simple example. If only the
+built-in DSQs are used, there is no need to implement ``ops.dispatch()`` as
+a task is never queued on the BPF scheduler and both the local and global
+DSQs are consumed automatically.
+
+``scx_bpf_dispatch()`` queues the task on the FIFO of the target DSQ. Use
+``scx_bpf_dispatch_vtime()`` for the priority queue. Internal DSQs such as
+``SCX_DSQ_LOCAL`` and ``SCX_DSQ_GLOBAL`` do not support priority-queue
+dispatching, and must be dispatched to with ``scx_bpf_dispatch()``.  See the
+function documentation and usage in ``tools/sched_ext/scx_simple.bpf.c`` for
+more information.
+
+Where to Look
+=============
+
+* ``include/linux/sched/ext.h`` defines the core data structures, ops table
+  and constants.
+
+* ``kernel/sched/ext.c`` contains sched_ext core implementation and helpers.
+  The functions prefixed with ``scx_bpf_`` can be called from the BPF
+  scheduler.
+
+* ``tools/sched_ext/`` hosts example BPF scheduler implementations.
+
+  * ``scx_simple[.bpf].c``: Minimal global FIFO scheduler example using a
+    custom DSQ.
+
+  * ``scx_qmap[.bpf].c``: A multi-level FIFO scheduler supporting five
+    levels of priority implemented with ``BPF_MAP_TYPE_QUEUE``.
+
+ABI Instability
+===============
+
+The APIs provided by sched_ext to BPF schedulers programs have no stability
+guarantees. This includes the ops table callbacks and constants defined in
+``include/linux/sched/ext.h``, as well as the ``scx_bpf_`` kfuncs defined in
+``kernel/sched/ext.c``.
+
+While we will attempt to provide a relatively stable API surface when
+possible, they are subject to change without warning between kernel
+versions.
diff --git a/MAINTAINERS b/MAINTAINERS
index cc40a9d9b8cd..c6708612e199 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -20338,6 +20338,19 @@ F:	include/linux/wait.h
 F:	include/uapi/linux/sched.h
 F:	kernel/sched/
 
+SCHEDULER - SCHED_EXT
+R:	Tejun Heo <tj@kernel.org>
+R:	David Vernet <void@manifault.com>
+L:	linux-kernel@vger.kernel.org
+S:	Maintained
+W:	https://github.com/sched-ext/scx
+T:	git://git.kernel.org/pub/scm/linux/kernel/git/tj/sched_ext.git
+F:	include/linux/sched/ext.h
+F:	kernel/sched/ext.h
+F:	kernel/sched/ext.c
+F:	tools/sched_ext/
+F:	tools/testing/selftests/sched_ext
+
 SCIOSENSE ENS160 MULTI-GAS SENSOR DRIVER
 M:	Gustavo Silva <gustavograzs@gmail.com>
 S:	Maintained
diff --git a/drivers/tty/sysrq.c b/drivers/tty/sysrq.c
index 14f8f00fdcf9..930b04e3d148 100644
--- a/drivers/tty/sysrq.c
+++ b/drivers/tty/sysrq.c
@@ -531,6 +531,7 @@ static const struct sysrq_key_op *sysrq_key_table[62] = {
 	NULL,				/* P */
 	NULL,				/* Q */
 	&sysrq_replay_logs_op,		/* R */
+	/* S: May be registered by sched_ext for resetting */
 	NULL,				/* S */
 	NULL,				/* T */
 	NULL,				/* U */
diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index 1ae44793132a..19ec49a9179b 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -133,6 +133,7 @@
 	*(__dl_sched_class)			\
 	*(__rt_sched_class)			\
 	*(__fair_sched_class)			\
+	*(__ext_sched_class)			\
 	*(__idle_sched_class)			\
 	__sched_class_lowest = .;
 
diff --git a/include/linux/cgroup.h b/include/linux/cgroup.h
index c60ba0ab1462..7139b33cb104 100644
--- a/include/linux/cgroup.h
+++ b/include/linux/cgroup.h
@@ -28,8 +28,6 @@
 
 struct kernel_clone_args;
 
-#ifdef CONFIG_CGROUPS
-
 /*
  * All weight knobs on the default hierarchy should use the following min,
  * default and max values.  The default value is the logarithmic center of
@@ -39,6 +37,8 @@ struct kernel_clone_args;
 #define CGROUP_WEIGHT_DFL		100
 #define CGROUP_WEIGHT_MAX		10000
 
+#ifdef CONFIG_CGROUPS
+
 enum {
 	CSS_TASK_ITER_PROCS    = (1U << 0),  /* walk only threadgroup leaders */
 	CSS_TASK_ITER_THREADED = (1U << 1),  /* walk all threaded css_sets in the domain */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1c771ea4481d..c5a7901b2580 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -82,6 +82,8 @@ struct task_group;
 struct task_struct;
 struct user_event_mm;
 
+#include <linux/sched/ext.h>
+
 /*
  * Task state bitmask. NOTE! These bits are also
  * encoded in fs/proc/array.c: get_task_state().
@@ -812,6 +814,9 @@ struct task_struct {
 	struct sched_rt_entity		rt;
 	struct sched_dl_entity		dl;
 	struct sched_dl_entity		*dl_server;
+#ifdef CONFIG_SCHED_CLASS_EXT
+	struct sched_ext_entity		scx;
+#endif
 	const struct sched_class	*sched_class;
 
 #ifdef CONFIG_SCHED_CORE
diff --git a/include/linux/sched/ext.h b/include/linux/sched/ext.h
new file mode 100644
index 000000000000..76166d3b14fc
--- /dev/null
+++ b/include/linux/sched/ext.h
@@ -0,0 +1,216 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * BPF extensible scheduler class: Documentation/scheduler/sched-ext.rst
+ *
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#ifndef _LINUX_SCHED_EXT_H
+#define _LINUX_SCHED_EXT_H
+
+#ifdef CONFIG_SCHED_CLASS_EXT
+
+#include <linux/llist.h>
+#include <linux/rhashtable-types.h>
+
+enum scx_public_consts {
+	SCX_OPS_NAME_LEN	= 128,
+
+	SCX_SLICE_DFL		= 20 * 1000000,	/* 20ms */
+	SCX_SLICE_INF		= U64_MAX,	/* infinite, implies nohz */
+};
+
+/*
+ * DSQ (dispatch queue) IDs are 64bit of the format:
+ *
+ *   Bits: [63] [62 ..  0]
+ *         [ B] [   ID   ]
+ *
+ *    B: 1 for IDs for built-in DSQs, 0 for ops-created user DSQs
+ *   ID: 63 bit ID
+ *
+ * Built-in IDs:
+ *
+ *   Bits: [63] [62] [61..32] [31 ..  0]
+ *         [ 1] [ L] [   R  ] [    V   ]
+ *
+ *    1: 1 for built-in DSQs.
+ *    L: 1 for LOCAL_ON DSQ IDs, 0 for others
+ *    V: For LOCAL_ON DSQ IDs, a CPU number. For others, a pre-defined value.
+ */
+enum scx_dsq_id_flags {
+	SCX_DSQ_FLAG_BUILTIN	= 1LLU << 63,
+	SCX_DSQ_FLAG_LOCAL_ON	= 1LLU << 62,
+
+	SCX_DSQ_INVALID		= SCX_DSQ_FLAG_BUILTIN | 0,
+	SCX_DSQ_GLOBAL		= SCX_DSQ_FLAG_BUILTIN | 1,
+	SCX_DSQ_LOCAL		= SCX_DSQ_FLAG_BUILTIN | 2,
+	SCX_DSQ_LOCAL_ON	= SCX_DSQ_FLAG_BUILTIN | SCX_DSQ_FLAG_LOCAL_ON,
+	SCX_DSQ_LOCAL_CPU_MASK	= 0xffffffffLLU,
+};
+
+/*
+ * A dispatch queue (DSQ) can be either a FIFO or p->scx.dsq_vtime ordered
+ * queue. A built-in DSQ is always a FIFO. The built-in local DSQs are used to
+ * buffer between the scheduler core and the BPF scheduler. See the
+ * documentation for more details.
+ */
+struct scx_dispatch_q {
+	raw_spinlock_t		lock;
+	struct list_head	list;	/* tasks in dispatch order */
+	struct rb_root		priq;	/* used to order by p->scx.dsq_vtime */
+	u32			nr;
+	u32			seq;	/* used by BPF iter */
+	u64			id;
+	struct rhash_head	hash_node;
+	struct llist_node	free_node;
+	struct rcu_head		rcu;
+};
+
+/* scx_entity.flags */
+enum scx_ent_flags {
+	SCX_TASK_QUEUED		= 1 << 0, /* on ext runqueue */
+	SCX_TASK_BAL_KEEP	= 1 << 1, /* balance decided to keep current */
+	SCX_TASK_RESET_RUNNABLE_AT = 1 << 2, /* runnable_at should be reset */
+	SCX_TASK_DEQD_FOR_SLEEP	= 1 << 3, /* last dequeue was for SLEEP */
+
+	SCX_TASK_STATE_SHIFT	= 8,	  /* bit 8 and 9 are used to carry scx_task_state */
+	SCX_TASK_STATE_BITS	= 2,
+	SCX_TASK_STATE_MASK	= ((1 << SCX_TASK_STATE_BITS) - 1) << SCX_TASK_STATE_SHIFT,
+
+	SCX_TASK_CURSOR		= 1 << 31, /* iteration cursor, not a task */
+};
+
+/* scx_entity.flags & SCX_TASK_STATE_MASK */
+enum scx_task_state {
+	SCX_TASK_NONE,		/* ops.init_task() not called yet */
+	SCX_TASK_INIT,		/* ops.init_task() succeeded, but task can be cancelled */
+	SCX_TASK_READY,		/* fully initialized, but not in sched_ext */
+	SCX_TASK_ENABLED,	/* fully initialized and in sched_ext */
+
+	SCX_TASK_NR_STATES,
+};
+
+/* scx_entity.dsq_flags */
+enum scx_ent_dsq_flags {
+	SCX_TASK_DSQ_ON_PRIQ	= 1 << 0, /* task is queued on the priority queue of a dsq */
+};
+
+/*
+ * Mask bits for scx_entity.kf_mask. Not all kfuncs can be called from
+ * everywhere and the following bits track which kfunc sets are currently
+ * allowed for %current. This simple per-task tracking works because SCX ops
+ * nest in a limited way. BPF will likely implement a way to allow and disallow
+ * kfuncs depending on the calling context which will replace this manual
+ * mechanism. See scx_kf_allow().
+ */
+enum scx_kf_mask {
+	SCX_KF_UNLOCKED		= 0,	  /* sleepable and not rq locked */
+	/* ENQUEUE and DISPATCH may be nested inside CPU_RELEASE */
+	SCX_KF_CPU_RELEASE	= 1 << 0, /* ops.cpu_release() */
+	/* ops.dequeue (in REST) may be nested inside DISPATCH */
+	SCX_KF_DISPATCH		= 1 << 1, /* ops.dispatch() */
+	SCX_KF_ENQUEUE		= 1 << 2, /* ops.enqueue() and ops.select_cpu() */
+	SCX_KF_SELECT_CPU	= 1 << 3, /* ops.select_cpu() */
+	SCX_KF_REST		= 1 << 4, /* other rq-locked operations */
+
+	__SCX_KF_RQ_LOCKED	= SCX_KF_CPU_RELEASE | SCX_KF_DISPATCH |
+				  SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU | SCX_KF_REST,
+	__SCX_KF_TERMINAL	= SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU | SCX_KF_REST,
+};
+
+enum scx_dsq_lnode_flags {
+	SCX_DSQ_LNODE_ITER_CURSOR = 1 << 0,
+
+	/* high 16 bits can be for iter cursor flags */
+	__SCX_DSQ_LNODE_PRIV_SHIFT = 16,
+};
+
+struct scx_dsq_list_node {
+	struct list_head	node;
+	u32			flags;
+	u32			priv;		/* can be used by iter cursor */
+};
+
+/*
+ * The following is embedded in task_struct and contains all fields necessary
+ * for a task to be scheduled by SCX.
+ */
+struct sched_ext_entity {
+	struct scx_dispatch_q	*dsq;
+	struct scx_dsq_list_node dsq_list;	/* dispatch order */
+	struct rb_node		dsq_priq;	/* p->scx.dsq_vtime order */
+	u32			dsq_seq;
+	u32			dsq_flags;	/* protected by DSQ lock */
+	u32			flags;		/* protected by rq lock */
+	u32			weight;
+	s32			sticky_cpu;
+	s32			holding_cpu;
+	u32			kf_mask;	/* see scx_kf_mask above */
+	struct task_struct	*kf_tasks[2];	/* see SCX_CALL_OP_TASK() */
+	atomic_long_t		ops_state;
+
+	struct list_head	runnable_node;	/* rq->scx.runnable_list */
+	unsigned long		runnable_at;
+
+#ifdef CONFIG_SCHED_CORE
+	u64			core_sched_at;	/* see scx_prio_less() */
+#endif
+	u64			ddsp_dsq_id;
+	u64			ddsp_enq_flags;
+
+	/* BPF scheduler modifiable fields */
+
+	/*
+	 * Runtime budget in nsecs. This is usually set through
+	 * scx_bpf_dispatch() but can also be modified directly by the BPF
+	 * scheduler. Automatically decreased by SCX as the task executes. On
+	 * depletion, a scheduling event is triggered.
+	 *
+	 * This value is cleared to zero if the task is preempted by
+	 * %SCX_KICK_PREEMPT and shouldn't be used to determine how long the
+	 * task ran. Use p->se.sum_exec_runtime instead.
+	 */
+	u64			slice;
+
+	/*
+	 * Used to order tasks when dispatching to the vtime-ordered priority
+	 * queue of a dsq. This is usually set through scx_bpf_dispatch_vtime()
+	 * but can also be modified directly by the BPF scheduler. Modifying it
+	 * while a task is queued on a dsq may mangle the ordering and is not
+	 * recommended.
+	 */
+	u64			dsq_vtime;
+
+	/*
+	 * If set, reject future sched_setscheduler(2) calls updating the policy
+	 * to %SCHED_EXT with -%EACCES.
+	 *
+	 * Can be set from ops.init_task() while the BPF scheduler is being
+	 * loaded (!scx_init_task_args->fork). If set and the task's policy is
+	 * already %SCHED_EXT, the task's policy is rejected and forcefully
+	 * reverted to %SCHED_NORMAL. The number of such events are reported
+	 * through /sys/kernel/debug/sched_ext::nr_rejected. Setting this flag
+	 * during fork is not allowed.
+	 */
+	bool			disallow;	/* reject switching into SCX */
+
+	/* cold fields */
+#ifdef CONFIG_EXT_GROUP_SCHED
+	struct cgroup		*cgrp_moving_from;
+#endif
+	/* must be the last field, see init_scx_entity() */
+	struct list_head	tasks_node;
+};
+
+void sched_ext_free(struct task_struct *p);
+void print_scx_info(const char *log_lvl, struct task_struct *p);
+
+#else	/* !CONFIG_SCHED_CLASS_EXT */
+
+static inline void sched_ext_free(struct task_struct *p) {}
+static inline void print_scx_info(const char *log_lvl, struct task_struct *p) {}
+
+#endif	/* CONFIG_SCHED_CLASS_EXT */
+#endif	/* _LINUX_SCHED_EXT_H */
diff --git a/include/linux/sched/task.h b/include/linux/sched/task.h
index d362aacf9f89..0f2aeb37bbb0 100644
--- a/include/linux/sched/task.h
+++ b/include/linux/sched/task.h
@@ -63,7 +63,8 @@ extern asmlinkage void schedule_tail(struct task_struct *prev);
 extern void init_idle(struct task_struct *idle, int cpu);
 
 extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
-extern void sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs);
+extern int sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs);
+extern void sched_cancel_fork(struct task_struct *p);
 extern void sched_post_fork(struct task_struct *p);
 extern void sched_dead(struct task_struct *p);
 
@@ -119,6 +120,11 @@ static inline struct task_struct *get_task_struct(struct task_struct *t)
 	return t;
 }
 
+static inline struct task_struct *tryget_task_struct(struct task_struct *t)
+{
+	return refcount_inc_not_zero(&t->usage) ? t : NULL;
+}
+
 extern void __put_task_struct(struct task_struct *t);
 extern void __put_task_struct_rcu_cb(struct rcu_head *rhp);
 
diff --git a/include/trace/events/sched_ext.h b/include/trace/events/sched_ext.h
new file mode 100644
index 000000000000..fe19da7315a9
--- /dev/null
+++ b/include/trace/events/sched_ext.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM sched_ext
+
+#if !defined(_TRACE_SCHED_EXT_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_SCHED_EXT_H
+
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(sched_ext_dump,
+
+	TP_PROTO(const char *line),
+
+	TP_ARGS(line),
+
+	TP_STRUCT__entry(
+		__string(line, line)
+	),
+
+	TP_fast_assign(
+		__assign_str(line);
+	),
+
+	TP_printk("%s",
+		__get_str(line)
+	)
+);
+
+#endif /* _TRACE_SCHED_EXT_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index 3bac0a8ceab2..359a14cc76a4 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -118,6 +118,7 @@ struct clone_args {
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
 #define SCHED_DEADLINE		6
+#define SCHED_EXT		7
 
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
diff --git a/init/Kconfig b/init/Kconfig
index 5783a0b87517..34cfb0d41b26 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1024,9 +1024,13 @@ menuconfig CGROUP_SCHED
 	  tasks.
 
 if CGROUP_SCHED
+config GROUP_SCHED_WEIGHT
+	def_bool n
+
 config FAIR_GROUP_SCHED
 	bool "Group scheduling for SCHED_OTHER"
 	depends on CGROUP_SCHED
+	select GROUP_SCHED_WEIGHT
 	default CGROUP_SCHED
 
 config CFS_BANDWIDTH
@@ -1051,6 +1055,12 @@ config RT_GROUP_SCHED
 	  realtime bandwidth for them.
 	  See Documentation/scheduler/sched-rt-group.rst for more information.
 
+config EXT_GROUP_SCHED
+	bool
+	depends on SCHED_CLASS_EXT && CGROUP_SCHED
+	select GROUP_SCHED_WEIGHT
+	default y
+
 endif #CGROUP_SCHED
 
 config SCHED_MM_CID
diff --git a/init/init_task.c b/init/init_task.c
index eeb110c65fe2..e222722e790b 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -6,6 +6,7 @@
 #include <linux/sched/sysctl.h>
 #include <linux/sched/rt.h>
 #include <linux/sched/task.h>
+#include <linux/sched/ext.h>
 #include <linux/init.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
@@ -98,6 +99,17 @@ struct task_struct init_task __aligned(L1_CACHE_BYTES) = {
 #endif
 #ifdef CONFIG_CGROUP_SCHED
 	.sched_task_group = &root_task_group,
+#endif
+#ifdef CONFIG_SCHED_CLASS_EXT
+	.scx		= {
+		.dsq_list.node	= LIST_HEAD_INIT(init_task.scx.dsq_list.node),
+		.sticky_cpu	= -1,
+		.holding_cpu	= -1,
+		.runnable_node	= LIST_HEAD_INIT(init_task.scx.runnable_node),
+		.runnable_at	= INITIAL_JIFFIES,
+		.ddsp_dsq_id	= SCX_DSQ_INVALID,
+		.slice		= SCX_SLICE_DFL,
+	},
 #endif
 	.ptraced	= LIST_HEAD_INIT(init_task.ptraced),
 	.ptrace_entry	= LIST_HEAD_INIT(init_task.ptrace_entry),
diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index c2f1fd95a821..fe782cd77388 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -133,4 +133,29 @@ config SCHED_CORE
 	  which is the likely usage by Linux distributions, there should
 	  be no measurable impact on performance.
 
-
+config SCHED_CLASS_EXT
+	bool "Extensible Scheduling Class"
+	depends on BPF_SYSCALL && BPF_JIT && DEBUG_INFO_BTF
+	select STACKTRACE if STACKTRACE_SUPPORT
+	help
+	  This option enables a new scheduler class sched_ext (SCX), which
+	  allows scheduling policies to be implemented as BPF programs to
+	  achieve the following:
+
+	  - Ease of experimentation and exploration: Enabling rapid
+	    iteration of new scheduling policies.
+	  - Customization: Building application-specific schedulers which
+	    implement policies that are not applicable to general-purpose
+	    schedulers.
+	  - Rapid scheduler deployments: Non-disruptive swap outs of
+	    scheduling policies in production environments.
+
+	  sched_ext leverages BPF struct_ops feature to define a structure
+	  which exports function callbacks and flags to BPF programs that
+	  wish to implement scheduling policies. The struct_ops structure
+	  exported by sched_ext is struct sched_ext_ops, and is conceptually
+	  similar to struct sched_class.
+
+	  For more information:
+	    Documentation/scheduler/sched-ext.rst
+	    https://github.com/sched-ext/scx
diff --git a/kernel/fork.c b/kernel/fork.c
index 6b97fb2ac4af..0844b59dc082 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -23,6 +23,7 @@
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/sched/cputime.h>
+#include <linux/sched/ext.h>
 #include <linux/seq_file.h>
 #include <linux/rtmutex.h>
 #include <linux/init.h>
@@ -969,6 +970,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(refcount_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	sched_ext_free(tsk);
 	io_uring_free(tsk);
 	cgroup_free(tsk);
 	task_numa_free(tsk, true);
@@ -2344,7 +2346,7 @@ __latent_entropy struct task_struct *copy_process(
 
 	retval = perf_event_init_task(p, clone_flags);
 	if (retval)
-		goto bad_fork_cleanup_policy;
+		goto bad_fork_sched_cancel_fork;
 	retval = audit_alloc(p);
 	if (retval)
 		goto bad_fork_cleanup_perf;
@@ -2477,7 +2479,9 @@ __latent_entropy struct task_struct *copy_process(
 	 * cgroup specific, it unconditionally needs to place the task on a
 	 * runqueue.
 	 */
-	sched_cgroup_fork(p, args);
+	retval = sched_cgroup_fork(p, args);
+	if (retval)
+		goto bad_fork_cancel_cgroup;
 
 	/*
 	 * From this point on we must avoid any synchronous user-space
@@ -2523,13 +2527,13 @@ __latent_entropy struct task_struct *copy_process(
 	/* Don't start children in a dying pid namespace */
 	if (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {
 		retval = -ENOMEM;
-		goto bad_fork_cancel_cgroup;
+		goto bad_fork_core_free;
 	}
 
 	/* Let kill terminate clone/fork in the middle */
 	if (fatal_signal_pending(current)) {
 		retval = -EINTR;
-		goto bad_fork_cancel_cgroup;
+		goto bad_fork_core_free;
 	}
 
 	/* No more failure paths after this point. */
@@ -2603,10 +2607,11 @@ __latent_entropy struct task_struct *copy_process(
 
 	return p;
 
-bad_fork_cancel_cgroup:
+bad_fork_core_free:
 	sched_core_free(p);
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
+bad_fork_cancel_cgroup:
 	cgroup_cancel_fork(p, args);
 bad_fork_put_pidfd:
 	if (clone_flags & CLONE_PIDFD) {
@@ -2645,6 +2650,8 @@ __latent_entropy struct task_struct *copy_process(
 	audit_free(p);
 bad_fork_cleanup_perf:
 	perf_event_free_task(p);
+bad_fork_sched_cancel_fork:
+	sched_cancel_fork(p);
 bad_fork_cleanup_policy:
 	lockdep_free_task(p);
 #ifdef CONFIG_NUMA
diff --git a/kernel/sched/build_policy.c b/kernel/sched/build_policy.c
index 39c315182b35..fae1f5c921eb 100644
--- a/kernel/sched/build_policy.c
+++ b/kernel/sched/build_policy.c
@@ -16,18 +16,25 @@
 #include <linux/sched/clock.h>
 #include <linux/sched/cputime.h>
 #include <linux/sched/hotplug.h>
+#include <linux/sched/isolation.h>
 #include <linux/sched/posix-timers.h>
 #include <linux/sched/rt.h>
 
 #include <linux/cpuidle.h>
 #include <linux/jiffies.h>
+#include <linux/kobject.h>
 #include <linux/livepatch.h>
+#include <linux/pm.h>
 #include <linux/psi.h>
+#include <linux/rhashtable.h>
+#include <linux/seq_buf.h>
 #include <linux/seqlock_api.h>
 #include <linux/slab.h>
 #include <linux/suspend.h>
 #include <linux/tsacct_kern.h>
 #include <linux/vtime.h>
+#include <linux/sysrq.h>
+#include <linux/percpu-rwsem.h>
 
 #include <uapi/linux/sched/types.h>
 
@@ -52,4 +59,8 @@
 #include "cputime.c"
 #include "deadline.c"
 
+#ifdef CONFIG_SCHED_CLASS_EXT
+# include "ext.c"
+#endif
+
 #include "syscalls.c"
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 1af59cf714cd..b6a3fbccf5d6 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -169,7 +169,10 @@ static inline int __task_prio(const struct task_struct *p)
 	if (p->sched_class == &idle_sched_class)
 		return MAX_RT_PRIO + NICE_WIDTH; /* 140 */
 
-	return MAX_RT_PRIO + MAX_NICE; /* 120, squash fair */
+	if (task_on_scx(p))
+		return MAX_RT_PRIO + MAX_NICE + 1; /* 120, squash ext */
+
+	return MAX_RT_PRIO + MAX_NICE; /* 119, squash fair */
 }
 
 /*
@@ -198,6 +201,11 @@ static inline bool prio_less(const struct task_struct *a,
 	if (pa == MAX_RT_PRIO + MAX_NICE)	/* fair */
 		return cfs_prio_less(a, b, in_fi);
 
+#ifdef CONFIG_SCHED_CLASS_EXT
+	if (pa == MAX_RT_PRIO + MAX_NICE + 1)	/* ext */
+		return scx_prio_less(a, b, in_fi);
+#endif
+
 	return false;
 }
 
@@ -1255,11 +1263,14 @@ bool sched_can_stop_tick(struct rq *rq)
 		return true;
 
 	/*
-	 * If there are no DL,RR/FIFO tasks, there must only be CFS tasks left;
-	 * if there's more than one we need the tick for involuntary
-	 * preemption.
+	 * If there are no DL,RR/FIFO tasks, there must only be CFS or SCX tasks
+	 * left. For CFS, if there's more than one we need the tick for
+	 * involuntary preemption. For SCX, ask.
 	 */
-	if (rq->nr_running > 1)
+	if (scx_enabled() && !scx_can_stop_tick(rq))
+		return false;
+
+	if (rq->cfs.nr_running > 1)
 		return false;
 
 	/*
@@ -1341,8 +1352,8 @@ void set_load_weight(struct task_struct *p, bool update_load)
 	 * SCHED_OTHER tasks have to update their load when changing their
 	 * weight
 	 */
-	if (update_load && p->sched_class == &fair_sched_class)
-		reweight_task(p, &lw);
+	if (update_load && p->sched_class->reweight_task)
+		p->sched_class->reweight_task(task_rq(p), p, &lw);
 	else
 		p->se.load = lw;
 }
@@ -2031,6 +2042,17 @@ inline int task_curr(const struct task_struct *p)
 	return cpu_curr(task_cpu(p)) == p;
 }
 
+/*
+ * ->switching_to() is called with the pi_lock and rq_lock held and must not
+ * mess with locking.
+ */
+void check_class_changing(struct rq *rq, struct task_struct *p,
+			  const struct sched_class *prev_class)
+{
+	if (prev_class != p->sched_class && p->sched_class->switching_to)
+		p->sched_class->switching_to(rq, p);
+}
+
 /*
  * switched_from, switched_to and prio_changed must _NOT_ drop rq->lock,
  * use the balance_callback list if you want balancing.
@@ -2289,7 +2311,7 @@ static inline bool rq_has_pinned_tasks(struct rq *rq)
 static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
 {
 	/* When not in the task's cpumask, no point in looking further. */
-	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
+	if (!task_allowed_on_cpu(p, cpu))
 		return false;
 
 	/* migrate_disabled() must be allowed to finish. */
@@ -2298,7 +2320,7 @@ static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
 
 	/* Non kernel threads are not allowed during either online or offline. */
 	if (!(p->flags & PF_KTHREAD))
-		return cpu_active(cpu) && task_cpu_possible(cpu, p);
+		return cpu_active(cpu);
 
 	/* KTHREAD_IS_PER_CPU is always allowed. */
 	if (kthread_is_per_cpu(p))
@@ -3775,6 +3797,15 @@ bool cpus_share_resources(int this_cpu, int that_cpu)
 
 static inline bool ttwu_queue_cond(struct task_struct *p, int cpu)
 {
+	/*
+	 * The BPF scheduler may depend on select_task_rq() being invoked during
+	 * wakeups. In addition, @p may end up executing on a different CPU
+	 * regardless of what happens in the wakeup path making the ttwu_queue
+	 * optimization less meaningful. Skip if on SCX.
+	 */
+	if (task_on_scx(p))
+		return false;
+
 	/*
 	 * Do not complicate things with the async wake_list while the CPU is
 	 * in hotplug state.
@@ -4342,6 +4373,10 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->rt.on_rq		= 0;
 	p->rt.on_list		= 0;
 
+#ifdef CONFIG_SCHED_CLASS_EXT
+	init_scx_entity(&p->scx);
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
 #endif
@@ -4582,10 +4617,18 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 
 	if (dl_prio(p->prio))
 		return -EAGAIN;
-	else if (rt_prio(p->prio))
+
+	scx_pre_fork(p);
+
+	if (rt_prio(p->prio)) {
 		p->sched_class = &rt_sched_class;
-	else
+#ifdef CONFIG_SCHED_CLASS_EXT
+	} else if (task_should_scx(p)) {
+		p->sched_class = &ext_sched_class;
+#endif
+	} else {
 		p->sched_class = &fair_sched_class;
+	}
 
 	init_entity_runnable_average(&p->se);
 
@@ -4605,7 +4648,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	return 0;
 }
 
-void sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs)
+int sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs)
 {
 	unsigned long flags;
 
@@ -4632,11 +4675,19 @@ void sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs)
 	if (p->sched_class->task_fork)
 		p->sched_class->task_fork(p);
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+
+	return scx_fork(p);
+}
+
+void sched_cancel_fork(struct task_struct *p)
+{
+	scx_cancel_fork(p);
 }
 
 void sched_post_fork(struct task_struct *p)
 {
 	uclamp_post_fork(p);
+	scx_post_fork(p);
 }
 
 unsigned long to_ratio(u64 period, u64 runtime)
@@ -5469,6 +5520,7 @@ void sched_tick(void)
 	calc_global_load_tick(rq);
 	sched_core_tick(rq);
 	task_tick_mm_cid(rq, curr);
+	scx_tick(rq);
 
 	rq_unlock(rq, &rf);
 
@@ -5481,8 +5533,10 @@ void sched_tick(void)
 		wq_worker_tick(curr);
 
 #ifdef CONFIG_SMP
-	rq->idle_balance = idle_cpu(cpu);
-	sched_balance_trigger(rq);
+	if (!scx_switched_all()) {
+		rq->idle_balance = idle_cpu(cpu);
+		sched_balance_trigger(rq);
+	}
 #endif
 }
 
@@ -5772,8 +5826,19 @@ static inline void schedule_debug(struct task_struct *prev, bool preempt)
 static void put_prev_task_balance(struct rq *rq, struct task_struct *prev,
 				  struct rq_flags *rf)
 {
-#ifdef CONFIG_SMP
+	const struct sched_class *start_class = prev->sched_class;
 	const struct sched_class *class;
+
+#ifdef CONFIG_SCHED_CLASS_EXT
+	/*
+	 * SCX requires a balance() call before every pick_next_task() including
+	 * when waking up from SCHED_IDLE. If @start_class is below SCX, start
+	 * from SCX instead.
+	 */
+	if (scx_enabled() && sched_class_above(&ext_sched_class, start_class))
+		start_class = &ext_sched_class;
+#endif
+
 	/*
 	 * We must do the balancing pass before put_prev_task(), such
 	 * that when we release the rq->lock the task is in the same
@@ -5782,11 +5847,10 @@ static void put_prev_task_balance(struct rq *rq, struct task_struct *prev,
 	 * We can terminate the balance pass as soon as we know there is
 	 * a runnable task of @class priority or higher.
 	 */
-	for_class_range(class, prev->sched_class, &idle_sched_class) {
-		if (class->balance(rq, prev, rf))
+	for_active_class_range(class, start_class, &idle_sched_class) {
+		if (class->balance && class->balance(rq, prev, rf))
 			break;
 	}
-#endif
 
 	put_prev_task(rq, prev);
 
@@ -5808,6 +5872,9 @@ __pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 	const struct sched_class *class;
 	struct task_struct *p;
 
+	if (scx_enabled())
+		goto restart;
+
 	/*
 	 * Optimization: we know that if all tasks are in the fair class we can
 	 * call that function directly, but only if the @prev task wasn't of a
@@ -5847,10 +5914,23 @@ __pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 restart:
 	put_prev_task_balance(rq, prev, rf);
 
-	for_each_class(class) {
+	/*
+	 * We've updated @prev and no longer need the server link, clear it.
+	 * Must be done before ->pick_next_task() because that can (re)set
+	 * ->dl_server.
+	 */
+	if (prev->dl_server)
+		prev->dl_server = NULL;
+
+	for_each_active_class(class) {
 		p = class->pick_next_task(rq);
-		if (p)
+		if (p) {
+			const struct sched_class *prev_class = prev->sched_class;
+
+			if (class != prev_class && prev_class->switch_class)
+				prev_class->switch_class(rq, p);
 			return p;
+		}
 	}
 
 	BUG(); /* The idle class should always have a runnable task. */
@@ -5880,7 +5960,7 @@ static inline struct task_struct *pick_task(struct rq *rq)
 	const struct sched_class *class;
 	struct task_struct *p;
 
-	for_each_class(class) {
+	for_each_active_class(class) {
 		p = class->pick_task(rq);
 		if (p)
 			return p;
@@ -6877,6 +6957,10 @@ void __setscheduler_prio(struct task_struct *p, int prio)
 		p->sched_class = &dl_sched_class;
 	else if (rt_prio(prio))
 		p->sched_class = &rt_sched_class;
+#ifdef CONFIG_SCHED_CLASS_EXT
+	else if (task_should_scx(p))
+		p->sched_class = &ext_sched_class;
+#endif
 	else
 		p->sched_class = &fair_sched_class;
 
@@ -7022,6 +7106,7 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 	}
 
 	__setscheduler_prio(p, prio);
+	check_class_changing(rq, p, prev_class);
 
 	if (queued)
 		enqueue_task(rq, p, queue_flag);
@@ -7436,6 +7521,7 @@ void sched_show_task(struct task_struct *p)
 
 	print_worker_info(KERN_INFO, p);
 	print_stop_info(KERN_INFO, p);
+	print_scx_info(KERN_INFO, p);
 	show_stack(p, NULL, KERN_INFO);
 	put_task_stack(p);
 }
@@ -7964,6 +8050,8 @@ int sched_cpu_activate(unsigned int cpu)
 		cpuset_cpu_active();
 	}
 
+	scx_rq_activate(rq);
+
 	/*
 	 * Put the rq online, if not already. This happens:
 	 *
@@ -8013,6 +8101,8 @@ int sched_cpu_deactivate(unsigned int cpu)
 
 	sched_set_rq_offline(rq, cpu);
 
+	scx_rq_deactivate(rq);
+
 	/*
 	 * When going down, decrement the number of cores with SMT present.
 	 */
@@ -8197,11 +8287,15 @@ void __init sched_init(void)
 	int i;
 
 	/* Make sure the linker didn't screw up */
-	BUG_ON(&idle_sched_class != &fair_sched_class + 1 ||
-	       &fair_sched_class != &rt_sched_class + 1 ||
-	       &rt_sched_class   != &dl_sched_class + 1);
 #ifdef CONFIG_SMP
-	BUG_ON(&dl_sched_class != &stop_sched_class + 1);
+	BUG_ON(!sched_class_above(&stop_sched_class, &dl_sched_class));
+#endif
+	BUG_ON(!sched_class_above(&dl_sched_class, &rt_sched_class));
+	BUG_ON(!sched_class_above(&rt_sched_class, &fair_sched_class));
+	BUG_ON(!sched_class_above(&fair_sched_class, &idle_sched_class));
+#ifdef CONFIG_SCHED_CLASS_EXT
+	BUG_ON(!sched_class_above(&fair_sched_class, &ext_sched_class));
+	BUG_ON(!sched_class_above(&ext_sched_class, &idle_sched_class));
 #endif
 
 	wait_bit_init();
@@ -8225,6 +8319,9 @@ void __init sched_init(void)
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		init_cfs_bandwidth(&root_task_group.cfs_bandwidth, NULL);
 #endif /* CONFIG_FAIR_GROUP_SCHED */
+#ifdef CONFIG_EXT_GROUP_SCHED
+		root_task_group.scx_weight = CGROUP_WEIGHT_DFL;
+#endif /* CONFIG_EXT_GROUP_SCHED */
 #ifdef CONFIG_RT_GROUP_SCHED
 		root_task_group.rt_se = (struct sched_rt_entity **)ptr;
 		ptr += nr_cpu_ids * sizeof(void **);
@@ -8370,6 +8467,7 @@ void __init sched_init(void)
 	balance_push_set(smp_processor_id(), false);
 #endif
 	init_sched_fair_class();
+	init_sched_ext_class();
 
 	psi_init();
 
@@ -8655,6 +8753,7 @@ struct task_group *sched_create_group(struct task_group *parent)
 	if (!alloc_rt_sched_group(tg, parent))
 		goto err;
 
+	scx_group_set_weight(tg, CGROUP_WEIGHT_DFL);
 	alloc_uclamp_sched_group(tg, parent);
 
 	return tg;
@@ -8782,6 +8881,7 @@ void sched_move_task(struct task_struct *tsk)
 		put_prev_task(rq, tsk);
 
 	sched_change_group(tsk, group);
+	scx_move_task(tsk);
 
 	if (queued)
 		enqueue_task(rq, tsk, queue_flags);
@@ -8796,11 +8896,6 @@ void sched_move_task(struct task_struct *tsk)
 	}
 }
 
-static inline struct task_group *css_tg(struct cgroup_subsys_state *css)
-{
-	return css ? container_of(css, struct task_group, css) : NULL;
-}
-
 static struct cgroup_subsys_state *
 cpu_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)
 {
@@ -8824,6 +8919,11 @@ static int cpu_cgroup_css_online(struct cgroup_subsys_state *css)
 {
 	struct task_group *tg = css_tg(css);
 	struct task_group *parent = css_tg(css->parent);
+	int ret;
+
+	ret = scx_tg_online(tg);
+	if (ret)
+		return ret;
 
 	if (parent)
 		sched_online_group(tg, parent);
@@ -8838,6 +8938,13 @@ static int cpu_cgroup_css_online(struct cgroup_subsys_state *css)
 	return 0;
 }
 
+static void cpu_cgroup_css_offline(struct cgroup_subsys_state *css)
+{
+	struct task_group *tg = css_tg(css);
+
+	scx_tg_offline(tg);
+}
+
 static void cpu_cgroup_css_released(struct cgroup_subsys_state *css)
 {
 	struct task_group *tg = css_tg(css);
@@ -8855,9 +8962,9 @@ static void cpu_cgroup_css_free(struct cgroup_subsys_state *css)
 	sched_unregister_group(tg);
 }
 
-#ifdef CONFIG_RT_GROUP_SCHED
 static int cpu_cgroup_can_attach(struct cgroup_taskset *tset)
 {
+#ifdef CONFIG_RT_GROUP_SCHED
 	struct task_struct *task;
 	struct cgroup_subsys_state *css;
 
@@ -8865,9 +8972,9 @@ static int cpu_cgroup_can_attach(struct cgroup_taskset *tset)
 		if (!sched_rt_can_attach(css_tg(css), task))
 			return -EINVAL;
 	}
-	return 0;
-}
 #endif
+	return scx_cgroup_can_attach(tset);
+}
 
 static void cpu_cgroup_attach(struct cgroup_taskset *tset)
 {
@@ -8876,6 +8983,13 @@ static void cpu_cgroup_attach(struct cgroup_taskset *tset)
 
 	cgroup_taskset_for_each(task, css, tset)
 		sched_move_task(task);
+
+	scx_cgroup_finish_attach();
+}
+
+static void cpu_cgroup_cancel_attach(struct cgroup_taskset *tset)
+{
+	scx_cgroup_cancel_attach(tset);
 }
 
 #ifdef CONFIG_UCLAMP_TASK_GROUP
@@ -9052,22 +9166,36 @@ static int cpu_uclamp_max_show(struct seq_file *sf, void *v)
 }
 #endif /* CONFIG_UCLAMP_TASK_GROUP */
 
+#ifdef CONFIG_GROUP_SCHED_WEIGHT
+static unsigned long tg_weight(struct task_group *tg)
+{
 #ifdef CONFIG_FAIR_GROUP_SCHED
+	return scale_load_down(tg->shares);
+#else
+	return sched_weight_from_cgroup(tg->scx_weight);
+#endif
+}
+
 static int cpu_shares_write_u64(struct cgroup_subsys_state *css,
 				struct cftype *cftype, u64 shareval)
 {
+	int ret;
+
 	if (shareval > scale_load_down(ULONG_MAX))
 		shareval = MAX_SHARES;
-	return sched_group_set_shares(css_tg(css), scale_load(shareval));
+	ret = sched_group_set_shares(css_tg(css), scale_load(shareval));
+	if (!ret)
+		scx_group_set_weight(css_tg(css),
+				     sched_weight_to_cgroup(shareval));
+	return ret;
 }
 
 static u64 cpu_shares_read_u64(struct cgroup_subsys_state *css,
 			       struct cftype *cft)
 {
-	struct task_group *tg = css_tg(css);
-
-	return (u64) scale_load_down(tg->shares);
+	return tg_weight(css_tg(css));
 }
+#endif /* CONFIG_GROUP_SCHED_WEIGHT */
 
 #ifdef CONFIG_CFS_BANDWIDTH
 static DEFINE_MUTEX(cfs_constraints_mutex);
@@ -9413,7 +9541,6 @@ static int cpu_cfs_local_stat_show(struct seq_file *sf, void *v)
 	return 0;
 }
 #endif /* CONFIG_CFS_BANDWIDTH */
-#endif /* CONFIG_FAIR_GROUP_SCHED */
 
 #ifdef CONFIG_RT_GROUP_SCHED
 static int cpu_rt_runtime_write(struct cgroup_subsys_state *css,
@@ -9441,7 +9568,7 @@ static u64 cpu_rt_period_read_uint(struct cgroup_subsys_state *css,
 }
 #endif /* CONFIG_RT_GROUP_SCHED */
 
-#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_GROUP_SCHED_WEIGHT
 static s64 cpu_idle_read_s64(struct cgroup_subsys_state *css,
 			       struct cftype *cft)
 {
@@ -9451,12 +9578,17 @@ static s64 cpu_idle_read_s64(struct cgroup_subsys_state *css,
 static int cpu_idle_write_s64(struct cgroup_subsys_state *css,
 				struct cftype *cft, s64 idle)
 {
-	return sched_group_set_idle(css_tg(css), idle);
+	int ret;
+
+	ret = sched_group_set_idle(css_tg(css), idle);
+	if (!ret)
+		scx_group_set_idle(css_tg(css), idle);
+	return ret;
 }
 #endif
 
 static struct cftype cpu_legacy_files[] = {
-#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_GROUP_SCHED_WEIGHT
 	{
 		.name = "shares",
 		.read_u64 = cpu_shares_read_u64,
@@ -9566,38 +9698,35 @@ static int cpu_local_stat_show(struct seq_file *sf,
 	return 0;
 }
 
-#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_GROUP_SCHED_WEIGHT
+
 static u64 cpu_weight_read_u64(struct cgroup_subsys_state *css,
 			       struct cftype *cft)
 {
-	struct task_group *tg = css_tg(css);
-	u64 weight = scale_load_down(tg->shares);
-
-	return DIV_ROUND_CLOSEST_ULL(weight * CGROUP_WEIGHT_DFL, 1024);
+	return sched_weight_to_cgroup(tg_weight(css_tg(css)));
 }
 
 static int cpu_weight_write_u64(struct cgroup_subsys_state *css,
-				struct cftype *cft, u64 weight)
+				struct cftype *cft, u64 cgrp_weight)
 {
-	/*
-	 * cgroup weight knobs should use the common MIN, DFL and MAX
-	 * values which are 1, 100 and 10000 respectively.  While it loses
-	 * a bit of range on both ends, it maps pretty well onto the shares
-	 * value used by scheduler and the round-trip conversions preserve
-	 * the original value over the entire range.
-	 */
-	if (weight < CGROUP_WEIGHT_MIN || weight > CGROUP_WEIGHT_MAX)
+	unsigned long weight;
+	int ret;
+
+	if (cgrp_weight < CGROUP_WEIGHT_MIN || cgrp_weight > CGROUP_WEIGHT_MAX)
 		return -ERANGE;
 
-	weight = DIV_ROUND_CLOSEST_ULL(weight * 1024, CGROUP_WEIGHT_DFL);
+	weight = sched_weight_from_cgroup(cgrp_weight);
 
-	return sched_group_set_shares(css_tg(css), scale_load(weight));
+	ret = sched_group_set_shares(css_tg(css), scale_load(weight));
+	if (!ret)
+		scx_group_set_weight(css_tg(css), cgrp_weight);
+	return ret;
 }
 
 static s64 cpu_weight_nice_read_s64(struct cgroup_subsys_state *css,
 				    struct cftype *cft)
 {
-	unsigned long weight = scale_load_down(css_tg(css)->shares);
+	unsigned long weight = tg_weight(css_tg(css));
 	int last_delta = INT_MAX;
 	int prio, delta;
 
@@ -9616,7 +9745,7 @@ static int cpu_weight_nice_write_s64(struct cgroup_subsys_state *css,
 				     struct cftype *cft, s64 nice)
 {
 	unsigned long weight;
-	int idx;
+	int idx, ret;
 
 	if (nice < MIN_NICE || nice > MAX_NICE)
 		return -ERANGE;
@@ -9625,9 +9754,13 @@ static int cpu_weight_nice_write_s64(struct cgroup_subsys_state *css,
 	idx = array_index_nospec(idx, 40);
 	weight = sched_prio_to_weight[idx];
 
-	return sched_group_set_shares(css_tg(css), scale_load(weight));
+	ret = sched_group_set_shares(css_tg(css), scale_load(weight));
+	if (!ret)
+		scx_group_set_weight(css_tg(css),
+				     sched_weight_to_cgroup(weight));
+	return ret;
 }
-#endif
+#endif /* CONFIG_GROUP_SCHED_WEIGHT */
 
 static void __maybe_unused cpu_period_quota_print(struct seq_file *sf,
 						  long period, long quota)
@@ -9687,7 +9820,7 @@ static ssize_t cpu_max_write(struct kernfs_open_file *of,
 #endif
 
 static struct cftype cpu_files[] = {
-#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_GROUP_SCHED_WEIGHT
 	{
 		.name = "weight",
 		.flags = CFTYPE_NOT_ON_ROOT,
@@ -9741,14 +9874,14 @@ static struct cftype cpu_files[] = {
 struct cgroup_subsys cpu_cgrp_subsys = {
 	.css_alloc	= cpu_cgroup_css_alloc,
 	.css_online	= cpu_cgroup_css_online,
+	.css_offline	= cpu_cgroup_css_offline,
 	.css_released	= cpu_cgroup_css_released,
 	.css_free	= cpu_cgroup_css_free,
 	.css_extra_stat_show = cpu_extra_stat_show,
 	.css_local_stat_show = cpu_local_stat_show,
-#ifdef CONFIG_RT_GROUP_SCHED
 	.can_attach	= cpu_cgroup_can_attach,
-#endif
 	.attach		= cpu_cgroup_attach,
+	.cancel_attach	= cpu_cgroup_cancel_attach,
 	.legacy_cftypes	= cpu_legacy_files,
 	.dfl_cftypes	= cpu_files,
 	.early_init	= true,
@@ -10338,3 +10471,38 @@ void sched_mm_cid_fork(struct task_struct *t)
 	t->mm_cid_active = 1;
 }
 #endif
+
+#ifdef CONFIG_SCHED_CLASS_EXT
+void sched_deq_and_put_task(struct task_struct *p, int queue_flags,
+			    struct sched_enq_and_set_ctx *ctx)
+{
+	struct rq *rq = task_rq(p);
+
+	lockdep_assert_rq_held(rq);
+
+	*ctx = (struct sched_enq_and_set_ctx){
+		.p = p,
+		.queue_flags = queue_flags,
+		.queued = task_on_rq_queued(p),
+		.running = task_current(rq, p),
+	};
+
+	update_rq_clock(rq);
+	if (ctx->queued)
+		dequeue_task(rq, p, queue_flags | DEQUEUE_NOCLOCK);
+	if (ctx->running)
+		put_prev_task(rq, p);
+}
+
+void sched_enq_and_set_task(struct sched_enq_and_set_ctx *ctx)
+{
+	struct rq *rq = task_rq(ctx->p);
+
+	lockdep_assert_rq_held(rq);
+
+	if (ctx->queued)
+		enqueue_task(rq, ctx->p, ctx->queue_flags | ENQUEUE_NOCLOCK);
+	if (ctx->running)
+		set_next_task(rq, ctx->p);
+}
+#endif	/* CONFIG_SCHED_CLASS_EXT */
diff --git a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c
index eece6244f9d2..e683e5d08daa 100644
--- a/kernel/sched/cpufreq_schedutil.c
+++ b/kernel/sched/cpufreq_schedutil.c
@@ -197,8 +197,10 @@ unsigned long sugov_effective_cpu_perf(int cpu, unsigned long actual,
 
 static void sugov_get_util(struct sugov_cpu *sg_cpu, unsigned long boost)
 {
-	unsigned long min, max, util = cpu_util_cfs_boost(sg_cpu->cpu);
+	unsigned long min, max, util = scx_cpuperf_target(sg_cpu->cpu);
 
+	if (!scx_switched_all())
+		util += cpu_util_cfs_boost(sg_cpu->cpu);
 	util = effective_cpu_util(sg_cpu->cpu, util, &min, &max);
 	util = max(util, boost);
 	sg_cpu->bw_min = min;
@@ -325,16 +327,35 @@ static unsigned long sugov_iowait_apply(struct sugov_cpu *sg_cpu, u64 time,
 }
 
 #ifdef CONFIG_NO_HZ_COMMON
-static bool sugov_cpu_is_busy(struct sugov_cpu *sg_cpu)
+static bool sugov_hold_freq(struct sugov_cpu *sg_cpu)
 {
-	unsigned long idle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu->cpu);
-	bool ret = idle_calls == sg_cpu->saved_idle_calls;
+	unsigned long idle_calls;
+	bool ret;
+
+	/*
+	 * The heuristics in this function is for the fair class. For SCX, the
+	 * performance target comes directly from the BPF scheduler. Let's just
+	 * follow it.
+	 */
+	if (scx_switched_all())
+		return false;
+
+	/* if capped by uclamp_max, always update to be in compliance */
+	if (uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)))
+		return false;
+
+	/*
+	 * Maintain the frequency if the CPU has not been idle recently, as
+	 * reduction is likely to be premature.
+	 */
+	idle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu->cpu);
+	ret = idle_calls == sg_cpu->saved_idle_calls;
 
 	sg_cpu->saved_idle_calls = idle_calls;
 	return ret;
 }
 #else
-static inline bool sugov_cpu_is_busy(struct sugov_cpu *sg_cpu) { return false; }
+static inline bool sugov_hold_freq(struct sugov_cpu *sg_cpu) { return false; }
 #endif /* CONFIG_NO_HZ_COMMON */
 
 /*
@@ -382,14 +403,8 @@ static void sugov_update_single_freq(struct update_util_data *hook, u64 time,
 		return;
 
 	next_f = get_next_freq(sg_policy, sg_cpu->util, max_cap);
-	/*
-	 * Do not reduce the frequency if the CPU has not been idle
-	 * recently, as the reduction is likely to be premature then.
-	 *
-	 * Except when the rq is capped by uclamp_max.
-	 */
-	if (!uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)) &&
-	    sugov_cpu_is_busy(sg_cpu) && next_f < sg_policy->next_freq &&
+
+	if (sugov_hold_freq(sg_cpu) && next_f < sg_policy->next_freq &&
 	    !sg_policy->need_freq_update) {
 		next_f = sg_policy->next_freq;
 
@@ -436,14 +451,7 @@ static void sugov_update_single_perf(struct update_util_data *hook, u64 time,
 	if (!sugov_update_single_common(sg_cpu, time, max_cap, flags))
 		return;
 
-	/*
-	 * Do not reduce the target performance level if the CPU has not been
-	 * idle recently, as the reduction is likely to be premature then.
-	 *
-	 * Except when the rq is capped by uclamp_max.
-	 */
-	if (!uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)) &&
-	    sugov_cpu_is_busy(sg_cpu) && sg_cpu->util < prev_util)
+	if (sugov_hold_freq(sg_cpu) && sg_cpu->util < prev_util)
 		sg_cpu->util = prev_util;
 
 	cpufreq_driver_adjust_perf(sg_cpu->cpu, sg_cpu->bw_min,
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index c1eb9a1afd13..c057ef46c5f8 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -1090,6 +1090,9 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 		P(dl.runtime);
 		P(dl.deadline);
 	}
+#ifdef CONFIG_SCHED_CLASS_EXT
+	__PS("ext.enabled", task_on_scx(p));
+#endif
 #undef PN_SCHEDSTAT
 #undef P_SCHEDSTAT
 
diff --git a/kernel/sched/ext.c b/kernel/sched/ext.c
new file mode 100644
index 000000000000..5fae2292ec29
--- /dev/null
+++ b/kernel/sched/ext.c
@@ -0,0 +1,7281 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * BPF extensible scheduler class: Documentation/scheduler/sched-ext.rst
+ *
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#define SCX_OP_IDX(op)		(offsetof(struct sched_ext_ops, op) / sizeof(void (*)(void)))
+
+enum scx_consts {
+	SCX_DSP_DFL_MAX_BATCH		= 32,
+	SCX_DSP_MAX_LOOPS		= 32,
+	SCX_WATCHDOG_MAX_TIMEOUT	= 30 * HZ,
+
+	SCX_EXIT_BT_LEN			= 64,
+	SCX_EXIT_MSG_LEN		= 1024,
+	SCX_EXIT_DUMP_DFL_LEN		= 32768,
+
+	SCX_CPUPERF_ONE			= SCHED_CAPACITY_SCALE,
+
+	/*
+	 * Iterating all tasks may take a while. Periodically drop
+	 * scx_tasks_lock to avoid causing e.g. CSD and RCU stalls.
+	 */
+	SCX_OPS_TASK_ITER_BATCH		= 32,
+};
+
+enum scx_exit_kind {
+	SCX_EXIT_NONE,
+	SCX_EXIT_DONE,
+
+	SCX_EXIT_UNREG = 64,	/* user-space initiated unregistration */
+	SCX_EXIT_UNREG_BPF,	/* BPF-initiated unregistration */
+	SCX_EXIT_UNREG_KERN,	/* kernel-initiated unregistration */
+	SCX_EXIT_SYSRQ,		/* requested by 'S' sysrq */
+
+	SCX_EXIT_ERROR = 1024,	/* runtime error, error msg contains details */
+	SCX_EXIT_ERROR_BPF,	/* ERROR but triggered through scx_bpf_error() */
+	SCX_EXIT_ERROR_STALL,	/* watchdog detected stalled runnable tasks */
+};
+
+/*
+ * An exit code can be specified when exiting with scx_bpf_exit() or
+ * scx_ops_exit(), corresponding to exit_kind UNREG_BPF and UNREG_KERN
+ * respectively. The codes are 64bit of the format:
+ *
+ *   Bits: [63  ..  48 47   ..  32 31 .. 0]
+ *         [ SYS ACT ] [ SYS RSN ] [ USR  ]
+ *
+ *   SYS ACT: System-defined exit actions
+ *   SYS RSN: System-defined exit reasons
+ *   USR    : User-defined exit codes and reasons
+ *
+ * Using the above, users may communicate intention and context by ORing system
+ * actions and/or system reasons with a user-defined exit code.
+ */
+enum scx_exit_code {
+	/* Reasons */
+	SCX_ECODE_RSN_HOTPLUG	= 1LLU << 32,
+
+	/* Actions */
+	SCX_ECODE_ACT_RESTART	= 1LLU << 48,
+};
+
+/*
+ * scx_exit_info is passed to ops.exit() to describe why the BPF scheduler is
+ * being disabled.
+ */
+struct scx_exit_info {
+	/* %SCX_EXIT_* - broad category of the exit reason */
+	enum scx_exit_kind	kind;
+
+	/* exit code if gracefully exiting */
+	s64			exit_code;
+
+	/* textual representation of the above */
+	const char		*reason;
+
+	/* backtrace if exiting due to an error */
+	unsigned long		*bt;
+	u32			bt_len;
+
+	/* informational message */
+	char			*msg;
+
+	/* debug dump */
+	char			*dump;
+};
+
+/* sched_ext_ops.flags */
+enum scx_ops_flags {
+	/*
+	 * Keep built-in idle tracking even if ops.update_idle() is implemented.
+	 */
+	SCX_OPS_KEEP_BUILTIN_IDLE = 1LLU << 0,
+
+	/*
+	 * By default, if there are no other task to run on the CPU, ext core
+	 * keeps running the current task even after its slice expires. If this
+	 * flag is specified, such tasks are passed to ops.enqueue() with
+	 * %SCX_ENQ_LAST. See the comment above %SCX_ENQ_LAST for more info.
+	 */
+	SCX_OPS_ENQ_LAST	= 1LLU << 1,
+
+	/*
+	 * An exiting task may schedule after PF_EXITING is set. In such cases,
+	 * bpf_task_from_pid() may not be able to find the task and if the BPF
+	 * scheduler depends on pid lookup for dispatching, the task will be
+	 * lost leading to various issues including RCU grace period stalls.
+	 *
+	 * To mask this problem, by default, unhashed tasks are automatically
+	 * dispatched to the local DSQ on enqueue. If the BPF scheduler doesn't
+	 * depend on pid lookups and wants to handle these tasks directly, the
+	 * following flag can be used.
+	 */
+	SCX_OPS_ENQ_EXITING	= 1LLU << 2,
+
+	/*
+	 * If set, only tasks with policy set to SCHED_EXT are attached to
+	 * sched_ext. If clear, SCHED_NORMAL tasks are also included.
+	 */
+	SCX_OPS_SWITCH_PARTIAL	= 1LLU << 3,
+
+	/*
+	 * CPU cgroup support flags
+	 */
+	SCX_OPS_HAS_CGROUP_WEIGHT = 1LLU << 16,	/* cpu.weight */
+
+	SCX_OPS_ALL_FLAGS	= SCX_OPS_KEEP_BUILTIN_IDLE |
+				  SCX_OPS_ENQ_LAST |
+				  SCX_OPS_ENQ_EXITING |
+				  SCX_OPS_SWITCH_PARTIAL |
+				  SCX_OPS_HAS_CGROUP_WEIGHT,
+};
+
+/* argument container for ops.init_task() */
+struct scx_init_task_args {
+	/*
+	 * Set if ops.init_task() is being invoked on the fork path, as opposed
+	 * to the scheduler transition path.
+	 */
+	bool			fork;
+#ifdef CONFIG_EXT_GROUP_SCHED
+	/* the cgroup the task is joining */
+	struct cgroup		*cgroup;
+#endif
+};
+
+/* argument container for ops.exit_task() */
+struct scx_exit_task_args {
+	/* Whether the task exited before running on sched_ext. */
+	bool cancelled;
+};
+
+/* argument container for ops->cgroup_init() */
+struct scx_cgroup_init_args {
+	/* the weight of the cgroup [1..10000] */
+	u32			weight;
+};
+
+enum scx_cpu_preempt_reason {
+	/* next task is being scheduled by &sched_class_rt */
+	SCX_CPU_PREEMPT_RT,
+	/* next task is being scheduled by &sched_class_dl */
+	SCX_CPU_PREEMPT_DL,
+	/* next task is being scheduled by &sched_class_stop */
+	SCX_CPU_PREEMPT_STOP,
+	/* unknown reason for SCX being preempted */
+	SCX_CPU_PREEMPT_UNKNOWN,
+};
+
+/*
+ * Argument container for ops->cpu_acquire(). Currently empty, but may be
+ * expanded in the future.
+ */
+struct scx_cpu_acquire_args {};
+
+/* argument container for ops->cpu_release() */
+struct scx_cpu_release_args {
+	/* the reason the CPU was preempted */
+	enum scx_cpu_preempt_reason reason;
+
+	/* the task that's going to be scheduled on the CPU */
+	struct task_struct	*task;
+};
+
+/*
+ * Informational context provided to dump operations.
+ */
+struct scx_dump_ctx {
+	enum scx_exit_kind	kind;
+	s64			exit_code;
+	const char		*reason;
+	u64			at_ns;
+	u64			at_jiffies;
+};
+
+/**
+ * struct sched_ext_ops - Operation table for BPF scheduler implementation
+ *
+ * Userland can implement an arbitrary scheduling policy by implementing and
+ * loading operations in this table.
+ */
+struct sched_ext_ops {
+	/**
+	 * select_cpu - Pick the target CPU for a task which is being woken up
+	 * @p: task being woken up
+	 * @prev_cpu: the cpu @p was on before sleeping
+	 * @wake_flags: SCX_WAKE_*
+	 *
+	 * Decision made here isn't final. @p may be moved to any CPU while it
+	 * is getting dispatched for execution later. However, as @p is not on
+	 * the rq at this point, getting the eventual execution CPU right here
+	 * saves a small bit of overhead down the line.
+	 *
+	 * If an idle CPU is returned, the CPU is kicked and will try to
+	 * dispatch. While an explicit custom mechanism can be added,
+	 * select_cpu() serves as the default way to wake up idle CPUs.
+	 *
+	 * @p may be dispatched directly by calling scx_bpf_dispatch(). If @p
+	 * is dispatched, the ops.enqueue() callback will be skipped. Finally,
+	 * if @p is dispatched to SCX_DSQ_LOCAL, it will be dispatched to the
+	 * local DSQ of whatever CPU is returned by this callback.
+	 */
+	s32 (*select_cpu)(struct task_struct *p, s32 prev_cpu, u64 wake_flags);
+
+	/**
+	 * enqueue - Enqueue a task on the BPF scheduler
+	 * @p: task being enqueued
+	 * @enq_flags: %SCX_ENQ_*
+	 *
+	 * @p is ready to run. Dispatch directly by calling scx_bpf_dispatch()
+	 * or enqueue on the BPF scheduler. If not directly dispatched, the bpf
+	 * scheduler owns @p and if it fails to dispatch @p, the task will
+	 * stall.
+	 *
+	 * If @p was dispatched from ops.select_cpu(), this callback is
+	 * skipped.
+	 */
+	void (*enqueue)(struct task_struct *p, u64 enq_flags);
+
+	/**
+	 * dequeue - Remove a task from the BPF scheduler
+	 * @p: task being dequeued
+	 * @deq_flags: %SCX_DEQ_*
+	 *
+	 * Remove @p from the BPF scheduler. This is usually called to isolate
+	 * the task while updating its scheduling properties (e.g. priority).
+	 *
+	 * The ext core keeps track of whether the BPF side owns a given task or
+	 * not and can gracefully ignore spurious dispatches from BPF side,
+	 * which makes it safe to not implement this method. However, depending
+	 * on the scheduling logic, this can lead to confusing behaviors - e.g.
+	 * scheduling position not being updated across a priority change.
+	 */
+	void (*dequeue)(struct task_struct *p, u64 deq_flags);
+
+	/**
+	 * dispatch - Dispatch tasks from the BPF scheduler and/or consume DSQs
+	 * @cpu: CPU to dispatch tasks for
+	 * @prev: previous task being switched out
+	 *
+	 * Called when a CPU's local dsq is empty. The operation should dispatch
+	 * one or more tasks from the BPF scheduler into the DSQs using
+	 * scx_bpf_dispatch() and/or consume user DSQs into the local DSQ using
+	 * scx_bpf_consume().
+	 *
+	 * The maximum number of times scx_bpf_dispatch() can be called without
+	 * an intervening scx_bpf_consume() is specified by
+	 * ops.dispatch_max_batch. See the comments on top of the two functions
+	 * for more details.
+	 *
+	 * When not %NULL, @prev is an SCX task with its slice depleted. If
+	 * @prev is still runnable as indicated by set %SCX_TASK_QUEUED in
+	 * @prev->scx.flags, it is not enqueued yet and will be enqueued after
+	 * ops.dispatch() returns. To keep executing @prev, return without
+	 * dispatching or consuming any tasks. Also see %SCX_OPS_ENQ_LAST.
+	 */
+	void (*dispatch)(s32 cpu, struct task_struct *prev);
+
+	/**
+	 * tick - Periodic tick
+	 * @p: task running currently
+	 *
+	 * This operation is called every 1/HZ seconds on CPUs which are
+	 * executing an SCX task. Setting @p->scx.slice to 0 will trigger an
+	 * immediate dispatch cycle on the CPU.
+	 */
+	void (*tick)(struct task_struct *p);
+
+	/**
+	 * runnable - A task is becoming runnable on its associated CPU
+	 * @p: task becoming runnable
+	 * @enq_flags: %SCX_ENQ_*
+	 *
+	 * This and the following three functions can be used to track a task's
+	 * execution state transitions. A task becomes ->runnable() on a CPU,
+	 * and then goes through one or more ->running() and ->stopping() pairs
+	 * as it runs on the CPU, and eventually becomes ->quiescent() when it's
+	 * done running on the CPU.
+	 *
+	 * @p is becoming runnable on the CPU because it's
+	 *
+	 * - waking up (%SCX_ENQ_WAKEUP)
+	 * - being moved from another CPU
+	 * - being restored after temporarily taken off the queue for an
+	 *   attribute change.
+	 *
+	 * This and ->enqueue() are related but not coupled. This operation
+	 * notifies @p's state transition and may not be followed by ->enqueue()
+	 * e.g. when @p is being dispatched to a remote CPU, or when @p is
+	 * being enqueued on a CPU experiencing a hotplug event. Likewise, a
+	 * task may be ->enqueue()'d without being preceded by this operation
+	 * e.g. after exhausting its slice.
+	 */
+	void (*runnable)(struct task_struct *p, u64 enq_flags);
+
+	/**
+	 * running - A task is starting to run on its associated CPU
+	 * @p: task starting to run
+	 *
+	 * See ->runnable() for explanation on the task state notifiers.
+	 */
+	void (*running)(struct task_struct *p);
+
+	/**
+	 * stopping - A task is stopping execution
+	 * @p: task stopping to run
+	 * @runnable: is task @p still runnable?
+	 *
+	 * See ->runnable() for explanation on the task state notifiers. If
+	 * !@runnable, ->quiescent() will be invoked after this operation
+	 * returns.
+	 */
+	void (*stopping)(struct task_struct *p, bool runnable);
+
+	/**
+	 * quiescent - A task is becoming not runnable on its associated CPU
+	 * @p: task becoming not runnable
+	 * @deq_flags: %SCX_DEQ_*
+	 *
+	 * See ->runnable() for explanation on the task state notifiers.
+	 *
+	 * @p is becoming quiescent on the CPU because it's
+	 *
+	 * - sleeping (%SCX_DEQ_SLEEP)
+	 * - being moved to another CPU
+	 * - being temporarily taken off the queue for an attribute change
+	 *   (%SCX_DEQ_SAVE)
+	 *
+	 * This and ->dequeue() are related but not coupled. This operation
+	 * notifies @p's state transition and may not be preceded by ->dequeue()
+	 * e.g. when @p is being dispatched to a remote CPU.
+	 */
+	void (*quiescent)(struct task_struct *p, u64 deq_flags);
+
+	/**
+	 * yield - Yield CPU
+	 * @from: yielding task
+	 * @to: optional yield target task
+	 *
+	 * If @to is NULL, @from is yielding the CPU to other runnable tasks.
+	 * The BPF scheduler should ensure that other available tasks are
+	 * dispatched before the yielding task. Return value is ignored in this
+	 * case.
+	 *
+	 * If @to is not-NULL, @from wants to yield the CPU to @to. If the bpf
+	 * scheduler can implement the request, return %true; otherwise, %false.
+	 */
+	bool (*yield)(struct task_struct *from, struct task_struct *to);
+
+	/**
+	 * core_sched_before - Task ordering for core-sched
+	 * @a: task A
+	 * @b: task B
+	 *
+	 * Used by core-sched to determine the ordering between two tasks. See
+	 * Documentation/admin-guide/hw-vuln/core-scheduling.rst for details on
+	 * core-sched.
+	 *
+	 * Both @a and @b are runnable and may or may not currently be queued on
+	 * the BPF scheduler. Should return %true if @a should run before @b.
+	 * %false if there's no required ordering or @b should run before @a.
+	 *
+	 * If not specified, the default is ordering them according to when they
+	 * became runnable.
+	 */
+	bool (*core_sched_before)(struct task_struct *a, struct task_struct *b);
+
+	/**
+	 * set_weight - Set task weight
+	 * @p: task to set weight for
+	 * @weight: new weight [1..10000]
+	 *
+	 * Update @p's weight to @weight.
+	 */
+	void (*set_weight)(struct task_struct *p, u32 weight);
+
+	/**
+	 * set_cpumask - Set CPU affinity
+	 * @p: task to set CPU affinity for
+	 * @cpumask: cpumask of cpus that @p can run on
+	 *
+	 * Update @p's CPU affinity to @cpumask.
+	 */
+	void (*set_cpumask)(struct task_struct *p,
+			    const struct cpumask *cpumask);
+
+	/**
+	 * update_idle - Update the idle state of a CPU
+	 * @cpu: CPU to udpate the idle state for
+	 * @idle: whether entering or exiting the idle state
+	 *
+	 * This operation is called when @rq's CPU goes or leaves the idle
+	 * state. By default, implementing this operation disables the built-in
+	 * idle CPU tracking and the following helpers become unavailable:
+	 *
+	 * - scx_bpf_select_cpu_dfl()
+	 * - scx_bpf_test_and_clear_cpu_idle()
+	 * - scx_bpf_pick_idle_cpu()
+	 *
+	 * The user also must implement ops.select_cpu() as the default
+	 * implementation relies on scx_bpf_select_cpu_dfl().
+	 *
+	 * Specify the %SCX_OPS_KEEP_BUILTIN_IDLE flag to keep the built-in idle
+	 * tracking.
+	 */
+	void (*update_idle)(s32 cpu, bool idle);
+
+	/**
+	 * cpu_acquire - A CPU is becoming available to the BPF scheduler
+	 * @cpu: The CPU being acquired by the BPF scheduler.
+	 * @args: Acquire arguments, see the struct definition.
+	 *
+	 * A CPU that was previously released from the BPF scheduler is now once
+	 * again under its control.
+	 */
+	void (*cpu_acquire)(s32 cpu, struct scx_cpu_acquire_args *args);
+
+	/**
+	 * cpu_release - A CPU is taken away from the BPF scheduler
+	 * @cpu: The CPU being released by the BPF scheduler.
+	 * @args: Release arguments, see the struct definition.
+	 *
+	 * The specified CPU is no longer under the control of the BPF
+	 * scheduler. This could be because it was preempted by a higher
+	 * priority sched_class, though there may be other reasons as well. The
+	 * caller should consult @args->reason to determine the cause.
+	 */
+	void (*cpu_release)(s32 cpu, struct scx_cpu_release_args *args);
+
+	/**
+	 * init_task - Initialize a task to run in a BPF scheduler
+	 * @p: task to initialize for BPF scheduling
+	 * @args: init arguments, see the struct definition
+	 *
+	 * Either we're loading a BPF scheduler or a new task is being forked.
+	 * Initialize @p for BPF scheduling. This operation may block and can
+	 * be used for allocations, and is called exactly once for a task.
+	 *
+	 * Return 0 for success, -errno for failure. An error return while
+	 * loading will abort loading of the BPF scheduler. During a fork, it
+	 * will abort that specific fork.
+	 */
+	s32 (*init_task)(struct task_struct *p, struct scx_init_task_args *args);
+
+	/**
+	 * exit_task - Exit a previously-running task from the system
+	 * @p: task to exit
+	 *
+	 * @p is exiting or the BPF scheduler is being unloaded. Perform any
+	 * necessary cleanup for @p.
+	 */
+	void (*exit_task)(struct task_struct *p, struct scx_exit_task_args *args);
+
+	/**
+	 * enable - Enable BPF scheduling for a task
+	 * @p: task to enable BPF scheduling for
+	 *
+	 * Enable @p for BPF scheduling. enable() is called on @p any time it
+	 * enters SCX, and is always paired with a matching disable().
+	 */
+	void (*enable)(struct task_struct *p);
+
+	/**
+	 * disable - Disable BPF scheduling for a task
+	 * @p: task to disable BPF scheduling for
+	 *
+	 * @p is exiting, leaving SCX or the BPF scheduler is being unloaded.
+	 * Disable BPF scheduling for @p. A disable() call is always matched
+	 * with a prior enable() call.
+	 */
+	void (*disable)(struct task_struct *p);
+
+	/**
+	 * dump - Dump BPF scheduler state on error
+	 * @ctx: debug dump context
+	 *
+	 * Use scx_bpf_dump() to generate BPF scheduler specific debug dump.
+	 */
+	void (*dump)(struct scx_dump_ctx *ctx);
+
+	/**
+	 * dump_cpu - Dump BPF scheduler state for a CPU on error
+	 * @ctx: debug dump context
+	 * @cpu: CPU to generate debug dump for
+	 * @idle: @cpu is currently idle without any runnable tasks
+	 *
+	 * Use scx_bpf_dump() to generate BPF scheduler specific debug dump for
+	 * @cpu. If @idle is %true and this operation doesn't produce any
+	 * output, @cpu is skipped for dump.
+	 */
+	void (*dump_cpu)(struct scx_dump_ctx *ctx, s32 cpu, bool idle);
+
+	/**
+	 * dump_task - Dump BPF scheduler state for a runnable task on error
+	 * @ctx: debug dump context
+	 * @p: runnable task to generate debug dump for
+	 *
+	 * Use scx_bpf_dump() to generate BPF scheduler specific debug dump for
+	 * @p.
+	 */
+	void (*dump_task)(struct scx_dump_ctx *ctx, struct task_struct *p);
+
+#ifdef CONFIG_EXT_GROUP_SCHED
+	/**
+	 * cgroup_init - Initialize a cgroup
+	 * @cgrp: cgroup being initialized
+	 * @args: init arguments, see the struct definition
+	 *
+	 * Either the BPF scheduler is being loaded or @cgrp created, initialize
+	 * @cgrp for sched_ext. This operation may block.
+	 *
+	 * Return 0 for success, -errno for failure. An error return while
+	 * loading will abort loading of the BPF scheduler. During cgroup
+	 * creation, it will abort the specific cgroup creation.
+	 */
+	s32 (*cgroup_init)(struct cgroup *cgrp,
+			   struct scx_cgroup_init_args *args);
+
+	/**
+	 * cgroup_exit - Exit a cgroup
+	 * @cgrp: cgroup being exited
+	 *
+	 * Either the BPF scheduler is being unloaded or @cgrp destroyed, exit
+	 * @cgrp for sched_ext. This operation my block.
+	 */
+	void (*cgroup_exit)(struct cgroup *cgrp);
+
+	/**
+	 * cgroup_prep_move - Prepare a task to be moved to a different cgroup
+	 * @p: task being moved
+	 * @from: cgroup @p is being moved from
+	 * @to: cgroup @p is being moved to
+	 *
+	 * Prepare @p for move from cgroup @from to @to. This operation may
+	 * block and can be used for allocations.
+	 *
+	 * Return 0 for success, -errno for failure. An error return aborts the
+	 * migration.
+	 */
+	s32 (*cgroup_prep_move)(struct task_struct *p,
+				struct cgroup *from, struct cgroup *to);
+
+	/**
+	 * cgroup_move - Commit cgroup move
+	 * @p: task being moved
+	 * @from: cgroup @p is being moved from
+	 * @to: cgroup @p is being moved to
+	 *
+	 * Commit the move. @p is dequeued during this operation.
+	 */
+	void (*cgroup_move)(struct task_struct *p,
+			    struct cgroup *from, struct cgroup *to);
+
+	/**
+	 * cgroup_cancel_move - Cancel cgroup move
+	 * @p: task whose cgroup move is being canceled
+	 * @from: cgroup @p was being moved from
+	 * @to: cgroup @p was being moved to
+	 *
+	 * @p was cgroup_prep_move()'d but failed before reaching cgroup_move().
+	 * Undo the preparation.
+	 */
+	void (*cgroup_cancel_move)(struct task_struct *p,
+				   struct cgroup *from, struct cgroup *to);
+
+	/**
+	 * cgroup_set_weight - A cgroup's weight is being changed
+	 * @cgrp: cgroup whose weight is being updated
+	 * @weight: new weight [1..10000]
+	 *
+	 * Update @tg's weight to @weight.
+	 */
+	void (*cgroup_set_weight)(struct cgroup *cgrp, u32 weight);
+#endif	/* CONFIG_CGROUPS */
+
+	/*
+	 * All online ops must come before ops.cpu_online().
+	 */
+
+	/**
+	 * cpu_online - A CPU became online
+	 * @cpu: CPU which just came up
+	 *
+	 * @cpu just came online. @cpu will not call ops.enqueue() or
+	 * ops.dispatch(), nor run tasks associated with other CPUs beforehand.
+	 */
+	void (*cpu_online)(s32 cpu);
+
+	/**
+	 * cpu_offline - A CPU is going offline
+	 * @cpu: CPU which is going offline
+	 *
+	 * @cpu is going offline. @cpu will not call ops.enqueue() or
+	 * ops.dispatch(), nor run tasks associated with other CPUs afterwards.
+	 */
+	void (*cpu_offline)(s32 cpu);
+
+	/*
+	 * All CPU hotplug ops must come before ops.init().
+	 */
+
+	/**
+	 * init - Initialize the BPF scheduler
+	 */
+	s32 (*init)(void);
+
+	/**
+	 * exit - Clean up after the BPF scheduler
+	 * @info: Exit info
+	 */
+	void (*exit)(struct scx_exit_info *info);
+
+	/**
+	 * dispatch_max_batch - Max nr of tasks that dispatch() can dispatch
+	 */
+	u32 dispatch_max_batch;
+
+	/**
+	 * flags - %SCX_OPS_* flags
+	 */
+	u64 flags;
+
+	/**
+	 * timeout_ms - The maximum amount of time, in milliseconds, that a
+	 * runnable task should be able to wait before being scheduled. The
+	 * maximum timeout may not exceed the default timeout of 30 seconds.
+	 *
+	 * Defaults to the maximum allowed timeout value of 30 seconds.
+	 */
+	u32 timeout_ms;
+
+	/**
+	 * exit_dump_len - scx_exit_info.dump buffer length. If 0, the default
+	 * value of 32768 is used.
+	 */
+	u32 exit_dump_len;
+
+	/**
+	 * hotplug_seq - A sequence number that may be set by the scheduler to
+	 * detect when a hotplug event has occurred during the loading process.
+	 * If 0, no detection occurs. Otherwise, the scheduler will fail to
+	 * load if the sequence number does not match @scx_hotplug_seq on the
+	 * enable path.
+	 */
+	u64 hotplug_seq;
+
+	/**
+	 * name - BPF scheduler's name
+	 *
+	 * Must be a non-zero valid BPF object name including only isalnum(),
+	 * '_' and '.' chars. Shows up in kernel.sched_ext_ops sysctl while the
+	 * BPF scheduler is enabled.
+	 */
+	char name[SCX_OPS_NAME_LEN];
+};
+
+enum scx_opi {
+	SCX_OPI_BEGIN			= 0,
+	SCX_OPI_NORMAL_BEGIN		= 0,
+	SCX_OPI_NORMAL_END		= SCX_OP_IDX(cpu_online),
+	SCX_OPI_CPU_HOTPLUG_BEGIN	= SCX_OP_IDX(cpu_online),
+	SCX_OPI_CPU_HOTPLUG_END		= SCX_OP_IDX(init),
+	SCX_OPI_END			= SCX_OP_IDX(init),
+};
+
+enum scx_wake_flags {
+	/* expose select WF_* flags as enums */
+	SCX_WAKE_FORK		= WF_FORK,
+	SCX_WAKE_TTWU		= WF_TTWU,
+	SCX_WAKE_SYNC		= WF_SYNC,
+};
+
+enum scx_enq_flags {
+	/* expose select ENQUEUE_* flags as enums */
+	SCX_ENQ_WAKEUP		= ENQUEUE_WAKEUP,
+	SCX_ENQ_HEAD		= ENQUEUE_HEAD,
+
+	/* high 32bits are SCX specific */
+
+	/*
+	 * Set the following to trigger preemption when calling
+	 * scx_bpf_dispatch() with a local dsq as the target. The slice of the
+	 * current task is cleared to zero and the CPU is kicked into the
+	 * scheduling path. Implies %SCX_ENQ_HEAD.
+	 */
+	SCX_ENQ_PREEMPT		= 1LLU << 32,
+
+	/*
+	 * The task being enqueued was previously enqueued on the current CPU's
+	 * %SCX_DSQ_LOCAL, but was removed from it in a call to the
+	 * bpf_scx_reenqueue_local() kfunc. If bpf_scx_reenqueue_local() was
+	 * invoked in a ->cpu_release() callback, and the task is again
+	 * dispatched back to %SCX_LOCAL_DSQ by this current ->enqueue(), the
+	 * task will not be scheduled on the CPU until at least the next invocation
+	 * of the ->cpu_acquire() callback.
+	 */
+	SCX_ENQ_REENQ		= 1LLU << 40,
+
+	/*
+	 * The task being enqueued is the only task available for the cpu. By
+	 * default, ext core keeps executing such tasks but when
+	 * %SCX_OPS_ENQ_LAST is specified, they're ops.enqueue()'d with the
+	 * %SCX_ENQ_LAST flag set.
+	 *
+	 * If the BPF scheduler wants to continue executing the task,
+	 * ops.enqueue() should dispatch the task to %SCX_DSQ_LOCAL immediately.
+	 * If the task gets queued on a different dsq or the BPF side, the BPF
+	 * scheduler is responsible for triggering a follow-up scheduling event.
+	 * Otherwise, Execution may stall.
+	 */
+	SCX_ENQ_LAST		= 1LLU << 41,
+
+	/* high 8 bits are internal */
+	__SCX_ENQ_INTERNAL_MASK	= 0xffLLU << 56,
+
+	SCX_ENQ_CLEAR_OPSS	= 1LLU << 56,
+	SCX_ENQ_DSQ_PRIQ	= 1LLU << 57,
+};
+
+enum scx_deq_flags {
+	/* expose select DEQUEUE_* flags as enums */
+	SCX_DEQ_SLEEP		= DEQUEUE_SLEEP,
+
+	/* high 32bits are SCX specific */
+
+	/*
+	 * The generic core-sched layer decided to execute the task even though
+	 * it hasn't been dispatched yet. Dequeue from the BPF side.
+	 */
+	SCX_DEQ_CORE_SCHED_EXEC	= 1LLU << 32,
+};
+
+enum scx_pick_idle_cpu_flags {
+	SCX_PICK_IDLE_CORE	= 1LLU << 0,	/* pick a CPU whose SMT siblings are also idle */
+};
+
+enum scx_kick_flags {
+	/*
+	 * Kick the target CPU if idle. Guarantees that the target CPU goes
+	 * through at least one full scheduling cycle before going idle. If the
+	 * target CPU can be determined to be currently not idle and going to go
+	 * through a scheduling cycle before going idle, noop.
+	 */
+	SCX_KICK_IDLE		= 1LLU << 0,
+
+	/*
+	 * Preempt the current task and execute the dispatch path. If the
+	 * current task of the target CPU is an SCX task, its ->scx.slice is
+	 * cleared to zero before the scheduling path is invoked so that the
+	 * task expires and the dispatch path is invoked.
+	 */
+	SCX_KICK_PREEMPT	= 1LLU << 1,
+
+	/*
+	 * Wait for the CPU to be rescheduled. The scx_bpf_kick_cpu() call will
+	 * return after the target CPU finishes picking the next task.
+	 */
+	SCX_KICK_WAIT		= 1LLU << 2,
+};
+
+enum scx_tg_flags {
+	SCX_TG_ONLINE		= 1U << 0,
+	SCX_TG_INITED		= 1U << 1,
+};
+
+enum scx_ops_enable_state {
+	SCX_OPS_ENABLING,
+	SCX_OPS_ENABLED,
+	SCX_OPS_DISABLING,
+	SCX_OPS_DISABLED,
+};
+
+static const char *scx_ops_enable_state_str[] = {
+	[SCX_OPS_ENABLING]	= "enabling",
+	[SCX_OPS_ENABLED]	= "enabled",
+	[SCX_OPS_DISABLING]	= "disabling",
+	[SCX_OPS_DISABLED]	= "disabled",
+};
+
+/*
+ * sched_ext_entity->ops_state
+ *
+ * Used to track the task ownership between the SCX core and the BPF scheduler.
+ * State transitions look as follows:
+ *
+ * NONE -> QUEUEING -> QUEUED -> DISPATCHING
+ *   ^              |                 |
+ *   |              v                 v
+ *   \-------------------------------/
+ *
+ * QUEUEING and DISPATCHING states can be waited upon. See wait_ops_state() call
+ * sites for explanations on the conditions being waited upon and why they are
+ * safe. Transitions out of them into NONE or QUEUED must store_release and the
+ * waiters should load_acquire.
+ *
+ * Tracking scx_ops_state enables sched_ext core to reliably determine whether
+ * any given task can be dispatched by the BPF scheduler at all times and thus
+ * relaxes the requirements on the BPF scheduler. This allows the BPF scheduler
+ * to try to dispatch any task anytime regardless of its state as the SCX core
+ * can safely reject invalid dispatches.
+ */
+enum scx_ops_state {
+	SCX_OPSS_NONE,		/* owned by the SCX core */
+	SCX_OPSS_QUEUEING,	/* in transit to the BPF scheduler */
+	SCX_OPSS_QUEUED,	/* owned by the BPF scheduler */
+	SCX_OPSS_DISPATCHING,	/* in transit back to the SCX core */
+
+	/*
+	 * QSEQ brands each QUEUED instance so that, when dispatch races
+	 * dequeue/requeue, the dispatcher can tell whether it still has a claim
+	 * on the task being dispatched.
+	 *
+	 * As some 32bit archs can't do 64bit store_release/load_acquire,
+	 * p->scx.ops_state is atomic_long_t which leaves 30 bits for QSEQ on
+	 * 32bit machines. The dispatch race window QSEQ protects is very narrow
+	 * and runs with IRQ disabled. 30 bits should be sufficient.
+	 */
+	SCX_OPSS_QSEQ_SHIFT	= 2,
+};
+
+/* Use macros to ensure that the type is unsigned long for the masks */
+#define SCX_OPSS_STATE_MASK	((1LU << SCX_OPSS_QSEQ_SHIFT) - 1)
+#define SCX_OPSS_QSEQ_MASK	(~SCX_OPSS_STATE_MASK)
+
+/*
+ * During exit, a task may schedule after losing its PIDs. When disabling the
+ * BPF scheduler, we need to be able to iterate tasks in every state to
+ * guarantee system safety. Maintain a dedicated task list which contains every
+ * task between its fork and eventual free.
+ */
+static DEFINE_SPINLOCK(scx_tasks_lock);
+static LIST_HEAD(scx_tasks);
+
+/* ops enable/disable */
+static struct kthread_worker *scx_ops_helper;
+static DEFINE_MUTEX(scx_ops_enable_mutex);
+DEFINE_STATIC_KEY_FALSE(__scx_ops_enabled);
+DEFINE_STATIC_PERCPU_RWSEM(scx_fork_rwsem);
+static atomic_t scx_ops_enable_state_var = ATOMIC_INIT(SCX_OPS_DISABLED);
+static atomic_t scx_ops_bypass_depth = ATOMIC_INIT(0);
+static bool scx_ops_init_task_enabled;
+static bool scx_switching_all;
+DEFINE_STATIC_KEY_FALSE(__scx_switched_all);
+
+static struct sched_ext_ops scx_ops;
+static bool scx_warned_zero_slice;
+
+static DEFINE_STATIC_KEY_FALSE(scx_ops_enq_last);
+static DEFINE_STATIC_KEY_FALSE(scx_ops_enq_exiting);
+static DEFINE_STATIC_KEY_FALSE(scx_ops_cpu_preempt);
+static DEFINE_STATIC_KEY_FALSE(scx_builtin_idle_enabled);
+
+static struct static_key_false scx_has_op[SCX_OPI_END] =
+	{ [0 ... SCX_OPI_END-1] = STATIC_KEY_FALSE_INIT };
+
+static atomic_t scx_exit_kind = ATOMIC_INIT(SCX_EXIT_DONE);
+static struct scx_exit_info *scx_exit_info;
+
+static atomic_long_t scx_nr_rejected = ATOMIC_LONG_INIT(0);
+static atomic_long_t scx_hotplug_seq = ATOMIC_LONG_INIT(0);
+
+/*
+ * A monotically increasing sequence number that is incremented every time a
+ * scheduler is enabled. This can be used by to check if any custom sched_ext
+ * scheduler has ever been used in the system.
+ */
+static atomic_long_t scx_enable_seq = ATOMIC_LONG_INIT(0);
+
+/*
+ * The maximum amount of time in jiffies that a task may be runnable without
+ * being scheduled on a CPU. If this timeout is exceeded, it will trigger
+ * scx_ops_error().
+ */
+static unsigned long scx_watchdog_timeout;
+
+/*
+ * The last time the delayed work was run. This delayed work relies on
+ * ksoftirqd being able to run to service timer interrupts, so it's possible
+ * that this work itself could get wedged. To account for this, we check that
+ * it's not stalled in the timer tick, and trigger an error if it is.
+ */
+static unsigned long scx_watchdog_timestamp = INITIAL_JIFFIES;
+
+static struct delayed_work scx_watchdog_work;
+
+/* idle tracking */
+#ifdef CONFIG_SMP
+#ifdef CONFIG_CPUMASK_OFFSTACK
+#define CL_ALIGNED_IF_ONSTACK
+#else
+#define CL_ALIGNED_IF_ONSTACK __cacheline_aligned_in_smp
+#endif
+
+static struct {
+	cpumask_var_t cpu;
+	cpumask_var_t smt;
+} idle_masks CL_ALIGNED_IF_ONSTACK;
+
+#endif	/* CONFIG_SMP */
+
+/* for %SCX_KICK_WAIT */
+static unsigned long __percpu *scx_kick_cpus_pnt_seqs;
+
+/*
+ * Direct dispatch marker.
+ *
+ * Non-NULL values are used for direct dispatch from enqueue path. A valid
+ * pointer points to the task currently being enqueued. An ERR_PTR value is used
+ * to indicate that direct dispatch has already happened.
+ */
+static DEFINE_PER_CPU(struct task_struct *, direct_dispatch_task);
+
+/*
+ * Dispatch queues.
+ *
+ * The global DSQ (%SCX_DSQ_GLOBAL) is split per-node for scalability. This is
+ * to avoid live-locking in bypass mode where all tasks are dispatched to
+ * %SCX_DSQ_GLOBAL and all CPUs consume from it. If per-node split isn't
+ * sufficient, it can be further split.
+ */
+static struct scx_dispatch_q **global_dsqs;
+
+static const struct rhashtable_params dsq_hash_params = {
+	.key_len		= 8,
+	.key_offset		= offsetof(struct scx_dispatch_q, id),
+	.head_offset		= offsetof(struct scx_dispatch_q, hash_node),
+};
+
+static struct rhashtable dsq_hash;
+static LLIST_HEAD(dsqs_to_free);
+
+/* dispatch buf */
+struct scx_dsp_buf_ent {
+	struct task_struct	*task;
+	unsigned long		qseq;
+	u64			dsq_id;
+	u64			enq_flags;
+};
+
+static u32 scx_dsp_max_batch;
+
+struct scx_dsp_ctx {
+	struct rq		*rq;
+	u32			cursor;
+	u32			nr_tasks;
+	struct scx_dsp_buf_ent	buf[];
+};
+
+static struct scx_dsp_ctx __percpu *scx_dsp_ctx;
+
+/* string formatting from BPF */
+struct scx_bstr_buf {
+	u64			data[MAX_BPRINTF_VARARGS];
+	char			line[SCX_EXIT_MSG_LEN];
+};
+
+static DEFINE_RAW_SPINLOCK(scx_exit_bstr_buf_lock);
+static struct scx_bstr_buf scx_exit_bstr_buf;
+
+/* ops debug dump */
+struct scx_dump_data {
+	s32			cpu;
+	bool			first;
+	s32			cursor;
+	struct seq_buf		*s;
+	const char		*prefix;
+	struct scx_bstr_buf	buf;
+};
+
+static struct scx_dump_data scx_dump_data = {
+	.cpu			= -1,
+};
+
+/* /sys/kernel/sched_ext interface */
+static struct kset *scx_kset;
+static struct kobject *scx_root_kobj;
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/sched_ext.h>
+
+static void process_ddsp_deferred_locals(struct rq *rq);
+static void scx_bpf_kick_cpu(s32 cpu, u64 flags);
+static __printf(3, 4) void scx_ops_exit_kind(enum scx_exit_kind kind,
+					     s64 exit_code,
+					     const char *fmt, ...);
+
+#define scx_ops_error_kind(err, fmt, args...)					\
+	scx_ops_exit_kind((err), 0, fmt, ##args)
+
+#define scx_ops_exit(code, fmt, args...)					\
+	scx_ops_exit_kind(SCX_EXIT_UNREG_KERN, (code), fmt, ##args)
+
+#define scx_ops_error(fmt, args...)						\
+	scx_ops_error_kind(SCX_EXIT_ERROR, fmt, ##args)
+
+#define SCX_HAS_OP(op)	static_branch_likely(&scx_has_op[SCX_OP_IDX(op)])
+
+static long jiffies_delta_msecs(unsigned long at, unsigned long now)
+{
+	if (time_after(at, now))
+		return jiffies_to_msecs(at - now);
+	else
+		return -(long)jiffies_to_msecs(now - at);
+}
+
+/* if the highest set bit is N, return a mask with bits [N+1, 31] set */
+static u32 higher_bits(u32 flags)
+{
+	return ~((1 << fls(flags)) - 1);
+}
+
+/* return the mask with only the highest bit set */
+static u32 highest_bit(u32 flags)
+{
+	int bit = fls(flags);
+	return ((u64)1 << bit) >> 1;
+}
+
+static bool u32_before(u32 a, u32 b)
+{
+	return (s32)(a - b) < 0;
+}
+
+static struct scx_dispatch_q *find_global_dsq(struct task_struct *p)
+{
+	return global_dsqs[cpu_to_node(task_cpu(p))];
+}
+
+static struct scx_dispatch_q *find_user_dsq(u64 dsq_id)
+{
+	return rhashtable_lookup_fast(&dsq_hash, &dsq_id, dsq_hash_params);
+}
+
+/*
+ * scx_kf_mask enforcement. Some kfuncs can only be called from specific SCX
+ * ops. When invoking SCX ops, SCX_CALL_OP[_RET]() should be used to indicate
+ * the allowed kfuncs and those kfuncs should use scx_kf_allowed() to check
+ * whether it's running from an allowed context.
+ *
+ * @mask is constant, always inline to cull the mask calculations.
+ */
+static __always_inline void scx_kf_allow(u32 mask)
+{
+	/* nesting is allowed only in increasing scx_kf_mask order */
+	WARN_ONCE((mask | higher_bits(mask)) & current->scx.kf_mask,
+		  "invalid nesting current->scx.kf_mask=0x%x mask=0x%x\n",
+		  current->scx.kf_mask, mask);
+	current->scx.kf_mask |= mask;
+	barrier();
+}
+
+static void scx_kf_disallow(u32 mask)
+{
+	barrier();
+	current->scx.kf_mask &= ~mask;
+}
+
+#define SCX_CALL_OP(mask, op, args...)						\
+do {										\
+	if (mask) {								\
+		scx_kf_allow(mask);						\
+		scx_ops.op(args);						\
+		scx_kf_disallow(mask);						\
+	} else {								\
+		scx_ops.op(args);						\
+	}									\
+} while (0)
+
+#define SCX_CALL_OP_RET(mask, op, args...)					\
+({										\
+	__typeof__(scx_ops.op(args)) __ret;					\
+	if (mask) {								\
+		scx_kf_allow(mask);						\
+		__ret = scx_ops.op(args);					\
+		scx_kf_disallow(mask);						\
+	} else {								\
+		__ret = scx_ops.op(args);					\
+	}									\
+	__ret;									\
+})
+
+/*
+ * Some kfuncs are allowed only on the tasks that are subjects of the
+ * in-progress scx_ops operation for, e.g., locking guarantees. To enforce such
+ * restrictions, the following SCX_CALL_OP_*() variants should be used when
+ * invoking scx_ops operations that take task arguments. These can only be used
+ * for non-nesting operations due to the way the tasks are tracked.
+ *
+ * kfuncs which can only operate on such tasks can in turn use
+ * scx_kf_allowed_on_arg_tasks() to test whether the invocation is allowed on
+ * the specific task.
+ */
+#define SCX_CALL_OP_TASK(mask, op, task, args...)				\
+do {										\
+	BUILD_BUG_ON((mask) & ~__SCX_KF_TERMINAL);				\
+	current->scx.kf_tasks[0] = task;					\
+	SCX_CALL_OP(mask, op, task, ##args);					\
+	current->scx.kf_tasks[0] = NULL;					\
+} while (0)
+
+#define SCX_CALL_OP_TASK_RET(mask, op, task, args...)				\
+({										\
+	__typeof__(scx_ops.op(task, ##args)) __ret;				\
+	BUILD_BUG_ON((mask) & ~__SCX_KF_TERMINAL);				\
+	current->scx.kf_tasks[0] = task;					\
+	__ret = SCX_CALL_OP_RET(mask, op, task, ##args);			\
+	current->scx.kf_tasks[0] = NULL;					\
+	__ret;									\
+})
+
+#define SCX_CALL_OP_2TASKS_RET(mask, op, task0, task1, args...)			\
+({										\
+	__typeof__(scx_ops.op(task0, task1, ##args)) __ret;			\
+	BUILD_BUG_ON((mask) & ~__SCX_KF_TERMINAL);				\
+	current->scx.kf_tasks[0] = task0;					\
+	current->scx.kf_tasks[1] = task1;					\
+	__ret = SCX_CALL_OP_RET(mask, op, task0, task1, ##args);		\
+	current->scx.kf_tasks[0] = NULL;					\
+	current->scx.kf_tasks[1] = NULL;					\
+	__ret;									\
+})
+
+/* @mask is constant, always inline to cull unnecessary branches */
+static __always_inline bool scx_kf_allowed(u32 mask)
+{
+	if (unlikely(!(current->scx.kf_mask & mask))) {
+		scx_ops_error("kfunc with mask 0x%x called from an operation only allowing 0x%x",
+			      mask, current->scx.kf_mask);
+		return false;
+	}
+
+	/*
+	 * Enforce nesting boundaries. e.g. A kfunc which can be called from
+	 * DISPATCH must not be called if we're running DEQUEUE which is nested
+	 * inside ops.dispatch(). We don't need to check boundaries for any
+	 * blocking kfuncs as the verifier ensures they're only called from
+	 * sleepable progs.
+	 */
+	if (unlikely(highest_bit(mask) == SCX_KF_CPU_RELEASE &&
+		     (current->scx.kf_mask & higher_bits(SCX_KF_CPU_RELEASE)))) {
+		scx_ops_error("cpu_release kfunc called from a nested operation");
+		return false;
+	}
+
+	if (unlikely(highest_bit(mask) == SCX_KF_DISPATCH &&
+		     (current->scx.kf_mask & higher_bits(SCX_KF_DISPATCH)))) {
+		scx_ops_error("dispatch kfunc called from a nested operation");
+		return false;
+	}
+
+	return true;
+}
+
+/* see SCX_CALL_OP_TASK() */
+static __always_inline bool scx_kf_allowed_on_arg_tasks(u32 mask,
+							struct task_struct *p)
+{
+	if (!scx_kf_allowed(mask))
+		return false;
+
+	if (unlikely((p != current->scx.kf_tasks[0] &&
+		      p != current->scx.kf_tasks[1]))) {
+		scx_ops_error("called on a task not being operated on");
+		return false;
+	}
+
+	return true;
+}
+
+static bool scx_kf_allowed_if_unlocked(void)
+{
+	return !current->scx.kf_mask;
+}
+
+/**
+ * nldsq_next_task - Iterate to the next task in a non-local DSQ
+ * @dsq: user dsq being interated
+ * @cur: current position, %NULL to start iteration
+ * @rev: walk backwards
+ *
+ * Returns %NULL when iteration is finished.
+ */
+static struct task_struct *nldsq_next_task(struct scx_dispatch_q *dsq,
+					   struct task_struct *cur, bool rev)
+{
+	struct list_head *list_node;
+	struct scx_dsq_list_node *dsq_lnode;
+
+	lockdep_assert_held(&dsq->lock);
+
+	if (cur)
+		list_node = &cur->scx.dsq_list.node;
+	else
+		list_node = &dsq->list;
+
+	/* find the next task, need to skip BPF iteration cursors */
+	do {
+		if (rev)
+			list_node = list_node->prev;
+		else
+			list_node = list_node->next;
+
+		if (list_node == &dsq->list)
+			return NULL;
+
+		dsq_lnode = container_of(list_node, struct scx_dsq_list_node,
+					 node);
+	} while (dsq_lnode->flags & SCX_DSQ_LNODE_ITER_CURSOR);
+
+	return container_of(dsq_lnode, struct task_struct, scx.dsq_list);
+}
+
+#define nldsq_for_each_task(p, dsq)						\
+	for ((p) = nldsq_next_task((dsq), NULL, false); (p);			\
+	     (p) = nldsq_next_task((dsq), (p), false))
+
+
+/*
+ * BPF DSQ iterator. Tasks in a non-local DSQ can be iterated in [reverse]
+ * dispatch order. BPF-visible iterator is opaque and larger to allow future
+ * changes without breaking backward compatibility. Can be used with
+ * bpf_for_each(). See bpf_iter_scx_dsq_*().
+ */
+enum scx_dsq_iter_flags {
+	/* iterate in the reverse dispatch order */
+	SCX_DSQ_ITER_REV		= 1U << 16,
+
+	__SCX_DSQ_ITER_HAS_SLICE	= 1U << 30,
+	__SCX_DSQ_ITER_HAS_VTIME	= 1U << 31,
+
+	__SCX_DSQ_ITER_USER_FLAGS	= SCX_DSQ_ITER_REV,
+	__SCX_DSQ_ITER_ALL_FLAGS	= __SCX_DSQ_ITER_USER_FLAGS |
+					  __SCX_DSQ_ITER_HAS_SLICE |
+					  __SCX_DSQ_ITER_HAS_VTIME,
+};
+
+struct bpf_iter_scx_dsq_kern {
+	struct scx_dsq_list_node	cursor;
+	struct scx_dispatch_q		*dsq;
+	u64				slice;
+	u64				vtime;
+} __attribute__((aligned(8)));
+
+struct bpf_iter_scx_dsq {
+	u64				__opaque[6];
+} __attribute__((aligned(8)));
+
+
+/*
+ * SCX task iterator.
+ */
+struct scx_task_iter {
+	struct sched_ext_entity		cursor;
+	struct task_struct		*locked;
+	struct rq			*rq;
+	struct rq_flags			rf;
+	u32				cnt;
+};
+
+/**
+ * scx_task_iter_start - Lock scx_tasks_lock and start a task iteration
+ * @iter: iterator to init
+ *
+ * Initialize @iter and return with scx_tasks_lock held. Once initialized, @iter
+ * must eventually be stopped with scx_task_iter_stop().
+ *
+ * scx_tasks_lock and the rq lock may be released using scx_task_iter_unlock()
+ * between this and the first next() call or between any two next() calls. If
+ * the locks are released between two next() calls, the caller is responsible
+ * for ensuring that the task being iterated remains accessible either through
+ * RCU read lock or obtaining a reference count.
+ *
+ * All tasks which existed when the iteration started are guaranteed to be
+ * visited as long as they still exist.
+ */
+static void scx_task_iter_start(struct scx_task_iter *iter)
+{
+	BUILD_BUG_ON(__SCX_DSQ_ITER_ALL_FLAGS &
+		     ((1U << __SCX_DSQ_LNODE_PRIV_SHIFT) - 1));
+
+	spin_lock_irq(&scx_tasks_lock);
+
+	iter->cursor = (struct sched_ext_entity){ .flags = SCX_TASK_CURSOR };
+	list_add(&iter->cursor.tasks_node, &scx_tasks);
+	iter->locked = NULL;
+	iter->cnt = 0;
+}
+
+static void __scx_task_iter_rq_unlock(struct scx_task_iter *iter)
+{
+	if (iter->locked) {
+		task_rq_unlock(iter->rq, iter->locked, &iter->rf);
+		iter->locked = NULL;
+	}
+}
+
+/**
+ * scx_task_iter_unlock - Unlock rq and scx_tasks_lock held by a task iterator
+ * @iter: iterator to unlock
+ *
+ * If @iter is in the middle of a locked iteration, it may be locking the rq of
+ * the task currently being visited in addition to scx_tasks_lock. Unlock both.
+ * This function can be safely called anytime during an iteration.
+ */
+static void scx_task_iter_unlock(struct scx_task_iter *iter)
+{
+	__scx_task_iter_rq_unlock(iter);
+	spin_unlock_irq(&scx_tasks_lock);
+}
+
+/**
+ * scx_task_iter_relock - Lock scx_tasks_lock released by scx_task_iter_unlock()
+ * @iter: iterator to re-lock
+ *
+ * Re-lock scx_tasks_lock unlocked by scx_task_iter_unlock(). Note that it
+ * doesn't re-lock the rq lock. Must be called before other iterator operations.
+ */
+static void scx_task_iter_relock(struct scx_task_iter *iter)
+{
+	spin_lock_irq(&scx_tasks_lock);
+}
+
+/**
+ * scx_task_iter_stop - Stop a task iteration and unlock scx_tasks_lock
+ * @iter: iterator to exit
+ *
+ * Exit a previously initialized @iter. Must be called with scx_tasks_lock held
+ * which is released on return. If the iterator holds a task's rq lock, that rq
+ * lock is also released. See scx_task_iter_start() for details.
+ */
+static void scx_task_iter_stop(struct scx_task_iter *iter)
+{
+	list_del_init(&iter->cursor.tasks_node);
+	scx_task_iter_unlock(iter);
+}
+
+/**
+ * scx_task_iter_next - Next task
+ * @iter: iterator to walk
+ *
+ * Visit the next task. See scx_task_iter_start() for details. Locks are dropped
+ * and re-acquired every %SCX_OPS_TASK_ITER_BATCH iterations to avoid causing
+ * stalls by holding scx_tasks_lock for too long.
+ */
+static struct task_struct *scx_task_iter_next(struct scx_task_iter *iter)
+{
+	struct list_head *cursor = &iter->cursor.tasks_node;
+	struct sched_ext_entity *pos;
+
+	if (!(++iter->cnt % SCX_OPS_TASK_ITER_BATCH)) {
+		scx_task_iter_unlock(iter);
+		cpu_relax();
+		cond_resched();
+		scx_task_iter_relock(iter);
+	}
+
+	list_for_each_entry(pos, cursor, tasks_node) {
+		if (&pos->tasks_node == &scx_tasks)
+			return NULL;
+		if (!(pos->flags & SCX_TASK_CURSOR)) {
+			list_move(cursor, &pos->tasks_node);
+			return container_of(pos, struct task_struct, scx);
+		}
+	}
+
+	/* can't happen, should always terminate at scx_tasks above */
+	BUG();
+}
+
+/**
+ * scx_task_iter_next_locked - Next non-idle task with its rq locked
+ * @iter: iterator to walk
+ * @include_dead: Whether we should include dead tasks in the iteration
+ *
+ * Visit the non-idle task with its rq lock held. Allows callers to specify
+ * whether they would like to filter out dead tasks. See scx_task_iter_start()
+ * for details.
+ */
+static struct task_struct *scx_task_iter_next_locked(struct scx_task_iter *iter)
+{
+	struct task_struct *p;
+
+	__scx_task_iter_rq_unlock(iter);
+
+	while ((p = scx_task_iter_next(iter))) {
+		/*
+		 * scx_task_iter is used to prepare and move tasks into SCX
+		 * while loading the BPF scheduler and vice-versa while
+		 * unloading. The init_tasks ("swappers") should be excluded
+		 * from the iteration because:
+		 *
+		 * - It's unsafe to use __setschduler_prio() on an init_task to
+		 *   determine the sched_class to use as it won't preserve its
+		 *   idle_sched_class.
+		 *
+		 * - ops.init/exit_task() can easily be confused if called with
+		 *   init_tasks as they, e.g., share PID 0.
+		 *
+		 * As init_tasks are never scheduled through SCX, they can be
+		 * skipped safely. Note that is_idle_task() which tests %PF_IDLE
+		 * doesn't work here:
+		 *
+		 * - %PF_IDLE may not be set for an init_task whose CPU hasn't
+		 *   yet been onlined.
+		 *
+		 * - %PF_IDLE can be set on tasks that are not init_tasks. See
+		 *   play_idle_precise() used by CONFIG_IDLE_INJECT.
+		 *
+		 * Test for idle_sched_class as only init_tasks are on it.
+		 */
+		if (p->sched_class != &idle_sched_class)
+			break;
+	}
+	if (!p)
+		return NULL;
+
+	iter->rq = task_rq_lock(p, &iter->rf);
+	iter->locked = p;
+
+	return p;
+}
+
+static enum scx_ops_enable_state scx_ops_enable_state(void)
+{
+	return atomic_read(&scx_ops_enable_state_var);
+}
+
+static enum scx_ops_enable_state
+scx_ops_set_enable_state(enum scx_ops_enable_state to)
+{
+	return atomic_xchg(&scx_ops_enable_state_var, to);
+}
+
+static bool scx_ops_tryset_enable_state(enum scx_ops_enable_state to,
+					enum scx_ops_enable_state from)
+{
+	int from_v = from;
+
+	return atomic_try_cmpxchg(&scx_ops_enable_state_var, &from_v, to);
+}
+
+static bool scx_rq_bypassing(struct rq *rq)
+{
+	return unlikely(rq->scx.flags & SCX_RQ_BYPASSING);
+}
+
+/**
+ * wait_ops_state - Busy-wait the specified ops state to end
+ * @p: target task
+ * @opss: state to wait the end of
+ *
+ * Busy-wait for @p to transition out of @opss. This can only be used when the
+ * state part of @opss is %SCX_QUEUEING or %SCX_DISPATCHING. This function also
+ * has load_acquire semantics to ensure that the caller can see the updates made
+ * in the enqueueing and dispatching paths.
+ */
+static void wait_ops_state(struct task_struct *p, unsigned long opss)
+{
+	do {
+		cpu_relax();
+	} while (atomic_long_read_acquire(&p->scx.ops_state) == opss);
+}
+
+/**
+ * ops_cpu_valid - Verify a cpu number
+ * @cpu: cpu number which came from a BPF ops
+ * @where: extra information reported on error
+ *
+ * @cpu is a cpu number which came from the BPF scheduler and can be any value.
+ * Verify that it is in range and one of the possible cpus. If invalid, trigger
+ * an ops error.
+ */
+static bool ops_cpu_valid(s32 cpu, const char *where)
+{
+	if (likely(cpu >= 0 && cpu < nr_cpu_ids && cpu_possible(cpu))) {
+		return true;
+	} else {
+		scx_ops_error("invalid CPU %d%s%s", cpu,
+			      where ? " " : "", where ?: "");
+		return false;
+	}
+}
+
+/**
+ * ops_sanitize_err - Sanitize a -errno value
+ * @ops_name: operation to blame on failure
+ * @err: -errno value to sanitize
+ *
+ * Verify @err is a valid -errno. If not, trigger scx_ops_error() and return
+ * -%EPROTO. This is necessary because returning a rogue -errno up the chain can
+ * cause misbehaviors. For an example, a large negative return from
+ * ops.init_task() triggers an oops when passed up the call chain because the
+ * value fails IS_ERR() test after being encoded with ERR_PTR() and then is
+ * handled as a pointer.
+ */
+static int ops_sanitize_err(const char *ops_name, s32 err)
+{
+	if (err < 0 && err >= -MAX_ERRNO)
+		return err;
+
+	scx_ops_error("ops.%s() returned an invalid errno %d", ops_name, err);
+	return -EPROTO;
+}
+
+static void run_deferred(struct rq *rq)
+{
+	process_ddsp_deferred_locals(rq);
+}
+
+#ifdef CONFIG_SMP
+static void deferred_bal_cb_workfn(struct rq *rq)
+{
+	run_deferred(rq);
+}
+#endif
+
+static void deferred_irq_workfn(struct irq_work *irq_work)
+{
+	struct rq *rq = container_of(irq_work, struct rq, scx.deferred_irq_work);
+
+	raw_spin_rq_lock(rq);
+	run_deferred(rq);
+	raw_spin_rq_unlock(rq);
+}
+
+/**
+ * schedule_deferred - Schedule execution of deferred actions on an rq
+ * @rq: target rq
+ *
+ * Schedule execution of deferred actions on @rq. Must be called with @rq
+ * locked. Deferred actions are executed with @rq locked but unpinned, and thus
+ * can unlock @rq to e.g. migrate tasks to other rqs.
+ */
+static void schedule_deferred(struct rq *rq)
+{
+	lockdep_assert_rq_held(rq);
+
+#ifdef CONFIG_SMP
+	/*
+	 * If in the middle of waking up a task, task_woken_scx() will be called
+	 * afterwards which will then run the deferred actions, no need to
+	 * schedule anything.
+	 */
+	if (rq->scx.flags & SCX_RQ_IN_WAKEUP)
+		return;
+
+	/*
+	 * If in balance, the balance callbacks will be called before rq lock is
+	 * released. Schedule one.
+	 */
+	if (rq->scx.flags & SCX_RQ_IN_BALANCE) {
+		queue_balance_callback(rq, &rq->scx.deferred_bal_cb,
+				       deferred_bal_cb_workfn);
+		return;
+	}
+#endif
+	/*
+	 * No scheduler hooks available. Queue an irq work. They are executed on
+	 * IRQ re-enable which may take a bit longer than the scheduler hooks.
+	 * The above WAKEUP and BALANCE paths should cover most of the cases and
+	 * the time to IRQ re-enable shouldn't be long.
+	 */
+	irq_work_queue(&rq->scx.deferred_irq_work);
+}
+
+/**
+ * touch_core_sched - Update timestamp used for core-sched task ordering
+ * @rq: rq to read clock from, must be locked
+ * @p: task to update the timestamp for
+ *
+ * Update @p->scx.core_sched_at timestamp. This is used by scx_prio_less() to
+ * implement global or local-DSQ FIFO ordering for core-sched. Should be called
+ * when a task becomes runnable and its turn on the CPU ends (e.g. slice
+ * exhaustion).
+ */
+static void touch_core_sched(struct rq *rq, struct task_struct *p)
+{
+	lockdep_assert_rq_held(rq);
+
+#ifdef CONFIG_SCHED_CORE
+	/*
+	 * It's okay to update the timestamp spuriously. Use
+	 * sched_core_disabled() which is cheaper than enabled().
+	 *
+	 * As this is used to determine ordering between tasks of sibling CPUs,
+	 * it may be better to use per-core dispatch sequence instead.
+	 */
+	if (!sched_core_disabled())
+		p->scx.core_sched_at = sched_clock_cpu(cpu_of(rq));
+#endif
+}
+
+/**
+ * touch_core_sched_dispatch - Update core-sched timestamp on dispatch
+ * @rq: rq to read clock from, must be locked
+ * @p: task being dispatched
+ *
+ * If the BPF scheduler implements custom core-sched ordering via
+ * ops.core_sched_before(), @p->scx.core_sched_at is used to implement FIFO
+ * ordering within each local DSQ. This function is called from dispatch paths
+ * and updates @p->scx.core_sched_at if custom core-sched ordering is in effect.
+ */
+static void touch_core_sched_dispatch(struct rq *rq, struct task_struct *p)
+{
+	lockdep_assert_rq_held(rq);
+
+#ifdef CONFIG_SCHED_CORE
+	if (SCX_HAS_OP(core_sched_before))
+		touch_core_sched(rq, p);
+#endif
+}
+
+static void update_curr_scx(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	s64 delta_exec;
+
+	delta_exec = update_curr_common(rq);
+	if (unlikely(delta_exec <= 0))
+		return;
+
+	if (curr->scx.slice != SCX_SLICE_INF) {
+		curr->scx.slice -= min_t(u64, curr->scx.slice, delta_exec);
+		if (!curr->scx.slice)
+			touch_core_sched(rq, curr);
+	}
+}
+
+static bool scx_dsq_priq_less(struct rb_node *node_a,
+			      const struct rb_node *node_b)
+{
+	const struct task_struct *a =
+		container_of(node_a, struct task_struct, scx.dsq_priq);
+	const struct task_struct *b =
+		container_of(node_b, struct task_struct, scx.dsq_priq);
+
+	return time_before64(a->scx.dsq_vtime, b->scx.dsq_vtime);
+}
+
+static void dsq_mod_nr(struct scx_dispatch_q *dsq, s32 delta)
+{
+	/* scx_bpf_dsq_nr_queued() reads ->nr without locking, use WRITE_ONCE() */
+	WRITE_ONCE(dsq->nr, dsq->nr + delta);
+}
+
+static void dispatch_enqueue(struct scx_dispatch_q *dsq, struct task_struct *p,
+			     u64 enq_flags)
+{
+	bool is_local = dsq->id == SCX_DSQ_LOCAL;
+
+	WARN_ON_ONCE(p->scx.dsq || !list_empty(&p->scx.dsq_list.node));
+	WARN_ON_ONCE((p->scx.dsq_flags & SCX_TASK_DSQ_ON_PRIQ) ||
+		     !RB_EMPTY_NODE(&p->scx.dsq_priq));
+
+	if (!is_local) {
+		raw_spin_lock(&dsq->lock);
+		if (unlikely(dsq->id == SCX_DSQ_INVALID)) {
+			scx_ops_error("attempting to dispatch to a destroyed dsq");
+			/* fall back to the global dsq */
+			raw_spin_unlock(&dsq->lock);
+			dsq = find_global_dsq(p);
+			raw_spin_lock(&dsq->lock);
+		}
+	}
+
+	if (unlikely((dsq->id & SCX_DSQ_FLAG_BUILTIN) &&
+		     (enq_flags & SCX_ENQ_DSQ_PRIQ))) {
+		/*
+		 * SCX_DSQ_LOCAL and SCX_DSQ_GLOBAL DSQs always consume from
+		 * their FIFO queues. To avoid confusion and accidentally
+		 * starving vtime-dispatched tasks by FIFO-dispatched tasks, we
+		 * disallow any internal DSQ from doing vtime ordering of
+		 * tasks.
+		 */
+		scx_ops_error("cannot use vtime ordering for built-in DSQs");
+		enq_flags &= ~SCX_ENQ_DSQ_PRIQ;
+	}
+
+	if (enq_flags & SCX_ENQ_DSQ_PRIQ) {
+		struct rb_node *rbp;
+
+		/*
+		 * A PRIQ DSQ shouldn't be using FIFO enqueueing. As tasks are
+		 * linked to both the rbtree and list on PRIQs, this can only be
+		 * tested easily when adding the first task.
+		 */
+		if (unlikely(RB_EMPTY_ROOT(&dsq->priq) &&
+			     nldsq_next_task(dsq, NULL, false)))
+			scx_ops_error("DSQ ID 0x%016llx already had FIFO-enqueued tasks",
+				      dsq->id);
+
+		p->scx.dsq_flags |= SCX_TASK_DSQ_ON_PRIQ;
+		rb_add(&p->scx.dsq_priq, &dsq->priq, scx_dsq_priq_less);
+
+		/*
+		 * Find the previous task and insert after it on the list so
+		 * that @dsq->list is vtime ordered.
+		 */
+		rbp = rb_prev(&p->scx.dsq_priq);
+		if (rbp) {
+			struct task_struct *prev =
+				container_of(rbp, struct task_struct,
+					     scx.dsq_priq);
+			list_add(&p->scx.dsq_list.node, &prev->scx.dsq_list.node);
+		} else {
+			list_add(&p->scx.dsq_list.node, &dsq->list);
+		}
+	} else {
+		/* a FIFO DSQ shouldn't be using PRIQ enqueuing */
+		if (unlikely(!RB_EMPTY_ROOT(&dsq->priq)))
+			scx_ops_error("DSQ ID 0x%016llx already had PRIQ-enqueued tasks",
+				      dsq->id);
+
+		if (enq_flags & (SCX_ENQ_HEAD | SCX_ENQ_PREEMPT))
+			list_add(&p->scx.dsq_list.node, &dsq->list);
+		else
+			list_add_tail(&p->scx.dsq_list.node, &dsq->list);
+	}
+
+	/* seq records the order tasks are queued, used by BPF DSQ iterator */
+	dsq->seq++;
+	p->scx.dsq_seq = dsq->seq;
+
+	dsq_mod_nr(dsq, 1);
+	p->scx.dsq = dsq;
+
+	/*
+	 * scx.ddsp_dsq_id and scx.ddsp_enq_flags are only relevant on the
+	 * direct dispatch path, but we clear them here because the direct
+	 * dispatch verdict may be overridden on the enqueue path during e.g.
+	 * bypass.
+	 */
+	p->scx.ddsp_dsq_id = SCX_DSQ_INVALID;
+	p->scx.ddsp_enq_flags = 0;
+
+	/*
+	 * We're transitioning out of QUEUEING or DISPATCHING. store_release to
+	 * match waiters' load_acquire.
+	 */
+	if (enq_flags & SCX_ENQ_CLEAR_OPSS)
+		atomic_long_set_release(&p->scx.ops_state, SCX_OPSS_NONE);
+
+	if (is_local) {
+		struct rq *rq = container_of(dsq, struct rq, scx.local_dsq);
+		bool preempt = false;
+
+		if ((enq_flags & SCX_ENQ_PREEMPT) && p != rq->curr &&
+		    rq->curr->sched_class == &ext_sched_class) {
+			rq->curr->scx.slice = 0;
+			preempt = true;
+		}
+
+		if (preempt || sched_class_above(&ext_sched_class,
+						 rq->curr->sched_class))
+			resched_curr(rq);
+	} else {
+		raw_spin_unlock(&dsq->lock);
+	}
+}
+
+static void task_unlink_from_dsq(struct task_struct *p,
+				 struct scx_dispatch_q *dsq)
+{
+	WARN_ON_ONCE(list_empty(&p->scx.dsq_list.node));
+
+	if (p->scx.dsq_flags & SCX_TASK_DSQ_ON_PRIQ) {
+		rb_erase(&p->scx.dsq_priq, &dsq->priq);
+		RB_CLEAR_NODE(&p->scx.dsq_priq);
+		p->scx.dsq_flags &= ~SCX_TASK_DSQ_ON_PRIQ;
+	}
+
+	list_del_init(&p->scx.dsq_list.node);
+	dsq_mod_nr(dsq, -1);
+}
+
+static void dispatch_dequeue(struct rq *rq, struct task_struct *p)
+{
+	struct scx_dispatch_q *dsq = p->scx.dsq;
+	bool is_local = dsq == &rq->scx.local_dsq;
+
+	if (!dsq) {
+		/*
+		 * If !dsq && on-list, @p is on @rq's ddsp_deferred_locals.
+		 * Unlinking is all that's needed to cancel.
+		 */
+		if (unlikely(!list_empty(&p->scx.dsq_list.node)))
+			list_del_init(&p->scx.dsq_list.node);
+
+		/*
+		 * When dispatching directly from the BPF scheduler to a local
+		 * DSQ, the task isn't associated with any DSQ but
+		 * @p->scx.holding_cpu may be set under the protection of
+		 * %SCX_OPSS_DISPATCHING.
+		 */
+		if (p->scx.holding_cpu >= 0)
+			p->scx.holding_cpu = -1;
+
+		return;
+	}
+
+	if (!is_local)
+		raw_spin_lock(&dsq->lock);
+
+	/*
+	 * Now that we hold @dsq->lock, @p->holding_cpu and @p->scx.dsq_* can't
+	 * change underneath us.
+	*/
+	if (p->scx.holding_cpu < 0) {
+		/* @p must still be on @dsq, dequeue */
+		task_unlink_from_dsq(p, dsq);
+	} else {
+		/*
+		 * We're racing against dispatch_to_local_dsq() which already
+		 * removed @p from @dsq and set @p->scx.holding_cpu. Clear the
+		 * holding_cpu which tells dispatch_to_local_dsq() that it lost
+		 * the race.
+		 */
+		WARN_ON_ONCE(!list_empty(&p->scx.dsq_list.node));
+		p->scx.holding_cpu = -1;
+	}
+	p->scx.dsq = NULL;
+
+	if (!is_local)
+		raw_spin_unlock(&dsq->lock);
+}
+
+static struct scx_dispatch_q *find_dsq_for_dispatch(struct rq *rq, u64 dsq_id,
+						    struct task_struct *p)
+{
+	struct scx_dispatch_q *dsq;
+
+	if (dsq_id == SCX_DSQ_LOCAL)
+		return &rq->scx.local_dsq;
+
+	if ((dsq_id & SCX_DSQ_LOCAL_ON) == SCX_DSQ_LOCAL_ON) {
+		s32 cpu = dsq_id & SCX_DSQ_LOCAL_CPU_MASK;
+
+		if (!ops_cpu_valid(cpu, "in SCX_DSQ_LOCAL_ON dispatch verdict"))
+			return find_global_dsq(p);
+
+		return &cpu_rq(cpu)->scx.local_dsq;
+	}
+
+	if (dsq_id == SCX_DSQ_GLOBAL)
+		dsq = find_global_dsq(p);
+	else
+		dsq = find_user_dsq(dsq_id);
+
+	if (unlikely(!dsq)) {
+		scx_ops_error("non-existent DSQ 0x%llx for %s[%d]",
+			      dsq_id, p->comm, p->pid);
+		return find_global_dsq(p);
+	}
+
+	return dsq;
+}
+
+static void mark_direct_dispatch(struct task_struct *ddsp_task,
+				 struct task_struct *p, u64 dsq_id,
+				 u64 enq_flags)
+{
+	/*
+	 * Mark that dispatch already happened from ops.select_cpu() or
+	 * ops.enqueue() by spoiling direct_dispatch_task with a non-NULL value
+	 * which can never match a valid task pointer.
+	 */
+	__this_cpu_write(direct_dispatch_task, ERR_PTR(-ESRCH));
+
+	/* @p must match the task on the enqueue path */
+	if (unlikely(p != ddsp_task)) {
+		if (IS_ERR(ddsp_task))
+			scx_ops_error("%s[%d] already direct-dispatched",
+				      p->comm, p->pid);
+		else
+			scx_ops_error("scheduling for %s[%d] but trying to direct-dispatch %s[%d]",
+				      ddsp_task->comm, ddsp_task->pid,
+				      p->comm, p->pid);
+		return;
+	}
+
+	WARN_ON_ONCE(p->scx.ddsp_dsq_id != SCX_DSQ_INVALID);
+	WARN_ON_ONCE(p->scx.ddsp_enq_flags);
+
+	p->scx.ddsp_dsq_id = dsq_id;
+	p->scx.ddsp_enq_flags = enq_flags;
+}
+
+static void direct_dispatch(struct task_struct *p, u64 enq_flags)
+{
+	struct rq *rq = task_rq(p);
+	struct scx_dispatch_q *dsq =
+		find_dsq_for_dispatch(rq, p->scx.ddsp_dsq_id, p);
+
+	touch_core_sched_dispatch(rq, p);
+
+	p->scx.ddsp_enq_flags |= enq_flags;
+
+	/*
+	 * We are in the enqueue path with @rq locked and pinned, and thus can't
+	 * double lock a remote rq and enqueue to its local DSQ. For
+	 * DSQ_LOCAL_ON verdicts targeting the local DSQ of a remote CPU, defer
+	 * the enqueue so that it's executed when @rq can be unlocked.
+	 */
+	if (dsq->id == SCX_DSQ_LOCAL && dsq != &rq->scx.local_dsq) {
+		unsigned long opss;
+
+		opss = atomic_long_read(&p->scx.ops_state) & SCX_OPSS_STATE_MASK;
+
+		switch (opss & SCX_OPSS_STATE_MASK) {
+		case SCX_OPSS_NONE:
+			break;
+		case SCX_OPSS_QUEUEING:
+			/*
+			 * As @p was never passed to the BPF side, _release is
+			 * not strictly necessary. Still do it for consistency.
+			 */
+			atomic_long_set_release(&p->scx.ops_state, SCX_OPSS_NONE);
+			break;
+		default:
+			WARN_ONCE(true, "sched_ext: %s[%d] has invalid ops state 0x%lx in direct_dispatch()",
+				  p->comm, p->pid, opss);
+			atomic_long_set_release(&p->scx.ops_state, SCX_OPSS_NONE);
+			break;
+		}
+
+		WARN_ON_ONCE(p->scx.dsq || !list_empty(&p->scx.dsq_list.node));
+		list_add_tail(&p->scx.dsq_list.node,
+			      &rq->scx.ddsp_deferred_locals);
+		schedule_deferred(rq);
+		return;
+	}
+
+	dispatch_enqueue(dsq, p, p->scx.ddsp_enq_flags | SCX_ENQ_CLEAR_OPSS);
+}
+
+static bool scx_rq_online(struct rq *rq)
+{
+	/*
+	 * Test both cpu_active() and %SCX_RQ_ONLINE. %SCX_RQ_ONLINE indicates
+	 * the online state as seen from the BPF scheduler. cpu_active() test
+	 * guarantees that, if this function returns %true, %SCX_RQ_ONLINE will
+	 * stay set until the current scheduling operation is complete even if
+	 * we aren't locking @rq.
+	 */
+	return likely((rq->scx.flags & SCX_RQ_ONLINE) && cpu_active(cpu_of(rq)));
+}
+
+static void do_enqueue_task(struct rq *rq, struct task_struct *p, u64 enq_flags,
+			    int sticky_cpu)
+{
+	struct task_struct **ddsp_taskp;
+	unsigned long qseq;
+
+	WARN_ON_ONCE(!(p->scx.flags & SCX_TASK_QUEUED));
+
+	/* rq migration */
+	if (sticky_cpu == cpu_of(rq))
+		goto local_norefill;
+
+	/*
+	 * If !scx_rq_online(), we already told the BPF scheduler that the CPU
+	 * is offline and are just running the hotplug path. Don't bother the
+	 * BPF scheduler.
+	 */
+	if (!scx_rq_online(rq))
+		goto local;
+
+	if (scx_rq_bypassing(rq)) {
+		if (enq_flags & SCX_ENQ_LAST)
+			goto local;
+		else
+			goto global;
+	}
+
+	if (p->scx.ddsp_dsq_id != SCX_DSQ_INVALID)
+		goto direct;
+
+	/* see %SCX_OPS_ENQ_EXITING */
+	if (!static_branch_unlikely(&scx_ops_enq_exiting) &&
+	    unlikely(p->flags & PF_EXITING))
+		goto local;
+
+	/* see %SCX_OPS_ENQ_LAST */
+	if (!static_branch_unlikely(&scx_ops_enq_last) &&
+	    (enq_flags & SCX_ENQ_LAST))
+		goto local;
+
+	if (!SCX_HAS_OP(enqueue))
+		goto global;
+
+	/* DSQ bypass didn't trigger, enqueue on the BPF scheduler */
+	qseq = rq->scx.ops_qseq++ << SCX_OPSS_QSEQ_SHIFT;
+
+	WARN_ON_ONCE(atomic_long_read(&p->scx.ops_state) != SCX_OPSS_NONE);
+	atomic_long_set(&p->scx.ops_state, SCX_OPSS_QUEUEING | qseq);
+
+	ddsp_taskp = this_cpu_ptr(&direct_dispatch_task);
+	WARN_ON_ONCE(*ddsp_taskp);
+	*ddsp_taskp = p;
+
+	SCX_CALL_OP_TASK(SCX_KF_ENQUEUE, enqueue, p, enq_flags);
+
+	*ddsp_taskp = NULL;
+	if (p->scx.ddsp_dsq_id != SCX_DSQ_INVALID)
+		goto direct;
+
+	/*
+	 * If not directly dispatched, QUEUEING isn't clear yet and dispatch or
+	 * dequeue may be waiting. The store_release matches their load_acquire.
+	 */
+	atomic_long_set_release(&p->scx.ops_state, SCX_OPSS_QUEUED | qseq);
+	return;
+
+direct:
+	direct_dispatch(p, enq_flags);
+	return;
+
+local:
+	/*
+	 * For task-ordering, slice refill must be treated as implying the end
+	 * of the current slice. Otherwise, the longer @p stays on the CPU, the
+	 * higher priority it becomes from scx_prio_less()'s POV.
+	 */
+	touch_core_sched(rq, p);
+	p->scx.slice = SCX_SLICE_DFL;
+local_norefill:
+	dispatch_enqueue(&rq->scx.local_dsq, p, enq_flags);
+	return;
+
+global:
+	touch_core_sched(rq, p);	/* see the comment in local: */
+	p->scx.slice = SCX_SLICE_DFL;
+	dispatch_enqueue(find_global_dsq(p), p, enq_flags);
+}
+
+static bool task_runnable(const struct task_struct *p)
+{
+	return !list_empty(&p->scx.runnable_node);
+}
+
+static void set_task_runnable(struct rq *rq, struct task_struct *p)
+{
+	lockdep_assert_rq_held(rq);
+
+	if (p->scx.flags & SCX_TASK_RESET_RUNNABLE_AT) {
+		p->scx.runnable_at = jiffies;
+		p->scx.flags &= ~SCX_TASK_RESET_RUNNABLE_AT;
+	}
+
+	/*
+	 * list_add_tail() must be used. scx_ops_bypass() depends on tasks being
+	 * appened to the runnable_list.
+	 */
+	list_add_tail(&p->scx.runnable_node, &rq->scx.runnable_list);
+}
+
+static void clr_task_runnable(struct task_struct *p, bool reset_runnable_at)
+{
+	list_del_init(&p->scx.runnable_node);
+	if (reset_runnable_at)
+		p->scx.flags |= SCX_TASK_RESET_RUNNABLE_AT;
+}
+
+static void enqueue_task_scx(struct rq *rq, struct task_struct *p, int enq_flags)
+{
+	int sticky_cpu = p->scx.sticky_cpu;
+
+	if (enq_flags & ENQUEUE_WAKEUP)
+		rq->scx.flags |= SCX_RQ_IN_WAKEUP;
+
+	enq_flags |= rq->scx.extra_enq_flags;
+
+	if (sticky_cpu >= 0)
+		p->scx.sticky_cpu = -1;
+
+	/*
+	 * Restoring a running task will be immediately followed by
+	 * set_next_task_scx() which expects the task to not be on the BPF
+	 * scheduler as tasks can only start running through local DSQs. Force
+	 * direct-dispatch into the local DSQ by setting the sticky_cpu.
+	 */
+	if (unlikely(enq_flags & ENQUEUE_RESTORE) && task_current(rq, p))
+		sticky_cpu = cpu_of(rq);
+
+	if (p->scx.flags & SCX_TASK_QUEUED) {
+		WARN_ON_ONCE(!task_runnable(p));
+		goto out;
+	}
+
+	set_task_runnable(rq, p);
+	p->scx.flags |= SCX_TASK_QUEUED;
+	rq->scx.nr_running++;
+	add_nr_running(rq, 1);
+
+	if (SCX_HAS_OP(runnable) && !task_on_rq_migrating(p))
+		SCX_CALL_OP_TASK(SCX_KF_REST, runnable, p, enq_flags);
+
+	if (enq_flags & SCX_ENQ_WAKEUP)
+		touch_core_sched(rq, p);
+
+	do_enqueue_task(rq, p, enq_flags, sticky_cpu);
+out:
+	rq->scx.flags &= ~SCX_RQ_IN_WAKEUP;
+}
+
+static void ops_dequeue(struct task_struct *p, u64 deq_flags)
+{
+	unsigned long opss;
+
+	/* dequeue is always temporary, don't reset runnable_at */
+	clr_task_runnable(p, false);
+
+	/* acquire ensures that we see the preceding updates on QUEUED */
+	opss = atomic_long_read_acquire(&p->scx.ops_state);
+
+	switch (opss & SCX_OPSS_STATE_MASK) {
+	case SCX_OPSS_NONE:
+		break;
+	case SCX_OPSS_QUEUEING:
+		/*
+		 * QUEUEING is started and finished while holding @p's rq lock.
+		 * As we're holding the rq lock now, we shouldn't see QUEUEING.
+		 */
+		BUG();
+	case SCX_OPSS_QUEUED:
+		if (SCX_HAS_OP(dequeue))
+			SCX_CALL_OP_TASK(SCX_KF_REST, dequeue, p, deq_flags);
+
+		if (atomic_long_try_cmpxchg(&p->scx.ops_state, &opss,
+					    SCX_OPSS_NONE))
+			break;
+		fallthrough;
+	case SCX_OPSS_DISPATCHING:
+		/*
+		 * If @p is being dispatched from the BPF scheduler to a DSQ,
+		 * wait for the transfer to complete so that @p doesn't get
+		 * added to its DSQ after dequeueing is complete.
+		 *
+		 * As we're waiting on DISPATCHING with the rq locked, the
+		 * dispatching side shouldn't try to lock the rq while
+		 * DISPATCHING is set. See dispatch_to_local_dsq().
+		 *
+		 * DISPATCHING shouldn't have qseq set and control can reach
+		 * here with NONE @opss from the above QUEUED case block.
+		 * Explicitly wait on %SCX_OPSS_DISPATCHING instead of @opss.
+		 */
+		wait_ops_state(p, SCX_OPSS_DISPATCHING);
+		BUG_ON(atomic_long_read(&p->scx.ops_state) != SCX_OPSS_NONE);
+		break;
+	}
+}
+
+static void dequeue_task_scx(struct rq *rq, struct task_struct *p, int deq_flags)
+{
+	if (!(p->scx.flags & SCX_TASK_QUEUED)) {
+		WARN_ON_ONCE(task_runnable(p));
+		return;
+	}
+
+	ops_dequeue(p, deq_flags);
+
+	/*
+	 * A currently running task which is going off @rq first gets dequeued
+	 * and then stops running. As we want running <-> stopping transitions
+	 * to be contained within runnable <-> quiescent transitions, trigger
+	 * ->stopping() early here instead of in put_prev_task_scx().
+	 *
+	 * @p may go through multiple stopping <-> running transitions between
+	 * here and put_prev_task_scx() if task attribute changes occur while
+	 * balance_scx() leaves @rq unlocked. However, they don't contain any
+	 * information meaningful to the BPF scheduler and can be suppressed by
+	 * skipping the callbacks if the task is !QUEUED.
+	 */
+	if (SCX_HAS_OP(stopping) && task_current(rq, p)) {
+		update_curr_scx(rq);
+		SCX_CALL_OP_TASK(SCX_KF_REST, stopping, p, false);
+	}
+
+	if (SCX_HAS_OP(quiescent) && !task_on_rq_migrating(p))
+		SCX_CALL_OP_TASK(SCX_KF_REST, quiescent, p, deq_flags);
+
+	if (deq_flags & SCX_DEQ_SLEEP)
+		p->scx.flags |= SCX_TASK_DEQD_FOR_SLEEP;
+	else
+		p->scx.flags &= ~SCX_TASK_DEQD_FOR_SLEEP;
+
+	p->scx.flags &= ~SCX_TASK_QUEUED;
+	rq->scx.nr_running--;
+	sub_nr_running(rq, 1);
+
+	dispatch_dequeue(rq, p);
+}
+
+static void yield_task_scx(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+
+	if (SCX_HAS_OP(yield))
+		SCX_CALL_OP_2TASKS_RET(SCX_KF_REST, yield, p, NULL);
+	else
+		p->scx.slice = 0;
+}
+
+static bool yield_to_task_scx(struct rq *rq, struct task_struct *to)
+{
+	struct task_struct *from = rq->curr;
+
+	if (SCX_HAS_OP(yield))
+		return SCX_CALL_OP_2TASKS_RET(SCX_KF_REST, yield, from, to);
+	else
+		return false;
+}
+
+static void move_local_task_to_local_dsq(struct task_struct *p, u64 enq_flags,
+					 struct scx_dispatch_q *src_dsq,
+					 struct rq *dst_rq)
+{
+	struct scx_dispatch_q *dst_dsq = &dst_rq->scx.local_dsq;
+
+	/* @dsq is locked and @p is on @dst_rq */
+	lockdep_assert_held(&src_dsq->lock);
+	lockdep_assert_rq_held(dst_rq);
+
+	WARN_ON_ONCE(p->scx.holding_cpu >= 0);
+
+	if (enq_flags & (SCX_ENQ_HEAD | SCX_ENQ_PREEMPT))
+		list_add(&p->scx.dsq_list.node, &dst_dsq->list);
+	else
+		list_add_tail(&p->scx.dsq_list.node, &dst_dsq->list);
+
+	dsq_mod_nr(dst_dsq, 1);
+	p->scx.dsq = dst_dsq;
+}
+
+#ifdef CONFIG_SMP
+/**
+ * move_remote_task_to_local_dsq - Move a task from a foreign rq to a local DSQ
+ * @p: task to move
+ * @enq_flags: %SCX_ENQ_*
+ * @src_rq: rq to move the task from, locked on entry, released on return
+ * @dst_rq: rq to move the task into, locked on return
+ *
+ * Move @p which is currently on @src_rq to @dst_rq's local DSQ.
+ */
+static void move_remote_task_to_local_dsq(struct task_struct *p, u64 enq_flags,
+					  struct rq *src_rq, struct rq *dst_rq)
+{
+	lockdep_assert_rq_held(src_rq);
+
+	/* the following marks @p MIGRATING which excludes dequeue */
+	deactivate_task(src_rq, p, 0);
+	set_task_cpu(p, cpu_of(dst_rq));
+	p->scx.sticky_cpu = cpu_of(dst_rq);
+
+	raw_spin_rq_unlock(src_rq);
+	raw_spin_rq_lock(dst_rq);
+
+	/*
+	 * We want to pass scx-specific enq_flags but activate_task() will
+	 * truncate the upper 32 bit. As we own @rq, we can pass them through
+	 * @rq->scx.extra_enq_flags instead.
+	 */
+	WARN_ON_ONCE(!cpumask_test_cpu(cpu_of(dst_rq), p->cpus_ptr));
+	WARN_ON_ONCE(dst_rq->scx.extra_enq_flags);
+	dst_rq->scx.extra_enq_flags = enq_flags;
+	activate_task(dst_rq, p, 0);
+	dst_rq->scx.extra_enq_flags = 0;
+}
+
+/*
+ * Similar to kernel/sched/core.c::is_cpu_allowed(). However, there are two
+ * differences:
+ *
+ * - is_cpu_allowed() asks "Can this task run on this CPU?" while
+ *   task_can_run_on_remote_rq() asks "Can the BPF scheduler migrate the task to
+ *   this CPU?".
+ *
+ *   While migration is disabled, is_cpu_allowed() has to say "yes" as the task
+ *   must be allowed to finish on the CPU that it's currently on regardless of
+ *   the CPU state. However, task_can_run_on_remote_rq() must say "no" as the
+ *   BPF scheduler shouldn't attempt to migrate a task which has migration
+ *   disabled.
+ *
+ * - The BPF scheduler is bypassed while the rq is offline and we can always say
+ *   no to the BPF scheduler initiated migrations while offline.
+ */
+static bool task_can_run_on_remote_rq(struct task_struct *p, struct rq *rq,
+				      bool trigger_error)
+{
+	int cpu = cpu_of(rq);
+
+	/*
+	 * We don't require the BPF scheduler to avoid dispatching to offline
+	 * CPUs mostly for convenience but also because CPUs can go offline
+	 * between scx_bpf_dispatch() calls and here. Trigger error iff the
+	 * picked CPU is outside the allowed mask.
+	 */
+	if (!task_allowed_on_cpu(p, cpu)) {
+		if (trigger_error)
+			scx_ops_error("SCX_DSQ_LOCAL[_ON] verdict target cpu %d not allowed for %s[%d]",
+				      cpu_of(rq), p->comm, p->pid);
+		return false;
+	}
+
+	if (unlikely(is_migration_disabled(p)))
+		return false;
+
+	if (!scx_rq_online(rq))
+		return false;
+
+	return true;
+}
+
+/**
+ * unlink_dsq_and_lock_src_rq() - Unlink task from its DSQ and lock its task_rq
+ * @p: target task
+ * @dsq: locked DSQ @p is currently on
+ * @src_rq: rq @p is currently on, stable with @dsq locked
+ *
+ * Called with @dsq locked but no rq's locked. We want to move @p to a different
+ * DSQ, including any local DSQ, but are not locking @src_rq. Locking @src_rq is
+ * required when transferring into a local DSQ. Even when transferring into a
+ * non-local DSQ, it's better to use the same mechanism to protect against
+ * dequeues and maintain the invariant that @p->scx.dsq can only change while
+ * @src_rq is locked, which e.g. scx_dump_task() depends on.
+ *
+ * We want to grab @src_rq but that can deadlock if we try while locking @dsq,
+ * so we want to unlink @p from @dsq, drop its lock and then lock @src_rq. As
+ * this may race with dequeue, which can't drop the rq lock or fail, do a little
+ * dancing from our side.
+ *
+ * @p->scx.holding_cpu is set to this CPU before @dsq is unlocked. If @p gets
+ * dequeued after we unlock @dsq but before locking @src_rq, the holding_cpu
+ * would be cleared to -1. While other cpus may have updated it to different
+ * values afterwards, as this operation can't be preempted or recurse, the
+ * holding_cpu can never become this CPU again before we're done. Thus, we can
+ * tell whether we lost to dequeue by testing whether the holding_cpu still
+ * points to this CPU. See dispatch_dequeue() for the counterpart.
+ *
+ * On return, @dsq is unlocked and @src_rq is locked. Returns %true if @p is
+ * still valid. %false if lost to dequeue.
+ */
+static bool unlink_dsq_and_lock_src_rq(struct task_struct *p,
+				       struct scx_dispatch_q *dsq,
+				       struct rq *src_rq)
+{
+	s32 cpu = raw_smp_processor_id();
+
+	lockdep_assert_held(&dsq->lock);
+
+	WARN_ON_ONCE(p->scx.holding_cpu >= 0);
+	task_unlink_from_dsq(p, dsq);
+	p->scx.holding_cpu = cpu;
+
+	raw_spin_unlock(&dsq->lock);
+	raw_spin_rq_lock(src_rq);
+
+	/* task_rq couldn't have changed if we're still the holding cpu */
+	return likely(p->scx.holding_cpu == cpu) &&
+		!WARN_ON_ONCE(src_rq != task_rq(p));
+}
+
+static bool consume_remote_task(struct rq *this_rq, struct task_struct *p,
+				struct scx_dispatch_q *dsq, struct rq *src_rq)
+{
+	raw_spin_rq_unlock(this_rq);
+
+	if (unlink_dsq_and_lock_src_rq(p, dsq, src_rq)) {
+		move_remote_task_to_local_dsq(p, 0, src_rq, this_rq);
+		return true;
+	} else {
+		raw_spin_rq_unlock(src_rq);
+		raw_spin_rq_lock(this_rq);
+		return false;
+	}
+}
+#else	/* CONFIG_SMP */
+static inline void move_remote_task_to_local_dsq(struct task_struct *p, u64 enq_flags, struct rq *src_rq, struct rq *dst_rq) { WARN_ON_ONCE(1); }
+static inline bool task_can_run_on_remote_rq(struct task_struct *p, struct rq *rq, bool trigger_error) { return false; }
+static inline bool consume_remote_task(struct rq *this_rq, struct task_struct *p, struct scx_dispatch_q *dsq, struct rq *task_rq) { return false; }
+#endif	/* CONFIG_SMP */
+
+static bool consume_dispatch_q(struct rq *rq, struct scx_dispatch_q *dsq)
+{
+	struct task_struct *p;
+retry:
+	/*
+	 * The caller can't expect to successfully consume a task if the task's
+	 * addition to @dsq isn't guaranteed to be visible somehow. Test
+	 * @dsq->list without locking and skip if it seems empty.
+	 */
+	if (list_empty(&dsq->list))
+		return false;
+
+	raw_spin_lock(&dsq->lock);
+
+	nldsq_for_each_task(p, dsq) {
+		struct rq *task_rq = task_rq(p);
+
+		if (rq == task_rq) {
+			task_unlink_from_dsq(p, dsq);
+			move_local_task_to_local_dsq(p, 0, dsq, rq);
+			raw_spin_unlock(&dsq->lock);
+			return true;
+		}
+
+		if (task_can_run_on_remote_rq(p, rq, false)) {
+			if (likely(consume_remote_task(rq, p, dsq, task_rq)))
+				return true;
+			goto retry;
+		}
+	}
+
+	raw_spin_unlock(&dsq->lock);
+	return false;
+}
+
+static bool consume_global_dsq(struct rq *rq)
+{
+	int node = cpu_to_node(cpu_of(rq));
+
+	return consume_dispatch_q(rq, global_dsqs[node]);
+}
+
+/**
+ * dispatch_to_local_dsq - Dispatch a task to a local dsq
+ * @rq: current rq which is locked
+ * @dst_dsq: destination DSQ
+ * @p: task to dispatch
+ * @enq_flags: %SCX_ENQ_*
+ *
+ * We're holding @rq lock and want to dispatch @p to @dst_dsq which is a local
+ * DSQ. This function performs all the synchronization dancing needed because
+ * local DSQs are protected with rq locks.
+ *
+ * The caller must have exclusive ownership of @p (e.g. through
+ * %SCX_OPSS_DISPATCHING).
+ */
+static void dispatch_to_local_dsq(struct rq *rq, struct scx_dispatch_q *dst_dsq,
+				  struct task_struct *p, u64 enq_flags)
+{
+	struct rq *src_rq = task_rq(p);
+	struct rq *dst_rq = container_of(dst_dsq, struct rq, scx.local_dsq);
+
+	/*
+	 * We're synchronized against dequeue through DISPATCHING. As @p can't
+	 * be dequeued, its task_rq and cpus_allowed are stable too.
+	 *
+	 * If dispatching to @rq that @p is already on, no lock dancing needed.
+	 */
+	if (rq == src_rq && rq == dst_rq) {
+		dispatch_enqueue(dst_dsq, p, enq_flags | SCX_ENQ_CLEAR_OPSS);
+		return;
+	}
+
+#ifdef CONFIG_SMP
+	if (unlikely(!task_can_run_on_remote_rq(p, dst_rq, true))) {
+		dispatch_enqueue(find_global_dsq(p), p,
+				 enq_flags | SCX_ENQ_CLEAR_OPSS);
+		return;
+	}
+
+	/*
+	 * @p is on a possibly remote @src_rq which we need to lock to move the
+	 * task. If dequeue is in progress, it'd be locking @src_rq and waiting
+	 * on DISPATCHING, so we can't grab @src_rq lock while holding
+	 * DISPATCHING.
+	 *
+	 * As DISPATCHING guarantees that @p is wholly ours, we can pretend that
+	 * we're moving from a DSQ and use the same mechanism - mark the task
+	 * under transfer with holding_cpu, release DISPATCHING and then follow
+	 * the same protocol. See unlink_dsq_and_lock_src_rq().
+	 */
+	p->scx.holding_cpu = raw_smp_processor_id();
+
+	/* store_release ensures that dequeue sees the above */
+	atomic_long_set_release(&p->scx.ops_state, SCX_OPSS_NONE);
+
+	/* switch to @src_rq lock */
+	if (rq != src_rq) {
+		raw_spin_rq_unlock(rq);
+		raw_spin_rq_lock(src_rq);
+	}
+
+	/* task_rq couldn't have changed if we're still the holding cpu */
+	if (likely(p->scx.holding_cpu == raw_smp_processor_id()) &&
+	    !WARN_ON_ONCE(src_rq != task_rq(p))) {
+		/*
+		 * If @p is staying on the same rq, there's no need to go
+		 * through the full deactivate/activate cycle. Optimize by
+		 * abbreviating move_remote_task_to_local_dsq().
+		 */
+		if (src_rq == dst_rq) {
+			p->scx.holding_cpu = -1;
+			dispatch_enqueue(&dst_rq->scx.local_dsq, p, enq_flags);
+		} else {
+			move_remote_task_to_local_dsq(p, enq_flags,
+						      src_rq, dst_rq);
+		}
+
+		/* if the destination CPU is idle, wake it up */
+		if (sched_class_above(p->sched_class, dst_rq->curr->sched_class))
+			resched_curr(dst_rq);
+	}
+
+	/* switch back to @rq lock */
+	if (rq != dst_rq) {
+		raw_spin_rq_unlock(dst_rq);
+		raw_spin_rq_lock(rq);
+	}
+#else	/* CONFIG_SMP */
+	BUG();	/* control can not reach here on UP */
+#endif	/* CONFIG_SMP */
+}
+
+/**
+ * finish_dispatch - Asynchronously finish dispatching a task
+ * @rq: current rq which is locked
+ * @p: task to finish dispatching
+ * @qseq_at_dispatch: qseq when @p started getting dispatched
+ * @dsq_id: destination DSQ ID
+ * @enq_flags: %SCX_ENQ_*
+ *
+ * Dispatching to local DSQs may need to wait for queueing to complete or
+ * require rq lock dancing. As we don't wanna do either while inside
+ * ops.dispatch() to avoid locking order inversion, we split dispatching into
+ * two parts. scx_bpf_dispatch() which is called by ops.dispatch() records the
+ * task and its qseq. Once ops.dispatch() returns, this function is called to
+ * finish up.
+ *
+ * There is no guarantee that @p is still valid for dispatching or even that it
+ * was valid in the first place. Make sure that the task is still owned by the
+ * BPF scheduler and claim the ownership before dispatching.
+ */
+static void finish_dispatch(struct rq *rq, struct task_struct *p,
+			    unsigned long qseq_at_dispatch,
+			    u64 dsq_id, u64 enq_flags)
+{
+	struct scx_dispatch_q *dsq;
+	unsigned long opss;
+
+	touch_core_sched_dispatch(rq, p);
+retry:
+	/*
+	 * No need for _acquire here. @p is accessed only after a successful
+	 * try_cmpxchg to DISPATCHING.
+	 */
+	opss = atomic_long_read(&p->scx.ops_state);
+
+	switch (opss & SCX_OPSS_STATE_MASK) {
+	case SCX_OPSS_DISPATCHING:
+	case SCX_OPSS_NONE:
+		/* someone else already got to it */
+		return;
+	case SCX_OPSS_QUEUED:
+		/*
+		 * If qseq doesn't match, @p has gone through at least one
+		 * dispatch/dequeue and re-enqueue cycle between
+		 * scx_bpf_dispatch() and here and we have no claim on it.
+		 */
+		if ((opss & SCX_OPSS_QSEQ_MASK) != qseq_at_dispatch)
+			return;
+
+		/*
+		 * While we know @p is accessible, we don't yet have a claim on
+		 * it - the BPF scheduler is allowed to dispatch tasks
+		 * spuriously and there can be a racing dequeue attempt. Let's
+		 * claim @p by atomically transitioning it from QUEUED to
+		 * DISPATCHING.
+		 */
+		if (likely(atomic_long_try_cmpxchg(&p->scx.ops_state, &opss,
+						   SCX_OPSS_DISPATCHING)))
+			break;
+		goto retry;
+	case SCX_OPSS_QUEUEING:
+		/*
+		 * do_enqueue_task() is in the process of transferring the task
+		 * to the BPF scheduler while holding @p's rq lock. As we aren't
+		 * holding any kernel or BPF resource that the enqueue path may
+		 * depend upon, it's safe to wait.
+		 */
+		wait_ops_state(p, opss);
+		goto retry;
+	}
+
+	BUG_ON(!(p->scx.flags & SCX_TASK_QUEUED));
+
+	dsq = find_dsq_for_dispatch(this_rq(), dsq_id, p);
+
+	if (dsq->id == SCX_DSQ_LOCAL)
+		dispatch_to_local_dsq(rq, dsq, p, enq_flags);
+	else
+		dispatch_enqueue(dsq, p, enq_flags | SCX_ENQ_CLEAR_OPSS);
+}
+
+static void flush_dispatch_buf(struct rq *rq)
+{
+	struct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);
+	u32 u;
+
+	for (u = 0; u < dspc->cursor; u++) {
+		struct scx_dsp_buf_ent *ent = &dspc->buf[u];
+
+		finish_dispatch(rq, ent->task, ent->qseq, ent->dsq_id,
+				ent->enq_flags);
+	}
+
+	dspc->nr_tasks += dspc->cursor;
+	dspc->cursor = 0;
+}
+
+static int balance_one(struct rq *rq, struct task_struct *prev, bool local)
+{
+	struct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);
+	bool prev_on_scx = prev->sched_class == &ext_sched_class;
+	int nr_loops = SCX_DSP_MAX_LOOPS;
+	bool has_tasks = false;
+
+	lockdep_assert_rq_held(rq);
+	rq->scx.flags |= SCX_RQ_IN_BALANCE;
+
+	if (static_branch_unlikely(&scx_ops_cpu_preempt) &&
+	    unlikely(rq->scx.cpu_released)) {
+		/*
+		 * If the previous sched_class for the current CPU was not SCX,
+		 * notify the BPF scheduler that it again has control of the
+		 * core. This callback complements ->cpu_release(), which is
+		 * emitted in scx_next_task_picked().
+		 */
+		if (SCX_HAS_OP(cpu_acquire))
+			SCX_CALL_OP(0, cpu_acquire, cpu_of(rq), NULL);
+		rq->scx.cpu_released = false;
+	}
+
+	if (prev_on_scx) {
+		WARN_ON_ONCE(local && (prev->scx.flags & SCX_TASK_BAL_KEEP));
+		update_curr_scx(rq);
+
+		/*
+		 * If @prev is runnable & has slice left, it has priority and
+		 * fetching more just increases latency for the fetched tasks.
+		 * Tell put_prev_task_scx() to put @prev on local_dsq. If the
+		 * BPF scheduler wants to handle this explicitly, it should
+		 * implement ->cpu_released().
+		 *
+		 * See scx_ops_disable_workfn() for the explanation on the
+		 * bypassing test.
+		 *
+		 * When balancing a remote CPU for core-sched, there won't be a
+		 * following put_prev_task_scx() call and we don't own
+		 * %SCX_TASK_BAL_KEEP. Instead, pick_task_scx() will test the
+		 * same conditions later and pick @rq->curr accordingly.
+		 */
+		if ((prev->scx.flags & SCX_TASK_QUEUED) &&
+		    prev->scx.slice && !scx_rq_bypassing(rq)) {
+			if (local)
+				prev->scx.flags |= SCX_TASK_BAL_KEEP;
+			goto has_tasks;
+		}
+	}
+
+	/* if there already are tasks to run, nothing to do */
+	if (rq->scx.local_dsq.nr)
+		goto has_tasks;
+
+	if (consume_global_dsq(rq))
+		goto has_tasks;
+
+	if (!SCX_HAS_OP(dispatch) || scx_rq_bypassing(rq) || !scx_rq_online(rq))
+		goto out;
+
+	dspc->rq = rq;
+
+	/*
+	 * The dispatch loop. Because flush_dispatch_buf() may drop the rq lock,
+	 * the local DSQ might still end up empty after a successful
+	 * ops.dispatch(). If the local DSQ is empty even after ops.dispatch()
+	 * produced some tasks, retry. The BPF scheduler may depend on this
+	 * looping behavior to simplify its implementation.
+	 */
+	do {
+		dspc->nr_tasks = 0;
+
+		SCX_CALL_OP(SCX_KF_DISPATCH, dispatch, cpu_of(rq),
+			    prev_on_scx ? prev : NULL);
+
+		flush_dispatch_buf(rq);
+
+		if (rq->scx.local_dsq.nr)
+			goto has_tasks;
+		if (consume_global_dsq(rq))
+			goto has_tasks;
+
+		/*
+		 * ops.dispatch() can trap us in this loop by repeatedly
+		 * dispatching ineligible tasks. Break out once in a while to
+		 * allow the watchdog to run. As IRQ can't be enabled in
+		 * balance(), we want to complete this scheduling cycle and then
+		 * start a new one. IOW, we want to call resched_curr() on the
+		 * next, most likely idle, task, not the current one. Use
+		 * scx_bpf_kick_cpu() for deferred kicking.
+		 */
+		if (unlikely(!--nr_loops)) {
+			scx_bpf_kick_cpu(cpu_of(rq), 0);
+			break;
+		}
+	} while (dspc->nr_tasks);
+
+	goto out;
+
+has_tasks:
+	has_tasks = true;
+out:
+	rq->scx.flags &= ~SCX_RQ_IN_BALANCE;
+	return has_tasks;
+}
+
+static int balance_scx(struct rq *rq, struct task_struct *prev,
+		       struct rq_flags *rf)
+{
+	int ret;
+
+	rq_unpin_lock(rq, rf);
+
+	ret = balance_one(rq, prev, true);
+
+#ifdef CONFIG_SCHED_SMT
+	/*
+	 * When core-sched is enabled, this ops.balance() call will be followed
+	 * by put_prev_scx() and pick_task_scx() on this CPU and pick_task_scx()
+	 * on the SMT siblings. Balance the siblings too.
+	 */
+	if (sched_core_enabled(rq)) {
+		const struct cpumask *smt_mask = cpu_smt_mask(cpu_of(rq));
+		int scpu;
+
+		for_each_cpu_andnot(scpu, smt_mask, cpumask_of(cpu_of(rq))) {
+			struct rq *srq = cpu_rq(scpu);
+			struct task_struct *sprev = srq->curr;
+
+			WARN_ON_ONCE(__rq_lockp(rq) != __rq_lockp(srq));
+			update_rq_clock(srq);
+			balance_one(srq, sprev, false);
+		}
+	}
+#endif
+	rq_repin_lock(rq, rf);
+
+	return ret;
+}
+
+static void process_ddsp_deferred_locals(struct rq *rq)
+{
+	struct task_struct *p;
+
+	lockdep_assert_rq_held(rq);
+
+	/*
+	 * Now that @rq can be unlocked, execute the deferred enqueueing of
+	 * tasks directly dispatched to the local DSQs of other CPUs. See
+	 * direct_dispatch(). Keep popping from the head instead of using
+	 * list_for_each_entry_safe() as dispatch_local_dsq() may unlock @rq
+	 * temporarily.
+	 */
+	while ((p = list_first_entry_or_null(&rq->scx.ddsp_deferred_locals,
+				struct task_struct, scx.dsq_list.node))) {
+		struct scx_dispatch_q *dsq;
+
+		list_del_init(&p->scx.dsq_list.node);
+
+		dsq = find_dsq_for_dispatch(rq, p->scx.ddsp_dsq_id, p);
+		if (!WARN_ON_ONCE(dsq->id != SCX_DSQ_LOCAL))
+			dispatch_to_local_dsq(rq, dsq, p, p->scx.ddsp_enq_flags);
+	}
+}
+
+static void set_next_task_scx(struct rq *rq, struct task_struct *p, bool first)
+{
+	if (p->scx.flags & SCX_TASK_QUEUED) {
+		/*
+		 * Core-sched might decide to execute @p before it is
+		 * dispatched. Call ops_dequeue() to notify the BPF scheduler.
+		 */
+		ops_dequeue(p, SCX_DEQ_CORE_SCHED_EXEC);
+		dispatch_dequeue(rq, p);
+	}
+
+	p->se.exec_start = rq_clock_task(rq);
+
+	/* see dequeue_task_scx() on why we skip when !QUEUED */
+	if (SCX_HAS_OP(running) && (p->scx.flags & SCX_TASK_QUEUED))
+		SCX_CALL_OP_TASK(SCX_KF_REST, running, p);
+
+	clr_task_runnable(p, true);
+
+	/*
+	 * @p is getting newly scheduled or got kicked after someone updated its
+	 * slice. Refresh whether tick can be stopped. See scx_can_stop_tick().
+	 */
+	if ((p->scx.slice == SCX_SLICE_INF) !=
+	    (bool)(rq->scx.flags & SCX_RQ_CAN_STOP_TICK)) {
+		if (p->scx.slice == SCX_SLICE_INF)
+			rq->scx.flags |= SCX_RQ_CAN_STOP_TICK;
+		else
+			rq->scx.flags &= ~SCX_RQ_CAN_STOP_TICK;
+
+		sched_update_tick_dependency(rq);
+
+		/*
+		 * For now, let's refresh the load_avgs just when transitioning
+		 * in and out of nohz. In the future, we might want to add a
+		 * mechanism which calls the following periodically on
+		 * tick-stopped CPUs.
+		 */
+		update_other_load_avgs(rq);
+	}
+}
+
+static enum scx_cpu_preempt_reason
+preempt_reason_from_class(const struct sched_class *class)
+{
+#ifdef CONFIG_SMP
+	if (class == &stop_sched_class)
+		return SCX_CPU_PREEMPT_STOP;
+#endif
+	if (class == &dl_sched_class)
+		return SCX_CPU_PREEMPT_DL;
+	if (class == &rt_sched_class)
+		return SCX_CPU_PREEMPT_RT;
+	return SCX_CPU_PREEMPT_UNKNOWN;
+}
+
+static void switch_class_scx(struct rq *rq, struct task_struct *next)
+{
+	const struct sched_class *next_class = next->sched_class;
+
+	if (!scx_enabled())
+		return;
+#ifdef CONFIG_SMP
+	/*
+	 * Pairs with the smp_load_acquire() issued by a CPU in
+	 * kick_cpus_irq_workfn() who is waiting for this CPU to perform a
+	 * resched.
+	 */
+	smp_store_release(&rq->scx.pnt_seq, rq->scx.pnt_seq + 1);
+#endif
+	if (!static_branch_unlikely(&scx_ops_cpu_preempt))
+		return;
+
+	/*
+	 * The callback is conceptually meant to convey that the CPU is no
+	 * longer under the control of SCX. Therefore, don't invoke the callback
+	 * if the next class is below SCX (in which case the BPF scheduler has
+	 * actively decided not to schedule any tasks on the CPU).
+	 */
+	if (sched_class_above(&ext_sched_class, next_class))
+		return;
+
+	/*
+	 * At this point we know that SCX was preempted by a higher priority
+	 * sched_class, so invoke the ->cpu_release() callback if we have not
+	 * done so already. We only send the callback once between SCX being
+	 * preempted, and it regaining control of the CPU.
+	 *
+	 * ->cpu_release() complements ->cpu_acquire(), which is emitted the
+	 *  next time that balance_scx() is invoked.
+	 */
+	if (!rq->scx.cpu_released) {
+		if (SCX_HAS_OP(cpu_release)) {
+			struct scx_cpu_release_args args = {
+				.reason = preempt_reason_from_class(next_class),
+				.task = next,
+			};
+
+			SCX_CALL_OP(SCX_KF_CPU_RELEASE,
+				    cpu_release, cpu_of(rq), &args);
+		}
+		rq->scx.cpu_released = true;
+	}
+}
+
+static void put_prev_task_scx(struct rq *rq, struct task_struct *p)
+{
+	update_curr_scx(rq);
+
+	/* see dequeue_task_scx() on why we skip when !QUEUED */
+	if (SCX_HAS_OP(stopping) && (p->scx.flags & SCX_TASK_QUEUED))
+		SCX_CALL_OP_TASK(SCX_KF_REST, stopping, p, true);
+
+	/*
+	 * If we're being called from put_prev_task_balance(), balance_scx() may
+	 * have decided that @p should keep running.
+	 */
+	if (p->scx.flags & SCX_TASK_BAL_KEEP) {
+		p->scx.flags &= ~SCX_TASK_BAL_KEEP;
+		set_task_runnable(rq, p);
+		dispatch_enqueue(&rq->scx.local_dsq, p, SCX_ENQ_HEAD);
+		return;
+	}
+
+	if (p->scx.flags & SCX_TASK_QUEUED) {
+		set_task_runnable(rq, p);
+
+		/*
+		 * If @p has slice left and balance_scx() didn't tag it for
+		 * keeping, @p is getting preempted by a higher priority
+		 * scheduler class or core-sched forcing a different task. Leave
+		 * it at the head of the local DSQ.
+		 */
+		if (p->scx.slice && !scx_rq_bypassing(rq)) {
+			dispatch_enqueue(&rq->scx.local_dsq, p, SCX_ENQ_HEAD);
+			return;
+		}
+
+		/*
+		 * If we're in the pick_next_task path, balance_scx() should
+		 * have already populated the local DSQ if there are any other
+		 * available tasks. If empty, tell ops.enqueue() that @p is the
+		 * only one available for this cpu. ops.enqueue() should put it
+		 * on the local DSQ so that the subsequent pick_next_task_scx()
+		 * can find the task unless it wants to trigger a separate
+		 * follow-up scheduling event.
+		 */
+		if (list_empty(&rq->scx.local_dsq.list))
+			do_enqueue_task(rq, p, SCX_ENQ_LAST, -1);
+		else
+			do_enqueue_task(rq, p, 0, -1);
+	}
+}
+
+static struct task_struct *first_local_task(struct rq *rq)
+{
+	return list_first_entry_or_null(&rq->scx.local_dsq.list,
+					struct task_struct, scx.dsq_list.node);
+}
+
+static struct task_struct *pick_next_task_scx(struct rq *rq)
+{
+	struct task_struct *p;
+
+	p = first_local_task(rq);
+	if (!p)
+		return NULL;
+
+	set_next_task_scx(rq, p, true);
+
+	if (unlikely(!p->scx.slice)) {
+		if (!scx_rq_bypassing(rq) && !scx_warned_zero_slice) {
+			printk_deferred(KERN_WARNING "sched_ext: %s[%d] has zero slice in pick_next_task_scx()\n",
+					p->comm, p->pid);
+			scx_warned_zero_slice = true;
+		}
+		p->scx.slice = SCX_SLICE_DFL;
+	}
+
+	return p;
+}
+
+#ifdef CONFIG_SCHED_CORE
+/**
+ * scx_prio_less - Task ordering for core-sched
+ * @a: task A
+ * @b: task B
+ *
+ * Core-sched is implemented as an additional scheduling layer on top of the
+ * usual sched_class'es and needs to find out the expected task ordering. For
+ * SCX, core-sched calls this function to interrogate the task ordering.
+ *
+ * Unless overridden by ops.core_sched_before(), @p->scx.core_sched_at is used
+ * to implement the default task ordering. The older the timestamp, the higher
+ * prority the task - the global FIFO ordering matching the default scheduling
+ * behavior.
+ *
+ * When ops.core_sched_before() is enabled, @p->scx.core_sched_at is used to
+ * implement FIFO ordering within each local DSQ. See pick_task_scx().
+ */
+bool scx_prio_less(const struct task_struct *a, const struct task_struct *b,
+		   bool in_fi)
+{
+	/*
+	 * The const qualifiers are dropped from task_struct pointers when
+	 * calling ops.core_sched_before(). Accesses are controlled by the
+	 * verifier.
+	 */
+	if (SCX_HAS_OP(core_sched_before) && !scx_rq_bypassing(task_rq(a)))
+		return SCX_CALL_OP_2TASKS_RET(SCX_KF_REST, core_sched_before,
+					      (struct task_struct *)a,
+					      (struct task_struct *)b);
+	else
+		return time_after64(a->scx.core_sched_at, b->scx.core_sched_at);
+}
+
+/**
+ * pick_task_scx - Pick a candidate task for core-sched
+ * @rq: rq to pick the candidate task from
+ *
+ * Core-sched calls this function on each SMT sibling to determine the next
+ * tasks to run on the SMT siblings. balance_one() has been called on all
+ * siblings and put_prev_task_scx() has been called only for the current CPU.
+ *
+ * As put_prev_task_scx() hasn't been called on remote CPUs, we can't just look
+ * at the first task in the local dsq. @rq->curr has to be considered explicitly
+ * to mimic %SCX_TASK_BAL_KEEP.
+ */
+static struct task_struct *pick_task_scx(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	struct task_struct *first = first_local_task(rq);
+
+	if (curr->scx.flags & SCX_TASK_QUEUED) {
+		/* is curr the only runnable task? */
+		if (!first)
+			return curr;
+
+		/*
+		 * Does curr trump first? We can always go by core_sched_at for
+		 * this comparison as it represents global FIFO ordering when
+		 * the default core-sched ordering is used and local-DSQ FIFO
+		 * ordering otherwise.
+		 *
+		 * We can have a task with an earlier timestamp on the DSQ. For
+		 * example, when a current task is preempted by a sibling
+		 * picking a different cookie, the task would be requeued at the
+		 * head of the local DSQ with an earlier timestamp than the
+		 * core-sched picked next task. Besides, the BPF scheduler may
+		 * dispatch any tasks to the local DSQ anytime.
+		 */
+		if (curr->scx.slice && time_before64(curr->scx.core_sched_at,
+						     first->scx.core_sched_at))
+			return curr;
+	}
+
+	return first;	/* this may be %NULL */
+}
+#endif	/* CONFIG_SCHED_CORE */
+
+#ifdef CONFIG_SMP
+
+static bool test_and_clear_cpu_idle(int cpu)
+{
+#ifdef CONFIG_SCHED_SMT
+	/*
+	 * SMT mask should be cleared whether we can claim @cpu or not. The SMT
+	 * cluster is not wholly idle either way. This also prevents
+	 * scx_pick_idle_cpu() from getting caught in an infinite loop.
+	 */
+	if (sched_smt_active()) {
+		const struct cpumask *smt = cpu_smt_mask(cpu);
+
+		/*
+		 * If offline, @cpu is not its own sibling and
+		 * scx_pick_idle_cpu() can get caught in an infinite loop as
+		 * @cpu is never cleared from idle_masks.smt. Ensure that @cpu
+		 * is eventually cleared.
+		 */
+		if (cpumask_intersects(smt, idle_masks.smt))
+			cpumask_andnot(idle_masks.smt, idle_masks.smt, smt);
+		else if (cpumask_test_cpu(cpu, idle_masks.smt))
+			__cpumask_clear_cpu(cpu, idle_masks.smt);
+	}
+#endif
+	return cpumask_test_and_clear_cpu(cpu, idle_masks.cpu);
+}
+
+static s32 scx_pick_idle_cpu(const struct cpumask *cpus_allowed, u64 flags)
+{
+	int cpu;
+
+retry:
+	if (sched_smt_active()) {
+		cpu = cpumask_any_and_distribute(idle_masks.smt, cpus_allowed);
+		if (cpu < nr_cpu_ids)
+			goto found;
+
+		if (flags & SCX_PICK_IDLE_CORE)
+			return -EBUSY;
+	}
+
+	cpu = cpumask_any_and_distribute(idle_masks.cpu, cpus_allowed);
+	if (cpu >= nr_cpu_ids)
+		return -EBUSY;
+
+found:
+	if (test_and_clear_cpu_idle(cpu))
+		return cpu;
+	else
+		goto retry;
+}
+
+static s32 scx_select_cpu_dfl(struct task_struct *p, s32 prev_cpu,
+			      u64 wake_flags, bool *found)
+{
+	s32 cpu;
+
+	*found = false;
+
+	/*
+	 * If WAKE_SYNC, the waker's local DSQ is empty, and the system is
+	 * under utilized, wake up @p to the local DSQ of the waker. Checking
+	 * only for an empty local DSQ is insufficient as it could give the
+	 * wakee an unfair advantage when the system is oversaturated.
+	 * Checking only for the presence of idle CPUs is also insufficient as
+	 * the local DSQ of the waker could have tasks piled up on it even if
+	 * there is an idle core elsewhere on the system.
+	 */
+	cpu = smp_processor_id();
+	if ((wake_flags & SCX_WAKE_SYNC) && p->nr_cpus_allowed > 1 &&
+	    !cpumask_empty(idle_masks.cpu) && !(current->flags & PF_EXITING) &&
+	    cpu_rq(cpu)->scx.local_dsq.nr == 0) {
+		if (cpumask_test_cpu(cpu, p->cpus_ptr))
+			goto cpu_found;
+	}
+
+	if (p->nr_cpus_allowed == 1) {
+		if (test_and_clear_cpu_idle(prev_cpu)) {
+			cpu = prev_cpu;
+			goto cpu_found;
+		} else {
+			return prev_cpu;
+		}
+	}
+
+	/*
+	 * If CPU has SMT, any wholly idle CPU is likely a better pick than
+	 * partially idle @prev_cpu.
+	 */
+	if (sched_smt_active()) {
+		if (cpumask_test_cpu(prev_cpu, idle_masks.smt) &&
+		    test_and_clear_cpu_idle(prev_cpu)) {
+			cpu = prev_cpu;
+			goto cpu_found;
+		}
+
+		cpu = scx_pick_idle_cpu(p->cpus_ptr, SCX_PICK_IDLE_CORE);
+		if (cpu >= 0)
+			goto cpu_found;
+	}
+
+	if (test_and_clear_cpu_idle(prev_cpu)) {
+		cpu = prev_cpu;
+		goto cpu_found;
+	}
+
+	cpu = scx_pick_idle_cpu(p->cpus_ptr, 0);
+	if (cpu >= 0)
+		goto cpu_found;
+
+	return prev_cpu;
+
+cpu_found:
+	*found = true;
+	return cpu;
+}
+
+static int select_task_rq_scx(struct task_struct *p, int prev_cpu, int wake_flags)
+{
+	/*
+	 * sched_exec() calls with %WF_EXEC when @p is about to exec(2) as it
+	 * can be a good migration opportunity with low cache and memory
+	 * footprint. Returning a CPU different than @prev_cpu triggers
+	 * immediate rq migration. However, for SCX, as the current rq
+	 * association doesn't dictate where the task is going to run, this
+	 * doesn't fit well. If necessary, we can later add a dedicated method
+	 * which can decide to preempt self to force it through the regular
+	 * scheduling path.
+	 */
+	if (unlikely(wake_flags & WF_EXEC))
+		return prev_cpu;
+
+	if (SCX_HAS_OP(select_cpu) && !scx_rq_bypassing(task_rq(p))) {
+		s32 cpu;
+		struct task_struct **ddsp_taskp;
+
+		ddsp_taskp = this_cpu_ptr(&direct_dispatch_task);
+		WARN_ON_ONCE(*ddsp_taskp);
+		*ddsp_taskp = p;
+
+		cpu = SCX_CALL_OP_TASK_RET(SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU,
+					   select_cpu, p, prev_cpu, wake_flags);
+		*ddsp_taskp = NULL;
+		if (ops_cpu_valid(cpu, "from ops.select_cpu()"))
+			return cpu;
+		else
+			return prev_cpu;
+	} else {
+		bool found;
+		s32 cpu;
+
+		cpu = scx_select_cpu_dfl(p, prev_cpu, wake_flags, &found);
+		if (found) {
+			p->scx.slice = SCX_SLICE_DFL;
+			p->scx.ddsp_dsq_id = SCX_DSQ_LOCAL;
+		}
+		return cpu;
+	}
+}
+
+static void task_woken_scx(struct rq *rq, struct task_struct *p)
+{
+	run_deferred(rq);
+}
+
+static void set_cpus_allowed_scx(struct task_struct *p,
+				 struct affinity_context *ac)
+{
+	set_cpus_allowed_common(p, ac);
+
+	/*
+	 * The effective cpumask is stored in @p->cpus_ptr which may temporarily
+	 * differ from the configured one in @p->cpus_mask. Always tell the bpf
+	 * scheduler the effective one.
+	 *
+	 * Fine-grained memory write control is enforced by BPF making the const
+	 * designation pointless. Cast it away when calling the operation.
+	 */
+	if (SCX_HAS_OP(set_cpumask))
+		SCX_CALL_OP_TASK(SCX_KF_REST, set_cpumask, p,
+				 (struct cpumask *)p->cpus_ptr);
+}
+
+static void reset_idle_masks(void)
+{
+	/*
+	 * Consider all online cpus idle. Should converge to the actual state
+	 * quickly.
+	 */
+	cpumask_copy(idle_masks.cpu, cpu_online_mask);
+	cpumask_copy(idle_masks.smt, cpu_online_mask);
+}
+
+void __scx_update_idle(struct rq *rq, bool idle)
+{
+	int cpu = cpu_of(rq);
+
+	if (SCX_HAS_OP(update_idle) && !scx_rq_bypassing(rq)) {
+		SCX_CALL_OP(SCX_KF_REST, update_idle, cpu_of(rq), idle);
+		if (!static_branch_unlikely(&scx_builtin_idle_enabled))
+			return;
+	}
+
+	if (idle)
+		cpumask_set_cpu(cpu, idle_masks.cpu);
+	else
+		cpumask_clear_cpu(cpu, idle_masks.cpu);
+
+#ifdef CONFIG_SCHED_SMT
+	if (sched_smt_active()) {
+		const struct cpumask *smt = cpu_smt_mask(cpu);
+
+		if (idle) {
+			/*
+			 * idle_masks.smt handling is racy but that's fine as
+			 * it's only for optimization and self-correcting.
+			 */
+			for_each_cpu(cpu, smt) {
+				if (!cpumask_test_cpu(cpu, idle_masks.cpu))
+					return;
+			}
+			cpumask_or(idle_masks.smt, idle_masks.smt, smt);
+		} else {
+			cpumask_andnot(idle_masks.smt, idle_masks.smt, smt);
+		}
+	}
+#endif
+}
+
+static void handle_hotplug(struct rq *rq, bool online)
+{
+	int cpu = cpu_of(rq);
+
+	atomic_long_inc(&scx_hotplug_seq);
+
+	if (online && SCX_HAS_OP(cpu_online))
+		SCX_CALL_OP(SCX_KF_UNLOCKED, cpu_online, cpu);
+	else if (!online && SCX_HAS_OP(cpu_offline))
+		SCX_CALL_OP(SCX_KF_UNLOCKED, cpu_offline, cpu);
+	else
+		scx_ops_exit(SCX_ECODE_ACT_RESTART | SCX_ECODE_RSN_HOTPLUG,
+			     "cpu %d going %s, exiting scheduler", cpu,
+			     online ? "online" : "offline");
+}
+
+void scx_rq_activate(struct rq *rq)
+{
+	handle_hotplug(rq, true);
+}
+
+void scx_rq_deactivate(struct rq *rq)
+{
+	handle_hotplug(rq, false);
+}
+
+static void rq_online_scx(struct rq *rq)
+{
+	rq->scx.flags |= SCX_RQ_ONLINE;
+}
+
+static void rq_offline_scx(struct rq *rq)
+{
+	rq->scx.flags &= ~SCX_RQ_ONLINE;
+}
+
+#else	/* CONFIG_SMP */
+
+static bool test_and_clear_cpu_idle(int cpu) { return false; }
+static s32 scx_pick_idle_cpu(const struct cpumask *cpus_allowed, u64 flags) { return -EBUSY; }
+static void reset_idle_masks(void) {}
+
+#endif	/* CONFIG_SMP */
+
+static bool check_rq_for_timeouts(struct rq *rq)
+{
+	struct task_struct *p;
+	struct rq_flags rf;
+	bool timed_out = false;
+
+	rq_lock_irqsave(rq, &rf);
+	list_for_each_entry(p, &rq->scx.runnable_list, scx.runnable_node) {
+		unsigned long last_runnable = p->scx.runnable_at;
+
+		if (unlikely(time_after(jiffies,
+					last_runnable + scx_watchdog_timeout))) {
+			u32 dur_ms = jiffies_to_msecs(jiffies - last_runnable);
+
+			scx_ops_error_kind(SCX_EXIT_ERROR_STALL,
+					   "%s[%d] failed to run for %u.%03us",
+					   p->comm, p->pid,
+					   dur_ms / 1000, dur_ms % 1000);
+			timed_out = true;
+			break;
+		}
+	}
+	rq_unlock_irqrestore(rq, &rf);
+
+	return timed_out;
+}
+
+static void scx_watchdog_workfn(struct work_struct *work)
+{
+	int cpu;
+
+	WRITE_ONCE(scx_watchdog_timestamp, jiffies);
+
+	for_each_online_cpu(cpu) {
+		if (unlikely(check_rq_for_timeouts(cpu_rq(cpu))))
+			break;
+
+		cond_resched();
+	}
+	queue_delayed_work(system_unbound_wq, to_delayed_work(work),
+			   scx_watchdog_timeout / 2);
+}
+
+void scx_tick(struct rq *rq)
+{
+	unsigned long last_check;
+
+	if (!scx_enabled())
+		return;
+
+	last_check = READ_ONCE(scx_watchdog_timestamp);
+	if (unlikely(time_after(jiffies,
+				last_check + READ_ONCE(scx_watchdog_timeout)))) {
+		u32 dur_ms = jiffies_to_msecs(jiffies - last_check);
+
+		scx_ops_error_kind(SCX_EXIT_ERROR_STALL,
+				   "watchdog failed to check in for %u.%03us",
+				   dur_ms / 1000, dur_ms % 1000);
+	}
+
+	update_other_load_avgs(rq);
+}
+
+static void task_tick_scx(struct rq *rq, struct task_struct *curr, int queued)
+{
+	update_curr_scx(rq);
+
+	/*
+	 * While disabling, always resched and refresh core-sched timestamp as
+	 * we can't trust the slice management or ops.core_sched_before().
+	 */
+	if (scx_rq_bypassing(rq)) {
+		curr->scx.slice = 0;
+		touch_core_sched(rq, curr);
+	} else if (SCX_HAS_OP(tick)) {
+		SCX_CALL_OP(SCX_KF_REST, tick, curr);
+	}
+
+	if (!curr->scx.slice)
+		resched_curr(rq);
+}
+
+#ifdef CONFIG_EXT_GROUP_SCHED
+static struct cgroup *tg_cgrp(struct task_group *tg)
+{
+	/*
+	 * If CGROUP_SCHED is disabled, @tg is NULL. If @tg is an autogroup,
+	 * @tg->css.cgroup is NULL. In both cases, @tg can be treated as the
+	 * root cgroup.
+	 */
+	if (tg && tg->css.cgroup)
+		return tg->css.cgroup;
+	else
+		return &cgrp_dfl_root.cgrp;
+}
+
+#define SCX_INIT_TASK_ARGS_CGROUP(tg)		.cgroup = tg_cgrp(tg),
+
+#else	/* CONFIG_EXT_GROUP_SCHED */
+
+#define SCX_INIT_TASK_ARGS_CGROUP(tg)
+
+#endif	/* CONFIG_EXT_GROUP_SCHED */
+
+static enum scx_task_state scx_get_task_state(const struct task_struct *p)
+{
+	return (p->scx.flags & SCX_TASK_STATE_MASK) >> SCX_TASK_STATE_SHIFT;
+}
+
+static void scx_set_task_state(struct task_struct *p, enum scx_task_state state)
+{
+	enum scx_task_state prev_state = scx_get_task_state(p);
+	bool warn = false;
+
+	BUILD_BUG_ON(SCX_TASK_NR_STATES > (1 << SCX_TASK_STATE_BITS));
+
+	switch (state) {
+	case SCX_TASK_NONE:
+		break;
+	case SCX_TASK_INIT:
+		warn = prev_state != SCX_TASK_NONE;
+		break;
+	case SCX_TASK_READY:
+		warn = prev_state == SCX_TASK_NONE;
+		break;
+	case SCX_TASK_ENABLED:
+		warn = prev_state != SCX_TASK_READY;
+		break;
+	default:
+		warn = true;
+		return;
+	}
+
+	WARN_ONCE(warn, "sched_ext: Invalid task state transition %d -> %d for %s[%d]",
+		  prev_state, state, p->comm, p->pid);
+
+	p->scx.flags &= ~SCX_TASK_STATE_MASK;
+	p->scx.flags |= state << SCX_TASK_STATE_SHIFT;
+}
+
+static int scx_ops_init_task(struct task_struct *p, struct task_group *tg, bool fork)
+{
+	int ret;
+
+	p->scx.disallow = false;
+
+	if (SCX_HAS_OP(init_task)) {
+		struct scx_init_task_args args = {
+			SCX_INIT_TASK_ARGS_CGROUP(tg)
+			.fork = fork,
+		};
+
+		ret = SCX_CALL_OP_RET(SCX_KF_UNLOCKED, init_task, p, &args);
+		if (unlikely(ret)) {
+			ret = ops_sanitize_err("init_task", ret);
+			return ret;
+		}
+	}
+
+	scx_set_task_state(p, SCX_TASK_INIT);
+
+	if (p->scx.disallow) {
+		if (!fork) {
+			struct rq *rq;
+			struct rq_flags rf;
+
+			rq = task_rq_lock(p, &rf);
+
+			/*
+			 * We're in the load path and @p->policy will be applied
+			 * right after. Reverting @p->policy here and rejecting
+			 * %SCHED_EXT transitions from scx_check_setscheduler()
+			 * guarantees that if ops.init_task() sets @p->disallow,
+			 * @p can never be in SCX.
+			 */
+			if (p->policy == SCHED_EXT) {
+				p->policy = SCHED_NORMAL;
+				atomic_long_inc(&scx_nr_rejected);
+			}
+
+			task_rq_unlock(rq, p, &rf);
+		} else if (p->policy == SCHED_EXT) {
+			scx_ops_error("ops.init_task() set task->scx.disallow for %s[%d] during fork",
+				      p->comm, p->pid);
+		}
+	}
+
+	p->scx.flags |= SCX_TASK_RESET_RUNNABLE_AT;
+	return 0;
+}
+
+static void scx_ops_enable_task(struct task_struct *p)
+{
+	u32 weight;
+
+	lockdep_assert_rq_held(task_rq(p));
+
+	/*
+	 * Set the weight before calling ops.enable() so that the scheduler
+	 * doesn't see a stale value if they inspect the task struct.
+	 */
+	if (task_has_idle_policy(p))
+		weight = WEIGHT_IDLEPRIO;
+	else
+		weight = sched_prio_to_weight[p->static_prio - MAX_RT_PRIO];
+
+	p->scx.weight = sched_weight_to_cgroup(weight);
+
+	if (SCX_HAS_OP(enable))
+		SCX_CALL_OP_TASK(SCX_KF_REST, enable, p);
+	scx_set_task_state(p, SCX_TASK_ENABLED);
+
+	if (SCX_HAS_OP(set_weight))
+		SCX_CALL_OP_TASK(SCX_KF_REST, set_weight, p, p->scx.weight);
+}
+
+static void scx_ops_disable_task(struct task_struct *p)
+{
+	lockdep_assert_rq_held(task_rq(p));
+	WARN_ON_ONCE(scx_get_task_state(p) != SCX_TASK_ENABLED);
+
+	if (SCX_HAS_OP(disable))
+		SCX_CALL_OP(SCX_KF_REST, disable, p);
+	scx_set_task_state(p, SCX_TASK_READY);
+}
+
+static void scx_ops_exit_task(struct task_struct *p)
+{
+	struct scx_exit_task_args args = {
+		.cancelled = false,
+	};
+
+	lockdep_assert_rq_held(task_rq(p));
+
+	switch (scx_get_task_state(p)) {
+	case SCX_TASK_NONE:
+		return;
+	case SCX_TASK_INIT:
+		args.cancelled = true;
+		break;
+	case SCX_TASK_READY:
+		break;
+	case SCX_TASK_ENABLED:
+		scx_ops_disable_task(p);
+		break;
+	default:
+		WARN_ON_ONCE(true);
+		return;
+	}
+
+	if (SCX_HAS_OP(exit_task))
+		SCX_CALL_OP(SCX_KF_REST, exit_task, p, &args);
+	scx_set_task_state(p, SCX_TASK_NONE);
+}
+
+void init_scx_entity(struct sched_ext_entity *scx)
+{
+	/*
+	 * init_idle() calls this function again after fork sequence is
+	 * complete. Don't touch ->tasks_node as it's already linked.
+	 */
+	memset(scx, 0, offsetof(struct sched_ext_entity, tasks_node));
+
+	INIT_LIST_HEAD(&scx->dsq_list.node);
+	RB_CLEAR_NODE(&scx->dsq_priq);
+	scx->sticky_cpu = -1;
+	scx->holding_cpu = -1;
+	INIT_LIST_HEAD(&scx->runnable_node);
+	scx->runnable_at = jiffies;
+	scx->ddsp_dsq_id = SCX_DSQ_INVALID;
+	scx->slice = SCX_SLICE_DFL;
+}
+
+void scx_pre_fork(struct task_struct *p)
+{
+	/*
+	 * BPF scheduler enable/disable paths want to be able to iterate and
+	 * update all tasks which can become complex when racing forks. As
+	 * enable/disable are very cold paths, let's use a percpu_rwsem to
+	 * exclude forks.
+	 */
+	percpu_down_read(&scx_fork_rwsem);
+}
+
+int scx_fork(struct task_struct *p)
+{
+	percpu_rwsem_assert_held(&scx_fork_rwsem);
+
+	if (scx_ops_init_task_enabled)
+		return scx_ops_init_task(p, task_group(p), true);
+	else
+		return 0;
+}
+
+void scx_post_fork(struct task_struct *p)
+{
+	if (scx_ops_init_task_enabled) {
+		scx_set_task_state(p, SCX_TASK_READY);
+
+		/*
+		 * Enable the task immediately if it's running on sched_ext.
+		 * Otherwise, it'll be enabled in switching_to_scx() if and
+		 * when it's ever configured to run with a SCHED_EXT policy.
+		 */
+		if (p->sched_class == &ext_sched_class) {
+			struct rq_flags rf;
+			struct rq *rq;
+
+			rq = task_rq_lock(p, &rf);
+			scx_ops_enable_task(p);
+			task_rq_unlock(rq, p, &rf);
+		}
+	}
+
+	spin_lock_irq(&scx_tasks_lock);
+	list_add_tail(&p->scx.tasks_node, &scx_tasks);
+	spin_unlock_irq(&scx_tasks_lock);
+
+	percpu_up_read(&scx_fork_rwsem);
+}
+
+void scx_cancel_fork(struct task_struct *p)
+{
+	if (scx_enabled()) {
+		struct rq *rq;
+		struct rq_flags rf;
+
+		rq = task_rq_lock(p, &rf);
+		WARN_ON_ONCE(scx_get_task_state(p) >= SCX_TASK_READY);
+		scx_ops_exit_task(p);
+		task_rq_unlock(rq, p, &rf);
+	}
+
+	percpu_up_read(&scx_fork_rwsem);
+}
+
+void sched_ext_free(struct task_struct *p)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&scx_tasks_lock, flags);
+	list_del_init(&p->scx.tasks_node);
+	spin_unlock_irqrestore(&scx_tasks_lock, flags);
+
+	/*
+	 * @p is off scx_tasks and wholly ours. scx_ops_enable()'s READY ->
+	 * ENABLED transitions can't race us. Disable ops for @p.
+	 */
+	if (scx_get_task_state(p) != SCX_TASK_NONE) {
+		struct rq_flags rf;
+		struct rq *rq;
+
+		rq = task_rq_lock(p, &rf);
+		scx_ops_exit_task(p);
+		task_rq_unlock(rq, p, &rf);
+	}
+}
+
+static void reweight_task_scx(struct rq *rq, struct task_struct *p,
+			      const struct load_weight *lw)
+{
+	lockdep_assert_rq_held(task_rq(p));
+
+	p->scx.weight = sched_weight_to_cgroup(scale_load_down(lw->weight));
+	if (SCX_HAS_OP(set_weight))
+		SCX_CALL_OP_TASK(SCX_KF_REST, set_weight, p, p->scx.weight);
+}
+
+static void prio_changed_scx(struct rq *rq, struct task_struct *p, int oldprio)
+{
+}
+
+static void switching_to_scx(struct rq *rq, struct task_struct *p)
+{
+	scx_ops_enable_task(p);
+
+	/*
+	 * set_cpus_allowed_scx() is not called while @p is associated with a
+	 * different scheduler class. Keep the BPF scheduler up-to-date.
+	 */
+	if (SCX_HAS_OP(set_cpumask))
+		SCX_CALL_OP_TASK(SCX_KF_REST, set_cpumask, p,
+				 (struct cpumask *)p->cpus_ptr);
+}
+
+static void switched_from_scx(struct rq *rq, struct task_struct *p)
+{
+	scx_ops_disable_task(p);
+}
+
+static void wakeup_preempt_scx(struct rq *rq, struct task_struct *p,int wake_flags) {}
+static void switched_to_scx(struct rq *rq, struct task_struct *p) {}
+
+int scx_check_setscheduler(struct task_struct *p, int policy)
+{
+	lockdep_assert_rq_held(task_rq(p));
+
+	/* if disallow, reject transitioning into SCX */
+	if (scx_enabled() && READ_ONCE(p->scx.disallow) &&
+	    p->policy != policy && policy == SCHED_EXT)
+		return -EACCES;
+
+	return 0;
+}
+
+#ifdef CONFIG_NO_HZ_FULL
+bool scx_can_stop_tick(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+
+	if (scx_rq_bypassing(rq))
+		return false;
+
+	if (p->sched_class != &ext_sched_class)
+		return true;
+
+	/*
+	 * @rq can dispatch from different DSQs, so we can't tell whether it
+	 * needs the tick or not by looking at nr_running. Allow stopping ticks
+	 * iff the BPF scheduler indicated so. See set_next_task_scx().
+	 */
+	return rq->scx.flags & SCX_RQ_CAN_STOP_TICK;
+}
+#endif
+
+#ifdef CONFIG_EXT_GROUP_SCHED
+
+DEFINE_STATIC_PERCPU_RWSEM(scx_cgroup_rwsem);
+static bool scx_cgroup_enabled;
+static bool cgroup_warned_missing_weight;
+static bool cgroup_warned_missing_idle;
+
+static void scx_cgroup_warn_missing_weight(struct task_group *tg)
+{
+	if (scx_ops_enable_state() == SCX_OPS_DISABLED ||
+	    cgroup_warned_missing_weight)
+		return;
+
+	if ((scx_ops.flags & SCX_OPS_HAS_CGROUP_WEIGHT) || !tg->css.parent)
+		return;
+
+	pr_warn("sched_ext: \"%s\" does not implement cgroup cpu.weight\n",
+		scx_ops.name);
+	cgroup_warned_missing_weight = true;
+}
+
+static void scx_cgroup_warn_missing_idle(struct task_group *tg)
+{
+	if (!scx_cgroup_enabled || cgroup_warned_missing_idle)
+		return;
+
+	if (!tg->idle)
+		return;
+
+	pr_warn("sched_ext: \"%s\" does not implement cgroup cpu.idle\n",
+		scx_ops.name);
+	cgroup_warned_missing_idle = true;
+}
+
+int scx_tg_online(struct task_group *tg)
+{
+	int ret = 0;
+
+	WARN_ON_ONCE(tg->scx_flags & (SCX_TG_ONLINE | SCX_TG_INITED));
+
+	percpu_down_read(&scx_cgroup_rwsem);
+
+	scx_cgroup_warn_missing_weight(tg);
+
+	if (scx_cgroup_enabled) {
+		if (SCX_HAS_OP(cgroup_init)) {
+			struct scx_cgroup_init_args args =
+				{ .weight = tg->scx_weight };
+
+			ret = SCX_CALL_OP_RET(SCX_KF_UNLOCKED, cgroup_init,
+					      tg->css.cgroup, &args);
+			if (ret)
+				ret = ops_sanitize_err("cgroup_init", ret);
+		}
+		if (ret == 0)
+			tg->scx_flags |= SCX_TG_ONLINE | SCX_TG_INITED;
+	} else {
+		tg->scx_flags |= SCX_TG_ONLINE;
+	}
+
+	percpu_up_read(&scx_cgroup_rwsem);
+	return ret;
+}
+
+void scx_tg_offline(struct task_group *tg)
+{
+	WARN_ON_ONCE(!(tg->scx_flags & SCX_TG_ONLINE));
+
+	percpu_down_read(&scx_cgroup_rwsem);
+
+	if (SCX_HAS_OP(cgroup_exit) && (tg->scx_flags & SCX_TG_INITED))
+		SCX_CALL_OP(SCX_KF_UNLOCKED, cgroup_exit, tg->css.cgroup);
+	tg->scx_flags &= ~(SCX_TG_ONLINE | SCX_TG_INITED);
+
+	percpu_up_read(&scx_cgroup_rwsem);
+}
+
+int scx_cgroup_can_attach(struct cgroup_taskset *tset)
+{
+	struct cgroup_subsys_state *css;
+	struct task_struct *p;
+	int ret;
+
+	/* released in scx_finish/cancel_attach() */
+	percpu_down_read(&scx_cgroup_rwsem);
+
+	if (!scx_cgroup_enabled)
+		return 0;
+
+	cgroup_taskset_for_each(p, css, tset) {
+		struct cgroup *from = tg_cgrp(task_group(p));
+		struct cgroup *to = tg_cgrp(css_tg(css));
+
+		WARN_ON_ONCE(p->scx.cgrp_moving_from);
+
+		/*
+		 * sched_move_task() omits identity migrations. Let's match the
+		 * behavior so that ops.cgroup_prep_move() and ops.cgroup_move()
+		 * always match one-to-one.
+		 */
+		if (from == to)
+			continue;
+
+		if (SCX_HAS_OP(cgroup_prep_move)) {
+			ret = SCX_CALL_OP_RET(SCX_KF_UNLOCKED, cgroup_prep_move,
+					      p, from, css->cgroup);
+			if (ret)
+				goto err;
+		}
+
+		p->scx.cgrp_moving_from = from;
+	}
+
+	return 0;
+
+err:
+	cgroup_taskset_for_each(p, css, tset) {
+		if (SCX_HAS_OP(cgroup_cancel_move) && p->scx.cgrp_moving_from)
+			SCX_CALL_OP(SCX_KF_UNLOCKED, cgroup_cancel_move, p,
+				    p->scx.cgrp_moving_from, css->cgroup);
+		p->scx.cgrp_moving_from = NULL;
+	}
+
+	percpu_up_read(&scx_cgroup_rwsem);
+	return ops_sanitize_err("cgroup_prep_move", ret);
+}
+
+void scx_move_task(struct task_struct *p)
+{
+	if (!scx_cgroup_enabled)
+		return;
+
+	/*
+	 * We're called from sched_move_task() which handles both cgroup and
+	 * autogroup moves. Ignore the latter.
+	 *
+	 * Also ignore exiting tasks, because in the exit path tasks transition
+	 * from the autogroup to the root group, so task_group_is_autogroup()
+	 * alone isn't able to catch exiting autogroup tasks. This is safe for
+	 * cgroup_move(), because cgroup migrations never happen for PF_EXITING
+	 * tasks.
+	 */
+	if (task_group_is_autogroup(task_group(p)) || (p->flags & PF_EXITING))
+		return;
+
+	/*
+	 * @p must have ops.cgroup_prep_move() called on it and thus
+	 * cgrp_moving_from set.
+	 */
+	if (SCX_HAS_OP(cgroup_move) && !WARN_ON_ONCE(!p->scx.cgrp_moving_from))
+		SCX_CALL_OP_TASK(SCX_KF_UNLOCKED, cgroup_move, p,
+			p->scx.cgrp_moving_from, tg_cgrp(task_group(p)));
+	p->scx.cgrp_moving_from = NULL;
+}
+
+void scx_cgroup_finish_attach(void)
+{
+	percpu_up_read(&scx_cgroup_rwsem);
+}
+
+void scx_cgroup_cancel_attach(struct cgroup_taskset *tset)
+{
+	struct cgroup_subsys_state *css;
+	struct task_struct *p;
+
+	if (!scx_cgroup_enabled)
+		goto out_unlock;
+
+	cgroup_taskset_for_each(p, css, tset) {
+		if (SCX_HAS_OP(cgroup_cancel_move) && p->scx.cgrp_moving_from)
+			SCX_CALL_OP(SCX_KF_UNLOCKED, cgroup_cancel_move, p,
+				    p->scx.cgrp_moving_from, css->cgroup);
+		p->scx.cgrp_moving_from = NULL;
+	}
+out_unlock:
+	percpu_up_read(&scx_cgroup_rwsem);
+}
+
+void scx_group_set_weight(struct task_group *tg, unsigned long weight)
+{
+	percpu_down_read(&scx_cgroup_rwsem);
+
+	if (scx_cgroup_enabled && tg->scx_weight != weight) {
+		if (SCX_HAS_OP(cgroup_set_weight))
+			SCX_CALL_OP(SCX_KF_UNLOCKED, cgroup_set_weight,
+				    tg_cgrp(tg), weight);
+		tg->scx_weight = weight;
+	}
+
+	percpu_up_read(&scx_cgroup_rwsem);
+}
+
+void scx_group_set_idle(struct task_group *tg, bool idle)
+{
+	percpu_down_read(&scx_cgroup_rwsem);
+	scx_cgroup_warn_missing_idle(tg);
+	percpu_up_read(&scx_cgroup_rwsem);
+}
+
+static void scx_cgroup_lock(void)
+{
+	percpu_down_write(&scx_cgroup_rwsem);
+}
+
+static void scx_cgroup_unlock(void)
+{
+	percpu_up_write(&scx_cgroup_rwsem);
+}
+
+#else	/* CONFIG_EXT_GROUP_SCHED */
+
+static inline void scx_cgroup_lock(void) {}
+static inline void scx_cgroup_unlock(void) {}
+
+#endif	/* CONFIG_EXT_GROUP_SCHED */
+
+/*
+ * Omitted operations:
+ *
+ * - wakeup_preempt: NOOP as it isn't useful in the wakeup path because the task
+ *   isn't tied to the CPU at that point. Preemption is implemented by resetting
+ *   the victim task's slice to 0 and triggering reschedule on the target CPU.
+ *
+ * - migrate_task_rq: Unnecessary as task to cpu mapping is transient.
+ *
+ * - task_fork/dead: We need fork/dead notifications for all tasks regardless of
+ *   their current sched_class. Call them directly from sched core instead.
+ */
+DEFINE_SCHED_CLASS(ext) = {
+	.enqueue_task		= enqueue_task_scx,
+	.dequeue_task		= dequeue_task_scx,
+	.yield_task		= yield_task_scx,
+	.yield_to_task		= yield_to_task_scx,
+
+	.wakeup_preempt		= wakeup_preempt_scx,
+
+	.balance		= balance_scx,
+	.pick_next_task		= pick_next_task_scx,
+
+	.put_prev_task		= put_prev_task_scx,
+	.set_next_task		= set_next_task_scx,
+
+	.switch_class		= switch_class_scx,
+
+#ifdef CONFIG_SMP
+	.select_task_rq		= select_task_rq_scx,
+	.task_woken		= task_woken_scx,
+	.set_cpus_allowed	= set_cpus_allowed_scx,
+
+	.rq_online		= rq_online_scx,
+	.rq_offline		= rq_offline_scx,
+#endif
+
+#ifdef CONFIG_SCHED_CORE
+	.pick_task		= pick_task_scx,
+#endif
+
+	.task_tick		= task_tick_scx,
+
+	.switching_to		= switching_to_scx,
+	.switched_from		= switched_from_scx,
+	.switched_to		= switched_to_scx,
+	.reweight_task		= reweight_task_scx,
+	.prio_changed		= prio_changed_scx,
+
+	.update_curr		= update_curr_scx,
+
+#ifdef CONFIG_UCLAMP_TASK
+	.uclamp_enabled		= 1,
+#endif
+};
+
+static void init_dsq(struct scx_dispatch_q *dsq, u64 dsq_id)
+{
+	memset(dsq, 0, sizeof(*dsq));
+
+	raw_spin_lock_init(&dsq->lock);
+	INIT_LIST_HEAD(&dsq->list);
+	dsq->id = dsq_id;
+}
+
+static struct scx_dispatch_q *create_dsq(u64 dsq_id, int node)
+{
+	struct scx_dispatch_q *dsq;
+	int ret;
+
+	if (dsq_id & SCX_DSQ_FLAG_BUILTIN)
+		return ERR_PTR(-EINVAL);
+
+	dsq = kmalloc_node(sizeof(*dsq), GFP_KERNEL, node);
+	if (!dsq)
+		return ERR_PTR(-ENOMEM);
+
+	init_dsq(dsq, dsq_id);
+
+	ret = rhashtable_insert_fast(&dsq_hash, &dsq->hash_node,
+				     dsq_hash_params);
+	if (ret) {
+		kfree(dsq);
+		return ERR_PTR(ret);
+	}
+	return dsq;
+}
+
+static void free_dsq_irq_workfn(struct irq_work *irq_work)
+{
+	struct llist_node *to_free = llist_del_all(&dsqs_to_free);
+	struct scx_dispatch_q *dsq, *tmp_dsq;
+
+	llist_for_each_entry_safe(dsq, tmp_dsq, to_free, free_node)
+		kfree_rcu(dsq, rcu);
+}
+
+static DEFINE_IRQ_WORK(free_dsq_irq_work, free_dsq_irq_workfn);
+
+static void destroy_dsq(u64 dsq_id)
+{
+	struct scx_dispatch_q *dsq;
+	unsigned long flags;
+
+	rcu_read_lock();
+
+	dsq = find_user_dsq(dsq_id);
+	if (!dsq)
+		goto out_unlock_rcu;
+
+	raw_spin_lock_irqsave(&dsq->lock, flags);
+
+	if (dsq->nr) {
+		scx_ops_error("attempting to destroy in-use dsq 0x%016llx (nr=%u)",
+			      dsq->id, dsq->nr);
+		goto out_unlock_dsq;
+	}
+
+	if (rhashtable_remove_fast(&dsq_hash, &dsq->hash_node, dsq_hash_params))
+		goto out_unlock_dsq;
+
+	/*
+	 * Mark dead by invalidating ->id to prevent dispatch_enqueue() from
+	 * queueing more tasks. As this function can be called from anywhere,
+	 * freeing is bounced through an irq work to avoid nesting RCU
+	 * operations inside scheduler locks.
+	 */
+	dsq->id = SCX_DSQ_INVALID;
+	llist_add(&dsq->free_node, &dsqs_to_free);
+	irq_work_queue(&free_dsq_irq_work);
+
+out_unlock_dsq:
+	raw_spin_unlock_irqrestore(&dsq->lock, flags);
+out_unlock_rcu:
+	rcu_read_unlock();
+}
+
+#ifdef CONFIG_EXT_GROUP_SCHED
+static void scx_cgroup_exit(void)
+{
+	struct cgroup_subsys_state *css;
+
+	percpu_rwsem_assert_held(&scx_cgroup_rwsem);
+
+	WARN_ON_ONCE(!scx_cgroup_enabled);
+	scx_cgroup_enabled = false;
+
+	/*
+	 * scx_tg_on/offline() are excluded through scx_cgroup_rwsem. If we walk
+	 * cgroups and exit all the inited ones, all online cgroups are exited.
+	 */
+	rcu_read_lock();
+	css_for_each_descendant_post(css, &root_task_group.css) {
+		struct task_group *tg = css_tg(css);
+
+		if (!(tg->scx_flags & SCX_TG_INITED))
+			continue;
+		tg->scx_flags &= ~SCX_TG_INITED;
+
+		if (!scx_ops.cgroup_exit)
+			continue;
+
+		if (WARN_ON_ONCE(!css_tryget(css)))
+			continue;
+		rcu_read_unlock();
+
+		SCX_CALL_OP(SCX_KF_UNLOCKED, cgroup_exit, css->cgroup);
+
+		rcu_read_lock();
+		css_put(css);
+	}
+	rcu_read_unlock();
+}
+
+static int scx_cgroup_init(void)
+{
+	struct cgroup_subsys_state *css;
+	int ret;
+
+	percpu_rwsem_assert_held(&scx_cgroup_rwsem);
+
+	cgroup_warned_missing_weight = false;
+	cgroup_warned_missing_idle = false;
+
+	/*
+	 * scx_tg_on/offline() are excluded thorugh scx_cgroup_rwsem. If we walk
+	 * cgroups and init, all online cgroups are initialized.
+	 */
+	rcu_read_lock();
+	css_for_each_descendant_pre(css, &root_task_group.css) {
+		struct task_group *tg = css_tg(css);
+		struct scx_cgroup_init_args args = { .weight = tg->scx_weight };
+
+		scx_cgroup_warn_missing_weight(tg);
+		scx_cgroup_warn_missing_idle(tg);
+
+		if ((tg->scx_flags &
+		     (SCX_TG_ONLINE | SCX_TG_INITED)) != SCX_TG_ONLINE)
+			continue;
+
+		if (!scx_ops.cgroup_init) {
+			tg->scx_flags |= SCX_TG_INITED;
+			continue;
+		}
+
+		if (WARN_ON_ONCE(!css_tryget(css)))
+			continue;
+		rcu_read_unlock();
+
+		ret = SCX_CALL_OP_RET(SCX_KF_UNLOCKED, cgroup_init,
+				      css->cgroup, &args);
+		if (ret) {
+			css_put(css);
+			return ret;
+		}
+		tg->scx_flags |= SCX_TG_INITED;
+
+		rcu_read_lock();
+		css_put(css);
+	}
+	rcu_read_unlock();
+
+	WARN_ON_ONCE(scx_cgroup_enabled);
+	scx_cgroup_enabled = true;
+
+	return 0;
+}
+
+#else
+static void scx_cgroup_exit(void) {}
+static int scx_cgroup_init(void) { return 0; }
+#endif
+
+
+/********************************************************************************
+ * Sysfs interface and ops enable/disable.
+ */
+
+#define SCX_ATTR(_name)								\
+	static struct kobj_attribute scx_attr_##_name = {			\
+		.attr = { .name = __stringify(_name), .mode = 0444 },		\
+		.show = scx_attr_##_name##_show,				\
+	}
+
+static ssize_t scx_attr_state_show(struct kobject *kobj,
+				   struct kobj_attribute *ka, char *buf)
+{
+	return sysfs_emit(buf, "%s\n",
+			  scx_ops_enable_state_str[scx_ops_enable_state()]);
+}
+SCX_ATTR(state);
+
+static ssize_t scx_attr_switch_all_show(struct kobject *kobj,
+					struct kobj_attribute *ka, char *buf)
+{
+	return sysfs_emit(buf, "%d\n", READ_ONCE(scx_switching_all));
+}
+SCX_ATTR(switch_all);
+
+static ssize_t scx_attr_nr_rejected_show(struct kobject *kobj,
+					 struct kobj_attribute *ka, char *buf)
+{
+	return sysfs_emit(buf, "%ld\n", atomic_long_read(&scx_nr_rejected));
+}
+SCX_ATTR(nr_rejected);
+
+static ssize_t scx_attr_hotplug_seq_show(struct kobject *kobj,
+					 struct kobj_attribute *ka, char *buf)
+{
+	return sysfs_emit(buf, "%ld\n", atomic_long_read(&scx_hotplug_seq));
+}
+SCX_ATTR(hotplug_seq);
+
+static ssize_t scx_attr_enable_seq_show(struct kobject *kobj,
+					struct kobj_attribute *ka, char *buf)
+{
+	return sysfs_emit(buf, "%ld\n", atomic_long_read(&scx_enable_seq));
+}
+SCX_ATTR(enable_seq);
+
+static struct attribute *scx_global_attrs[] = {
+	&scx_attr_state.attr,
+	&scx_attr_switch_all.attr,
+	&scx_attr_nr_rejected.attr,
+	&scx_attr_hotplug_seq.attr,
+	&scx_attr_enable_seq.attr,
+	NULL,
+};
+
+static const struct attribute_group scx_global_attr_group = {
+	.attrs = scx_global_attrs,
+};
+
+static void scx_kobj_release(struct kobject *kobj)
+{
+	kfree(kobj);
+}
+
+static ssize_t scx_attr_ops_show(struct kobject *kobj,
+				 struct kobj_attribute *ka, char *buf)
+{
+	return sysfs_emit(buf, "%s\n", scx_ops.name);
+}
+SCX_ATTR(ops);
+
+static struct attribute *scx_sched_attrs[] = {
+	&scx_attr_ops.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(scx_sched);
+
+static const struct kobj_type scx_ktype = {
+	.release = scx_kobj_release,
+	.sysfs_ops = &kobj_sysfs_ops,
+	.default_groups = scx_sched_groups,
+};
+
+static int scx_uevent(const struct kobject *kobj, struct kobj_uevent_env *env)
+{
+	return add_uevent_var(env, "SCXOPS=%s", scx_ops.name);
+}
+
+static const struct kset_uevent_ops scx_uevent_ops = {
+	.uevent = scx_uevent,
+};
+
+/*
+ * Used by sched_fork() and __setscheduler_prio() to pick the matching
+ * sched_class. dl/rt are already handled.
+ */
+bool task_should_scx(struct task_struct *p)
+{
+	if (!scx_enabled() ||
+	    unlikely(scx_ops_enable_state() == SCX_OPS_DISABLING))
+		return false;
+	if (READ_ONCE(scx_switching_all))
+		return true;
+	return p->policy == SCHED_EXT;
+}
+
+/**
+ * scx_ops_bypass - [Un]bypass scx_ops and guarantee forward progress
+ *
+ * Bypassing guarantees that all runnable tasks make forward progress without
+ * trusting the BPF scheduler. We can't grab any mutexes or rwsems as they might
+ * be held by tasks that the BPF scheduler is forgetting to run, which
+ * unfortunately also excludes toggling the static branches.
+ *
+ * Let's work around by overriding a couple ops and modifying behaviors based on
+ * the DISABLING state and then cycling the queued tasks through dequeue/enqueue
+ * to force global FIFO scheduling.
+ *
+ * - ops.select_cpu() is ignored and the default select_cpu() is used.
+ *
+ * - ops.enqueue() is ignored and tasks are queued in simple global FIFO order.
+ *
+ * - ops.dispatch() is ignored.
+ *
+ * - balance_scx() does not set %SCX_RQ_BAL_KEEP on no*n-zero slice as slice
+ *   can't be trusted. Whenever a tick triggers, the running task is rotated to
+ *   the tail of the queue with core_sched_at touched.
+ *
+ * - pick_next_task() suppresses zero slice warning.
+ *
+ * - scx_bpf_kick_cpu() is disabled to avoid irq_work malfunction during PM
+ *   operations.
+ *
+ * - scx_prio_less() reverts to the default core_sched_at order.
+ */
+static void scx_ops_bypass(bool bypass)
+{
+	int depth, cpu;
+
+	if (bypass) {
+		depth = atomic_inc_return(&scx_ops_bypass_depth);
+		WARN_ON_ONCE(depth <= 0);
+		if (depth != 1)
+			return;
+	} else {
+		depth = atomic_dec_return(&scx_ops_bypass_depth);
+		WARN_ON_ONCE(depth < 0);
+		if (depth != 0)
+			return;
+	}
+
+	/*
+	 * No task property is changing. We just need to make sure all currently
+	 * queued tasks are re-queued according to the new scx_rq_bypassing()
+	 * state. As an optimization, walk each rq's runnable_list instead of
+	 * the scx_tasks list.
+	 *
+	 * This function can't trust the scheduler and thus can't use
+	 * cpus_read_lock(). Walk all possible CPUs instead of online.
+	 */
+	for_each_possible_cpu(cpu) {
+		struct rq *rq = cpu_rq(cpu);
+		struct rq_flags rf;
+		struct task_struct *p, *n;
+
+		rq_lock_irqsave(rq, &rf);
+
+		if (bypass) {
+			WARN_ON_ONCE(rq->scx.flags & SCX_RQ_BYPASSING);
+			rq->scx.flags |= SCX_RQ_BYPASSING;
+		} else {
+			WARN_ON_ONCE(!(rq->scx.flags & SCX_RQ_BYPASSING));
+			rq->scx.flags &= ~SCX_RQ_BYPASSING;
+		}
+
+		/*
+		 * We need to guarantee that no tasks are on the BPF scheduler
+		 * while bypassing. Either we see enabled or the enable path
+		 * sees scx_rq_bypassing() before moving tasks to SCX.
+		 */
+		if (!scx_enabled()) {
+			rq_unlock_irqrestore(rq, &rf);
+			continue;
+		}
+
+		/*
+		 * The use of list_for_each_entry_safe_reverse() is required
+		 * because each task is going to be removed from and added back
+		 * to the runnable_list during iteration. Because they're added
+		 * to the tail of the list, safe reverse iteration can still
+		 * visit all nodes.
+		 */
+		list_for_each_entry_safe_reverse(p, n, &rq->scx.runnable_list,
+						 scx.runnable_node) {
+			struct sched_enq_and_set_ctx ctx;
+
+			/* cycling deq/enq is enough, see the function comment */
+			sched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE, &ctx);
+			sched_enq_and_set_task(&ctx);
+		}
+
+		rq_unlock_irqrestore(rq, &rf);
+
+		/* resched to restore ticks and idle state */
+		resched_cpu(cpu);
+	}
+}
+
+static void free_exit_info(struct scx_exit_info *ei)
+{
+	kfree(ei->dump);
+	kfree(ei->msg);
+	kfree(ei->bt);
+	kfree(ei);
+}
+
+static struct scx_exit_info *alloc_exit_info(size_t exit_dump_len)
+{
+	struct scx_exit_info *ei;
+
+	ei = kzalloc(sizeof(*ei), GFP_KERNEL);
+	if (!ei)
+		return NULL;
+
+	ei->bt = kcalloc(SCX_EXIT_BT_LEN, sizeof(ei->bt[0]), GFP_KERNEL);
+	ei->msg = kzalloc(SCX_EXIT_MSG_LEN, GFP_KERNEL);
+	ei->dump = kzalloc(exit_dump_len, GFP_KERNEL);
+
+	if (!ei->bt || !ei->msg || !ei->dump) {
+		free_exit_info(ei);
+		return NULL;
+	}
+
+	return ei;
+}
+
+static const char *scx_exit_reason(enum scx_exit_kind kind)
+{
+	switch (kind) {
+	case SCX_EXIT_UNREG:
+		return "unregistered from user space";
+	case SCX_EXIT_UNREG_BPF:
+		return "unregistered from BPF";
+	case SCX_EXIT_UNREG_KERN:
+		return "unregistered from the main kernel";
+	case SCX_EXIT_SYSRQ:
+		return "disabled by sysrq-S";
+	case SCX_EXIT_ERROR:
+		return "runtime error";
+	case SCX_EXIT_ERROR_BPF:
+		return "scx_bpf_error";
+	case SCX_EXIT_ERROR_STALL:
+		return "runnable task stall";
+	default:
+		return "<UNKNOWN>";
+	}
+}
+
+static void scx_ops_disable_workfn(struct kthread_work *work)
+{
+	struct scx_exit_info *ei = scx_exit_info;
+	struct scx_task_iter sti;
+	struct task_struct *p;
+	struct rhashtable_iter rht_iter;
+	struct scx_dispatch_q *dsq;
+	int i, kind;
+
+	kind = atomic_read(&scx_exit_kind);
+	while (true) {
+		/*
+		 * NONE indicates that a new scx_ops has been registered since
+		 * disable was scheduled - don't kill the new ops. DONE
+		 * indicates that the ops has already been disabled.
+		 */
+		if (kind == SCX_EXIT_NONE || kind == SCX_EXIT_DONE)
+			return;
+		if (atomic_try_cmpxchg(&scx_exit_kind, &kind, SCX_EXIT_DONE))
+			break;
+	}
+	ei->kind = kind;
+	ei->reason = scx_exit_reason(ei->kind);
+
+	/* guarantee forward progress by bypassing scx_ops */
+	scx_ops_bypass(true);
+
+	switch (scx_ops_set_enable_state(SCX_OPS_DISABLING)) {
+	case SCX_OPS_DISABLING:
+		WARN_ONCE(true, "sched_ext: duplicate disabling instance?");
+		break;
+	case SCX_OPS_DISABLED:
+		pr_warn("sched_ext: ops error detected without ops (%s)\n",
+			scx_exit_info->msg);
+		WARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=
+			     SCX_OPS_DISABLING);
+		goto done;
+	default:
+		break;
+	}
+
+	/*
+	 * Here, every runnable task is guaranteed to make forward progress and
+	 * we can safely use blocking synchronization constructs. Actually
+	 * disable ops.
+	 */
+	mutex_lock(&scx_ops_enable_mutex);
+
+	static_branch_disable(&__scx_switched_all);
+	WRITE_ONCE(scx_switching_all, false);
+
+	/*
+	 * Shut down cgroup support before tasks so that the cgroup attach path
+	 * doesn't race against scx_ops_exit_task().
+	 */
+	scx_cgroup_lock();
+	scx_cgroup_exit();
+	scx_cgroup_unlock();
+
+	/*
+	 * The BPF scheduler is going away. All tasks including %TASK_DEAD ones
+	 * must be switched out and exited synchronously.
+	 */
+	percpu_down_write(&scx_fork_rwsem);
+
+	scx_ops_init_task_enabled = false;
+
+	scx_task_iter_start(&sti);
+	while ((p = scx_task_iter_next_locked(&sti))) {
+		const struct sched_class *old_class = p->sched_class;
+		struct sched_enq_and_set_ctx ctx;
+
+		sched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE, &ctx);
+
+		__setscheduler_prio(p, p->prio);
+		check_class_changing(task_rq(p), p, old_class);
+
+		sched_enq_and_set_task(&ctx);
+
+		check_class_changed(task_rq(p), p, old_class, p->prio);
+		scx_ops_exit_task(p);
+	}
+	scx_task_iter_stop(&sti);
+	percpu_up_write(&scx_fork_rwsem);
+
+	/* no task is on scx, turn off all the switches and flush in-progress calls */
+	static_branch_disable(&__scx_ops_enabled);
+	for (i = SCX_OPI_BEGIN; i < SCX_OPI_END; i++)
+		static_branch_disable(&scx_has_op[i]);
+	static_branch_disable(&scx_ops_enq_last);
+	static_branch_disable(&scx_ops_enq_exiting);
+	static_branch_disable(&scx_ops_cpu_preempt);
+	static_branch_disable(&scx_builtin_idle_enabled);
+	synchronize_rcu();
+
+	if (ei->kind >= SCX_EXIT_ERROR) {
+		pr_err("sched_ext: BPF scheduler \"%s\" disabled (%s)\n",
+		       scx_ops.name, ei->reason);
+
+		if (ei->msg[0] != '\0')
+			pr_err("sched_ext: %s: %s\n", scx_ops.name, ei->msg);
+#ifdef CONFIG_STACKTRACE
+		stack_trace_print(ei->bt, ei->bt_len, 2);
+#endif
+	} else {
+		pr_info("sched_ext: BPF scheduler \"%s\" disabled (%s)\n",
+			scx_ops.name, ei->reason);
+	}
+
+	if (scx_ops.exit)
+		SCX_CALL_OP(SCX_KF_UNLOCKED, exit, ei);
+
+	cancel_delayed_work_sync(&scx_watchdog_work);
+
+	/*
+	 * Delete the kobject from the hierarchy eagerly in addition to just
+	 * dropping a reference. Otherwise, if the object is deleted
+	 * asynchronously, sysfs could observe an object of the same name still
+	 * in the hierarchy when another scheduler is loaded.
+	 */
+	kobject_del(scx_root_kobj);
+	kobject_put(scx_root_kobj);
+	scx_root_kobj = NULL;
+
+	memset(&scx_ops, 0, sizeof(scx_ops));
+
+	rhashtable_walk_enter(&dsq_hash, &rht_iter);
+	do {
+		rhashtable_walk_start(&rht_iter);
+
+		while ((dsq = rhashtable_walk_next(&rht_iter)) && !IS_ERR(dsq))
+			destroy_dsq(dsq->id);
+
+		rhashtable_walk_stop(&rht_iter);
+	} while (dsq == ERR_PTR(-EAGAIN));
+	rhashtable_walk_exit(&rht_iter);
+
+	free_percpu(scx_dsp_ctx);
+	scx_dsp_ctx = NULL;
+	scx_dsp_max_batch = 0;
+
+	free_exit_info(scx_exit_info);
+	scx_exit_info = NULL;
+
+	mutex_unlock(&scx_ops_enable_mutex);
+
+	WARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_DISABLED) !=
+		     SCX_OPS_DISABLING);
+done:
+	scx_ops_bypass(false);
+}
+
+static DEFINE_KTHREAD_WORK(scx_ops_disable_work, scx_ops_disable_workfn);
+
+static void schedule_scx_ops_disable_work(void)
+{
+	struct kthread_worker *helper = READ_ONCE(scx_ops_helper);
+
+	/*
+	 * We may be called spuriously before the first bpf_sched_ext_reg(). If
+	 * scx_ops_helper isn't set up yet, there's nothing to do.
+	 */
+	if (helper)
+		kthread_queue_work(helper, &scx_ops_disable_work);
+}
+
+static void scx_ops_disable(enum scx_exit_kind kind)
+{
+	int none = SCX_EXIT_NONE;
+
+	if (WARN_ON_ONCE(kind == SCX_EXIT_NONE || kind == SCX_EXIT_DONE))
+		kind = SCX_EXIT_ERROR;
+
+	atomic_try_cmpxchg(&scx_exit_kind, &none, kind);
+
+	schedule_scx_ops_disable_work();
+}
+
+static void dump_newline(struct seq_buf *s)
+{
+	trace_sched_ext_dump("");
+
+	/* @s may be zero sized and seq_buf triggers WARN if so */
+	if (s->size)
+		seq_buf_putc(s, '\n');
+}
+
+static __printf(2, 3) void dump_line(struct seq_buf *s, const char *fmt, ...)
+{
+	va_list args;
+
+#ifdef CONFIG_TRACEPOINTS
+	if (trace_sched_ext_dump_enabled()) {
+		/* protected by scx_dump_state()::dump_lock */
+		static char line_buf[SCX_EXIT_MSG_LEN];
+
+		va_start(args, fmt);
+		vscnprintf(line_buf, sizeof(line_buf), fmt, args);
+		va_end(args);
+
+		trace_sched_ext_dump(line_buf);
+	}
+#endif
+	/* @s may be zero sized and seq_buf triggers WARN if so */
+	if (s->size) {
+		va_start(args, fmt);
+		seq_buf_vprintf(s, fmt, args);
+		va_end(args);
+
+		seq_buf_putc(s, '\n');
+	}
+}
+
+static void dump_stack_trace(struct seq_buf *s, const char *prefix,
+			     const unsigned long *bt, unsigned int len)
+{
+	unsigned int i;
+
+	for (i = 0; i < len; i++)
+		dump_line(s, "%s%pS", prefix, (void *)bt[i]);
+}
+
+static void ops_dump_init(struct seq_buf *s, const char *prefix)
+{
+	struct scx_dump_data *dd = &scx_dump_data;
+
+	lockdep_assert_irqs_disabled();
+
+	dd->cpu = smp_processor_id();		/* allow scx_bpf_dump() */
+	dd->first = true;
+	dd->cursor = 0;
+	dd->s = s;
+	dd->prefix = prefix;
+}
+
+static void ops_dump_flush(void)
+{
+	struct scx_dump_data *dd = &scx_dump_data;
+	char *line = dd->buf.line;
+
+	if (!dd->cursor)
+		return;
+
+	/*
+	 * There's something to flush and this is the first line. Insert a blank
+	 * line to distinguish ops dump.
+	 */
+	if (dd->first) {
+		dump_newline(dd->s);
+		dd->first = false;
+	}
+
+	/*
+	 * There may be multiple lines in $line. Scan and emit each line
+	 * separately.
+	 */
+	while (true) {
+		char *end = line;
+		char c;
+
+		while (*end != '\n' && *end != '\0')
+			end++;
+
+		/*
+		 * If $line overflowed, it may not have newline at the end.
+		 * Always emit with a newline.
+		 */
+		c = *end;
+		*end = '\0';
+		dump_line(dd->s, "%s%s", dd->prefix, line);
+		if (c == '\0')
+			break;
+
+		/* move to the next line */
+		end++;
+		if (*end == '\0')
+			break;
+		line = end;
+	}
+
+	dd->cursor = 0;
+}
+
+static void ops_dump_exit(void)
+{
+	ops_dump_flush();
+	scx_dump_data.cpu = -1;
+}
+
+static void scx_dump_task(struct seq_buf *s, struct scx_dump_ctx *dctx,
+			  struct task_struct *p, char marker)
+{
+	static unsigned long bt[SCX_EXIT_BT_LEN];
+	char dsq_id_buf[19] = "(n/a)";
+	unsigned long ops_state = atomic_long_read(&p->scx.ops_state);
+	unsigned int bt_len = 0;
+
+	if (p->scx.dsq)
+		scnprintf(dsq_id_buf, sizeof(dsq_id_buf), "0x%llx",
+			  (unsigned long long)p->scx.dsq->id);
+
+	dump_newline(s);
+	dump_line(s, " %c%c %s[%d] %+ldms",
+		  marker, task_state_to_char(p), p->comm, p->pid,
+		  jiffies_delta_msecs(p->scx.runnable_at, dctx->at_jiffies));
+	dump_line(s, "      scx_state/flags=%u/0x%x dsq_flags=0x%x ops_state/qseq=%lu/%lu",
+		  scx_get_task_state(p), p->scx.flags & ~SCX_TASK_STATE_MASK,
+		  p->scx.dsq_flags, ops_state & SCX_OPSS_STATE_MASK,
+		  ops_state >> SCX_OPSS_QSEQ_SHIFT);
+	dump_line(s, "      sticky/holding_cpu=%d/%d dsq_id=%s dsq_vtime=%llu",
+		  p->scx.sticky_cpu, p->scx.holding_cpu, dsq_id_buf,
+		  p->scx.dsq_vtime);
+	dump_line(s, "      cpus=%*pb", cpumask_pr_args(p->cpus_ptr));
+
+	if (SCX_HAS_OP(dump_task)) {
+		ops_dump_init(s, "    ");
+		SCX_CALL_OP(SCX_KF_REST, dump_task, dctx, p);
+		ops_dump_exit();
+	}
+
+#ifdef CONFIG_STACKTRACE
+	bt_len = stack_trace_save_tsk(p, bt, SCX_EXIT_BT_LEN, 1);
+#endif
+	if (bt_len) {
+		dump_newline(s);
+		dump_stack_trace(s, "    ", bt, bt_len);
+	}
+}
+
+static void scx_dump_state(struct scx_exit_info *ei, size_t dump_len)
+{
+	static DEFINE_SPINLOCK(dump_lock);
+	static const char trunc_marker[] = "\n\n~~~~ TRUNCATED ~~~~\n";
+	struct scx_dump_ctx dctx = {
+		.kind = ei->kind,
+		.exit_code = ei->exit_code,
+		.reason = ei->reason,
+		.at_ns = ktime_get_ns(),
+		.at_jiffies = jiffies,
+	};
+	struct seq_buf s;
+	unsigned long flags;
+	char *buf;
+	int cpu;
+
+	spin_lock_irqsave(&dump_lock, flags);
+
+	seq_buf_init(&s, ei->dump, dump_len);
+
+	if (ei->kind == SCX_EXIT_NONE) {
+		dump_line(&s, "Debug dump triggered by %s", ei->reason);
+	} else {
+		dump_line(&s, "%s[%d] triggered exit kind %d:",
+			  current->comm, current->pid, ei->kind);
+		dump_line(&s, "  %s (%s)", ei->reason, ei->msg);
+		dump_newline(&s);
+		dump_line(&s, "Backtrace:");
+		dump_stack_trace(&s, "  ", ei->bt, ei->bt_len);
+	}
+
+	if (SCX_HAS_OP(dump)) {
+		ops_dump_init(&s, "");
+		SCX_CALL_OP(SCX_KF_UNLOCKED, dump, &dctx);
+		ops_dump_exit();
+	}
+
+	dump_newline(&s);
+	dump_line(&s, "CPU states");
+	dump_line(&s, "----------");
+
+	for_each_possible_cpu(cpu) {
+		struct rq *rq = cpu_rq(cpu);
+		struct rq_flags rf;
+		struct task_struct *p;
+		struct seq_buf ns;
+		size_t avail, used;
+		bool idle;
+
+		rq_lock(rq, &rf);
+
+		idle = list_empty(&rq->scx.runnable_list) &&
+			rq->curr->sched_class == &idle_sched_class;
+
+		if (idle && !SCX_HAS_OP(dump_cpu))
+			goto next;
+
+		/*
+		 * We don't yet know whether ops.dump_cpu() will produce output
+		 * and we may want to skip the default CPU dump if it doesn't.
+		 * Use a nested seq_buf to generate the standard dump so that we
+		 * can decide whether to commit later.
+		 */
+		avail = seq_buf_get_buf(&s, &buf);
+		seq_buf_init(&ns, buf, avail);
+
+		dump_newline(&ns);
+		dump_line(&ns, "CPU %-4d: nr_run=%u flags=0x%x cpu_rel=%d ops_qseq=%lu pnt_seq=%lu",
+			  cpu, rq->scx.nr_running, rq->scx.flags,
+			  rq->scx.cpu_released, rq->scx.ops_qseq,
+			  rq->scx.pnt_seq);
+		dump_line(&ns, "          curr=%s[%d] class=%ps",
+			  rq->curr->comm, rq->curr->pid,
+			  rq->curr->sched_class);
+		if (!cpumask_empty(rq->scx.cpus_to_kick))
+			dump_line(&ns, "  cpus_to_kick   : %*pb",
+				  cpumask_pr_args(rq->scx.cpus_to_kick));
+		if (!cpumask_empty(rq->scx.cpus_to_kick_if_idle))
+			dump_line(&ns, "  idle_to_kick   : %*pb",
+				  cpumask_pr_args(rq->scx.cpus_to_kick_if_idle));
+		if (!cpumask_empty(rq->scx.cpus_to_preempt))
+			dump_line(&ns, "  cpus_to_preempt: %*pb",
+				  cpumask_pr_args(rq->scx.cpus_to_preempt));
+		if (!cpumask_empty(rq->scx.cpus_to_wait))
+			dump_line(&ns, "  cpus_to_wait   : %*pb",
+				  cpumask_pr_args(rq->scx.cpus_to_wait));
+
+		used = seq_buf_used(&ns);
+		if (SCX_HAS_OP(dump_cpu)) {
+			ops_dump_init(&ns, "  ");
+			SCX_CALL_OP(SCX_KF_REST, dump_cpu, &dctx, cpu, idle);
+			ops_dump_exit();
+		}
+
+		/*
+		 * If idle && nothing generated by ops.dump_cpu(), there's
+		 * nothing interesting. Skip.
+		 */
+		if (idle && used == seq_buf_used(&ns))
+			goto next;
+
+		/*
+		 * $s may already have overflowed when $ns was created. If so,
+		 * calling commit on it will trigger BUG.
+		 */
+		if (avail) {
+			seq_buf_commit(&s, seq_buf_used(&ns));
+			if (seq_buf_has_overflowed(&ns))
+				seq_buf_set_overflow(&s);
+		}
+
+		if (rq->curr->sched_class == &ext_sched_class)
+			scx_dump_task(&s, &dctx, rq->curr, '*');
+
+		list_for_each_entry(p, &rq->scx.runnable_list, scx.runnable_node)
+			scx_dump_task(&s, &dctx, p, ' ');
+	next:
+		rq_unlock(rq, &rf);
+	}
+
+	if (seq_buf_has_overflowed(&s) && dump_len >= sizeof(trunc_marker))
+		memcpy(ei->dump + dump_len - sizeof(trunc_marker),
+		       trunc_marker, sizeof(trunc_marker));
+
+	spin_unlock_irqrestore(&dump_lock, flags);
+}
+
+static void scx_ops_error_irq_workfn(struct irq_work *irq_work)
+{
+	struct scx_exit_info *ei = scx_exit_info;
+
+	if (ei->kind >= SCX_EXIT_ERROR)
+		scx_dump_state(ei, scx_ops.exit_dump_len);
+
+	schedule_scx_ops_disable_work();
+}
+
+static DEFINE_IRQ_WORK(scx_ops_error_irq_work, scx_ops_error_irq_workfn);
+
+static __printf(3, 4) void scx_ops_exit_kind(enum scx_exit_kind kind,
+					     s64 exit_code,
+					     const char *fmt, ...)
+{
+	struct scx_exit_info *ei = scx_exit_info;
+	int none = SCX_EXIT_NONE;
+	va_list args;
+
+	if (!atomic_try_cmpxchg(&scx_exit_kind, &none, kind))
+		return;
+
+	ei->exit_code = exit_code;
+#ifdef CONFIG_STACKTRACE
+	if (kind >= SCX_EXIT_ERROR)
+		ei->bt_len = stack_trace_save(ei->bt, SCX_EXIT_BT_LEN, 1);
+#endif
+	va_start(args, fmt);
+	vscnprintf(ei->msg, SCX_EXIT_MSG_LEN, fmt, args);
+	va_end(args);
+
+	/*
+	 * Set ei->kind and ->reason for scx_dump_state(). They'll be set again
+	 * in scx_ops_disable_workfn().
+	 */
+	ei->kind = kind;
+	ei->reason = scx_exit_reason(ei->kind);
+
+	irq_work_queue(&scx_ops_error_irq_work);
+}
+
+static struct kthread_worker *scx_create_rt_helper(const char *name)
+{
+	struct kthread_worker *helper;
+
+	helper = kthread_create_worker(0, name);
+	if (helper)
+		sched_set_fifo(helper->task);
+	return helper;
+}
+
+static void check_hotplug_seq(const struct sched_ext_ops *ops)
+{
+	unsigned long long global_hotplug_seq;
+
+	/*
+	 * If a hotplug event has occurred between when a scheduler was
+	 * initialized, and when we were able to attach, exit and notify user
+	 * space about it.
+	 */
+	if (ops->hotplug_seq) {
+		global_hotplug_seq = atomic_long_read(&scx_hotplug_seq);
+		if (ops->hotplug_seq != global_hotplug_seq) {
+			scx_ops_exit(SCX_ECODE_ACT_RESTART | SCX_ECODE_RSN_HOTPLUG,
+				     "expected hotplug seq %llu did not match actual %llu",
+				     ops->hotplug_seq, global_hotplug_seq);
+		}
+	}
+}
+
+static int validate_ops(const struct sched_ext_ops *ops)
+{
+	/*
+	 * It doesn't make sense to specify the SCX_OPS_ENQ_LAST flag if the
+	 * ops.enqueue() callback isn't implemented.
+	 */
+	if ((ops->flags & SCX_OPS_ENQ_LAST) && !ops->enqueue) {
+		scx_ops_error("SCX_OPS_ENQ_LAST requires ops.enqueue() to be implemented");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int scx_ops_enable(struct sched_ext_ops *ops, struct bpf_link *link)
+{
+	struct scx_task_iter sti;
+	struct task_struct *p;
+	unsigned long timeout;
+	int i, cpu, node, ret;
+
+	if (!cpumask_equal(housekeeping_cpumask(HK_TYPE_DOMAIN),
+			   cpu_possible_mask)) {
+		pr_err("sched_ext: Not compatible with \"isolcpus=\" domain isolation");
+		return -EINVAL;
+	}
+
+	mutex_lock(&scx_ops_enable_mutex);
+
+	if (!scx_ops_helper) {
+		WRITE_ONCE(scx_ops_helper,
+			   scx_create_rt_helper("sched_ext_ops_helper"));
+		if (!scx_ops_helper) {
+			ret = -ENOMEM;
+			goto err_unlock;
+		}
+	}
+
+	if (!global_dsqs) {
+		struct scx_dispatch_q **dsqs;
+
+		dsqs = kcalloc(nr_node_ids, sizeof(dsqs[0]), GFP_KERNEL);
+		if (!dsqs) {
+			ret = -ENOMEM;
+			goto err_unlock;
+		}
+
+		for_each_node_state(node, N_POSSIBLE) {
+			struct scx_dispatch_q *dsq;
+
+			dsq = kzalloc_node(sizeof(*dsq), GFP_KERNEL, node);
+			if (!dsq) {
+				for_each_node_state(node, N_POSSIBLE)
+					kfree(dsqs[node]);
+				kfree(dsqs);
+				ret = -ENOMEM;
+				goto err_unlock;
+			}
+
+			init_dsq(dsq, SCX_DSQ_GLOBAL);
+			dsqs[node] = dsq;
+		}
+
+		global_dsqs = dsqs;
+	}
+
+	if (scx_ops_enable_state() != SCX_OPS_DISABLED) {
+		ret = -EBUSY;
+		goto err_unlock;
+	}
+
+	scx_root_kobj = kzalloc(sizeof(*scx_root_kobj), GFP_KERNEL);
+	if (!scx_root_kobj) {
+		ret = -ENOMEM;
+		goto err_unlock;
+	}
+
+	scx_root_kobj->kset = scx_kset;
+	ret = kobject_init_and_add(scx_root_kobj, &scx_ktype, NULL, "root");
+	if (ret < 0)
+		goto err;
+
+	scx_exit_info = alloc_exit_info(ops->exit_dump_len);
+	if (!scx_exit_info) {
+		ret = -ENOMEM;
+		goto err_del;
+	}
+
+	/*
+	 * Set scx_ops, transition to ENABLING and clear exit info to arm the
+	 * disable path. Failure triggers full disabling from here on.
+	 */
+	scx_ops = *ops;
+
+	WARN_ON_ONCE(scx_ops_set_enable_state(SCX_OPS_ENABLING) !=
+		     SCX_OPS_DISABLED);
+
+	atomic_set(&scx_exit_kind, SCX_EXIT_NONE);
+	scx_warned_zero_slice = false;
+
+	atomic_long_set(&scx_nr_rejected, 0);
+
+	for_each_possible_cpu(cpu)
+		cpu_rq(cpu)->scx.cpuperf_target = SCX_CPUPERF_ONE;
+
+	/*
+	 * Keep CPUs stable during enable so that the BPF scheduler can track
+	 * online CPUs by watching ->on/offline_cpu() after ->init().
+	 */
+	cpus_read_lock();
+
+	if (scx_ops.init) {
+		ret = SCX_CALL_OP_RET(SCX_KF_UNLOCKED, init);
+		if (ret) {
+			ret = ops_sanitize_err("init", ret);
+			cpus_read_unlock();
+			goto err_disable;
+		}
+	}
+
+	for (i = SCX_OPI_CPU_HOTPLUG_BEGIN; i < SCX_OPI_CPU_HOTPLUG_END; i++)
+		if (((void (**)(void))ops)[i])
+			static_branch_enable_cpuslocked(&scx_has_op[i]);
+
+	check_hotplug_seq(ops);
+	cpus_read_unlock();
+
+	ret = validate_ops(ops);
+	if (ret)
+		goto err_disable;
+
+	WARN_ON_ONCE(scx_dsp_ctx);
+	scx_dsp_max_batch = ops->dispatch_max_batch ?: SCX_DSP_DFL_MAX_BATCH;
+	scx_dsp_ctx = __alloc_percpu(struct_size_t(struct scx_dsp_ctx, buf,
+						   scx_dsp_max_batch),
+				     __alignof__(struct scx_dsp_ctx));
+	if (!scx_dsp_ctx) {
+		ret = -ENOMEM;
+		goto err_disable;
+	}
+
+	if (ops->timeout_ms)
+		timeout = msecs_to_jiffies(ops->timeout_ms);
+	else
+		timeout = SCX_WATCHDOG_MAX_TIMEOUT;
+
+	WRITE_ONCE(scx_watchdog_timeout, timeout);
+	WRITE_ONCE(scx_watchdog_timestamp, jiffies);
+	queue_delayed_work(system_unbound_wq, &scx_watchdog_work,
+			   scx_watchdog_timeout / 2);
+
+	/*
+	 * Once __scx_ops_enabled is set, %current can be switched to SCX
+	 * anytime. This can lead to stalls as some BPF schedulers (e.g.
+	 * userspace scheduling) may not function correctly before all tasks are
+	 * switched. Init in bypass mode to guarantee forward progress.
+	 */
+	scx_ops_bypass(true);
+
+	for (i = SCX_OPI_NORMAL_BEGIN; i < SCX_OPI_NORMAL_END; i++)
+		if (((void (**)(void))ops)[i])
+			static_branch_enable(&scx_has_op[i]);
+
+	if (ops->flags & SCX_OPS_ENQ_LAST)
+		static_branch_enable(&scx_ops_enq_last);
+
+	if (ops->flags & SCX_OPS_ENQ_EXITING)
+		static_branch_enable(&scx_ops_enq_exiting);
+	if (scx_ops.cpu_acquire || scx_ops.cpu_release)
+		static_branch_enable(&scx_ops_cpu_preempt);
+
+	if (!ops->update_idle || (ops->flags & SCX_OPS_KEEP_BUILTIN_IDLE)) {
+		reset_idle_masks();
+		static_branch_enable(&scx_builtin_idle_enabled);
+	} else {
+		static_branch_disable(&scx_builtin_idle_enabled);
+	}
+
+	/*
+	 * Lock out forks, cgroup on/offlining and moves before opening the
+	 * floodgate so that they don't wander into the operations prematurely.
+	 */
+	percpu_down_write(&scx_fork_rwsem);
+
+	WARN_ON_ONCE(scx_ops_init_task_enabled);
+	scx_ops_init_task_enabled = true;
+
+	/*
+	 * Enable ops for every task. Fork is excluded by scx_fork_rwsem
+	 * preventing new tasks from being added. No need to exclude tasks
+	 * leaving as sched_ext_free() can handle both prepped and enabled
+	 * tasks. Prep all tasks first and then enable them with preemption
+	 * disabled.
+	 *
+	 * All cgroups should be initialized before scx_ops_init_task() so that
+	 * the BPF scheduler can reliably track each task's cgroup membership
+	 * from scx_ops_init_task(). Lock out cgroup on/offlining and task
+	 * migrations while tasks are being initialized so that
+	 * scx_cgroup_can_attach() never sees uninitialized tasks.
+	 */
+	scx_cgroup_lock();
+	ret = scx_cgroup_init();
+	if (ret)
+		goto err_disable_unlock_all;
+
+	scx_task_iter_start(&sti);
+	while ((p = scx_task_iter_next_locked(&sti))) {
+		/*
+		 * @p may already be dead, have lost all its usages counts and
+		 * be waiting for RCU grace period before being freed. @p can't
+		 * be initialized for SCX in such cases and should be ignored.
+		 */
+		if (!tryget_task_struct(p))
+			continue;
+
+		scx_task_iter_unlock(&sti);
+
+		ret = scx_ops_init_task(p, task_group(p), false);
+		if (ret) {
+			put_task_struct(p);
+			scx_task_iter_relock(&sti);
+			scx_task_iter_stop(&sti);
+			pr_err("sched_ext: ops.init_task() failed (%d) for %s[%d] while loading\n",
+			       ret, p->comm, p->pid);
+			goto err_disable_unlock_all;
+		}
+
+		scx_set_task_state(p, SCX_TASK_READY);
+
+		put_task_struct(p);
+		scx_task_iter_relock(&sti);
+	}
+	scx_task_iter_stop(&sti);
+	scx_cgroup_unlock();
+	percpu_up_write(&scx_fork_rwsem);
+
+	/*
+	 * All tasks are READY. It's safe to turn on scx_enabled() and switch
+	 * all eligible tasks.
+	 */
+	WRITE_ONCE(scx_switching_all, !(ops->flags & SCX_OPS_SWITCH_PARTIAL));
+	static_branch_enable(&__scx_ops_enabled);
+
+	/*
+	 * We're fully committed and can't fail. The task READY -> ENABLED
+	 * transitions here are synchronized against sched_ext_free() through
+	 * scx_tasks_lock.
+	 */
+	percpu_down_write(&scx_fork_rwsem);
+	scx_task_iter_start(&sti);
+	while ((p = scx_task_iter_next_locked(&sti))) {
+		const struct sched_class *old_class = p->sched_class;
+		struct sched_enq_and_set_ctx ctx;
+
+		sched_deq_and_put_task(p, DEQUEUE_SAVE | DEQUEUE_MOVE, &ctx);
+
+		p->scx.slice = SCX_SLICE_DFL;
+		__setscheduler_prio(p, p->prio);
+		check_class_changing(task_rq(p), p, old_class);
+
+		sched_enq_and_set_task(&ctx);
+
+		check_class_changed(task_rq(p), p, old_class, p->prio);
+	}
+	scx_task_iter_stop(&sti);
+	percpu_up_write(&scx_fork_rwsem);
+
+	scx_ops_bypass(false);
+
+	/*
+	 * Returning an error code here would lose the recorded error
+	 * information. Exit indicating success so that the error is notified
+	 * through ops.exit() with all the details.
+	 */
+	if (!scx_ops_tryset_enable_state(SCX_OPS_ENABLED, SCX_OPS_ENABLING)) {
+		WARN_ON_ONCE(atomic_read(&scx_exit_kind) == SCX_EXIT_NONE);
+		ret = 0;
+		goto err_disable;
+	}
+
+	if (!(ops->flags & SCX_OPS_SWITCH_PARTIAL))
+		static_branch_enable(&__scx_switched_all);
+
+	pr_info("sched_ext: BPF scheduler \"%s\" enabled%s\n",
+		scx_ops.name, scx_switched_all() ? "" : " (partial)");
+	kobject_uevent(scx_root_kobj, KOBJ_ADD);
+	mutex_unlock(&scx_ops_enable_mutex);
+
+	atomic_long_inc(&scx_enable_seq);
+
+	return 0;
+
+err_del:
+	kobject_del(scx_root_kobj);
+err:
+	kobject_put(scx_root_kobj);
+	scx_root_kobj = NULL;
+	if (scx_exit_info) {
+		free_exit_info(scx_exit_info);
+		scx_exit_info = NULL;
+	}
+err_unlock:
+	mutex_unlock(&scx_ops_enable_mutex);
+	return ret;
+
+err_disable_unlock_all:
+	scx_cgroup_unlock();
+	percpu_up_write(&scx_fork_rwsem);
+	scx_ops_bypass(false);
+err_disable:
+	mutex_unlock(&scx_ops_enable_mutex);
+	/* must be fully disabled before returning */
+	scx_ops_disable(SCX_EXIT_ERROR);
+	kthread_flush_work(&scx_ops_disable_work);
+	return ret;
+}
+
+
+/********************************************************************************
+ * bpf_struct_ops plumbing.
+ */
+#include <linux/bpf_verifier.h>
+#include <linux/bpf.h>
+#include <linux/btf.h>
+
+extern struct btf *btf_vmlinux;
+static const struct btf_type *task_struct_type;
+static u32 task_struct_type_id;
+
+static bool set_arg_maybe_null(const char *op, int arg_n, int off, int size,
+			       enum bpf_access_type type,
+			       const struct bpf_prog *prog,
+			       struct bpf_insn_access_aux *info)
+{
+	struct btf *btf = bpf_get_btf_vmlinux();
+	const struct bpf_struct_ops_desc *st_ops_desc;
+	const struct btf_member *member;
+	const struct btf_type *t;
+	u32 btf_id, member_idx;
+	const char *mname;
+
+	/* struct_ops op args are all sequential, 64-bit numbers */
+	if (off != arg_n * sizeof(__u64))
+		return false;
+
+	/* btf_id should be the type id of struct sched_ext_ops */
+	btf_id = prog->aux->attach_btf_id;
+	st_ops_desc = bpf_struct_ops_find(btf, btf_id);
+	if (!st_ops_desc)
+		return false;
+
+	/* BTF type of struct sched_ext_ops */
+	t = st_ops_desc->type;
+
+	member_idx = prog->expected_attach_type;
+	if (member_idx >= btf_type_vlen(t))
+		return false;
+
+	/*
+	 * Get the member name of this struct_ops program, which corresponds to
+	 * a field in struct sched_ext_ops. For example, the member name of the
+	 * dispatch struct_ops program (callback) is "dispatch".
+	 */
+	member = &btf_type_member(t)[member_idx];
+	mname = btf_name_by_offset(btf_vmlinux, member->name_off);
+
+	if (!strcmp(mname, op)) {
+		/*
+		 * The value is a pointer to a type (struct task_struct) given
+		 * by a BTF ID (PTR_TO_BTF_ID). It is trusted (PTR_TRUSTED),
+		 * however, can be a NULL (PTR_MAYBE_NULL). The BPF program
+		 * should check the pointer to make sure it is not NULL before
+		 * using it, or the verifier will reject the program.
+		 *
+		 * Longer term, this is something that should be addressed by
+		 * BTF, and be fully contained within the verifier.
+		 */
+		info->reg_type = PTR_MAYBE_NULL | PTR_TO_BTF_ID | PTR_TRUSTED;
+		info->btf = btf_vmlinux;
+		info->btf_id = task_struct_type_id;
+
+		return true;
+	}
+
+	return false;
+}
+
+static bool bpf_scx_is_valid_access(int off, int size,
+				    enum bpf_access_type type,
+				    const struct bpf_prog *prog,
+				    struct bpf_insn_access_aux *info)
+{
+	if (type != BPF_READ)
+		return false;
+	if (set_arg_maybe_null("dispatch", 1, off, size, type, prog, info) ||
+	    set_arg_maybe_null("yield", 1, off, size, type, prog, info))
+		return true;
+	if (off < 0 || off >= sizeof(__u64) * MAX_BPF_FUNC_ARGS)
+		return false;
+	if (off % size != 0)
+		return false;
+
+	return btf_ctx_access(off, size, type, prog, info);
+}
+
+static int bpf_scx_btf_struct_access(struct bpf_verifier_log *log,
+				     const struct bpf_reg_state *reg, int off,
+				     int size)
+{
+	const struct btf_type *t;
+
+	t = btf_type_by_id(reg->btf, reg->btf_id);
+	if (t == task_struct_type) {
+		if (off >= offsetof(struct task_struct, scx.slice) &&
+		    off + size <= offsetofend(struct task_struct, scx.slice))
+			return SCALAR_VALUE;
+		if (off >= offsetof(struct task_struct, scx.dsq_vtime) &&
+		    off + size <= offsetofend(struct task_struct, scx.dsq_vtime))
+			return SCALAR_VALUE;
+		if (off >= offsetof(struct task_struct, scx.disallow) &&
+		    off + size <= offsetofend(struct task_struct, scx.disallow))
+			return SCALAR_VALUE;
+	}
+
+	return -EACCES;
+}
+
+static const struct bpf_func_proto *
+bpf_scx_get_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
+{
+	switch (func_id) {
+	case BPF_FUNC_task_storage_get:
+		return &bpf_task_storage_get_proto;
+	case BPF_FUNC_task_storage_delete:
+		return &bpf_task_storage_delete_proto;
+	default:
+		return bpf_base_func_proto(func_id, prog);
+	}
+}
+
+static const struct bpf_verifier_ops bpf_scx_verifier_ops = {
+	.get_func_proto = bpf_scx_get_func_proto,
+	.is_valid_access = bpf_scx_is_valid_access,
+	.btf_struct_access = bpf_scx_btf_struct_access,
+};
+
+static int bpf_scx_init_member(const struct btf_type *t,
+			       const struct btf_member *member,
+			       void *kdata, const void *udata)
+{
+	const struct sched_ext_ops *uops = udata;
+	struct sched_ext_ops *ops = kdata;
+	u32 moff = __btf_member_bit_offset(t, member) / 8;
+	int ret;
+
+	switch (moff) {
+	case offsetof(struct sched_ext_ops, dispatch_max_batch):
+		if (*(u32 *)(udata + moff) > INT_MAX)
+			return -E2BIG;
+		ops->dispatch_max_batch = *(u32 *)(udata + moff);
+		return 1;
+	case offsetof(struct sched_ext_ops, flags):
+		if (*(u64 *)(udata + moff) & ~SCX_OPS_ALL_FLAGS)
+			return -EINVAL;
+		ops->flags = *(u64 *)(udata + moff);
+		return 1;
+	case offsetof(struct sched_ext_ops, name):
+		ret = bpf_obj_name_cpy(ops->name, uops->name,
+				       sizeof(ops->name));
+		if (ret < 0)
+			return ret;
+		if (ret == 0)
+			return -EINVAL;
+		return 1;
+	case offsetof(struct sched_ext_ops, timeout_ms):
+		if (msecs_to_jiffies(*(u32 *)(udata + moff)) >
+		    SCX_WATCHDOG_MAX_TIMEOUT)
+			return -E2BIG;
+		ops->timeout_ms = *(u32 *)(udata + moff);
+		return 1;
+	case offsetof(struct sched_ext_ops, exit_dump_len):
+		ops->exit_dump_len =
+			*(u32 *)(udata + moff) ?: SCX_EXIT_DUMP_DFL_LEN;
+		return 1;
+	case offsetof(struct sched_ext_ops, hotplug_seq):
+		ops->hotplug_seq = *(u64 *)(udata + moff);
+		return 1;
+	}
+
+	return 0;
+}
+
+static int bpf_scx_check_member(const struct btf_type *t,
+				const struct btf_member *member,
+				const struct bpf_prog *prog)
+{
+	u32 moff = __btf_member_bit_offset(t, member) / 8;
+
+	switch (moff) {
+	case offsetof(struct sched_ext_ops, init_task):
+#ifdef CONFIG_EXT_GROUP_SCHED
+	case offsetof(struct sched_ext_ops, cgroup_init):
+	case offsetof(struct sched_ext_ops, cgroup_exit):
+	case offsetof(struct sched_ext_ops, cgroup_prep_move):
+#endif
+	case offsetof(struct sched_ext_ops, cpu_online):
+	case offsetof(struct sched_ext_ops, cpu_offline):
+	case offsetof(struct sched_ext_ops, init):
+	case offsetof(struct sched_ext_ops, exit):
+		break;
+	default:
+		if (prog->sleepable)
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int bpf_scx_reg(void *kdata, struct bpf_link *link)
+{
+	return scx_ops_enable(kdata, link);
+}
+
+static void bpf_scx_unreg(void *kdata, struct bpf_link *link)
+{
+	scx_ops_disable(SCX_EXIT_UNREG);
+	kthread_flush_work(&scx_ops_disable_work);
+}
+
+static int bpf_scx_init(struct btf *btf)
+{
+	s32 type_id;
+
+	type_id = btf_find_by_name_kind(btf, "task_struct", BTF_KIND_STRUCT);
+	if (type_id < 0)
+		return -EINVAL;
+	task_struct_type = btf_type_by_id(btf, type_id);
+	task_struct_type_id = type_id;
+
+	return 0;
+}
+
+static int bpf_scx_update(void *kdata, void *old_kdata, struct bpf_link *link)
+{
+	/*
+	 * sched_ext does not support updating the actively-loaded BPF
+	 * scheduler, as registering a BPF scheduler can always fail if the
+	 * scheduler returns an error code for e.g. ops.init(), ops.init_task(),
+	 * etc. Similarly, we can always race with unregistration happening
+	 * elsewhere, such as with sysrq.
+	 */
+	return -EOPNOTSUPP;
+}
+
+static int bpf_scx_validate(void *kdata)
+{
+	return 0;
+}
+
+static s32 select_cpu_stub(struct task_struct *p, s32 prev_cpu, u64 wake_flags) { return -EINVAL; }
+static void enqueue_stub(struct task_struct *p, u64 enq_flags) {}
+static void dequeue_stub(struct task_struct *p, u64 enq_flags) {}
+static void dispatch_stub(s32 prev_cpu, struct task_struct *p) {}
+static void tick_stub(struct task_struct *p) {}
+static void runnable_stub(struct task_struct *p, u64 enq_flags) {}
+static void running_stub(struct task_struct *p) {}
+static void stopping_stub(struct task_struct *p, bool runnable) {}
+static void quiescent_stub(struct task_struct *p, u64 deq_flags) {}
+static bool yield_stub(struct task_struct *from, struct task_struct *to) { return false; }
+static bool core_sched_before_stub(struct task_struct *a, struct task_struct *b) { return false; }
+static void set_weight_stub(struct task_struct *p, u32 weight) {}
+static void set_cpumask_stub(struct task_struct *p, const struct cpumask *mask) {}
+static void update_idle_stub(s32 cpu, bool idle) {}
+static void cpu_acquire_stub(s32 cpu, struct scx_cpu_acquire_args *args) {}
+static void cpu_release_stub(s32 cpu, struct scx_cpu_release_args *args) {}
+static s32 init_task_stub(struct task_struct *p, struct scx_init_task_args *args) { return -EINVAL; }
+static void exit_task_stub(struct task_struct *p, struct scx_exit_task_args *args) {}
+static void enable_stub(struct task_struct *p) {}
+static void disable_stub(struct task_struct *p) {}
+#ifdef CONFIG_EXT_GROUP_SCHED
+static s32 cgroup_init_stub(struct cgroup *cgrp, struct scx_cgroup_init_args *args) { return -EINVAL; }
+static void cgroup_exit_stub(struct cgroup *cgrp) {}
+static s32 cgroup_prep_move_stub(struct task_struct *p, struct cgroup *from, struct cgroup *to) { return -EINVAL; }
+static void cgroup_move_stub(struct task_struct *p, struct cgroup *from, struct cgroup *to) {}
+static void cgroup_cancel_move_stub(struct task_struct *p, struct cgroup *from, struct cgroup *to) {}
+static void cgroup_set_weight_stub(struct cgroup *cgrp, u32 weight) {}
+#endif
+static void cpu_online_stub(s32 cpu) {}
+static void cpu_offline_stub(s32 cpu) {}
+static s32 init_stub(void) { return -EINVAL; }
+static void exit_stub(struct scx_exit_info *info) {}
+static void dump_stub(struct scx_dump_ctx *ctx) {}
+static void dump_cpu_stub(struct scx_dump_ctx *ctx, s32 cpu, bool idle) {}
+static void dump_task_stub(struct scx_dump_ctx *ctx, struct task_struct *p) {}
+
+static struct sched_ext_ops __bpf_ops_sched_ext_ops = {
+	.select_cpu = select_cpu_stub,
+	.enqueue = enqueue_stub,
+	.dequeue = dequeue_stub,
+	.dispatch = dispatch_stub,
+	.tick = tick_stub,
+	.runnable = runnable_stub,
+	.running = running_stub,
+	.stopping = stopping_stub,
+	.quiescent = quiescent_stub,
+	.yield = yield_stub,
+	.core_sched_before = core_sched_before_stub,
+	.set_weight = set_weight_stub,
+	.set_cpumask = set_cpumask_stub,
+	.update_idle = update_idle_stub,
+	.cpu_acquire = cpu_acquire_stub,
+	.cpu_release = cpu_release_stub,
+	.init_task = init_task_stub,
+	.exit_task = exit_task_stub,
+	.enable = enable_stub,
+	.disable = disable_stub,
+#ifdef CONFIG_EXT_GROUP_SCHED
+	.cgroup_init = cgroup_init_stub,
+	.cgroup_exit = cgroup_exit_stub,
+	.cgroup_prep_move = cgroup_prep_move_stub,
+	.cgroup_move = cgroup_move_stub,
+	.cgroup_cancel_move = cgroup_cancel_move_stub,
+	.cgroup_set_weight = cgroup_set_weight_stub,
+#endif
+	.cpu_online = cpu_online_stub,
+	.cpu_offline = cpu_offline_stub,
+	.init = init_stub,
+	.exit = exit_stub,
+	.dump = dump_stub,
+	.dump_cpu = dump_cpu_stub,
+	.dump_task = dump_task_stub,
+};
+
+static struct bpf_struct_ops bpf_sched_ext_ops = {
+	.verifier_ops = &bpf_scx_verifier_ops,
+	.reg = bpf_scx_reg,
+	.unreg = bpf_scx_unreg,
+	.check_member = bpf_scx_check_member,
+	.init_member = bpf_scx_init_member,
+	.init = bpf_scx_init,
+	.update = bpf_scx_update,
+	.validate = bpf_scx_validate,
+	.name = "sched_ext_ops",
+	.owner = THIS_MODULE,
+	.cfi_stubs = &__bpf_ops_sched_ext_ops
+};
+
+
+/********************************************************************************
+ * System integration and init.
+ */
+
+static void sysrq_handle_sched_ext_reset(u8 key)
+{
+	if (scx_ops_helper)
+		scx_ops_disable(SCX_EXIT_SYSRQ);
+	else
+		pr_info("sched_ext: BPF scheduler not yet used\n");
+}
+
+static const struct sysrq_key_op sysrq_sched_ext_reset_op = {
+	.handler	= sysrq_handle_sched_ext_reset,
+	.help_msg	= "reset-sched-ext(S)",
+	.action_msg	= "Disable sched_ext and revert all tasks to CFS",
+	.enable_mask	= SYSRQ_ENABLE_RTNICE,
+};
+
+static void sysrq_handle_sched_ext_dump(u8 key)
+{
+	struct scx_exit_info ei = { .kind = SCX_EXIT_NONE, .reason = "SysRq-D" };
+
+	if (scx_enabled())
+		scx_dump_state(&ei, 0);
+}
+
+static const struct sysrq_key_op sysrq_sched_ext_dump_op = {
+	.handler	= sysrq_handle_sched_ext_dump,
+	.help_msg	= "dump-sched-ext(D)",
+	.action_msg	= "Trigger sched_ext debug dump",
+	.enable_mask	= SYSRQ_ENABLE_RTNICE,
+};
+
+static bool can_skip_idle_kick(struct rq *rq)
+{
+	lockdep_assert_rq_held(rq);
+
+	/*
+	 * We can skip idle kicking if @rq is going to go through at least one
+	 * full SCX scheduling cycle before going idle. Just checking whether
+	 * curr is not idle is insufficient because we could be racing
+	 * balance_one() trying to pull the next task from a remote rq, which
+	 * may fail, and @rq may become idle afterwards.
+	 *
+	 * The race window is small and we don't and can't guarantee that @rq is
+	 * only kicked while idle anyway. Skip only when sure.
+	 */
+	return !is_idle_task(rq->curr) && !(rq->scx.flags & SCX_RQ_IN_BALANCE);
+}
+
+static bool kick_one_cpu(s32 cpu, struct rq *this_rq, unsigned long *pseqs)
+{
+	struct rq *rq = cpu_rq(cpu);
+	struct scx_rq *this_scx = &this_rq->scx;
+	bool should_wait = false;
+	unsigned long flags;
+
+	raw_spin_rq_lock_irqsave(rq, flags);
+
+	/*
+	 * During CPU hotplug, a CPU may depend on kicking itself to make
+	 * forward progress. Allow kicking self regardless of online state.
+	 */
+	if (cpu_online(cpu) || cpu == cpu_of(this_rq)) {
+		if (cpumask_test_cpu(cpu, this_scx->cpus_to_preempt)) {
+			if (rq->curr->sched_class == &ext_sched_class)
+				rq->curr->scx.slice = 0;
+			cpumask_clear_cpu(cpu, this_scx->cpus_to_preempt);
+		}
+
+		if (cpumask_test_cpu(cpu, this_scx->cpus_to_wait)) {
+			pseqs[cpu] = rq->scx.pnt_seq;
+			should_wait = true;
+		}
+
+		resched_curr(rq);
+	} else {
+		cpumask_clear_cpu(cpu, this_scx->cpus_to_preempt);
+		cpumask_clear_cpu(cpu, this_scx->cpus_to_wait);
+	}
+
+	raw_spin_rq_unlock_irqrestore(rq, flags);
+
+	return should_wait;
+}
+
+static void kick_one_cpu_if_idle(s32 cpu, struct rq *this_rq)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long flags;
+
+	raw_spin_rq_lock_irqsave(rq, flags);
+
+	if (!can_skip_idle_kick(rq) &&
+	    (cpu_online(cpu) || cpu == cpu_of(this_rq)))
+		resched_curr(rq);
+
+	raw_spin_rq_unlock_irqrestore(rq, flags);
+}
+
+static void kick_cpus_irq_workfn(struct irq_work *irq_work)
+{
+	struct rq *this_rq = this_rq();
+	struct scx_rq *this_scx = &this_rq->scx;
+	unsigned long *pseqs = this_cpu_ptr(scx_kick_cpus_pnt_seqs);
+	bool should_wait = false;
+	s32 cpu;
+
+	for_each_cpu(cpu, this_scx->cpus_to_kick) {
+		should_wait |= kick_one_cpu(cpu, this_rq, pseqs);
+		cpumask_clear_cpu(cpu, this_scx->cpus_to_kick);
+		cpumask_clear_cpu(cpu, this_scx->cpus_to_kick_if_idle);
+	}
+
+	for_each_cpu(cpu, this_scx->cpus_to_kick_if_idle) {
+		kick_one_cpu_if_idle(cpu, this_rq);
+		cpumask_clear_cpu(cpu, this_scx->cpus_to_kick_if_idle);
+	}
+
+	if (!should_wait)
+		return;
+
+	for_each_cpu(cpu, this_scx->cpus_to_wait) {
+		unsigned long *wait_pnt_seq = &cpu_rq(cpu)->scx.pnt_seq;
+
+		if (cpu != cpu_of(this_rq)) {
+			/*
+			 * Pairs with smp_store_release() issued by this CPU in
+			 * scx_next_task_picked() on the resched path.
+			 *
+			 * We busy-wait here to guarantee that no other task can
+			 * be scheduled on our core before the target CPU has
+			 * entered the resched path.
+			 */
+			while (smp_load_acquire(wait_pnt_seq) == pseqs[cpu])
+				cpu_relax();
+		}
+
+		cpumask_clear_cpu(cpu, this_scx->cpus_to_wait);
+	}
+}
+
+/**
+ * print_scx_info - print out sched_ext scheduler state
+ * @log_lvl: the log level to use when printing
+ * @p: target task
+ *
+ * If a sched_ext scheduler is enabled, print the name and state of the
+ * scheduler. If @p is on sched_ext, print further information about the task.
+ *
+ * This function can be safely called on any task as long as the task_struct
+ * itself is accessible. While safe, this function isn't synchronized and may
+ * print out mixups or garbages of limited length.
+ */
+void print_scx_info(const char *log_lvl, struct task_struct *p)
+{
+	enum scx_ops_enable_state state = scx_ops_enable_state();
+	const char *all = READ_ONCE(scx_switching_all) ? "+all" : "";
+	char runnable_at_buf[22] = "?";
+	struct sched_class *class;
+	unsigned long runnable_at;
+
+	if (state == SCX_OPS_DISABLED)
+		return;
+
+	/*
+	 * Carefully check if the task was running on sched_ext, and then
+	 * carefully copy the time it's been runnable, and its state.
+	 */
+	if (copy_from_kernel_nofault(&class, &p->sched_class, sizeof(class)) ||
+	    class != &ext_sched_class) {
+		printk("%sSched_ext: %s (%s%s)", log_lvl, scx_ops.name,
+		       scx_ops_enable_state_str[state], all);
+		return;
+	}
+
+	if (!copy_from_kernel_nofault(&runnable_at, &p->scx.runnable_at,
+				      sizeof(runnable_at)))
+		scnprintf(runnable_at_buf, sizeof(runnable_at_buf), "%+ldms",
+			  jiffies_delta_msecs(runnable_at, jiffies));
+
+	/* print everything onto one line to conserve console space */
+	printk("%sSched_ext: %s (%s%s), task: runnable_at=%s",
+	       log_lvl, scx_ops.name, scx_ops_enable_state_str[state], all,
+	       runnable_at_buf);
+}
+
+static int scx_pm_handler(struct notifier_block *nb, unsigned long event, void *ptr)
+{
+	/*
+	 * SCX schedulers often have userspace components which are sometimes
+	 * involved in critial scheduling paths. PM operations involve freezing
+	 * userspace which can lead to scheduling misbehaviors including stalls.
+	 * Let's bypass while PM operations are in progress.
+	 */
+	switch (event) {
+	case PM_HIBERNATION_PREPARE:
+	case PM_SUSPEND_PREPARE:
+	case PM_RESTORE_PREPARE:
+		scx_ops_bypass(true);
+		break;
+	case PM_POST_HIBERNATION:
+	case PM_POST_SUSPEND:
+	case PM_POST_RESTORE:
+		scx_ops_bypass(false);
+		break;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block scx_pm_notifier = {
+	.notifier_call = scx_pm_handler,
+};
+
+void __init init_sched_ext_class(void)
+{
+	s32 cpu, v;
+
+	/*
+	 * The following is to prevent the compiler from optimizing out the enum
+	 * definitions so that BPF scheduler implementations can use them
+	 * through the generated vmlinux.h.
+	 */
+	WRITE_ONCE(v, SCX_ENQ_WAKEUP | SCX_DEQ_SLEEP | SCX_KICK_PREEMPT |
+		   SCX_TG_ONLINE);
+
+	BUG_ON(rhashtable_init(&dsq_hash, &dsq_hash_params));
+#ifdef CONFIG_SMP
+	BUG_ON(!alloc_cpumask_var(&idle_masks.cpu, GFP_KERNEL));
+	BUG_ON(!alloc_cpumask_var(&idle_masks.smt, GFP_KERNEL));
+#endif
+	scx_kick_cpus_pnt_seqs =
+		__alloc_percpu(sizeof(scx_kick_cpus_pnt_seqs[0]) * nr_cpu_ids,
+			       __alignof__(scx_kick_cpus_pnt_seqs[0]));
+	BUG_ON(!scx_kick_cpus_pnt_seqs);
+
+	for_each_possible_cpu(cpu) {
+		struct rq *rq = cpu_rq(cpu);
+
+		init_dsq(&rq->scx.local_dsq, SCX_DSQ_LOCAL);
+		INIT_LIST_HEAD(&rq->scx.runnable_list);
+		INIT_LIST_HEAD(&rq->scx.ddsp_deferred_locals);
+
+		BUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_kick, GFP_KERNEL));
+		BUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_kick_if_idle, GFP_KERNEL));
+		BUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_preempt, GFP_KERNEL));
+		BUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_wait, GFP_KERNEL));
+		init_irq_work(&rq->scx.deferred_irq_work, deferred_irq_workfn);
+		init_irq_work(&rq->scx.kick_cpus_irq_work, kick_cpus_irq_workfn);
+
+		if (cpu_online(cpu))
+			cpu_rq(cpu)->scx.flags |= SCX_RQ_ONLINE;
+	}
+
+	register_sysrq_key('S', &sysrq_sched_ext_reset_op);
+	register_sysrq_key('D', &sysrq_sched_ext_dump_op);
+	INIT_DELAYED_WORK(&scx_watchdog_work, scx_watchdog_workfn);
+}
+
+
+/********************************************************************************
+ * Helpers that can be called from the BPF scheduler.
+ */
+#include <linux/btf_ids.h>
+
+__bpf_kfunc_start_defs();
+
+/**
+ * scx_bpf_select_cpu_dfl - The default implementation of ops.select_cpu()
+ * @p: task_struct to select a CPU for
+ * @prev_cpu: CPU @p was on previously
+ * @wake_flags: %SCX_WAKE_* flags
+ * @is_idle: out parameter indicating whether the returned CPU is idle
+ *
+ * Can only be called from ops.select_cpu() if the built-in CPU selection is
+ * enabled - ops.update_idle() is missing or %SCX_OPS_KEEP_BUILTIN_IDLE is set.
+ * @p, @prev_cpu and @wake_flags match ops.select_cpu().
+ *
+ * Returns the picked CPU with *@is_idle indicating whether the picked CPU is
+ * currently idle and thus a good candidate for direct dispatching.
+ */
+__bpf_kfunc s32 scx_bpf_select_cpu_dfl(struct task_struct *p, s32 prev_cpu,
+				       u64 wake_flags, bool *is_idle)
+{
+	if (!static_branch_likely(&scx_builtin_idle_enabled)) {
+		scx_ops_error("built-in idle tracking is disabled");
+		goto prev_cpu;
+	}
+
+	if (!scx_kf_allowed(SCX_KF_SELECT_CPU))
+		goto prev_cpu;
+
+#ifdef CONFIG_SMP
+	return scx_select_cpu_dfl(p, prev_cpu, wake_flags, is_idle);
+#endif
+
+prev_cpu:
+	*is_idle = false;
+	return prev_cpu;
+}
+
+__bpf_kfunc_end_defs();
+
+BTF_KFUNCS_START(scx_kfunc_ids_select_cpu)
+BTF_ID_FLAGS(func, scx_bpf_select_cpu_dfl, KF_RCU)
+BTF_KFUNCS_END(scx_kfunc_ids_select_cpu)
+
+static const struct btf_kfunc_id_set scx_kfunc_set_select_cpu = {
+	.owner			= THIS_MODULE,
+	.set			= &scx_kfunc_ids_select_cpu,
+};
+
+static bool scx_dispatch_preamble(struct task_struct *p, u64 enq_flags)
+{
+	if (!scx_kf_allowed(SCX_KF_ENQUEUE | SCX_KF_DISPATCH))
+		return false;
+
+	lockdep_assert_irqs_disabled();
+
+	if (unlikely(!p)) {
+		scx_ops_error("called with NULL task");
+		return false;
+	}
+
+	if (unlikely(enq_flags & __SCX_ENQ_INTERNAL_MASK)) {
+		scx_ops_error("invalid enq_flags 0x%llx", enq_flags);
+		return false;
+	}
+
+	return true;
+}
+
+static void scx_dispatch_commit(struct task_struct *p, u64 dsq_id, u64 enq_flags)
+{
+	struct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);
+	struct task_struct *ddsp_task;
+
+	ddsp_task = __this_cpu_read(direct_dispatch_task);
+	if (ddsp_task) {
+		mark_direct_dispatch(ddsp_task, p, dsq_id, enq_flags);
+		return;
+	}
+
+	if (unlikely(dspc->cursor >= scx_dsp_max_batch)) {
+		scx_ops_error("dispatch buffer overflow");
+		return;
+	}
+
+	dspc->buf[dspc->cursor++] = (struct scx_dsp_buf_ent){
+		.task = p,
+		.qseq = atomic_long_read(&p->scx.ops_state) & SCX_OPSS_QSEQ_MASK,
+		.dsq_id = dsq_id,
+		.enq_flags = enq_flags,
+	};
+}
+
+__bpf_kfunc_start_defs();
+
+/**
+ * scx_bpf_dispatch - Dispatch a task into the FIFO queue of a DSQ
+ * @p: task_struct to dispatch
+ * @dsq_id: DSQ to dispatch to
+ * @slice: duration @p can run for in nsecs, 0 to keep the current value
+ * @enq_flags: SCX_ENQ_*
+ *
+ * Dispatch @p into the FIFO queue of the DSQ identified by @dsq_id. It is safe
+ * to call this function spuriously. Can be called from ops.enqueue(),
+ * ops.select_cpu(), and ops.dispatch().
+ *
+ * When called from ops.select_cpu() or ops.enqueue(), it's for direct dispatch
+ * and @p must match the task being enqueued. Also, %SCX_DSQ_LOCAL_ON can't be
+ * used to target the local DSQ of a CPU other than the enqueueing one. Use
+ * ops.select_cpu() to be on the target CPU in the first place.
+ *
+ * When called from ops.select_cpu(), @enq_flags and @dsp_id are stored, and @p
+ * will be directly dispatched to the corresponding dispatch queue after
+ * ops.select_cpu() returns. If @p is dispatched to SCX_DSQ_LOCAL, it will be
+ * dispatched to the local DSQ of the CPU returned by ops.select_cpu().
+ * @enq_flags are OR'd with the enqueue flags on the enqueue path before the
+ * task is dispatched.
+ *
+ * When called from ops.dispatch(), there are no restrictions on @p or @dsq_id
+ * and this function can be called upto ops.dispatch_max_batch times to dispatch
+ * multiple tasks. scx_bpf_dispatch_nr_slots() returns the number of the
+ * remaining slots. scx_bpf_consume() flushes the batch and resets the counter.
+ *
+ * This function doesn't have any locking restrictions and may be called under
+ * BPF locks (in the future when BPF introduces more flexible locking).
+ *
+ * @p is allowed to run for @slice. The scheduling path is triggered on slice
+ * exhaustion. If zero, the current residual slice is maintained. If
+ * %SCX_SLICE_INF, @p never expires and the BPF scheduler must kick the CPU with
+ * scx_bpf_kick_cpu() to trigger scheduling.
+ */
+__bpf_kfunc void scx_bpf_dispatch(struct task_struct *p, u64 dsq_id, u64 slice,
+				  u64 enq_flags)
+{
+	if (!scx_dispatch_preamble(p, enq_flags))
+		return;
+
+	if (slice)
+		p->scx.slice = slice;
+	else
+		p->scx.slice = p->scx.slice ?: 1;
+
+	scx_dispatch_commit(p, dsq_id, enq_flags);
+}
+
+/**
+ * scx_bpf_dispatch_vtime - Dispatch a task into the vtime priority queue of a DSQ
+ * @p: task_struct to dispatch
+ * @dsq_id: DSQ to dispatch to
+ * @slice: duration @p can run for in nsecs, 0 to keep the current value
+ * @vtime: @p's ordering inside the vtime-sorted queue of the target DSQ
+ * @enq_flags: SCX_ENQ_*
+ *
+ * Dispatch @p into the vtime priority queue of the DSQ identified by @dsq_id.
+ * Tasks queued into the priority queue are ordered by @vtime and always
+ * consumed after the tasks in the FIFO queue. All other aspects are identical
+ * to scx_bpf_dispatch().
+ *
+ * @vtime ordering is according to time_before64() which considers wrapping. A
+ * numerically larger vtime may indicate an earlier position in the ordering and
+ * vice-versa.
+ */
+__bpf_kfunc void scx_bpf_dispatch_vtime(struct task_struct *p, u64 dsq_id,
+					u64 slice, u64 vtime, u64 enq_flags)
+{
+	if (!scx_dispatch_preamble(p, enq_flags))
+		return;
+
+	if (slice)
+		p->scx.slice = slice;
+	else
+		p->scx.slice = p->scx.slice ?: 1;
+
+	p->scx.dsq_vtime = vtime;
+
+	scx_dispatch_commit(p, dsq_id, enq_flags | SCX_ENQ_DSQ_PRIQ);
+}
+
+__bpf_kfunc_end_defs();
+
+BTF_KFUNCS_START(scx_kfunc_ids_enqueue_dispatch)
+BTF_ID_FLAGS(func, scx_bpf_dispatch, KF_RCU)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_vtime, KF_RCU)
+BTF_KFUNCS_END(scx_kfunc_ids_enqueue_dispatch)
+
+static const struct btf_kfunc_id_set scx_kfunc_set_enqueue_dispatch = {
+	.owner			= THIS_MODULE,
+	.set			= &scx_kfunc_ids_enqueue_dispatch,
+};
+
+static bool scx_dispatch_from_dsq(struct bpf_iter_scx_dsq_kern *kit,
+				  struct task_struct *p, u64 dsq_id,
+				  u64 enq_flags)
+{
+	struct scx_dispatch_q *src_dsq = kit->dsq, *dst_dsq;
+	struct rq *this_rq, *src_rq, *dst_rq, *locked_rq;
+	bool dispatched = false;
+	bool in_balance;
+	unsigned long flags;
+
+	if (!scx_kf_allowed_if_unlocked() && !scx_kf_allowed(SCX_KF_DISPATCH))
+		return false;
+
+	/*
+	 * Can be called from either ops.dispatch() locking this_rq() or any
+	 * context where no rq lock is held. If latter, lock @p's task_rq which
+	 * we'll likely need anyway.
+	 */
+	src_rq = task_rq(p);
+
+	local_irq_save(flags);
+	this_rq = this_rq();
+	in_balance = this_rq->scx.flags & SCX_RQ_IN_BALANCE;
+
+	if (in_balance) {
+		if (this_rq != src_rq) {
+			raw_spin_rq_unlock(this_rq);
+			raw_spin_rq_lock(src_rq);
+		}
+	} else {
+		raw_spin_rq_lock(src_rq);
+	}
+
+	locked_rq = src_rq;
+	raw_spin_lock(&src_dsq->lock);
+
+	/*
+	 * Did someone else get to it? @p could have already left $src_dsq, got
+	 * re-enqueud, or be in the process of being consumed by someone else.
+	 */
+	if (unlikely(p->scx.dsq != src_dsq ||
+		     u32_before(kit->cursor.priv, p->scx.dsq_seq) ||
+		     p->scx.holding_cpu >= 0) ||
+	    WARN_ON_ONCE(src_rq != task_rq(p))) {
+		raw_spin_unlock(&src_dsq->lock);
+		goto out;
+	}
+
+	/* @p is still on $src_dsq and stable, determine the destination */
+	dst_dsq = find_dsq_for_dispatch(this_rq, dsq_id, p);
+
+	if (dst_dsq->id == SCX_DSQ_LOCAL) {
+		dst_rq = container_of(dst_dsq, struct rq, scx.local_dsq);
+		if (!task_can_run_on_remote_rq(p, dst_rq, true)) {
+			dst_dsq = find_global_dsq(p);
+			dst_rq = src_rq;
+		}
+	} else {
+		/* no need to migrate if destination is a non-local DSQ */
+		dst_rq = src_rq;
+	}
+
+	/*
+	 * Move @p into $dst_dsq. If $dst_dsq is the local DSQ of a different
+	 * CPU, @p will be migrated.
+	 */
+	if (dst_dsq->id == SCX_DSQ_LOCAL) {
+		/* @p is going from a non-local DSQ to a local DSQ */
+		if (src_rq == dst_rq) {
+			task_unlink_from_dsq(p, src_dsq);
+			move_local_task_to_local_dsq(p, enq_flags,
+						     src_dsq, dst_rq);
+			raw_spin_unlock(&src_dsq->lock);
+		} else {
+			raw_spin_unlock(&src_dsq->lock);
+			move_remote_task_to_local_dsq(p, enq_flags,
+						      src_rq, dst_rq);
+			locked_rq = dst_rq;
+		}
+	} else {
+		/*
+		 * @p is going from a non-local DSQ to a non-local DSQ. As
+		 * $src_dsq is already locked, do an abbreviated dequeue.
+		 */
+		task_unlink_from_dsq(p, src_dsq);
+		p->scx.dsq = NULL;
+		raw_spin_unlock(&src_dsq->lock);
+
+		if (kit->cursor.flags & __SCX_DSQ_ITER_HAS_VTIME)
+			p->scx.dsq_vtime = kit->vtime;
+		dispatch_enqueue(dst_dsq, p, enq_flags);
+	}
+
+	if (kit->cursor.flags & __SCX_DSQ_ITER_HAS_SLICE)
+		p->scx.slice = kit->slice;
+
+	dispatched = true;
+out:
+	if (in_balance) {
+		if (this_rq != locked_rq) {
+			raw_spin_rq_unlock(locked_rq);
+			raw_spin_rq_lock(this_rq);
+		}
+	} else {
+		raw_spin_rq_unlock_irqrestore(locked_rq, flags);
+	}
+
+	kit->cursor.flags &= ~(__SCX_DSQ_ITER_HAS_SLICE |
+			       __SCX_DSQ_ITER_HAS_VTIME);
+	return dispatched;
+}
+
+__bpf_kfunc_start_defs();
+
+/**
+ * scx_bpf_dispatch_nr_slots - Return the number of remaining dispatch slots
+ *
+ * Can only be called from ops.dispatch().
+ */
+__bpf_kfunc u32 scx_bpf_dispatch_nr_slots(void)
+{
+	if (!scx_kf_allowed(SCX_KF_DISPATCH))
+		return 0;
+
+	return scx_dsp_max_batch - __this_cpu_read(scx_dsp_ctx->cursor);
+}
+
+/**
+ * scx_bpf_dispatch_cancel - Cancel the latest dispatch
+ *
+ * Cancel the latest dispatch. Can be called multiple times to cancel further
+ * dispatches. Can only be called from ops.dispatch().
+ */
+__bpf_kfunc void scx_bpf_dispatch_cancel(void)
+{
+	struct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);
+
+	if (!scx_kf_allowed(SCX_KF_DISPATCH))
+		return;
+
+	if (dspc->cursor > 0)
+		dspc->cursor--;
+	else
+		scx_ops_error("dispatch buffer underflow");
+}
+
+/**
+ * scx_bpf_consume - Transfer a task from a DSQ to the current CPU's local DSQ
+ * @dsq_id: DSQ to consume
+ *
+ * Consume a task from the non-local DSQ identified by @dsq_id and transfer it
+ * to the current CPU's local DSQ for execution. Can only be called from
+ * ops.dispatch().
+ *
+ * This function flushes the in-flight dispatches from scx_bpf_dispatch() before
+ * trying to consume the specified DSQ. It may also grab rq locks and thus can't
+ * be called under any BPF locks.
+ *
+ * Returns %true if a task has been consumed, %false if there isn't any task to
+ * consume.
+ */
+__bpf_kfunc bool scx_bpf_consume(u64 dsq_id)
+{
+	struct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);
+	struct scx_dispatch_q *dsq;
+
+	if (!scx_kf_allowed(SCX_KF_DISPATCH))
+		return false;
+
+	flush_dispatch_buf(dspc->rq);
+
+	dsq = find_user_dsq(dsq_id);
+	if (unlikely(!dsq)) {
+		scx_ops_error("invalid DSQ ID 0x%016llx", dsq_id);
+		return false;
+	}
+
+	if (consume_dispatch_q(dspc->rq, dsq)) {
+		/*
+		 * A successfully consumed task can be dequeued before it starts
+		 * running while the CPU is trying to migrate other dispatched
+		 * tasks. Bump nr_tasks to tell balance_scx() to retry on empty
+		 * local DSQ.
+		 */
+		dspc->nr_tasks++;
+		return true;
+	} else {
+		return false;
+	}
+}
+
+/**
+ * scx_bpf_dispatch_from_dsq_set_slice - Override slice when dispatching from DSQ
+ * @it__iter: DSQ iterator in progress
+ * @slice: duration the dispatched task can run for in nsecs
+ *
+ * Override the slice of the next task that will be dispatched from @it__iter
+ * using scx_bpf_dispatch_from_dsq[_vtime](). If this function is not called,
+ * the previous slice duration is kept.
+ */
+__bpf_kfunc void scx_bpf_dispatch_from_dsq_set_slice(
+				struct bpf_iter_scx_dsq *it__iter, u64 slice)
+{
+	struct bpf_iter_scx_dsq_kern *kit = (void *)it__iter;
+
+	kit->slice = slice;
+	kit->cursor.flags |= __SCX_DSQ_ITER_HAS_SLICE;
+}
+
+/**
+ * scx_bpf_dispatch_from_dsq_set_vtime - Override vtime when dispatching from DSQ
+ * @it__iter: DSQ iterator in progress
+ * @vtime: task's ordering inside the vtime-sorted queue of the target DSQ
+ *
+ * Override the vtime of the next task that will be dispatched from @it__iter
+ * using scx_bpf_dispatch_from_dsq_vtime(). If this function is not called, the
+ * previous slice vtime is kept. If scx_bpf_dispatch_from_dsq() is used to
+ * dispatch the next task, the override is ignored and cleared.
+ */
+__bpf_kfunc void scx_bpf_dispatch_from_dsq_set_vtime(
+				struct bpf_iter_scx_dsq *it__iter, u64 vtime)
+{
+	struct bpf_iter_scx_dsq_kern *kit = (void *)it__iter;
+
+	kit->vtime = vtime;
+	kit->cursor.flags |= __SCX_DSQ_ITER_HAS_VTIME;
+}
+
+/**
+ * scx_bpf_dispatch_from_dsq - Move a task from DSQ iteration to a DSQ
+ * @it__iter: DSQ iterator in progress
+ * @p: task to transfer
+ * @dsq_id: DSQ to move @p to
+ * @enq_flags: SCX_ENQ_*
+ *
+ * Transfer @p which is on the DSQ currently iterated by @it__iter to the DSQ
+ * specified by @dsq_id. All DSQs - local DSQs, global DSQ and user DSQs - can
+ * be the destination.
+ *
+ * For the transfer to be successful, @p must still be on the DSQ and have been
+ * queued before the DSQ iteration started. This function doesn't care whether
+ * @p was obtained from the DSQ iteration. @p just has to be on the DSQ and have
+ * been queued before the iteration started.
+ *
+ * @p's slice is kept by default. Use scx_bpf_dispatch_from_dsq_set_slice() to
+ * update.
+ *
+ * Can be called from ops.dispatch() or any BPF context which doesn't hold a rq
+ * lock (e.g. BPF timers or SYSCALL programs).
+ *
+ * Returns %true if @p has been consumed, %false if @p had already been consumed
+ * or dequeued.
+ */
+__bpf_kfunc bool scx_bpf_dispatch_from_dsq(struct bpf_iter_scx_dsq *it__iter,
+					   struct task_struct *p, u64 dsq_id,
+					   u64 enq_flags)
+{
+	return scx_dispatch_from_dsq((struct bpf_iter_scx_dsq_kern *)it__iter,
+				     p, dsq_id, enq_flags);
+}
+
+/**
+ * scx_bpf_dispatch_vtime_from_dsq - Move a task from DSQ iteration to a PRIQ DSQ
+ * @it__iter: DSQ iterator in progress
+ * @p: task to transfer
+ * @dsq_id: DSQ to move @p to
+ * @enq_flags: SCX_ENQ_*
+ *
+ * Transfer @p which is on the DSQ currently iterated by @it__iter to the
+ * priority queue of the DSQ specified by @dsq_id. The destination must be a
+ * user DSQ as only user DSQs support priority queue.
+ *
+ * @p's slice and vtime are kept by default. Use
+ * scx_bpf_dispatch_from_dsq_set_slice() and
+ * scx_bpf_dispatch_from_dsq_set_vtime() to update.
+ *
+ * All other aspects are identical to scx_bpf_dispatch_from_dsq(). See
+ * scx_bpf_dispatch_vtime() for more information on @vtime.
+ */
+__bpf_kfunc bool scx_bpf_dispatch_vtime_from_dsq(struct bpf_iter_scx_dsq *it__iter,
+						 struct task_struct *p, u64 dsq_id,
+						 u64 enq_flags)
+{
+	return scx_dispatch_from_dsq((struct bpf_iter_scx_dsq_kern *)it__iter,
+				     p, dsq_id, enq_flags | SCX_ENQ_DSQ_PRIQ);
+}
+
+__bpf_kfunc_end_defs();
+
+BTF_KFUNCS_START(scx_kfunc_ids_dispatch)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_nr_slots)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_cancel)
+BTF_ID_FLAGS(func, scx_bpf_consume)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_from_dsq_set_slice)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_from_dsq_set_vtime)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_from_dsq, KF_RCU)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_vtime_from_dsq, KF_RCU)
+BTF_KFUNCS_END(scx_kfunc_ids_dispatch)
+
+static const struct btf_kfunc_id_set scx_kfunc_set_dispatch = {
+	.owner			= THIS_MODULE,
+	.set			= &scx_kfunc_ids_dispatch,
+};
+
+__bpf_kfunc_start_defs();
+
+/**
+ * scx_bpf_reenqueue_local - Re-enqueue tasks on a local DSQ
+ *
+ * Iterate over all of the tasks currently enqueued on the local DSQ of the
+ * caller's CPU, and re-enqueue them in the BPF scheduler. Returns the number of
+ * processed tasks. Can only be called from ops.cpu_release().
+ */
+__bpf_kfunc u32 scx_bpf_reenqueue_local(void)
+{
+	LIST_HEAD(tasks);
+	u32 nr_enqueued = 0;
+	struct rq *rq;
+	struct task_struct *p, *n;
+
+	if (!scx_kf_allowed(SCX_KF_CPU_RELEASE))
+		return 0;
+
+	rq = cpu_rq(smp_processor_id());
+	lockdep_assert_rq_held(rq);
+
+	/*
+	 * The BPF scheduler may choose to dispatch tasks back to
+	 * @rq->scx.local_dsq. Move all candidate tasks off to a private list
+	 * first to avoid processing the same tasks repeatedly.
+	 */
+	list_for_each_entry_safe(p, n, &rq->scx.local_dsq.list,
+				 scx.dsq_list.node) {
+		/*
+		 * If @p is being migrated, @p's current CPU may not agree with
+		 * its allowed CPUs and the migration_cpu_stop is about to
+		 * deactivate and re-activate @p anyway. Skip re-enqueueing.
+		 *
+		 * While racing sched property changes may also dequeue and
+		 * re-enqueue a migrating task while its current CPU and allowed
+		 * CPUs disagree, they use %ENQUEUE_RESTORE which is bypassed to
+		 * the current local DSQ for running tasks and thus are not
+		 * visible to the BPF scheduler.
+		 */
+		if (p->migration_pending)
+			continue;
+
+		dispatch_dequeue(rq, p);
+		list_add_tail(&p->scx.dsq_list.node, &tasks);
+	}
+
+	list_for_each_entry_safe(p, n, &tasks, scx.dsq_list.node) {
+		list_del_init(&p->scx.dsq_list.node);
+		do_enqueue_task(rq, p, SCX_ENQ_REENQ, -1);
+		nr_enqueued++;
+	}
+
+	return nr_enqueued;
+}
+
+__bpf_kfunc_end_defs();
+
+BTF_KFUNCS_START(scx_kfunc_ids_cpu_release)
+BTF_ID_FLAGS(func, scx_bpf_reenqueue_local)
+BTF_KFUNCS_END(scx_kfunc_ids_cpu_release)
+
+static const struct btf_kfunc_id_set scx_kfunc_set_cpu_release = {
+	.owner			= THIS_MODULE,
+	.set			= &scx_kfunc_ids_cpu_release,
+};
+
+__bpf_kfunc_start_defs();
+
+/**
+ * scx_bpf_create_dsq - Create a custom DSQ
+ * @dsq_id: DSQ to create
+ * @node: NUMA node to allocate from
+ *
+ * Create a custom DSQ identified by @dsq_id. Can be called from any sleepable
+ * scx callback, and any BPF_PROG_TYPE_SYSCALL prog.
+ */
+__bpf_kfunc s32 scx_bpf_create_dsq(u64 dsq_id, s32 node)
+{
+	if (unlikely(node >= (int)nr_node_ids ||
+		     (node < 0 && node != NUMA_NO_NODE)))
+		return -EINVAL;
+	return PTR_ERR_OR_ZERO(create_dsq(dsq_id, node));
+}
+
+__bpf_kfunc_end_defs();
+
+BTF_KFUNCS_START(scx_kfunc_ids_unlocked)
+BTF_ID_FLAGS(func, scx_bpf_create_dsq, KF_SLEEPABLE)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_from_dsq, KF_RCU)
+BTF_ID_FLAGS(func, scx_bpf_dispatch_vtime_from_dsq, KF_RCU)
+BTF_KFUNCS_END(scx_kfunc_ids_unlocked)
+
+static const struct btf_kfunc_id_set scx_kfunc_set_unlocked = {
+	.owner			= THIS_MODULE,
+	.set			= &scx_kfunc_ids_unlocked,
+};
+
+__bpf_kfunc_start_defs();
+
+/**
+ * scx_bpf_kick_cpu - Trigger reschedule on a CPU
+ * @cpu: cpu to kick
+ * @flags: %SCX_KICK_* flags
+ *
+ * Kick @cpu into rescheduling. This can be used to wake up an idle CPU or
+ * trigger rescheduling on a busy CPU. This can be called from any online
+ * scx_ops operation and the actual kicking is performed asynchronously through
+ * an irq work.
+ */
+__bpf_kfunc void scx_bpf_kick_cpu(s32 cpu, u64 flags)
+{
+	struct rq *this_rq;
+	unsigned long irq_flags;
+
+	if (!ops_cpu_valid(cpu, NULL))
+		return;
+
+	local_irq_save(irq_flags);
+
+	this_rq = this_rq();
+
+	/*
+	 * While bypassing for PM ops, IRQ handling may not be online which can
+	 * lead to irq_work_queue() malfunction such as infinite busy wait for
+	 * IRQ status update. Suppress kicking.
+	 */
+	if (scx_rq_bypassing(this_rq))
+		goto out;
+
+	/*
+	 * Actual kicking is bounced to kick_cpus_irq_workfn() to avoid nesting
+	 * rq locks. We can probably be smarter and avoid bouncing if called
+	 * from ops which don't hold a rq lock.
+	 */
+	if (flags & SCX_KICK_IDLE) {
+		struct rq *target_rq = cpu_rq(cpu);
+
+		if (unlikely(flags & (SCX_KICK_PREEMPT | SCX_KICK_WAIT)))
+			scx_ops_error("PREEMPT/WAIT cannot be used with SCX_KICK_IDLE");
+
+		if (raw_spin_rq_trylock(target_rq)) {
+			if (can_skip_idle_kick(target_rq)) {
+				raw_spin_rq_unlock(target_rq);
+				goto out;
+			}
+			raw_spin_rq_unlock(target_rq);
+		}
+		cpumask_set_cpu(cpu, this_rq->scx.cpus_to_kick_if_idle);
+	} else {
+		cpumask_set_cpu(cpu, this_rq->scx.cpus_to_kick);
+
+		if (flags & SCX_KICK_PREEMPT)
+			cpumask_set_cpu(cpu, this_rq->scx.cpus_to_preempt);
+		if (flags & SCX_KICK_WAIT)
+			cpumask_set_cpu(cpu, this_rq->scx.cpus_to_wait);
+	}
+
+	irq_work_queue(&this_rq->scx.kick_cpus_irq_work);
+out:
+	local_irq_restore(irq_flags);
+}
+
+/**
+ * scx_bpf_dsq_nr_queued - Return the number of queued tasks
+ * @dsq_id: id of the DSQ
+ *
+ * Return the number of tasks in the DSQ matching @dsq_id. If not found,
+ * -%ENOENT is returned.
+ */
+__bpf_kfunc s32 scx_bpf_dsq_nr_queued(u64 dsq_id)
+{
+	struct scx_dispatch_q *dsq;
+	s32 ret;
+
+	preempt_disable();
+
+	if (dsq_id == SCX_DSQ_LOCAL) {
+		ret = READ_ONCE(this_rq()->scx.local_dsq.nr);
+		goto out;
+	} else if ((dsq_id & SCX_DSQ_LOCAL_ON) == SCX_DSQ_LOCAL_ON) {
+		s32 cpu = dsq_id & SCX_DSQ_LOCAL_CPU_MASK;
+
+		if (ops_cpu_valid(cpu, NULL)) {
+			ret = READ_ONCE(cpu_rq(cpu)->scx.local_dsq.nr);
+			goto out;
+		}
+	} else {
+		dsq = find_user_dsq(dsq_id);
+		if (dsq) {
+			ret = READ_ONCE(dsq->nr);
+			goto out;
+		}
+	}
+	ret = -ENOENT;
+out:
+	preempt_enable();
+	return ret;
+}
+
+/**
+ * scx_bpf_destroy_dsq - Destroy a custom DSQ
+ * @dsq_id: DSQ to destroy
+ *
+ * Destroy the custom DSQ identified by @dsq_id. Only DSQs created with
+ * scx_bpf_create_dsq() can be destroyed. The caller must ensure that the DSQ is
+ * empty and no further tasks are dispatched to it. Ignored if called on a DSQ
+ * which doesn't exist. Can be called from any online scx_ops operations.
+ */
+__bpf_kfunc void scx_bpf_destroy_dsq(u64 dsq_id)
+{
+	destroy_dsq(dsq_id);
+}
+
+/**
+ * bpf_iter_scx_dsq_new - Create a DSQ iterator
+ * @it: iterator to initialize
+ * @dsq_id: DSQ to iterate
+ * @flags: %SCX_DSQ_ITER_*
+ *
+ * Initialize BPF iterator @it which can be used with bpf_for_each() to walk
+ * tasks in the DSQ specified by @dsq_id. Iteration using @it only includes
+ * tasks which are already queued when this function is invoked.
+ */
+__bpf_kfunc int bpf_iter_scx_dsq_new(struct bpf_iter_scx_dsq *it, u64 dsq_id,
+				     u64 flags)
+{
+	struct bpf_iter_scx_dsq_kern *kit = (void *)it;
+
+	BUILD_BUG_ON(sizeof(struct bpf_iter_scx_dsq_kern) >
+		     sizeof(struct bpf_iter_scx_dsq));
+	BUILD_BUG_ON(__alignof__(struct bpf_iter_scx_dsq_kern) !=
+		     __alignof__(struct bpf_iter_scx_dsq));
+
+	if (flags & ~__SCX_DSQ_ITER_USER_FLAGS)
+		return -EINVAL;
+
+	kit->dsq = find_user_dsq(dsq_id);
+	if (!kit->dsq)
+		return -ENOENT;
+
+	INIT_LIST_HEAD(&kit->cursor.node);
+	kit->cursor.flags |= SCX_DSQ_LNODE_ITER_CURSOR | flags;
+	kit->cursor.priv = READ_ONCE(kit->dsq->seq);
+
+	return 0;
+}
+
+/**
+ * bpf_iter_scx_dsq_next - Progress a DSQ iterator
+ * @it: iterator to progress
+ *
+ * Return the next task. See bpf_iter_scx_dsq_new().
+ */
+__bpf_kfunc struct task_struct *bpf_iter_scx_dsq_next(struct bpf_iter_scx_dsq *it)
+{
+	struct bpf_iter_scx_dsq_kern *kit = (void *)it;
+	bool rev = kit->cursor.flags & SCX_DSQ_ITER_REV;
+	struct task_struct *p;
+	unsigned long flags;
+
+	if (!kit->dsq)
+		return NULL;
+
+	raw_spin_lock_irqsave(&kit->dsq->lock, flags);
+
+	if (list_empty(&kit->cursor.node))
+		p = NULL;
+	else
+		p = container_of(&kit->cursor, struct task_struct, scx.dsq_list);
+
+	/*
+	 * Only tasks which were queued before the iteration started are
+	 * visible. This bounds BPF iterations and guarantees that vtime never
+	 * jumps in the other direction while iterating.
+	 */
+	do {
+		p = nldsq_next_task(kit->dsq, p, rev);
+	} while (p && unlikely(u32_before(kit->cursor.priv, p->scx.dsq_seq)));
+
+	if (p) {
+		if (rev)
+			list_move_tail(&kit->cursor.node, &p->scx.dsq_list.node);
+		else
+			list_move(&kit->cursor.node, &p->scx.dsq_list.node);
+	} else {
+		list_del_init(&kit->cursor.node);
+	}
+
+	raw_spin_unlock_irqrestore(&kit->dsq->lock, flags);
+
+	return p;
+}
+
+/**
+ * bpf_iter_scx_dsq_destroy - Destroy a DSQ iterator
+ * @it: iterator to destroy
+ *
+ * Undo scx_iter_scx_dsq_new().
+ */
+__bpf_kfunc void bpf_iter_scx_dsq_destroy(struct bpf_iter_scx_dsq *it)
+{
+	struct bpf_iter_scx_dsq_kern *kit = (void *)it;
+
+	if (!kit->dsq)
+		return;
+
+	if (!list_empty(&kit->cursor.node)) {
+		unsigned long flags;
+
+		raw_spin_lock_irqsave(&kit->dsq->lock, flags);
+		list_del_init(&kit->cursor.node);
+		raw_spin_unlock_irqrestore(&kit->dsq->lock, flags);
+	}
+	kit->dsq = NULL;
+}
+
+__bpf_kfunc_end_defs();
+
+static s32 __bstr_format(u64 *data_buf, char *line_buf, size_t line_size,
+			 char *fmt, unsigned long long *data, u32 data__sz)
+{
+	struct bpf_bprintf_data bprintf_data = { .get_bin_args = true };
+	s32 ret;
+
+	if (data__sz % 8 || data__sz > MAX_BPRINTF_VARARGS * 8 ||
+	    (data__sz && !data)) {
+		scx_ops_error("invalid data=%p and data__sz=%u",
+			      (void *)data, data__sz);
+		return -EINVAL;
+	}
+
+	ret = copy_from_kernel_nofault(data_buf, data, data__sz);
+	if (ret < 0) {
+		scx_ops_error("failed to read data fields (%d)", ret);
+		return ret;
+	}
+
+	ret = bpf_bprintf_prepare(fmt, UINT_MAX, data_buf, data__sz / 8,
+				  &bprintf_data);
+	if (ret < 0) {
+		scx_ops_error("format preparation failed (%d)", ret);
+		return ret;
+	}
+
+	ret = bstr_printf(line_buf, line_size, fmt,
+			  bprintf_data.bin_args);
+	bpf_bprintf_cleanup(&bprintf_data);
+	if (ret < 0) {
+		scx_ops_error("(\"%s\", %p, %u) failed to format",
+			      fmt, data, data__sz);
+		return ret;
+	}
+
+	return ret;
+}
+
+static s32 bstr_format(struct scx_bstr_buf *buf,
+		       char *fmt, unsigned long long *data, u32 data__sz)
+{
+	return __bstr_format(buf->data, buf->line, sizeof(buf->line),
+			     fmt, data, data__sz);
+}
+
+__bpf_kfunc_start_defs();
+
+/**
+ * scx_bpf_exit_bstr - Gracefully exit the BPF scheduler.
+ * @exit_code: Exit value to pass to user space via struct scx_exit_info.
+ * @fmt: error message format string
+ * @data: format string parameters packaged using ___bpf_fill() macro
+ * @data__sz: @data len, must end in '__sz' for the verifier
+ *
+ * Indicate that the BPF scheduler wants to exit gracefully, and initiate ops
+ * disabling.
+ */
+__bpf_kfunc void scx_bpf_exit_bstr(s64 exit_code, char *fmt,
+				   unsigned long long *data, u32 data__sz)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&scx_exit_bstr_buf_lock, flags);
+	if (bstr_format(&scx_exit_bstr_buf, fmt, data, data__sz) >= 0)
+		scx_ops_exit_kind(SCX_EXIT_UNREG_BPF, exit_code, "%s",
+				  scx_exit_bstr_buf.line);
+	raw_spin_unlock_irqrestore(&scx_exit_bstr_buf_lock, flags);
+}
+
+/**
+ * scx_bpf_error_bstr - Indicate fatal error
+ * @fmt: error message format string
+ * @data: format string parameters packaged using ___bpf_fill() macro
+ * @data__sz: @data len, must end in '__sz' for the verifier
+ *
+ * Indicate that the BPF scheduler encountered a fatal error and initiate ops
+ * disabling.
+ */
+__bpf_kfunc void scx_bpf_error_bstr(char *fmt, unsigned long long *data,
+				    u32 data__sz)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&scx_exit_bstr_buf_lock, flags);
+	if (bstr_format(&scx_exit_bstr_buf, fmt, data, data__sz) >= 0)
+		scx_ops_exit_kind(SCX_EXIT_ERROR_BPF, 0, "%s",
+				  scx_exit_bstr_buf.line);
+	raw_spin_unlock_irqrestore(&scx_exit_bstr_buf_lock, flags);
+}
+
+/**
+ * scx_bpf_dump - Generate extra debug dump specific to the BPF scheduler
+ * @fmt: format string
+ * @data: format string parameters packaged using ___bpf_fill() macro
+ * @data__sz: @data len, must end in '__sz' for the verifier
+ *
+ * To be called through scx_bpf_dump() helper from ops.dump(), dump_cpu() and
+ * dump_task() to generate extra debug dump specific to the BPF scheduler.
+ *
+ * The extra dump may be multiple lines. A single line may be split over
+ * multiple calls. The last line is automatically terminated.
+ */
+__bpf_kfunc void scx_bpf_dump_bstr(char *fmt, unsigned long long *data,
+				   u32 data__sz)
+{
+	struct scx_dump_data *dd = &scx_dump_data;
+	struct scx_bstr_buf *buf = &dd->buf;
+	s32 ret;
+
+	if (raw_smp_processor_id() != dd->cpu) {
+		scx_ops_error("scx_bpf_dump() must only be called from ops.dump() and friends");
+		return;
+	}
+
+	/* append the formatted string to the line buf */
+	ret = __bstr_format(buf->data, buf->line + dd->cursor,
+			    sizeof(buf->line) - dd->cursor, fmt, data, data__sz);
+	if (ret < 0) {
+		dump_line(dd->s, "%s[!] (\"%s\", %p, %u) failed to format (%d)",
+			  dd->prefix, fmt, data, data__sz, ret);
+		return;
+	}
+
+	dd->cursor += ret;
+	dd->cursor = min_t(s32, dd->cursor, sizeof(buf->line));
+
+	if (!dd->cursor)
+		return;
+
+	/*
+	 * If the line buf overflowed or ends in a newline, flush it into the
+	 * dump. This is to allow the caller to generate a single line over
+	 * multiple calls. As ops_dump_flush() can also handle multiple lines in
+	 * the line buf, the only case which can lead to an unexpected
+	 * truncation is when the caller keeps generating newlines in the middle
+	 * instead of the end consecutively. Don't do that.
+	 */
+	if (dd->cursor >= sizeof(buf->line) || buf->line[dd->cursor - 1] == '\n')
+		ops_dump_flush();
+}
+
+/**
+ * scx_bpf_cpuperf_cap - Query the maximum relative capacity of a CPU
+ * @cpu: CPU of interest
+ *
+ * Return the maximum relative capacity of @cpu in relation to the most
+ * performant CPU in the system. The return value is in the range [1,
+ * %SCX_CPUPERF_ONE]. See scx_bpf_cpuperf_cur().
+ */
+__bpf_kfunc u32 scx_bpf_cpuperf_cap(s32 cpu)
+{
+	if (ops_cpu_valid(cpu, NULL))
+		return arch_scale_cpu_capacity(cpu);
+	else
+		return SCX_CPUPERF_ONE;
+}
+
+/**
+ * scx_bpf_cpuperf_cur - Query the current relative performance of a CPU
+ * @cpu: CPU of interest
+ *
+ * Return the current relative performance of @cpu in relation to its maximum.
+ * The return value is in the range [1, %SCX_CPUPERF_ONE].
+ *
+ * The current performance level of a CPU in relation to the maximum performance
+ * available in the system can be calculated as follows:
+ *
+ *   scx_bpf_cpuperf_cap() * scx_bpf_cpuperf_cur() / %SCX_CPUPERF_ONE
+ *
+ * The result is in the range [1, %SCX_CPUPERF_ONE].
+ */
+__bpf_kfunc u32 scx_bpf_cpuperf_cur(s32 cpu)
+{
+	if (ops_cpu_valid(cpu, NULL))
+		return arch_scale_freq_capacity(cpu);
+	else
+		return SCX_CPUPERF_ONE;
+}
+
+/**
+ * scx_bpf_cpuperf_set - Set the relative performance target of a CPU
+ * @cpu: CPU of interest
+ * @perf: target performance level [0, %SCX_CPUPERF_ONE]
+ * @flags: %SCX_CPUPERF_* flags
+ *
+ * Set the target performance level of @cpu to @perf. @perf is in linear
+ * relative scale between 0 and %SCX_CPUPERF_ONE. This determines how the
+ * schedutil cpufreq governor chooses the target frequency.
+ *
+ * The actual performance level chosen, CPU grouping, and the overhead and
+ * latency of the operations are dependent on the hardware and cpufreq driver in
+ * use. Consult hardware and cpufreq documentation for more information. The
+ * current performance level can be monitored using scx_bpf_cpuperf_cur().
+ */
+__bpf_kfunc void scx_bpf_cpuperf_set(s32 cpu, u32 perf)
+{
+	if (unlikely(perf > SCX_CPUPERF_ONE)) {
+		scx_ops_error("Invalid cpuperf target %u for CPU %d", perf, cpu);
+		return;
+	}
+
+	if (ops_cpu_valid(cpu, NULL)) {
+		struct rq *rq = cpu_rq(cpu);
+
+		rq->scx.cpuperf_target = perf;
+
+		rcu_read_lock_sched_notrace();
+		cpufreq_update_util(cpu_rq(cpu), 0);
+		rcu_read_unlock_sched_notrace();
+	}
+}
+
+/**
+ * scx_bpf_nr_cpu_ids - Return the number of possible CPU IDs
+ *
+ * All valid CPU IDs in the system are smaller than the returned value.
+ */
+__bpf_kfunc u32 scx_bpf_nr_cpu_ids(void)
+{
+	return nr_cpu_ids;
+}
+
+/**
+ * scx_bpf_get_possible_cpumask - Get a referenced kptr to cpu_possible_mask
+ */
+__bpf_kfunc const struct cpumask *scx_bpf_get_possible_cpumask(void)
+{
+	return cpu_possible_mask;
+}
+
+/**
+ * scx_bpf_get_online_cpumask - Get a referenced kptr to cpu_online_mask
+ */
+__bpf_kfunc const struct cpumask *scx_bpf_get_online_cpumask(void)
+{
+	return cpu_online_mask;
+}
+
+/**
+ * scx_bpf_put_cpumask - Release a possible/online cpumask
+ * @cpumask: cpumask to release
+ */
+__bpf_kfunc void scx_bpf_put_cpumask(const struct cpumask *cpumask)
+{
+	/*
+	 * Empty function body because we aren't actually acquiring or releasing
+	 * a reference to a global cpumask, which is read-only in the caller and
+	 * is never released. The acquire / release semantics here are just used
+	 * to make the cpumask is a trusted pointer in the caller.
+	 */
+}
+
+/**
+ * scx_bpf_get_idle_cpumask - Get a referenced kptr to the idle-tracking
+ * per-CPU cpumask.
+ *
+ * Returns NULL if idle tracking is not enabled, or running on a UP kernel.
+ */
+__bpf_kfunc const struct cpumask *scx_bpf_get_idle_cpumask(void)
+{
+	if (!static_branch_likely(&scx_builtin_idle_enabled)) {
+		scx_ops_error("built-in idle tracking is disabled");
+		return cpu_none_mask;
+	}
+
+#ifdef CONFIG_SMP
+	return idle_masks.cpu;
+#else
+	return cpu_none_mask;
+#endif
+}
+
+/**
+ * scx_bpf_get_idle_smtmask - Get a referenced kptr to the idle-tracking,
+ * per-physical-core cpumask. Can be used to determine if an entire physical
+ * core is free.
+ *
+ * Returns NULL if idle tracking is not enabled, or running on a UP kernel.
+ */
+__bpf_kfunc const struct cpumask *scx_bpf_get_idle_smtmask(void)
+{
+	if (!static_branch_likely(&scx_builtin_idle_enabled)) {
+		scx_ops_error("built-in idle tracking is disabled");
+		return cpu_none_mask;
+	}
+
+#ifdef CONFIG_SMP
+	if (sched_smt_active())
+		return idle_masks.smt;
+	else
+		return idle_masks.cpu;
+#else
+	return cpu_none_mask;
+#endif
+}
+
+/**
+ * scx_bpf_put_idle_cpumask - Release a previously acquired referenced kptr to
+ * either the percpu, or SMT idle-tracking cpumask.
+ */
+__bpf_kfunc void scx_bpf_put_idle_cpumask(const struct cpumask *idle_mask)
+{
+	/*
+	 * Empty function body because we aren't actually acquiring or releasing
+	 * a reference to a global idle cpumask, which is read-only in the
+	 * caller and is never released. The acquire / release semantics here
+	 * are just used to make the cpumask a trusted pointer in the caller.
+	 */
+}
+
+/**
+ * scx_bpf_test_and_clear_cpu_idle - Test and clear @cpu's idle state
+ * @cpu: cpu to test and clear idle for
+ *
+ * Returns %true if @cpu was idle and its idle state was successfully cleared.
+ * %false otherwise.
+ *
+ * Unavailable if ops.update_idle() is implemented and
+ * %SCX_OPS_KEEP_BUILTIN_IDLE is not set.
+ */
+__bpf_kfunc bool scx_bpf_test_and_clear_cpu_idle(s32 cpu)
+{
+	if (!static_branch_likely(&scx_builtin_idle_enabled)) {
+		scx_ops_error("built-in idle tracking is disabled");
+		return false;
+	}
+
+	if (ops_cpu_valid(cpu, NULL))
+		return test_and_clear_cpu_idle(cpu);
+	else
+		return false;
+}
+
+/**
+ * scx_bpf_pick_idle_cpu - Pick and claim an idle cpu
+ * @cpus_allowed: Allowed cpumask
+ * @flags: %SCX_PICK_IDLE_CPU_* flags
+ *
+ * Pick and claim an idle cpu in @cpus_allowed. Returns the picked idle cpu
+ * number on success. -%EBUSY if no matching cpu was found.
+ *
+ * Idle CPU tracking may race against CPU scheduling state transitions. For
+ * example, this function may return -%EBUSY as CPUs are transitioning into the
+ * idle state. If the caller then assumes that there will be dispatch events on
+ * the CPUs as they were all busy, the scheduler may end up stalling with CPUs
+ * idling while there are pending tasks. Use scx_bpf_pick_any_cpu() and
+ * scx_bpf_kick_cpu() to guarantee that there will be at least one dispatch
+ * event in the near future.
+ *
+ * Unavailable if ops.update_idle() is implemented and
+ * %SCX_OPS_KEEP_BUILTIN_IDLE is not set.
+ */
+__bpf_kfunc s32 scx_bpf_pick_idle_cpu(const struct cpumask *cpus_allowed,
+				      u64 flags)
+{
+	if (!static_branch_likely(&scx_builtin_idle_enabled)) {
+		scx_ops_error("built-in idle tracking is disabled");
+		return -EBUSY;
+	}
+
+	return scx_pick_idle_cpu(cpus_allowed, flags);
+}
+
+/**
+ * scx_bpf_pick_any_cpu - Pick and claim an idle cpu if available or pick any CPU
+ * @cpus_allowed: Allowed cpumask
+ * @flags: %SCX_PICK_IDLE_CPU_* flags
+ *
+ * Pick and claim an idle cpu in @cpus_allowed. If none is available, pick any
+ * CPU in @cpus_allowed. Guaranteed to succeed and returns the picked idle cpu
+ * number if @cpus_allowed is not empty. -%EBUSY is returned if @cpus_allowed is
+ * empty.
+ *
+ * If ops.update_idle() is implemented and %SCX_OPS_KEEP_BUILTIN_IDLE is not
+ * set, this function can't tell which CPUs are idle and will always pick any
+ * CPU.
+ */
+__bpf_kfunc s32 scx_bpf_pick_any_cpu(const struct cpumask *cpus_allowed,
+				     u64 flags)
+{
+	s32 cpu;
+
+	if (static_branch_likely(&scx_builtin_idle_enabled)) {
+		cpu = scx_pick_idle_cpu(cpus_allowed, flags);
+		if (cpu >= 0)
+			return cpu;
+	}
+
+	cpu = cpumask_any_distribute(cpus_allowed);
+	if (cpu < nr_cpu_ids)
+		return cpu;
+	else
+		return -EBUSY;
+}
+
+/**
+ * scx_bpf_task_running - Is task currently running?
+ * @p: task of interest
+ */
+__bpf_kfunc bool scx_bpf_task_running(const struct task_struct *p)
+{
+	return task_rq(p)->curr == p;
+}
+
+/**
+ * scx_bpf_task_cpu - CPU a task is currently associated with
+ * @p: task of interest
+ */
+__bpf_kfunc s32 scx_bpf_task_cpu(const struct task_struct *p)
+{
+	return task_cpu(p);
+}
+
+/**
+ * scx_bpf_cpu_rq - Fetch the rq of a CPU
+ * @cpu: CPU of the rq
+ */
+__bpf_kfunc struct rq *scx_bpf_cpu_rq(s32 cpu)
+{
+	if (!ops_cpu_valid(cpu, NULL))
+		return NULL;
+
+	return cpu_rq(cpu);
+}
+
+/**
+ * scx_bpf_task_cgroup - Return the sched cgroup of a task
+ * @p: task of interest
+ *
+ * @p->sched_task_group->css.cgroup represents the cgroup @p is associated with
+ * from the scheduler's POV. SCX operations should use this function to
+ * determine @p's current cgroup as, unlike following @p->cgroups,
+ * @p->sched_task_group is protected by @p's rq lock and thus atomic w.r.t. all
+ * rq-locked operations. Can be called on the parameter tasks of rq-locked
+ * operations. The restriction guarantees that @p's rq is locked by the caller.
+ */
+#ifdef CONFIG_CGROUP_SCHED
+__bpf_kfunc struct cgroup *scx_bpf_task_cgroup(struct task_struct *p)
+{
+	struct task_group *tg = p->sched_task_group;
+	struct cgroup *cgrp = &cgrp_dfl_root.cgrp;
+
+	if (!scx_kf_allowed_on_arg_tasks(__SCX_KF_RQ_LOCKED, p))
+		goto out;
+
+	/*
+	 * A task_group may either be a cgroup or an autogroup. In the latter
+	 * case, @tg->css.cgroup is %NULL. A task_group can't become the other
+	 * kind once created.
+	 */
+	if (tg && tg->css.cgroup)
+		cgrp = tg->css.cgroup;
+	else
+		cgrp = &cgrp_dfl_root.cgrp;
+out:
+	cgroup_get(cgrp);
+	return cgrp;
+}
+#endif
+
+__bpf_kfunc_end_defs();
+
+BTF_KFUNCS_START(scx_kfunc_ids_any)
+BTF_ID_FLAGS(func, scx_bpf_kick_cpu)
+BTF_ID_FLAGS(func, scx_bpf_dsq_nr_queued)
+BTF_ID_FLAGS(func, scx_bpf_destroy_dsq)
+BTF_ID_FLAGS(func, bpf_iter_scx_dsq_new, KF_ITER_NEW | KF_RCU_PROTECTED)
+BTF_ID_FLAGS(func, bpf_iter_scx_dsq_next, KF_ITER_NEXT | KF_RET_NULL)
+BTF_ID_FLAGS(func, bpf_iter_scx_dsq_destroy, KF_ITER_DESTROY)
+BTF_ID_FLAGS(func, scx_bpf_exit_bstr, KF_TRUSTED_ARGS)
+BTF_ID_FLAGS(func, scx_bpf_error_bstr, KF_TRUSTED_ARGS)
+BTF_ID_FLAGS(func, scx_bpf_dump_bstr, KF_TRUSTED_ARGS)
+BTF_ID_FLAGS(func, scx_bpf_cpuperf_cap)
+BTF_ID_FLAGS(func, scx_bpf_cpuperf_cur)
+BTF_ID_FLAGS(func, scx_bpf_cpuperf_set)
+BTF_ID_FLAGS(func, scx_bpf_nr_cpu_ids)
+BTF_ID_FLAGS(func, scx_bpf_get_possible_cpumask, KF_ACQUIRE)
+BTF_ID_FLAGS(func, scx_bpf_get_online_cpumask, KF_ACQUIRE)
+BTF_ID_FLAGS(func, scx_bpf_put_cpumask, KF_RELEASE)
+BTF_ID_FLAGS(func, scx_bpf_get_idle_cpumask, KF_ACQUIRE)
+BTF_ID_FLAGS(func, scx_bpf_get_idle_smtmask, KF_ACQUIRE)
+BTF_ID_FLAGS(func, scx_bpf_put_idle_cpumask, KF_RELEASE)
+BTF_ID_FLAGS(func, scx_bpf_test_and_clear_cpu_idle)
+BTF_ID_FLAGS(func, scx_bpf_pick_idle_cpu, KF_RCU)
+BTF_ID_FLAGS(func, scx_bpf_pick_any_cpu, KF_RCU)
+BTF_ID_FLAGS(func, scx_bpf_task_running, KF_RCU)
+BTF_ID_FLAGS(func, scx_bpf_task_cpu, KF_RCU)
+BTF_ID_FLAGS(func, scx_bpf_cpu_rq)
+#ifdef CONFIG_CGROUP_SCHED
+BTF_ID_FLAGS(func, scx_bpf_task_cgroup, KF_RCU | KF_ACQUIRE)
+#endif
+BTF_KFUNCS_END(scx_kfunc_ids_any)
+
+static const struct btf_kfunc_id_set scx_kfunc_set_any = {
+	.owner			= THIS_MODULE,
+	.set			= &scx_kfunc_ids_any,
+};
+
+static int __init scx_init(void)
+{
+	int ret;
+
+	/*
+	 * kfunc registration can't be done from init_sched_ext_class() as
+	 * register_btf_kfunc_id_set() needs most of the system to be up.
+	 *
+	 * Some kfuncs are context-sensitive and can only be called from
+	 * specific SCX ops. They are grouped into BTF sets accordingly.
+	 * Unfortunately, BPF currently doesn't have a way of enforcing such
+	 * restrictions. Eventually, the verifier should be able to enforce
+	 * them. For now, register them the same and make each kfunc explicitly
+	 * check using scx_kf_allowed().
+	 */
+	if ((ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
+					     &scx_kfunc_set_select_cpu)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
+					     &scx_kfunc_set_enqueue_dispatch)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
+					     &scx_kfunc_set_dispatch)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
+					     &scx_kfunc_set_cpu_release)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
+					     &scx_kfunc_set_unlocked)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_SYSCALL,
+					     &scx_kfunc_set_unlocked)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS,
+					     &scx_kfunc_set_any)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING,
+					     &scx_kfunc_set_any)) ||
+	    (ret = register_btf_kfunc_id_set(BPF_PROG_TYPE_SYSCALL,
+					     &scx_kfunc_set_any))) {
+		pr_err("sched_ext: Failed to register kfunc sets (%d)\n", ret);
+		return ret;
+	}
+
+	ret = register_bpf_struct_ops(&bpf_sched_ext_ops, sched_ext_ops);
+	if (ret) {
+		pr_err("sched_ext: Failed to register struct_ops (%d)\n", ret);
+		return ret;
+	}
+
+	ret = register_pm_notifier(&scx_pm_notifier);
+	if (ret) {
+		pr_err("sched_ext: Failed to register PM notifier (%d)\n", ret);
+		return ret;
+	}
+
+	scx_kset = kset_create_and_add("sched_ext", &scx_uevent_ops, kernel_kobj);
+	if (!scx_kset) {
+		pr_err("sched_ext: Failed to create /sys/kernel/sched_ext\n");
+		return -ENOMEM;
+	}
+
+	ret = sysfs_create_group(&scx_kset->kobj, &scx_global_attr_group);
+	if (ret < 0) {
+		pr_err("sched_ext: Failed to add global attributes\n");
+		return ret;
+	}
+
+	return 0;
+}
+__initcall(scx_init);
diff --git a/kernel/sched/ext.h b/kernel/sched/ext.h
new file mode 100644
index 000000000000..246019519231
--- /dev/null
+++ b/kernel/sched/ext.h
@@ -0,0 +1,91 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * BPF extensible scheduler class: Documentation/scheduler/sched-ext.rst
+ *
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#ifdef CONFIG_SCHED_CLASS_EXT
+
+void scx_tick(struct rq *rq);
+void init_scx_entity(struct sched_ext_entity *scx);
+void scx_pre_fork(struct task_struct *p);
+int scx_fork(struct task_struct *p);
+void scx_post_fork(struct task_struct *p);
+void scx_cancel_fork(struct task_struct *p);
+bool scx_can_stop_tick(struct rq *rq);
+void scx_rq_activate(struct rq *rq);
+void scx_rq_deactivate(struct rq *rq);
+int scx_check_setscheduler(struct task_struct *p, int policy);
+bool task_should_scx(struct task_struct *p);
+void init_sched_ext_class(void);
+
+static inline u32 scx_cpuperf_target(s32 cpu)
+{
+	if (scx_enabled())
+		return cpu_rq(cpu)->scx.cpuperf_target;
+	else
+		return 0;
+}
+
+static inline bool task_on_scx(const struct task_struct *p)
+{
+	return scx_enabled() && p->sched_class == &ext_sched_class;
+}
+
+#ifdef CONFIG_SCHED_CORE
+bool scx_prio_less(const struct task_struct *a, const struct task_struct *b,
+		   bool in_fi);
+#endif
+
+#else	/* CONFIG_SCHED_CLASS_EXT */
+
+static inline void scx_tick(struct rq *rq) {}
+static inline void scx_pre_fork(struct task_struct *p) {}
+static inline int scx_fork(struct task_struct *p) { return 0; }
+static inline void scx_post_fork(struct task_struct *p) {}
+static inline void scx_cancel_fork(struct task_struct *p) {}
+static inline u32 scx_cpuperf_target(s32 cpu) { return 0; }
+static inline bool scx_can_stop_tick(struct rq *rq) { return true; }
+static inline void scx_rq_activate(struct rq *rq) {}
+static inline void scx_rq_deactivate(struct rq *rq) {}
+static inline int scx_check_setscheduler(struct task_struct *p, int policy) { return 0; }
+static inline bool task_on_scx(const struct task_struct *p) { return false; }
+static inline void init_sched_ext_class(void) {}
+
+#endif	/* CONFIG_SCHED_CLASS_EXT */
+
+#if defined(CONFIG_SCHED_CLASS_EXT) && defined(CONFIG_SMP)
+void __scx_update_idle(struct rq *rq, bool idle);
+
+static inline void scx_update_idle(struct rq *rq, bool idle)
+{
+	if (scx_enabled())
+		__scx_update_idle(rq, idle);
+}
+#else
+static inline void scx_update_idle(struct rq *rq, bool idle) {}
+#endif
+
+#ifdef CONFIG_CGROUP_SCHED
+#ifdef CONFIG_EXT_GROUP_SCHED
+int scx_tg_online(struct task_group *tg);
+void scx_tg_offline(struct task_group *tg);
+int scx_cgroup_can_attach(struct cgroup_taskset *tset);
+void scx_move_task(struct task_struct *p);
+void scx_cgroup_finish_attach(void);
+void scx_cgroup_cancel_attach(struct cgroup_taskset *tset);
+void scx_group_set_weight(struct task_group *tg, unsigned long cgrp_weight);
+void scx_group_set_idle(struct task_group *tg, bool idle);
+#else	/* CONFIG_EXT_GROUP_SCHED */
+static inline int scx_tg_online(struct task_group *tg) { return 0; }
+static inline void scx_tg_offline(struct task_group *tg) {}
+static inline int scx_cgroup_can_attach(struct cgroup_taskset *tset) { return 0; }
+static inline void scx_move_task(struct task_struct *p) {}
+static inline void scx_cgroup_finish_attach(void) {}
+static inline void scx_cgroup_cancel_attach(struct cgroup_taskset *tset) {}
+static inline void scx_group_set_weight(struct task_group *tg, unsigned long cgrp_weight) {}
+static inline void scx_group_set_idle(struct task_group *tg, bool idle) {}
+#endif	/* CONFIG_EXT_GROUP_SCHED */
+#endif	/* CONFIG_CGROUP_SCHED */
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 1d2cbdb162a6..718c92335979 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3844,7 +3844,8 @@ static void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,
 	}
 }
 
-void reweight_task(struct task_struct *p, const struct load_weight *lw)
+static void reweight_task_fair(struct rq *rq, struct task_struct *p,
+			       const struct load_weight *lw)
 {
 	struct sched_entity *se = &p->se;
 	struct cfs_rq *cfs_rq = cfs_rq_of(se);
@@ -9352,29 +9353,18 @@ static inline void update_blocked_load_status(struct rq *rq, bool has_blocked) {
 
 static bool __update_blocked_others(struct rq *rq, bool *done)
 {
-	const struct sched_class *curr_class;
-	u64 now = rq_clock_pelt(rq);
-	unsigned long hw_pressure;
-	bool decayed;
+	bool updated;
 
 	/*
 	 * update_load_avg() can call cpufreq_update_util(). Make sure that RT,
 	 * DL and IRQ signals have been updated before updating CFS.
 	 */
-	curr_class = rq->curr->sched_class;
-
-	hw_pressure = arch_scale_hw_pressure(cpu_of(rq));
-
-	/* hw_pressure doesn't care about invariance */
-	decayed = update_rt_rq_load_avg(now, rq, curr_class == &rt_sched_class) |
-		  update_dl_rq_load_avg(now, rq, curr_class == &dl_sched_class) |
-		  update_hw_load_avg(rq_clock_task(rq), rq, hw_pressure) |
-		  update_irq_load_avg(rq, 0);
+	updated = update_other_load_avgs(rq);
 
 	if (others_have_blocked(rq))
 		*done = false;
 
-	return decayed;
+	return updated;
 }
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -13220,6 +13210,7 @@ DEFINE_SCHED_CLASS(fair) = {
 	.task_tick		= task_tick_fair,
 	.task_fork		= task_fork_fair,
 
+	.reweight_task		= reweight_task_fair,
 	.prio_changed		= prio_changed_fair,
 	.switched_from		= switched_from_fair,
 	.switched_to		= switched_to_fair,
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 6e78d071beb5..c7a218123b7a 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -452,11 +452,13 @@ static void wakeup_preempt_idle(struct rq *rq, struct task_struct *p, int flags)
 
 static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
 {
+	scx_update_idle(rq, false);
 }
 
 static void set_next_task_idle(struct rq *rq, struct task_struct *next, bool first)
 {
 	update_idle_core(rq);
+	scx_update_idle(rq, true);
 	schedstat_inc(rq->sched_goidle);
 }
 
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4c36cc680361..4965853277e2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -192,9 +192,18 @@ static inline int idle_policy(int policy)
 	return policy == SCHED_IDLE;
 }
 
+static inline int normal_policy(int policy)
+{
+#ifdef CONFIG_SCHED_CLASS_EXT
+	if (policy == SCHED_EXT)
+		return true;
+#endif
+	return policy == SCHED_NORMAL;
+}
+
 static inline int fair_policy(int policy)
 {
-	return policy == SCHED_NORMAL || policy == SCHED_BATCH;
+	return normal_policy(policy) || policy == SCHED_BATCH;
 }
 
 static inline int rt_policy(int policy)
@@ -244,6 +253,24 @@ static inline void update_avg(u64 *avg, u64 sample)
 #define shr_bound(val, shift)							\
 	(val >> min_t(typeof(shift), shift, BITS_PER_TYPE(typeof(val)) - 1))
 
+/*
+ * cgroup weight knobs should use the common MIN, DFL and MAX values which are
+ * 1, 100 and 10000 respectively. While it loses a bit of range on both ends, it
+ * maps pretty well onto the shares value used by scheduler and the round-trip
+ * conversions preserve the original value over the entire range.
+ */
+static inline unsigned long sched_weight_from_cgroup(unsigned long cgrp_weight)
+{
+	return DIV_ROUND_CLOSEST_ULL(cgrp_weight * 1024, CGROUP_WEIGHT_DFL);
+}
+
+static inline unsigned long sched_weight_to_cgroup(unsigned long weight)
+{
+	return clamp_t(unsigned long,
+		       DIV_ROUND_CLOSEST_ULL(weight * CGROUP_WEIGHT_DFL, 1024),
+		       CGROUP_WEIGHT_MIN, CGROUP_WEIGHT_MAX);
+}
+
 /*
  * !! For sched_setattr_nocheck() (kernel) only !!
  *
@@ -397,16 +424,17 @@ struct cfs_bandwidth {
 struct task_group {
 	struct cgroup_subsys_state css;
 
+#ifdef CONFIG_GROUP_SCHED_WEIGHT
+	/* A positive value indicates that this is a SCHED_IDLE group. */
+	int			idle;
+#endif
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* schedulable entities of this group on each CPU */
 	struct sched_entity	**se;
 	/* runqueue "owned" by this group on each CPU */
 	struct cfs_rq		**cfs_rq;
 	unsigned long		shares;
-
-	/* A positive value indicates that this is a SCHED_IDLE group. */
-	int			idle;
-
 #ifdef	CONFIG_SMP
 	/*
 	 * load_avg can be heavily contended at clock tick time, so put
@@ -424,6 +452,11 @@ struct task_group {
 	struct rt_bandwidth	rt_bandwidth;
 #endif
 
+#ifdef CONFIG_EXT_GROUP_SCHED
+	u32			scx_flags;	/* SCX_TG_* */
+	u32			scx_weight;
+#endif
+
 	struct rcu_head		rcu;
 	struct list_head	list;
 
@@ -448,7 +481,7 @@ struct task_group {
 
 };
 
-#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_GROUP_SCHED_WEIGHT
 #define ROOT_TASK_GROUP_LOAD	NICE_0_LOAD
 
 /*
@@ -479,6 +512,11 @@ static inline int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)
 	return walk_tg_tree_from(&root_task_group, down, up, data);
 }
 
+static inline struct task_group *css_tg(struct cgroup_subsys_state *css)
+{
+	return css ? container_of(css, struct task_group, css) : NULL;
+}
+
 extern int tg_nop(struct task_group *tg, void *data);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -535,6 +573,9 @@ extern void set_task_rq_fair(struct sched_entity *se,
 static inline void set_task_rq_fair(struct sched_entity *se,
 			     struct cfs_rq *prev, struct cfs_rq *next) { }
 #endif /* CONFIG_SMP */
+#else /* !CONFIG_FAIR_GROUP_SCHED */
+static inline int sched_group_set_shares(struct task_group *tg, unsigned long shares) { return 0; }
+static inline int sched_group_set_idle(struct task_group *tg, long idle) { return 0; }
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 #else /* CONFIG_CGROUP_SCHED */
@@ -588,6 +629,11 @@ do {									\
 # define u64_u32_load(var)		u64_u32_load_copy(var, var##_copy)
 # define u64_u32_store(var, val)	u64_u32_store_copy(var, var##_copy, val)
 
+struct balance_callback {
+	struct balance_callback *next;
+	void (*func)(struct rq *rq);
+};
+
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
 	struct load_weight	load;
@@ -696,6 +742,43 @@ struct cfs_rq {
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
 
+#ifdef CONFIG_SCHED_CLASS_EXT
+/* scx_rq->flags, protected by the rq lock */
+enum scx_rq_flags {
+	/*
+	 * A hotplugged CPU starts scheduling before rq_online_scx(). Track
+	 * ops.cpu_on/offline() state so that ops.enqueue/dispatch() are called
+	 * only while the BPF scheduler considers the CPU to be online.
+	 */
+	SCX_RQ_ONLINE		= 1 << 0,
+	SCX_RQ_CAN_STOP_TICK	= 1 << 1,
+	SCX_RQ_BYPASSING	= 1 << 3,
+
+	SCX_RQ_IN_WAKEUP	= 1 << 16,
+	SCX_RQ_IN_BALANCE	= 1 << 17,
+};
+
+struct scx_rq {
+	struct scx_dispatch_q	local_dsq;
+	struct list_head	runnable_list;		/* runnable tasks on this rq */
+	struct list_head	ddsp_deferred_locals;	/* deferred ddsps from enq */
+	unsigned long		ops_qseq;
+	u64			extra_enq_flags;	/* see move_task_to_local_dsq() */
+	u32			nr_running;
+	u32			flags;
+	u32			cpuperf_target;		/* [0, SCHED_CAPACITY_SCALE] */
+	bool			cpu_released;
+	cpumask_var_t		cpus_to_kick;
+	cpumask_var_t		cpus_to_kick_if_idle;
+	cpumask_var_t		cpus_to_preempt;
+	cpumask_var_t		cpus_to_wait;
+	unsigned long		pnt_seq;
+	struct balance_callback	deferred_bal_cb;
+	struct irq_work		deferred_irq_work;
+	struct irq_work		kick_cpus_irq_work;
+};
+#endif /* CONFIG_SCHED_CLASS_EXT */
+
 static inline int rt_bandwidth_enabled(void)
 {
 	return sysctl_sched_rt_runtime >= 0;
@@ -996,11 +1079,6 @@ struct uclamp_rq {
 DECLARE_STATIC_KEY_FALSE(sched_uclamp_used);
 #endif /* CONFIG_UCLAMP_TASK */
 
-struct balance_callback {
-	struct balance_callback *next;
-	void (*func)(struct rq *rq);
-};
-
 /*
  * This is the main, per-CPU runqueue data structure.
  *
@@ -1043,6 +1121,9 @@ struct rq {
 	struct cfs_rq		cfs;
 	struct rt_rq		rt;
 	struct dl_rq		dl;
+#ifdef CONFIG_SCHED_CLASS_EXT
+	struct scx_rq		scx;
+#endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this CPU: */
@@ -2291,13 +2372,15 @@ struct sched_class {
 
 	void (*wakeup_preempt)(struct rq *rq, struct task_struct *p, int flags);
 
+	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
 	struct task_struct *(*pick_next_task)(struct rq *rq);
 
 	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
 	void (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);
 
+	void (*switch_class)(struct rq *rq, struct task_struct *next);
+
 #ifdef CONFIG_SMP
-	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int flags);
 
 	struct task_struct * (*pick_task)(struct rq *rq);
@@ -2323,8 +2406,11 @@ struct sched_class {
 	 * cannot assume the switched_from/switched_to pair is serialized by
 	 * rq->lock. They are however serialized by p->pi_lock.
 	 */
+	void (*switching_to) (struct rq *this_rq, struct task_struct *task);
 	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
 	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
+	void (*reweight_task)(struct rq *this_rq, struct task_struct *task,
+			      const struct load_weight *lw);
 	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
 			      int oldprio);
 
@@ -2373,19 +2459,54 @@ const struct sched_class name##_sched_class \
 extern struct sched_class __sched_class_highest[];
 extern struct sched_class __sched_class_lowest[];
 
+extern const struct sched_class stop_sched_class;
+extern const struct sched_class dl_sched_class;
+extern const struct sched_class rt_sched_class;
+extern const struct sched_class fair_sched_class;
+extern const struct sched_class idle_sched_class;
+
+#ifdef CONFIG_SCHED_CLASS_EXT
+extern const struct sched_class ext_sched_class;
+
+DECLARE_STATIC_KEY_FALSE(__scx_ops_enabled);	/* SCX BPF scheduler loaded */
+DECLARE_STATIC_KEY_FALSE(__scx_switched_all);	/* all fair class tasks on SCX */
+
+#define scx_enabled()		static_branch_unlikely(&__scx_ops_enabled)
+#define scx_switched_all()	static_branch_unlikely(&__scx_switched_all)
+#else /* !CONFIG_SCHED_CLASS_EXT */
+#define scx_enabled()		false
+#define scx_switched_all()	false
+#endif /* !CONFIG_SCHED_CLASS_EXT */
+
+/*
+ * Iterate only active classes. SCX can take over all fair tasks or be
+ * completely disabled. If the former, skip fair. If the latter, skip SCX.
+ */
+static inline const struct sched_class *next_active_class(const struct sched_class *class)
+{
+	class++;
+#ifdef CONFIG_SCHED_CLASS_EXT
+	if (scx_switched_all() && class == &fair_sched_class)
+		class++;
+	if (!scx_enabled() && class == &ext_sched_class)
+		class++;
+#endif
+	return class;
+}
+
 #define for_class_range(class, _from, _to) \
 	for (class = (_from); class < (_to); class++)
 
 #define for_each_class(class) \
 	for_class_range(class, __sched_class_highest, __sched_class_lowest)
 
-#define sched_class_above(_a, _b)	((_a) < (_b))
+#define for_active_class_range(class, _from, _to)				\
+	for (class = (_from); class != (_to); class = next_active_class(class))
 
-extern const struct sched_class stop_sched_class;
-extern const struct sched_class dl_sched_class;
-extern const struct sched_class rt_sched_class;
-extern const struct sched_class fair_sched_class;
-extern const struct sched_class idle_sched_class;
+#define for_each_active_class(class)						\
+	for_active_class_range(class, __sched_class_highest, __sched_class_lowest)
+
+#define sched_class_above(_a, _b)	((_a) < (_b))
 
 static inline bool sched_stop_runnable(struct rq *rq)
 {
@@ -2424,6 +2545,19 @@ extern void sched_balance_trigger(struct rq *rq);
 extern int __set_cpus_allowed_ptr(struct task_struct *p, struct affinity_context *ctx);
 extern void set_cpus_allowed_common(struct task_struct *p, struct affinity_context *ctx);
 
+static inline bool task_allowed_on_cpu(struct task_struct *p, int cpu)
+{
+	/* When not in the task's cpumask, no point in looking further. */
+	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
+		return false;
+
+	/* Can @cpu run a user thread? */
+	if (!(p->flags & PF_KTHREAD) && !task_cpu_possible(cpu, p))
+		return false;
+
+	return true;
+}
+
 static inline cpumask_t *alloc_user_cpus_ptr(int node)
 {
 	/*
@@ -2457,6 +2591,11 @@ extern int push_cpu_stop(void *arg);
 
 #else /* !CONFIG_SMP: */
 
+static inline bool task_allowed_on_cpu(struct task_struct *p, int cpu)
+{
+	return true;
+}
+
 static inline int __set_cpus_allowed_ptr(struct task_struct *p,
 					 struct affinity_context *ctx)
 {
@@ -2510,8 +2649,6 @@ extern void init_sched_dl_class(void);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
 
-extern void reweight_task(struct task_struct *p, const struct load_weight *lw);
-
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
@@ -3056,6 +3193,8 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) { }
 
 #ifdef CONFIG_SMP
 
+bool update_other_load_avgs(struct rq *rq);
+
 unsigned long effective_cpu_util(int cpu, unsigned long util_cfs,
 				 unsigned long *min,
 				 unsigned long *max);
@@ -3099,6 +3238,8 @@ static inline unsigned long cpu_util_rt(struct rq *rq)
 	return READ_ONCE(rq->avg_rt.util_avg);
 }
 
+#else /* !CONFIG_SMP */
+static inline bool update_other_load_avgs(struct rq *rq) { return false; }
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_UCLAMP_TASK
@@ -3609,6 +3750,8 @@ extern void set_load_weight(struct task_struct *p, bool update_load);
 extern void enqueue_task(struct rq *rq, struct task_struct *p, int flags);
 extern void dequeue_task(struct rq *rq, struct task_struct *p, int flags);
 
+extern void check_class_changing(struct rq *rq, struct task_struct *p,
+				 const struct sched_class *prev_class);
 extern void check_class_changed(struct rq *rq, struct task_struct *p,
 				const struct sched_class *prev_class,
 				int oldprio);
@@ -3629,4 +3772,24 @@ static inline void balance_callbacks(struct rq *rq, struct balance_callback *hea
 
 #endif
 
+#ifdef CONFIG_SCHED_CLASS_EXT
+/*
+ * Used by SCX in the enable/disable paths to move tasks between sched_classes
+ * and establish invariants.
+ */
+struct sched_enq_and_set_ctx {
+	struct task_struct	*p;
+	int			queue_flags;
+	bool			queued;
+	bool			running;
+};
+
+void sched_deq_and_put_task(struct task_struct *p, int queue_flags,
+			    struct sched_enq_and_set_ctx *ctx);
+void sched_enq_and_set_task(struct sched_enq_and_set_ctx *ctx);
+
+#endif /* CONFIG_SCHED_CLASS_EXT */
+
+#include "ext.h"
+
 #endif /* _KERNEL_SCHED_SCHED_H */
diff --git a/kernel/sched/syscalls.c b/kernel/sched/syscalls.c
index ae1b42775ef9..4fa59c9f69ac 100644
--- a/kernel/sched/syscalls.c
+++ b/kernel/sched/syscalls.c
@@ -259,6 +259,25 @@ int sched_core_idle_cpu(int cpu)
 #endif
 
 #ifdef CONFIG_SMP
+/*
+ * Load avg and utiliztion metrics need to be updated periodically and before
+ * consumption. This function updates the metrics for all subsystems except for
+ * the fair class. @rq must be locked and have its clock updated.
+ */
+bool update_other_load_avgs(struct rq *rq)
+{
+	u64 now = rq_clock_pelt(rq);
+	const struct sched_class *curr_class = rq->curr->sched_class;
+	unsigned long hw_pressure = arch_scale_hw_pressure(cpu_of(rq));
+
+	lockdep_assert_rq_held(rq);
+
+	return update_rt_rq_load_avg(now, rq, curr_class == &rt_sched_class) |
+		update_dl_rq_load_avg(now, rq, curr_class == &dl_sched_class) |
+		update_hw_load_avg(now, rq, hw_pressure) |
+		update_irq_load_avg(rq, 0);
+}
+
 /*
  * This function computes an effective utilization for the given CPU, to be
  * used for frequency selection given the linear relation: f = u * f_max.
@@ -695,6 +714,10 @@ int __sched_setscheduler(struct task_struct *p,
 		goto unlock;
 	}
 
+	retval = scx_check_setscheduler(p, policy);
+	if (retval)
+		goto unlock;
+
 	/*
 	 * If not changing anything there's no need to proceed further,
 	 * but store a possible modification of reset_on_fork.
@@ -797,6 +820,7 @@ int __sched_setscheduler(struct task_struct *p,
 		__setscheduler_prio(p, newprio);
 	}
 	__setscheduler_uclamp(p, attr);
+	check_class_changing(rq, p, prev_class);
 
 	if (queued) {
 		/*
@@ -1602,6 +1626,7 @@ SYSCALL_DEFINE1(sched_get_priority_max, int, policy)
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
 	case SCHED_IDLE:
+	case SCHED_EXT:
 		ret = 0;
 		break;
 	}
@@ -1629,6 +1654,7 @@ SYSCALL_DEFINE1(sched_get_priority_min, int, policy)
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
 	case SCHED_IDLE:
+	case SCHED_EXT:
 		ret = 0;
 	}
 	return ret;
diff --git a/lib/dump_stack.c b/lib/dump_stack.c
index 1a996fbbf50a..388da1aea14a 100644
--- a/lib/dump_stack.c
+++ b/lib/dump_stack.c
@@ -73,6 +73,7 @@ void dump_stack_print_info(const char *log_lvl)
 
 	print_worker_info(log_lvl, current);
 	print_stop_info(log_lvl, current);
+	print_scx_info(log_lvl, current);
 }
 
 /**
diff --git a/tools/Makefile b/tools/Makefile
index 276f5d0d53a4..278d24723b74 100644
--- a/tools/Makefile
+++ b/tools/Makefile
@@ -28,6 +28,7 @@ help:
 	@echo '  pci                    - PCI tools'
 	@echo '  perf                   - Linux performance measurement and analysis tool'
 	@echo '  selftests              - various kernel selftests'
+	@echo '  sched_ext              - sched_ext example schedulers'
 	@echo '  bootconfig             - boot config tool'
 	@echo '  spi                    - spi tools'
 	@echo '  tmon                   - thermal monitoring and tuning tool'
@@ -91,6 +92,9 @@ perf: FORCE
 	$(Q)mkdir -p $(PERF_O) .
 	$(Q)$(MAKE) --no-print-directory -C perf O=$(PERF_O) subdir=
 
+sched_ext: FORCE
+	$(call descend,sched_ext)
+
 selftests: FORCE
 	$(call descend,testing/$@)
 
@@ -184,6 +188,9 @@ perf_clean:
 	$(Q)mkdir -p $(PERF_O) .
 	$(Q)$(MAKE) --no-print-directory -C perf O=$(PERF_O) subdir= clean
 
+sched_ext_clean:
+	$(call descend,sched_ext,clean)
+
 selftests_clean:
 	$(call descend,testing/$(@:_clean=),clean)
 
@@ -213,6 +220,7 @@ clean: acpi_clean counter_clean cpupower_clean hv_clean firewire_clean \
 		mm_clean bpf_clean iio_clean x86_energy_perf_policy_clean tmon_clean \
 		freefall_clean build_clean libbpf_clean libsubcmd_clean \
 		gpio_clean objtool_clean leds_clean wmi_clean pci_clean firmware_clean debugging_clean \
-		intel-speed-select_clean tracing_clean thermal_clean thermometer_clean thermal-engine_clean
+		intel-speed-select_clean tracing_clean thermal_clean thermometer_clean thermal-engine_clean \
+		sched_ext_clean
 
 .PHONY: FORCE
diff --git a/tools/sched_ext/.gitignore b/tools/sched_ext/.gitignore
new file mode 100644
index 000000000000..d6264fe1c8cd
--- /dev/null
+++ b/tools/sched_ext/.gitignore
@@ -0,0 +1,2 @@
+tools/
+build/
diff --git a/tools/sched_ext/Makefile b/tools/sched_ext/Makefile
new file mode 100644
index 000000000000..ca3815e572d8
--- /dev/null
+++ b/tools/sched_ext/Makefile
@@ -0,0 +1,246 @@
+# SPDX-License-Identifier: GPL-2.0
+# Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+include ../build/Build.include
+include ../scripts/Makefile.arch
+include ../scripts/Makefile.include
+
+all: all_targets
+
+ifneq ($(LLVM),)
+ifneq ($(filter %/,$(LLVM)),)
+LLVM_PREFIX := $(LLVM)
+else ifneq ($(filter -%,$(LLVM)),)
+LLVM_SUFFIX := $(LLVM)
+endif
+
+CLANG_TARGET_FLAGS_arm          := arm-linux-gnueabi
+CLANG_TARGET_FLAGS_arm64        := aarch64-linux-gnu
+CLANG_TARGET_FLAGS_hexagon      := hexagon-linux-musl
+CLANG_TARGET_FLAGS_m68k         := m68k-linux-gnu
+CLANG_TARGET_FLAGS_mips         := mipsel-linux-gnu
+CLANG_TARGET_FLAGS_powerpc      := powerpc64le-linux-gnu
+CLANG_TARGET_FLAGS_riscv        := riscv64-linux-gnu
+CLANG_TARGET_FLAGS_s390         := s390x-linux-gnu
+CLANG_TARGET_FLAGS_x86          := x86_64-linux-gnu
+CLANG_TARGET_FLAGS              := $(CLANG_TARGET_FLAGS_$(ARCH))
+
+ifeq ($(CROSS_COMPILE),)
+ifeq ($(CLANG_TARGET_FLAGS),)
+$(error Specify CROSS_COMPILE or add '--target=' option to lib.mk)
+else
+CLANG_FLAGS     += --target=$(CLANG_TARGET_FLAGS)
+endif # CLANG_TARGET_FLAGS
+else
+CLANG_FLAGS     += --target=$(notdir $(CROSS_COMPILE:%-=%))
+endif # CROSS_COMPILE
+
+CC := $(LLVM_PREFIX)clang$(LLVM_SUFFIX) $(CLANG_FLAGS) -fintegrated-as
+else
+CC := $(CROSS_COMPILE)gcc
+endif # LLVM
+
+CURDIR := $(abspath .)
+TOOLSDIR := $(abspath ..)
+LIBDIR := $(TOOLSDIR)/lib
+BPFDIR := $(LIBDIR)/bpf
+TOOLSINCDIR := $(TOOLSDIR)/include
+BPFTOOLDIR := $(TOOLSDIR)/bpf/bpftool
+APIDIR := $(TOOLSINCDIR)/uapi
+GENDIR := $(abspath ../../include/generated)
+GENHDR := $(GENDIR)/autoconf.h
+
+ifeq ($(O),)
+OUTPUT_DIR := $(CURDIR)/build
+else
+OUTPUT_DIR := $(O)/build
+endif # O
+OBJ_DIR := $(OUTPUT_DIR)/obj
+INCLUDE_DIR := $(OUTPUT_DIR)/include
+BPFOBJ_DIR := $(OBJ_DIR)/libbpf
+SCXOBJ_DIR := $(OBJ_DIR)/sched_ext
+BINDIR := $(OUTPUT_DIR)/bin
+BPFOBJ := $(BPFOBJ_DIR)/libbpf.a
+ifneq ($(CROSS_COMPILE),)
+HOST_BUILD_DIR		:= $(OBJ_DIR)/host
+HOST_OUTPUT_DIR	:= host-tools
+HOST_INCLUDE_DIR	:= $(HOST_OUTPUT_DIR)/include
+else
+HOST_BUILD_DIR		:= $(OBJ_DIR)
+HOST_OUTPUT_DIR	:= $(OUTPUT_DIR)
+HOST_INCLUDE_DIR	:= $(INCLUDE_DIR)
+endif
+HOST_BPFOBJ := $(HOST_BUILD_DIR)/libbpf/libbpf.a
+RESOLVE_BTFIDS := $(HOST_BUILD_DIR)/resolve_btfids/resolve_btfids
+DEFAULT_BPFTOOL := $(HOST_OUTPUT_DIR)/sbin/bpftool
+
+VMLINUX_BTF_PATHS ?= $(if $(O),$(O)/vmlinux)					\
+		     $(if $(KBUILD_OUTPUT),$(KBUILD_OUTPUT)/vmlinux)		\
+		     ../../vmlinux						\
+		     /sys/kernel/btf/vmlinux					\
+		     /boot/vmlinux-$(shell uname -r)
+VMLINUX_BTF ?= $(abspath $(firstword $(wildcard $(VMLINUX_BTF_PATHS))))
+ifeq ($(VMLINUX_BTF),)
+$(error Cannot find a vmlinux for VMLINUX_BTF at any of "$(VMLINUX_BTF_PATHS)")
+endif
+
+BPFTOOL ?= $(DEFAULT_BPFTOOL)
+
+ifneq ($(wildcard $(GENHDR)),)
+  GENFLAGS := -DHAVE_GENHDR
+endif
+
+CFLAGS += -g -O2 -rdynamic -pthread -Wall -Werror $(GENFLAGS)			\
+	  -I$(INCLUDE_DIR) -I$(GENDIR) -I$(LIBDIR)				\
+	  -I$(TOOLSINCDIR) -I$(APIDIR) -I$(CURDIR)/include
+
+# Silence some warnings when compiled with clang
+ifneq ($(LLVM),)
+CFLAGS += -Wno-unused-command-line-argument
+endif
+
+LDFLAGS = -lelf -lz -lpthread
+
+IS_LITTLE_ENDIAN = $(shell $(CC) -dM -E - </dev/null |				\
+			grep 'define __BYTE_ORDER__ __ORDER_LITTLE_ENDIAN__')
+
+# Get Clang's default includes on this system, as opposed to those seen by
+# '-target bpf'. This fixes "missing" files on some architectures/distros,
+# such as asm/byteorder.h, asm/socket.h, asm/sockios.h, sys/cdefs.h etc.
+#
+# Use '-idirafter': Don't interfere with include mechanics except where the
+# build would have failed anyways.
+define get_sys_includes
+$(shell $(1) -v -E - </dev/null 2>&1 \
+	| sed -n '/<...> search starts here:/,/End of search list./{ s| \(/.*\)|-idirafter \1|p }') \
+$(shell $(1) -dM -E - </dev/null | grep '__riscv_xlen ' | awk '{printf("-D__riscv_xlen=%d -D__BITS_PER_LONG=%d", $$3, $$3)}')
+endef
+
+BPF_CFLAGS = -g -D__TARGET_ARCH_$(SRCARCH)					\
+	     $(if $(IS_LITTLE_ENDIAN),-mlittle-endian,-mbig-endian)		\
+	     -I$(CURDIR)/include -I$(CURDIR)/include/bpf-compat			\
+	     -I$(INCLUDE_DIR) -I$(APIDIR)					\
+	     -I../../include							\
+	     $(call get_sys_includes,$(CLANG))					\
+	     -Wall -Wno-compare-distinct-pointer-types				\
+	     -O2 -mcpu=v3
+
+# sort removes libbpf duplicates when not cross-building
+MAKE_DIRS := $(sort $(OBJ_DIR)/libbpf $(HOST_BUILD_DIR)/libbpf			\
+	       $(HOST_BUILD_DIR)/bpftool $(HOST_BUILD_DIR)/resolve_btfids	\
+	       $(INCLUDE_DIR) $(SCXOBJ_DIR) $(BINDIR))
+
+$(MAKE_DIRS):
+	$(call msg,MKDIR,,$@)
+	$(Q)mkdir -p $@
+
+$(BPFOBJ): $(wildcard $(BPFDIR)/*.[ch] $(BPFDIR)/Makefile)			\
+	   $(APIDIR)/linux/bpf.h						\
+	   | $(OBJ_DIR)/libbpf
+	$(Q)$(MAKE) $(submake_extras) -C $(BPFDIR) OUTPUT=$(OBJ_DIR)/libbpf/	\
+		    EXTRA_CFLAGS='-g -O0 -fPIC'					\
+		    DESTDIR=$(OUTPUT_DIR) prefix= all install_headers
+
+$(DEFAULT_BPFTOOL): $(wildcard $(BPFTOOLDIR)/*.[ch] $(BPFTOOLDIR)/Makefile)	\
+		    $(HOST_BPFOBJ) | $(HOST_BUILD_DIR)/bpftool
+	$(Q)$(MAKE) $(submake_extras)  -C $(BPFTOOLDIR)				\
+		    ARCH= CROSS_COMPILE= CC=$(HOSTCC) LD=$(HOSTLD)		\
+		    EXTRA_CFLAGS='-g -O0'					\
+		    OUTPUT=$(HOST_BUILD_DIR)/bpftool/				\
+		    LIBBPF_OUTPUT=$(HOST_BUILD_DIR)/libbpf/			\
+		    LIBBPF_DESTDIR=$(HOST_OUTPUT_DIR)/				\
+		    prefix= DESTDIR=$(HOST_OUTPUT_DIR)/ install-bin
+
+$(INCLUDE_DIR)/vmlinux.h: $(VMLINUX_BTF) $(BPFTOOL) | $(INCLUDE_DIR)
+ifeq ($(VMLINUX_H),)
+	$(call msg,GEN,,$@)
+	$(Q)$(BPFTOOL) btf dump file $(VMLINUX_BTF) format c > $@
+else
+	$(call msg,CP,,$@)
+	$(Q)cp "$(VMLINUX_H)" $@
+endif
+
+$(SCXOBJ_DIR)/%.bpf.o: %.bpf.c $(INCLUDE_DIR)/vmlinux.h include/scx/*.h		\
+		       | $(BPFOBJ) $(SCXOBJ_DIR)
+	$(call msg,CLNG-BPF,,$(notdir $@))
+	$(Q)$(CLANG) $(BPF_CFLAGS) -target bpf -c $< -o $@
+
+$(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BPFTOOL)
+	$(eval sched=$(notdir $@))
+	$(call msg,GEN-SKEL,,$(sched))
+	$(Q)$(BPFTOOL) gen object $(<:.o=.linked1.o) $<
+	$(Q)$(BPFTOOL) gen object $(<:.o=.linked2.o) $(<:.o=.linked1.o)
+	$(Q)$(BPFTOOL) gen object $(<:.o=.linked3.o) $(<:.o=.linked2.o)
+	$(Q)diff $(<:.o=.linked2.o) $(<:.o=.linked3.o)
+	$(Q)$(BPFTOOL) gen skeleton $(<:.o=.linked3.o) name $(subst .bpf.skel.h,,$(sched)) > $@
+	$(Q)$(BPFTOOL) gen subskeleton $(<:.o=.linked3.o) name $(subst .bpf.skel.h,,$(sched)) > $(@:.skel.h=.subskel.h)
+
+SCX_COMMON_DEPS := include/scx/common.h include/scx/user_exit_info.h | $(BINDIR)
+
+c-sched-targets = scx_simple scx_qmap scx_central scx_flatcg
+
+$(addprefix $(BINDIR)/,$(c-sched-targets)): \
+	$(BINDIR)/%: \
+		$(filter-out %.bpf.c,%.c) \
+		$(INCLUDE_DIR)/%.bpf.skel.h \
+		$(SCX_COMMON_DEPS)
+	$(eval sched=$(notdir $@))
+	$(CC) $(CFLAGS) -c $(sched).c -o $(SCXOBJ_DIR)/$(sched).o
+	$(CC) -o $@ $(SCXOBJ_DIR)/$(sched).o $(HOST_BPFOBJ) $(LDFLAGS)
+
+$(c-sched-targets): %: $(BINDIR)/%
+
+install: all
+	$(Q)mkdir -p $(DESTDIR)/usr/local/bin/
+	$(Q)cp $(BINDIR)/* $(DESTDIR)/usr/local/bin/
+
+clean:
+	rm -rf $(OUTPUT_DIR) $(HOST_OUTPUT_DIR)
+	rm -f *.o *.bpf.o *.bpf.skel.h *.bpf.subskel.h
+	rm -f $(c-sched-targets)
+
+help:
+	@echo   'Building targets'
+	@echo   '================'
+	@echo   ''
+	@echo   '  all		  - Compile all schedulers'
+	@echo   ''
+	@echo   'Alternatively, you may compile individual schedulers:'
+	@echo   ''
+	@printf '  %s\n' $(c-sched-targets)
+	@echo   ''
+	@echo   'For any scheduler build target, you may specify an alternative'
+	@echo   'build output path with the O= environment variable. For example:'
+	@echo   ''
+	@echo   '   O=/tmp/sched_ext make all'
+	@echo   ''
+	@echo   'will compile all schedulers, and emit the build artifacts to'
+	@echo   '/tmp/sched_ext/build.'
+	@echo   ''
+	@echo   ''
+	@echo   'Installing targets'
+	@echo   '=================='
+	@echo   ''
+	@echo   '  install	  - Compile and install all schedulers to /usr/bin.'
+	@echo   '		    You may specify the DESTDIR= environment variable'
+	@echo   '		    to indicate a prefix for /usr/bin. For example:'
+	@echo   ''
+	@echo   '                     DESTDIR=/tmp/sched_ext make install'
+	@echo   ''
+	@echo   '		    will build the schedulers in CWD/build, and'
+	@echo   '		    install the schedulers to /tmp/sched_ext/usr/bin.'
+	@echo   ''
+	@echo   ''
+	@echo   'Cleaning targets'
+	@echo   '================'
+	@echo   ''
+	@echo   '  clean		  - Remove all generated files'
+
+all_targets: $(c-sched-targets)
+
+.PHONY: all all_targets $(c-sched-targets) clean help
+
+# delete failed targets
+.DELETE_ON_ERROR:
+
+# keep intermediate (.bpf.skel.h, .bpf.o, etc) targets
+.SECONDARY:
diff --git a/tools/sched_ext/README.md b/tools/sched_ext/README.md
new file mode 100644
index 000000000000..16a42e4060f6
--- /dev/null
+++ b/tools/sched_ext/README.md
@@ -0,0 +1,270 @@
+SCHED_EXT EXAMPLE SCHEDULERS
+============================
+
+# Introduction
+
+This directory contains a number of example sched_ext schedulers. These
+schedulers are meant to provide examples of different types of schedulers
+that can be built using sched_ext, and illustrate how various features of
+sched_ext can be used.
+
+Some of the examples are performant, production-ready schedulers. That is, for
+the correct workload and with the correct tuning, they may be deployed in a
+production environment with acceptable or possibly even improved performance.
+Others are just examples that in practice, would not provide acceptable
+performance (though they could be improved to get there).
+
+This README will describe these example schedulers, including describing the
+types of workloads or scenarios they're designed to accommodate, and whether or
+not they're production ready. For more details on any of these schedulers,
+please see the header comment in their .bpf.c file.
+
+
+# Compiling the examples
+
+There are a few toolchain dependencies for compiling the example schedulers.
+
+## Toolchain dependencies
+
+1. clang >= 16.0.0
+
+The schedulers are BPF programs, and therefore must be compiled with clang. gcc
+is actively working on adding a BPF backend compiler as well, but are still
+missing some features such as BTF type tags which are necessary for using
+kptrs.
+
+2. pahole >= 1.25
+
+You may need pahole in order to generate BTF from DWARF.
+
+3. rust >= 1.70.0
+
+Rust schedulers uses features present in the rust toolchain >= 1.70.0. You
+should be able to use the stable build from rustup, but if that doesn't
+work, try using the rustup nightly build.
+
+There are other requirements as well, such as make, but these are the main /
+non-trivial ones.
+
+## Compiling the kernel
+
+In order to run a sched_ext scheduler, you'll have to run a kernel compiled
+with the patches in this repository, and with a minimum set of necessary
+Kconfig options:
+
+```
+CONFIG_BPF=y
+CONFIG_SCHED_CLASS_EXT=y
+CONFIG_BPF_SYSCALL=y
+CONFIG_BPF_JIT=y
+CONFIG_DEBUG_INFO_BTF=y
+```
+
+It's also recommended that you also include the following Kconfig options:
+
+```
+CONFIG_BPF_JIT_ALWAYS_ON=y
+CONFIG_BPF_JIT_DEFAULT_ON=y
+CONFIG_PAHOLE_HAS_SPLIT_BTF=y
+CONFIG_PAHOLE_HAS_BTF_TAG=y
+```
+
+There is a `Kconfig` file in this directory whose contents you can append to
+your local `.config` file, as long as there are no conflicts with any existing
+options in the file.
+
+## Getting a vmlinux.h file
+
+You may notice that most of the example schedulers include a "vmlinux.h" file.
+This is a large, auto-generated header file that contains all of the types
+defined in some vmlinux binary that was compiled with
+[BTF](https://docs.kernel.org/bpf/btf.html) (i.e. with the BTF-related Kconfig
+options specified above).
+
+The header file is created using `bpftool`, by passing it a vmlinux binary
+compiled with BTF as follows:
+
+```bash
+$ bpftool btf dump file /path/to/vmlinux format c > vmlinux.h
+```
+
+`bpftool` analyzes all of the BTF encodings in the binary, and produces a
+header file that can be included by BPF programs to access those types.  For
+example, using vmlinux.h allows a scheduler to access fields defined directly
+in vmlinux as follows:
+
+```c
+#include "vmlinux.h"
+// vmlinux.h is also implicitly included by scx_common.bpf.h.
+#include "scx_common.bpf.h"
+
+/*
+ * vmlinux.h provides definitions for struct task_struct and
+ * struct scx_enable_args.
+ */
+void BPF_STRUCT_OPS(example_enable, struct task_struct *p,
+		    struct scx_enable_args *args)
+{
+	bpf_printk("Task %s enabled in example scheduler", p->comm);
+}
+
+// vmlinux.h provides the definition for struct sched_ext_ops.
+SEC(".struct_ops.link")
+struct sched_ext_ops example_ops {
+	.enable	= (void *)example_enable,
+	.name	= "example",
+}
+```
+
+The scheduler build system will generate this vmlinux.h file as part of the
+scheduler build pipeline. It looks for a vmlinux file in the following
+dependency order:
+
+1. If the O= environment variable is defined, at `$O/vmlinux`
+2. If the KBUILD_OUTPUT= environment variable is defined, at
+   `$KBUILD_OUTPUT/vmlinux`
+3. At `../../vmlinux` (i.e. at the root of the kernel tree where you're
+   compiling the schedulers)
+3. `/sys/kernel/btf/vmlinux`
+4. `/boot/vmlinux-$(uname -r)`
+
+In other words, if you have compiled a kernel in your local repo, its vmlinux
+file will be used to generate vmlinux.h. Otherwise, it will be the vmlinux of
+the kernel you're currently running on. This means that if you're running on a
+kernel with sched_ext support, you may not need to compile a local kernel at
+all.
+
+### Aside on CO-RE
+
+One of the cooler features of BPF is that it supports
+[CO-RE](https://nakryiko.com/posts/bpf-core-reference-guide/) (Compile Once Run
+Everywhere). This feature allows you to reference fields inside of structs with
+types defined internal to the kernel, and not have to recompile if you load the
+BPF program on a different kernel with the field at a different offset. In our
+example above, we print out a task name with `p->comm`. CO-RE would perform
+relocations for that access when the program is loaded to ensure that it's
+referencing the correct offset for the currently running kernel.
+
+## Compiling the schedulers
+
+Once you have your toolchain setup, and a vmlinux that can be used to generate
+a full vmlinux.h file, you can compile the schedulers using `make`:
+
+```bash
+$ make -j($nproc)
+```
+
+# Example schedulers
+
+This directory contains the following example schedulers. These schedulers are
+for testing and demonstrating different aspects of sched_ext. While some may be
+useful in limited scenarios, they are not intended to be practical.
+
+For more scheduler implementations, tools and documentation, visit
+https://github.com/sched-ext/scx.
+
+## scx_simple
+
+A simple scheduler that provides an example of a minimal sched_ext scheduler.
+scx_simple can be run in either global weighted vtime mode, or FIFO mode.
+
+Though very simple, in limited scenarios, this scheduler can perform reasonably
+well on single-socket systems with a unified L3 cache.
+
+## scx_qmap
+
+Another simple, yet slightly more complex scheduler that provides an example of
+a basic weighted FIFO queuing policy. It also provides examples of some common
+useful BPF features, such as sleepable per-task storage allocation in the
+`ops.prep_enable()` callback, and using the `BPF_MAP_TYPE_QUEUE` map type to
+enqueue tasks. It also illustrates how core-sched support could be implemented.
+
+## scx_central
+
+A "central" scheduler where scheduling decisions are made from a single CPU.
+This scheduler illustrates how scheduling decisions can be dispatched from a
+single CPU, allowing other cores to run with infinite slices, without timer
+ticks, and without having to incur the overhead of making scheduling decisions.
+
+The approach demonstrated by this scheduler may be useful for any workload that
+benefits from minimizing scheduling overhead and timer ticks. An example of
+where this could be particularly useful is running VMs, where running with
+infinite slices and no timer ticks allows the VM to avoid unnecessary expensive
+vmexits.
+
+## scx_flatcg
+
+A flattened cgroup hierarchy scheduler. This scheduler implements hierarchical
+weight-based cgroup CPU control by flattening the cgroup hierarchy into a single
+layer, by compounding the active weight share at each level. The effect of this
+is a much more performant CPU controller, which does not need to descend down
+cgroup trees in order to properly compute a cgroup's share.
+
+Similar to scx_simple, in limited scenarios, this scheduler can perform
+reasonably well on single socket-socket systems with a unified L3 cache and show
+significantly lowered hierarchical scheduling overhead.
+
+
+# Troubleshooting
+
+There are a number of common issues that you may run into when building the
+schedulers. We'll go over some of the common ones here.
+
+## Build Failures
+
+### Old version of clang
+
+```
+error: static assertion failed due to requirement 'SCX_DSQ_FLAG_BUILTIN': bpftool generated vmlinux.h is missing high bits for 64bit enums, upgrade clang and pahole
+        _Static_assert(SCX_DSQ_FLAG_BUILTIN,
+                       ^~~~~~~~~~~~~~~~~~~~
+1 error generated.
+```
+
+This means you built the kernel or the schedulers with an older version of
+clang than what's supported (i.e. older than 16.0.0). To remediate this:
+
+1. `which clang` to make sure you're using a sufficiently new version of clang.
+
+2. `make fullclean` in the root path of the repository, and rebuild the kernel
+   and schedulers.
+
+3. Rebuild the kernel, and then your example schedulers.
+
+The schedulers are also cleaned if you invoke `make mrproper` in the root
+directory of the tree.
+
+### Stale kernel build / incomplete vmlinux.h file
+
+As described above, you'll need a `vmlinux.h` file that was generated from a
+vmlinux built with BTF, and with sched_ext support enabled. If you don't,
+you'll see errors such as the following which indicate that a type being
+referenced in a scheduler is unknown:
+
+```
+/path/to/sched_ext/tools/sched_ext/user_exit_info.h:25:23: note: forward declaration of 'struct scx_exit_info'
+
+const struct scx_exit_info *ei)
+
+^
+```
+
+In order to resolve this, please follow the steps above in
+[Getting a vmlinux.h file](#getting-a-vmlinuxh-file) in order to ensure your
+schedulers are using a vmlinux.h file that includes the requisite types.
+
+## Misc
+
+### llvm: [OFF]
+
+You may see the following output when building the schedulers:
+
+```
+Auto-detecting system features:
+...                         clang-bpf-co-re: [ on  ]
+...                                    llvm: [ OFF ]
+...                                  libcap: [ on  ]
+...                                  libbfd: [ on  ]
+```
+
+Seeing `llvm: [ OFF ]` here is not an issue. You can safely ignore.
diff --git a/tools/sched_ext/include/bpf-compat/gnu/stubs.h b/tools/sched_ext/include/bpf-compat/gnu/stubs.h
new file mode 100644
index 000000000000..ad7d139ce907
--- /dev/null
+++ b/tools/sched_ext/include/bpf-compat/gnu/stubs.h
@@ -0,0 +1,11 @@
+/*
+ * Dummy gnu/stubs.h. clang can end up including /usr/include/gnu/stubs.h when
+ * compiling BPF files although its content doesn't play any role. The file in
+ * turn includes stubs-64.h or stubs-32.h depending on whether __x86_64__ is
+ * defined. When compiling a BPF source, __x86_64__ isn't set and thus
+ * stubs-32.h is selected. However, the file is not there if the system doesn't
+ * have 32bit glibc devel package installed leading to a build failure.
+ *
+ * The problem is worked around by making this file available in the include
+ * search paths before the system one when building BPF.
+ */
diff --git a/tools/sched_ext/include/scx/common.bpf.h b/tools/sched_ext/include/scx/common.bpf.h
new file mode 100644
index 000000000000..225f61f9bfca
--- /dev/null
+++ b/tools/sched_ext/include/scx/common.bpf.h
@@ -0,0 +1,427 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#ifndef __SCX_COMMON_BPF_H
+#define __SCX_COMMON_BPF_H
+
+#ifdef LSP
+#define __bpf__
+#include "../vmlinux/vmlinux.h"
+#else
+#include "vmlinux.h"
+#endif
+
+#include <bpf/bpf_helpers.h>
+#include <bpf/bpf_tracing.h>
+#include <asm-generic/errno.h>
+#include "user_exit_info.h"
+
+#define PF_WQ_WORKER			0x00000020	/* I'm a workqueue worker */
+#define PF_KTHREAD			0x00200000	/* I am a kernel thread */
+#define PF_EXITING			0x00000004
+#define CLOCK_MONOTONIC			1
+
+/*
+ * Earlier versions of clang/pahole lost upper 32bits in 64bit enums which can
+ * lead to really confusing misbehaviors. Let's trigger a build failure.
+ */
+static inline void ___vmlinux_h_sanity_check___(void)
+{
+	_Static_assert(SCX_DSQ_FLAG_BUILTIN,
+		       "bpftool generated vmlinux.h is missing high bits for 64bit enums, upgrade clang and pahole");
+}
+
+s32 scx_bpf_create_dsq(u64 dsq_id, s32 node) __ksym;
+s32 scx_bpf_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flags, bool *is_idle) __ksym;
+void scx_bpf_dispatch(struct task_struct *p, u64 dsq_id, u64 slice, u64 enq_flags) __ksym;
+void scx_bpf_dispatch_vtime(struct task_struct *p, u64 dsq_id, u64 slice, u64 vtime, u64 enq_flags) __ksym;
+u32 scx_bpf_dispatch_nr_slots(void) __ksym;
+void scx_bpf_dispatch_cancel(void) __ksym;
+bool scx_bpf_consume(u64 dsq_id) __ksym;
+void scx_bpf_dispatch_from_dsq_set_slice(struct bpf_iter_scx_dsq *it__iter, u64 slice) __ksym;
+void scx_bpf_dispatch_from_dsq_set_vtime(struct bpf_iter_scx_dsq *it__iter, u64 vtime) __ksym;
+bool scx_bpf_dispatch_from_dsq(struct bpf_iter_scx_dsq *it__iter, struct task_struct *p, u64 dsq_id, u64 enq_flags) __ksym __weak;
+bool scx_bpf_dispatch_vtime_from_dsq(struct bpf_iter_scx_dsq *it__iter, struct task_struct *p, u64 dsq_id, u64 enq_flags) __ksym __weak;
+u32 scx_bpf_reenqueue_local(void) __ksym;
+void scx_bpf_kick_cpu(s32 cpu, u64 flags) __ksym;
+s32 scx_bpf_dsq_nr_queued(u64 dsq_id) __ksym;
+void scx_bpf_destroy_dsq(u64 dsq_id) __ksym;
+int bpf_iter_scx_dsq_new(struct bpf_iter_scx_dsq *it, u64 dsq_id, u64 flags) __ksym __weak;
+struct task_struct *bpf_iter_scx_dsq_next(struct bpf_iter_scx_dsq *it) __ksym __weak;
+void bpf_iter_scx_dsq_destroy(struct bpf_iter_scx_dsq *it) __ksym __weak;
+void scx_bpf_exit_bstr(s64 exit_code, char *fmt, unsigned long long *data, u32 data__sz) __ksym __weak;
+void scx_bpf_error_bstr(char *fmt, unsigned long long *data, u32 data_len) __ksym;
+void scx_bpf_dump_bstr(char *fmt, unsigned long long *data, u32 data_len) __ksym __weak;
+u32 scx_bpf_cpuperf_cap(s32 cpu) __ksym __weak;
+u32 scx_bpf_cpuperf_cur(s32 cpu) __ksym __weak;
+void scx_bpf_cpuperf_set(s32 cpu, u32 perf) __ksym __weak;
+u32 scx_bpf_nr_cpu_ids(void) __ksym __weak;
+const struct cpumask *scx_bpf_get_possible_cpumask(void) __ksym __weak;
+const struct cpumask *scx_bpf_get_online_cpumask(void) __ksym __weak;
+void scx_bpf_put_cpumask(const struct cpumask *cpumask) __ksym __weak;
+const struct cpumask *scx_bpf_get_idle_cpumask(void) __ksym;
+const struct cpumask *scx_bpf_get_idle_smtmask(void) __ksym;
+void scx_bpf_put_idle_cpumask(const struct cpumask *cpumask) __ksym;
+bool scx_bpf_test_and_clear_cpu_idle(s32 cpu) __ksym;
+s32 scx_bpf_pick_idle_cpu(const cpumask_t *cpus_allowed, u64 flags) __ksym;
+s32 scx_bpf_pick_any_cpu(const cpumask_t *cpus_allowed, u64 flags) __ksym;
+bool scx_bpf_task_running(const struct task_struct *p) __ksym;
+s32 scx_bpf_task_cpu(const struct task_struct *p) __ksym;
+struct rq *scx_bpf_cpu_rq(s32 cpu) __ksym;
+struct cgroup *scx_bpf_task_cgroup(struct task_struct *p) __ksym;
+
+/*
+ * Use the following as @it__iter when calling
+ * scx_bpf_dispatch[_vtime]_from_dsq() from within bpf_for_each() loops.
+ */
+#define BPF_FOR_EACH_ITER	(&___it)
+
+static inline __attribute__((format(printf, 1, 2)))
+void ___scx_bpf_bstr_format_checker(const char *fmt, ...) {}
+
+/*
+ * Helper macro for initializing the fmt and variadic argument inputs to both
+ * bstr exit kfuncs. Callers to this function should use ___fmt and ___param to
+ * refer to the initialized list of inputs to the bstr kfunc.
+ */
+#define scx_bpf_bstr_preamble(fmt, args...)					\
+	static char ___fmt[] = fmt;						\
+	/*									\
+	 * Note that __param[] must have at least one				\
+	 * element to keep the verifier happy.					\
+	 */									\
+	unsigned long long ___param[___bpf_narg(args) ?: 1] = {};		\
+										\
+	_Pragma("GCC diagnostic push")						\
+	_Pragma("GCC diagnostic ignored \"-Wint-conversion\"")			\
+	___bpf_fill(___param, args);						\
+	_Pragma("GCC diagnostic pop")						\
+
+/*
+ * scx_bpf_exit() wraps the scx_bpf_exit_bstr() kfunc with variadic arguments
+ * instead of an array of u64. Using this macro will cause the scheduler to
+ * exit cleanly with the specified exit code being passed to user space.
+ */
+#define scx_bpf_exit(code, fmt, args...)					\
+({										\
+	scx_bpf_bstr_preamble(fmt, args)					\
+	scx_bpf_exit_bstr(code, ___fmt, ___param, sizeof(___param));		\
+	___scx_bpf_bstr_format_checker(fmt, ##args);				\
+})
+
+/*
+ * scx_bpf_error() wraps the scx_bpf_error_bstr() kfunc with variadic arguments
+ * instead of an array of u64. Invoking this macro will cause the scheduler to
+ * exit in an erroneous state, with diagnostic information being passed to the
+ * user.
+ */
+#define scx_bpf_error(fmt, args...)						\
+({										\
+	scx_bpf_bstr_preamble(fmt, args)					\
+	scx_bpf_error_bstr(___fmt, ___param, sizeof(___param));			\
+	___scx_bpf_bstr_format_checker(fmt, ##args);				\
+})
+
+/*
+ * scx_bpf_dump() wraps the scx_bpf_dump_bstr() kfunc with variadic arguments
+ * instead of an array of u64. To be used from ops.dump() and friends.
+ */
+#define scx_bpf_dump(fmt, args...)						\
+({										\
+	scx_bpf_bstr_preamble(fmt, args)					\
+	scx_bpf_dump_bstr(___fmt, ___param, sizeof(___param));			\
+	___scx_bpf_bstr_format_checker(fmt, ##args);				\
+})
+
+#define BPF_STRUCT_OPS(name, args...)						\
+SEC("struct_ops/"#name)								\
+BPF_PROG(name, ##args)
+
+#define BPF_STRUCT_OPS_SLEEPABLE(name, args...)					\
+SEC("struct_ops.s/"#name)							\
+BPF_PROG(name, ##args)
+
+/**
+ * RESIZABLE_ARRAY - Generates annotations for an array that may be resized
+ * @elfsec: the data section of the BPF program in which to place the array
+ * @arr: the name of the array
+ *
+ * libbpf has an API for setting map value sizes. Since data sections (i.e.
+ * bss, data, rodata) themselves are maps, a data section can be resized. If
+ * a data section has an array as its last element, the BTF info for that
+ * array will be adjusted so that length of the array is extended to meet the
+ * new length of the data section. This macro annotates an array to have an
+ * element count of one with the assumption that this array can be resized
+ * within the userspace program. It also annotates the section specifier so
+ * this array exists in a custom sub data section which can be resized
+ * independently.
+ *
+ * See RESIZE_ARRAY() for the userspace convenience macro for resizing an
+ * array declared with RESIZABLE_ARRAY().
+ */
+#define RESIZABLE_ARRAY(elfsec, arr) arr[1] SEC("."#elfsec"."#arr)
+
+/**
+ * MEMBER_VPTR - Obtain the verified pointer to a struct or array member
+ * @base: struct or array to index
+ * @member: dereferenced member (e.g. .field, [idx0][idx1], .field[idx0] ...)
+ *
+ * The verifier often gets confused by the instruction sequence the compiler
+ * generates for indexing struct fields or arrays. This macro forces the
+ * compiler to generate a code sequence which first calculates the byte offset,
+ * checks it against the struct or array size and add that byte offset to
+ * generate the pointer to the member to help the verifier.
+ *
+ * Ideally, we want to abort if the calculated offset is out-of-bounds. However,
+ * BPF currently doesn't support abort, so evaluate to %NULL instead. The caller
+ * must check for %NULL and take appropriate action to appease the verifier. To
+ * avoid confusing the verifier, it's best to check for %NULL and dereference
+ * immediately.
+ *
+ *	vptr = MEMBER_VPTR(my_array, [i][j]);
+ *	if (!vptr)
+ *		return error;
+ *	*vptr = new_value;
+ *
+ * sizeof(@base) should encompass the memory area to be accessed and thus can't
+ * be a pointer to the area. Use `MEMBER_VPTR(*ptr, .member)` instead of
+ * `MEMBER_VPTR(ptr, ->member)`.
+ */
+#define MEMBER_VPTR(base, member) (typeof((base) member) *)			\
+({										\
+	u64 __base = (u64)&(base);						\
+	u64 __addr = (u64)&((base) member) - __base;				\
+	_Static_assert(sizeof(base) >= sizeof((base) member),			\
+		       "@base is smaller than @member, is @base a pointer?");	\
+	asm volatile (								\
+		"if %0 <= %[max] goto +2\n"					\
+		"%0 = 0\n"							\
+		"goto +1\n"							\
+		"%0 += %1\n"							\
+		: "+r"(__addr)							\
+		: "r"(__base),							\
+		  [max]"i"(sizeof(base) - sizeof((base) member)));		\
+	__addr;									\
+})
+
+/**
+ * ARRAY_ELEM_PTR - Obtain the verified pointer to an array element
+ * @arr: array to index into
+ * @i: array index
+ * @n: number of elements in array
+ *
+ * Similar to MEMBER_VPTR() but is intended for use with arrays where the
+ * element count needs to be explicit.
+ * It can be used in cases where a global array is defined with an initial
+ * size but is intended to be be resized before loading the BPF program.
+ * Without this version of the macro, MEMBER_VPTR() will use the compile time
+ * size of the array to compute the max, which will result in rejection by
+ * the verifier.
+ */
+#define ARRAY_ELEM_PTR(arr, i, n) (typeof(arr[i]) *)				\
+({										\
+	u64 __base = (u64)arr;							\
+	u64 __addr = (u64)&(arr[i]) - __base;					\
+	asm volatile (								\
+		"if %0 <= %[max] goto +2\n"					\
+		"%0 = 0\n"							\
+		"goto +1\n"							\
+		"%0 += %1\n"							\
+		: "+r"(__addr)							\
+		: "r"(__base),							\
+		  [max]"r"(sizeof(arr[0]) * ((n) - 1)));			\
+	__addr;									\
+})
+
+
+/*
+ * BPF declarations and helpers
+ */
+
+/* list and rbtree */
+#define __contains(name, node) __attribute__((btf_decl_tag("contains:" #name ":" #node)))
+#define private(name) SEC(".data." #name) __hidden __attribute__((aligned(8)))
+
+void *bpf_obj_new_impl(__u64 local_type_id, void *meta) __ksym;
+void bpf_obj_drop_impl(void *kptr, void *meta) __ksym;
+
+#define bpf_obj_new(type) ((type *)bpf_obj_new_impl(bpf_core_type_id_local(type), NULL))
+#define bpf_obj_drop(kptr) bpf_obj_drop_impl(kptr, NULL)
+
+void bpf_list_push_front(struct bpf_list_head *head, struct bpf_list_node *node) __ksym;
+void bpf_list_push_back(struct bpf_list_head *head, struct bpf_list_node *node) __ksym;
+struct bpf_list_node *bpf_list_pop_front(struct bpf_list_head *head) __ksym;
+struct bpf_list_node *bpf_list_pop_back(struct bpf_list_head *head) __ksym;
+struct bpf_rb_node *bpf_rbtree_remove(struct bpf_rb_root *root,
+				      struct bpf_rb_node *node) __ksym;
+int bpf_rbtree_add_impl(struct bpf_rb_root *root, struct bpf_rb_node *node,
+			bool (less)(struct bpf_rb_node *a, const struct bpf_rb_node *b),
+			void *meta, __u64 off) __ksym;
+#define bpf_rbtree_add(head, node, less) bpf_rbtree_add_impl(head, node, less, NULL, 0)
+
+struct bpf_rb_node *bpf_rbtree_first(struct bpf_rb_root *root) __ksym;
+
+void *bpf_refcount_acquire_impl(void *kptr, void *meta) __ksym;
+#define bpf_refcount_acquire(kptr) bpf_refcount_acquire_impl(kptr, NULL)
+
+/* task */
+struct task_struct *bpf_task_from_pid(s32 pid) __ksym;
+struct task_struct *bpf_task_acquire(struct task_struct *p) __ksym;
+void bpf_task_release(struct task_struct *p) __ksym;
+
+/* cgroup */
+struct cgroup *bpf_cgroup_ancestor(struct cgroup *cgrp, int level) __ksym;
+void bpf_cgroup_release(struct cgroup *cgrp) __ksym;
+struct cgroup *bpf_cgroup_from_id(u64 cgid) __ksym;
+
+/* css iteration */
+struct bpf_iter_css;
+struct cgroup_subsys_state;
+extern int bpf_iter_css_new(struct bpf_iter_css *it,
+			    struct cgroup_subsys_state *start,
+			    unsigned int flags) __weak __ksym;
+extern struct cgroup_subsys_state *
+bpf_iter_css_next(struct bpf_iter_css *it) __weak __ksym;
+extern void bpf_iter_css_destroy(struct bpf_iter_css *it) __weak __ksym;
+
+/* cpumask */
+struct bpf_cpumask *bpf_cpumask_create(void) __ksym;
+struct bpf_cpumask *bpf_cpumask_acquire(struct bpf_cpumask *cpumask) __ksym;
+void bpf_cpumask_release(struct bpf_cpumask *cpumask) __ksym;
+u32 bpf_cpumask_first(const struct cpumask *cpumask) __ksym;
+u32 bpf_cpumask_first_zero(const struct cpumask *cpumask) __ksym;
+void bpf_cpumask_set_cpu(u32 cpu, struct bpf_cpumask *cpumask) __ksym;
+void bpf_cpumask_clear_cpu(u32 cpu, struct bpf_cpumask *cpumask) __ksym;
+bool bpf_cpumask_test_cpu(u32 cpu, const struct cpumask *cpumask) __ksym;
+bool bpf_cpumask_test_and_set_cpu(u32 cpu, struct bpf_cpumask *cpumask) __ksym;
+bool bpf_cpumask_test_and_clear_cpu(u32 cpu, struct bpf_cpumask *cpumask) __ksym;
+void bpf_cpumask_setall(struct bpf_cpumask *cpumask) __ksym;
+void bpf_cpumask_clear(struct bpf_cpumask *cpumask) __ksym;
+bool bpf_cpumask_and(struct bpf_cpumask *dst, const struct cpumask *src1,
+		     const struct cpumask *src2) __ksym;
+void bpf_cpumask_or(struct bpf_cpumask *dst, const struct cpumask *src1,
+		    const struct cpumask *src2) __ksym;
+void bpf_cpumask_xor(struct bpf_cpumask *dst, const struct cpumask *src1,
+		     const struct cpumask *src2) __ksym;
+bool bpf_cpumask_equal(const struct cpumask *src1, const struct cpumask *src2) __ksym;
+bool bpf_cpumask_intersects(const struct cpumask *src1, const struct cpumask *src2) __ksym;
+bool bpf_cpumask_subset(const struct cpumask *src1, const struct cpumask *src2) __ksym;
+bool bpf_cpumask_empty(const struct cpumask *cpumask) __ksym;
+bool bpf_cpumask_full(const struct cpumask *cpumask) __ksym;
+void bpf_cpumask_copy(struct bpf_cpumask *dst, const struct cpumask *src) __ksym;
+u32 bpf_cpumask_any_distribute(const struct cpumask *cpumask) __ksym;
+u32 bpf_cpumask_any_and_distribute(const struct cpumask *src1,
+				   const struct cpumask *src2) __ksym;
+u32 bpf_cpumask_weight(const struct cpumask *cpumask) __ksym;
+
+/*
+ * Access a cpumask in read-only mode (typically to check bits).
+ */
+const struct cpumask *cast_mask(struct bpf_cpumask *mask)
+{
+	return (const struct cpumask *)mask;
+}
+
+/* rcu */
+void bpf_rcu_read_lock(void) __ksym;
+void bpf_rcu_read_unlock(void) __ksym;
+
+
+/*
+ * Other helpers
+ */
+
+/* useful compiler attributes */
+#define likely(x) __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#define __maybe_unused __attribute__((__unused__))
+
+/*
+ * READ/WRITE_ONCE() are from kernel (include/asm-generic/rwonce.h). They
+ * prevent compiler from caching, redoing or reordering reads or writes.
+ */
+typedef __u8  __attribute__((__may_alias__))  __u8_alias_t;
+typedef __u16 __attribute__((__may_alias__)) __u16_alias_t;
+typedef __u32 __attribute__((__may_alias__)) __u32_alias_t;
+typedef __u64 __attribute__((__may_alias__)) __u64_alias_t;
+
+static __always_inline void __read_once_size(const volatile void *p, void *res, int size)
+{
+	switch (size) {
+	case 1: *(__u8_alias_t  *) res = *(volatile __u8_alias_t  *) p; break;
+	case 2: *(__u16_alias_t *) res = *(volatile __u16_alias_t *) p; break;
+	case 4: *(__u32_alias_t *) res = *(volatile __u32_alias_t *) p; break;
+	case 8: *(__u64_alias_t *) res = *(volatile __u64_alias_t *) p; break;
+	default:
+		barrier();
+		__builtin_memcpy((void *)res, (const void *)p, size);
+		barrier();
+	}
+}
+
+static __always_inline void __write_once_size(volatile void *p, void *res, int size)
+{
+	switch (size) {
+	case 1: *(volatile  __u8_alias_t *) p = *(__u8_alias_t  *) res; break;
+	case 2: *(volatile __u16_alias_t *) p = *(__u16_alias_t *) res; break;
+	case 4: *(volatile __u32_alias_t *) p = *(__u32_alias_t *) res; break;
+	case 8: *(volatile __u64_alias_t *) p = *(__u64_alias_t *) res; break;
+	default:
+		barrier();
+		__builtin_memcpy((void *)p, (const void *)res, size);
+		barrier();
+	}
+}
+
+#define READ_ONCE(x)					\
+({							\
+	union { typeof(x) __val; char __c[1]; } __u =	\
+		{ .__c = { 0 } };			\
+	__read_once_size(&(x), __u.__c, sizeof(x));	\
+	__u.__val;					\
+})
+
+#define WRITE_ONCE(x, val)				\
+({							\
+	union { typeof(x) __val; char __c[1]; } __u =	\
+		{ .__val = (val) }; 			\
+	__write_once_size(&(x), __u.__c, sizeof(x));	\
+	__u.__val;					\
+})
+
+/*
+ * log2_u32 - Compute the base 2 logarithm of a 32-bit exponential value.
+ * @v: The value for which we're computing the base 2 logarithm.
+ */
+static inline u32 log2_u32(u32 v)
+{
+        u32 r;
+        u32 shift;
+
+        r = (v > 0xFFFF) << 4; v >>= r;
+        shift = (v > 0xFF) << 3; v >>= shift; r |= shift;
+        shift = (v > 0xF) << 2; v >>= shift; r |= shift;
+        shift = (v > 0x3) << 1; v >>= shift; r |= shift;
+        r |= (v >> 1);
+        return r;
+}
+
+/*
+ * log2_u64 - Compute the base 2 logarithm of a 64-bit exponential value.
+ * @v: The value for which we're computing the base 2 logarithm.
+ */
+static inline u32 log2_u64(u64 v)
+{
+        u32 hi = v >> 32;
+        if (hi)
+                return log2_u32(hi) + 32 + 1;
+        else
+                return log2_u32(v) + 1;
+}
+
+#include "compat.bpf.h"
+
+#endif	/* __SCX_COMMON_BPF_H */
diff --git a/tools/sched_ext/include/scx/common.h b/tools/sched_ext/include/scx/common.h
new file mode 100644
index 000000000000..5b0f90152152
--- /dev/null
+++ b/tools/sched_ext/include/scx/common.h
@@ -0,0 +1,75 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ */
+#ifndef __SCHED_EXT_COMMON_H
+#define __SCHED_EXT_COMMON_H
+
+#ifdef __KERNEL__
+#error "Should not be included by BPF programs"
+#endif
+
+#include <stdarg.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <errno.h>
+
+typedef uint8_t u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+
+#define SCX_BUG(__fmt, ...)							\
+	do {									\
+		fprintf(stderr, "[SCX_BUG] %s:%d", __FILE__, __LINE__);		\
+		if (errno)							\
+			fprintf(stderr, " (%s)\n", strerror(errno));		\
+		else								\
+			fprintf(stderr, "\n");					\
+		fprintf(stderr, __fmt __VA_OPT__(,) __VA_ARGS__);		\
+		fprintf(stderr, "\n");						\
+										\
+		exit(EXIT_FAILURE);						\
+	} while (0)
+
+#define SCX_BUG_ON(__cond, __fmt, ...)					\
+	do {								\
+		if (__cond)						\
+			SCX_BUG((__fmt) __VA_OPT__(,) __VA_ARGS__);	\
+	} while (0)
+
+/**
+ * RESIZE_ARRAY - Convenience macro for resizing a BPF array
+ * @__skel: the skeleton containing the array
+ * @elfsec: the data section of the BPF program in which the array exists
+ * @arr: the name of the array
+ * @n: the desired array element count
+ *
+ * For BPF arrays declared with RESIZABLE_ARRAY(), this macro performs two
+ * operations. It resizes the map which corresponds to the custom data
+ * section that contains the target array. As a side effect, the BTF info for
+ * the array is adjusted so that the array length is sized to cover the new
+ * data section size. The second operation is reassigning the skeleton pointer
+ * for that custom data section so that it points to the newly memory mapped
+ * region.
+ */
+#define RESIZE_ARRAY(__skel, elfsec, arr, n)						\
+	do {										\
+		size_t __sz;								\
+		bpf_map__set_value_size((__skel)->maps.elfsec##_##arr,			\
+				sizeof((__skel)->elfsec##_##arr->arr[0]) * (n));	\
+		(__skel)->elfsec##_##arr =						\
+			bpf_map__initial_value((__skel)->maps.elfsec##_##arr, &__sz);	\
+	} while (0)
+
+#include "user_exit_info.h"
+#include "compat.h"
+
+#endif	/* __SCHED_EXT_COMMON_H */
diff --git a/tools/sched_ext/include/scx/compat.bpf.h b/tools/sched_ext/include/scx/compat.bpf.h
new file mode 100644
index 000000000000..e5afe9efd3f3
--- /dev/null
+++ b/tools/sched_ext/include/scx/compat.bpf.h
@@ -0,0 +1,47 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#ifndef __SCX_COMPAT_BPF_H
+#define __SCX_COMPAT_BPF_H
+
+#define __COMPAT_ENUM_OR_ZERO(__type, __ent)					\
+({										\
+	__type __ret = 0;							\
+	if (bpf_core_enum_value_exists(__type, __ent))				\
+		__ret = __ent;							\
+	__ret;									\
+})
+
+/* v6.12: 819513666966 ("sched_ext: Add cgroup support") */
+#define __COMPAT_scx_bpf_task_cgroup(p)						\
+	(bpf_ksym_exists(scx_bpf_task_cgroup) ?					\
+	 scx_bpf_task_cgroup((p)) : NULL)
+
+/* v6.12: 4c30f5ce4f7a ("sched_ext: Implement scx_bpf_dispatch[_vtime]_from_dsq()") */
+#define __COMPAT_scx_bpf_dispatch_from_dsq_set_slice(it, slice)			\
+	(bpf_ksym_exists(scx_bpf_dispatch_from_dsq_set_slice) ?			\
+	 scx_bpf_dispatch_from_dsq_set_slice((it), (slice)) : (void)0)
+#define __COMPAT_scx_bpf_dispatch_from_dsq_set_vtime(it, vtime)			\
+	(bpf_ksym_exists(scx_bpf_dispatch_from_dsq_set_vtime) ?			\
+	 scx_bpf_dispatch_from_dsq_set_vtime((it), (vtime)) : (void)0)
+#define __COMPAT_scx_bpf_dispatch_from_dsq(it, p, dsq_id, enq_flags)		\
+	(bpf_ksym_exists(scx_bpf_dispatch_from_dsq) ?				\
+	 scx_bpf_dispatch_from_dsq((it), (p), (dsq_id), (enq_flags)) : false)
+#define __COMPAT_scx_bpf_dispatch_vtime_from_dsq(it, p, dsq_id, enq_flags)	\
+	(bpf_ksym_exists(scx_bpf_dispatch_vtime_from_dsq) ?			\
+	 scx_bpf_dispatch_vtime_from_dsq((it), (p), (dsq_id), (enq_flags)) : false)
+
+/*
+ * Define sched_ext_ops. This may be expanded to define multiple variants for
+ * backward compatibility. See compat.h::SCX_OPS_LOAD/ATTACH().
+ */
+#define SCX_OPS_DEFINE(__name, ...)						\
+	SEC(".struct_ops.link")							\
+	struct sched_ext_ops __name = {						\
+		__VA_ARGS__,							\
+	};
+
+#endif	/* __SCX_COMPAT_BPF_H */
diff --git a/tools/sched_ext/include/scx/compat.h b/tools/sched_ext/include/scx/compat.h
new file mode 100644
index 000000000000..cc56ff9aa252
--- /dev/null
+++ b/tools/sched_ext/include/scx/compat.h
@@ -0,0 +1,186 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#ifndef __SCX_COMPAT_H
+#define __SCX_COMPAT_H
+
+#include <bpf/btf.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include <unistd.h>
+
+struct btf *__COMPAT_vmlinux_btf __attribute__((weak));
+
+static inline void __COMPAT_load_vmlinux_btf(void)
+{
+	if (!__COMPAT_vmlinux_btf) {
+		__COMPAT_vmlinux_btf = btf__load_vmlinux_btf();
+		SCX_BUG_ON(!__COMPAT_vmlinux_btf, "btf__load_vmlinux_btf()");
+	}
+}
+
+static inline bool __COMPAT_read_enum(const char *type, const char *name, u64 *v)
+{
+	const struct btf_type *t;
+	const char *n;
+	s32 tid;
+	int i;
+
+	__COMPAT_load_vmlinux_btf();
+
+	tid = btf__find_by_name(__COMPAT_vmlinux_btf, type);
+	if (tid < 0)
+		return false;
+
+	t = btf__type_by_id(__COMPAT_vmlinux_btf, tid);
+	SCX_BUG_ON(!t, "btf__type_by_id(%d)", tid);
+
+	if (btf_is_enum(t)) {
+		struct btf_enum *e = btf_enum(t);
+
+		for (i = 0; i < BTF_INFO_VLEN(t->info); i++) {
+			n = btf__name_by_offset(__COMPAT_vmlinux_btf, e[i].name_off);
+			SCX_BUG_ON(!n, "btf__name_by_offset()");
+			if (!strcmp(n, name)) {
+				*v = e[i].val;
+				return true;
+			}
+		}
+	} else if (btf_is_enum64(t)) {
+		struct btf_enum64 *e = btf_enum64(t);
+
+		for (i = 0; i < BTF_INFO_VLEN(t->info); i++) {
+			n = btf__name_by_offset(__COMPAT_vmlinux_btf, e[i].name_off);
+			SCX_BUG_ON(!n, "btf__name_by_offset()");
+			if (!strcmp(n, name)) {
+				*v = btf_enum64_value(&e[i]);
+				return true;
+			}
+		}
+	}
+
+	return false;
+}
+
+#define __COMPAT_ENUM_OR_ZERO(__type, __ent)					\
+({										\
+	u64 __val = 0;								\
+	__COMPAT_read_enum(__type, __ent, &__val);				\
+	__val;									\
+})
+
+static inline bool __COMPAT_has_ksym(const char *ksym)
+{
+	__COMPAT_load_vmlinux_btf();
+	return btf__find_by_name(__COMPAT_vmlinux_btf, ksym) >= 0;
+}
+
+static inline bool __COMPAT_struct_has_field(const char *type, const char *field)
+{
+	const struct btf_type *t;
+	const struct btf_member *m;
+	const char *n;
+	s32 tid;
+	int i;
+
+	__COMPAT_load_vmlinux_btf();
+	tid = btf__find_by_name_kind(__COMPAT_vmlinux_btf, type, BTF_KIND_STRUCT);
+	if (tid < 0)
+		return false;
+
+	t = btf__type_by_id(__COMPAT_vmlinux_btf, tid);
+	SCX_BUG_ON(!t, "btf__type_by_id(%d)", tid);
+
+	m = btf_members(t);
+
+	for (i = 0; i < BTF_INFO_VLEN(t->info); i++) {
+		n = btf__name_by_offset(__COMPAT_vmlinux_btf, m[i].name_off);
+		SCX_BUG_ON(!n, "btf__name_by_offset()");
+			if (!strcmp(n, field))
+				return true;
+	}
+
+	return false;
+}
+
+#define SCX_OPS_SWITCH_PARTIAL							\
+	__COMPAT_ENUM_OR_ZERO("scx_ops_flags", "SCX_OPS_SWITCH_PARTIAL")
+
+static inline long scx_hotplug_seq(void)
+{
+	int fd;
+	char buf[32];
+	ssize_t len;
+	long val;
+
+	fd = open("/sys/kernel/sched_ext/hotplug_seq", O_RDONLY);
+	if (fd < 0)
+		return -ENOENT;
+
+	len = read(fd, buf, sizeof(buf) - 1);
+	SCX_BUG_ON(len <= 0, "read failed (%ld)", len);
+	buf[len] = 0;
+	close(fd);
+
+	val = strtoul(buf, NULL, 10);
+	SCX_BUG_ON(val < 0, "invalid num hotplug events: %lu", val);
+
+	return val;
+}
+
+/*
+ * struct sched_ext_ops can change over time. If compat.bpf.h::SCX_OPS_DEFINE()
+ * is used to define ops and compat.h::SCX_OPS_LOAD/ATTACH() are used to load
+ * and attach it, backward compatibility is automatically maintained where
+ * reasonable.
+ *
+ * ec7e3b0463e1 ("implement-ops") in https://github.com/sched-ext/sched_ext is
+ * the current minimum required kernel version.
+ */
+#define SCX_OPS_OPEN(__ops_name, __scx_name) ({					\
+	struct __scx_name *__skel;						\
+										\
+	SCX_BUG_ON(!__COMPAT_struct_has_field("sched_ext_ops", "dump"),		\
+		   "sched_ext_ops.dump() missing, kernel too old?");		\
+										\
+	__skel = __scx_name##__open();						\
+	SCX_BUG_ON(!__skel, "Could not open " #__scx_name);			\
+	__skel->struct_ops.__ops_name->hotplug_seq = scx_hotplug_seq();		\
+	__skel; 								\
+})
+
+#define SCX_OPS_LOAD(__skel, __ops_name, __scx_name, __uei_name) ({		\
+	UEI_SET_SIZE(__skel, __ops_name, __uei_name);				\
+	SCX_BUG_ON(__scx_name##__load((__skel)), "Failed to load skel");	\
+})
+
+/*
+ * New versions of bpftool now emit additional link placeholders for BPF maps,
+ * and set up BPF skeleton in such a way that libbpf will auto-attach BPF maps
+ * automatically, assumming libbpf is recent enough (v1.5+). Old libbpf will do
+ * nothing with those links and won't attempt to auto-attach maps.
+ *
+ * To maintain compatibility with older libbpf while avoiding trying to attach
+ * twice, disable the autoattach feature on newer libbpf.
+ */
+#if LIBBPF_MAJOR_VERSION > 1 ||							\
+	(LIBBPF_MAJOR_VERSION == 1 && LIBBPF_MINOR_VERSION >= 5)
+#define __SCX_OPS_DISABLE_AUTOATTACH(__skel, __ops_name)			\
+	bpf_map__set_autoattach((__skel)->maps.__ops_name, false)
+#else
+#define __SCX_OPS_DISABLE_AUTOATTACH(__skel, __ops_name) do {} while (0)
+#endif
+
+#define SCX_OPS_ATTACH(__skel, __ops_name, __scx_name) ({			\
+	struct bpf_link *__link;						\
+	__SCX_OPS_DISABLE_AUTOATTACH(__skel, __ops_name);			\
+	SCX_BUG_ON(__scx_name##__attach((__skel)), "Failed to attach skel");	\
+	__link = bpf_map__attach_struct_ops((__skel)->maps.__ops_name);		\
+	SCX_BUG_ON(!__link, "Failed to attach struct_ops");			\
+	__link;									\
+})
+
+#endif	/* __SCX_COMPAT_H */
diff --git a/tools/sched_ext/include/scx/user_exit_info.h b/tools/sched_ext/include/scx/user_exit_info.h
new file mode 100644
index 000000000000..8ce2734402e1
--- /dev/null
+++ b/tools/sched_ext/include/scx/user_exit_info.h
@@ -0,0 +1,115 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Define struct user_exit_info which is shared between BPF and userspace parts
+ * to communicate exit status and other information.
+ *
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#ifndef __USER_EXIT_INFO_H
+#define __USER_EXIT_INFO_H
+
+enum uei_sizes {
+	UEI_REASON_LEN		= 128,
+	UEI_MSG_LEN		= 1024,
+	UEI_DUMP_DFL_LEN	= 32768,
+};
+
+struct user_exit_info {
+	int		kind;
+	s64		exit_code;
+	char		reason[UEI_REASON_LEN];
+	char		msg[UEI_MSG_LEN];
+};
+
+#ifdef __bpf__
+
+#ifdef LSP
+#include "../vmlinux/vmlinux.h"
+#else
+#include "vmlinux.h"
+#endif
+#include <bpf/bpf_core_read.h>
+
+#define UEI_DEFINE(__name)							\
+	char RESIZABLE_ARRAY(data, __name##_dump);				\
+	const volatile u32 __name##_dump_len;					\
+	struct user_exit_info __name SEC(".data")
+
+#define UEI_RECORD(__uei_name, __ei) ({						\
+	bpf_probe_read_kernel_str(__uei_name.reason,				\
+				  sizeof(__uei_name.reason), (__ei)->reason);	\
+	bpf_probe_read_kernel_str(__uei_name.msg,				\
+				  sizeof(__uei_name.msg), (__ei)->msg);		\
+	bpf_probe_read_kernel_str(__uei_name##_dump,				\
+				  __uei_name##_dump_len, (__ei)->dump);		\
+	if (bpf_core_field_exists((__ei)->exit_code))				\
+		__uei_name.exit_code = (__ei)->exit_code;			\
+	/* use __sync to force memory barrier */				\
+	__sync_val_compare_and_swap(&__uei_name.kind, __uei_name.kind,		\
+				    (__ei)->kind);				\
+})
+
+#else	/* !__bpf__ */
+
+#include <stdio.h>
+#include <stdbool.h>
+
+/* no need to call the following explicitly if SCX_OPS_LOAD() is used */
+#define UEI_SET_SIZE(__skel, __ops_name, __uei_name) ({					\
+	u32 __len = (__skel)->struct_ops.__ops_name->exit_dump_len ?: UEI_DUMP_DFL_LEN;	\
+	(__skel)->rodata->__uei_name##_dump_len = __len;				\
+	RESIZE_ARRAY((__skel), data, __uei_name##_dump, __len);				\
+})
+
+#define UEI_EXITED(__skel, __uei_name) ({					\
+	/* use __sync to force memory barrier */				\
+	__sync_val_compare_and_swap(&(__skel)->data->__uei_name.kind, -1, -1);	\
+})
+
+#define UEI_REPORT(__skel, __uei_name) ({					\
+	struct user_exit_info *__uei = &(__skel)->data->__uei_name;		\
+	char *__uei_dump = (__skel)->data_##__uei_name##_dump->__uei_name##_dump; \
+	if (__uei_dump[0] != '\0') {						\
+		fputs("\nDEBUG DUMP\n", stderr);				\
+		fputs("================================================================================\n\n", stderr); \
+		fputs(__uei_dump, stderr);					\
+		fputs("\n================================================================================\n\n", stderr); \
+	}									\
+	fprintf(stderr, "EXIT: %s", __uei->reason);				\
+	if (__uei->msg[0] != '\0')						\
+		fprintf(stderr, " (%s)", __uei->msg);				\
+	fputs("\n", stderr);							\
+	__uei->exit_code;							\
+})
+
+/*
+ * We can't import vmlinux.h while compiling user C code. Let's duplicate
+ * scx_exit_code definition.
+ */
+enum scx_exit_code {
+	/* Reasons */
+	SCX_ECODE_RSN_HOTPLUG		= 1LLU << 32,
+
+	/* Actions */
+	SCX_ECODE_ACT_RESTART		= 1LLU << 48,
+};
+
+enum uei_ecode_mask {
+	UEI_ECODE_USER_MASK		= ((1LLU << 32) - 1),
+	UEI_ECODE_SYS_RSN_MASK		= ((1LLU << 16) - 1) << 32,
+	UEI_ECODE_SYS_ACT_MASK		= ((1LLU << 16) - 1) << 48,
+};
+
+/*
+ * These macro interpret the ecode returned from UEI_REPORT().
+ */
+#define UEI_ECODE_USER(__ecode)		((__ecode) & UEI_ECODE_USER_MASK)
+#define UEI_ECODE_SYS_RSN(__ecode)	((__ecode) & UEI_ECODE_SYS_RSN_MASK)
+#define UEI_ECODE_SYS_ACT(__ecode)	((__ecode) & UEI_ECODE_SYS_ACT_MASK)
+
+#define UEI_ECODE_RESTART(__ecode)	(UEI_ECODE_SYS_ACT((__ecode)) == SCX_ECODE_ACT_RESTART)
+
+#endif	/* __bpf__ */
+#endif	/* __USER_EXIT_INFO_H */
diff --git a/tools/sched_ext/scx_central.bpf.c b/tools/sched_ext/scx_central.bpf.c
new file mode 100644
index 000000000000..8dd8eb73b6b8
--- /dev/null
+++ b/tools/sched_ext/scx_central.bpf.c
@@ -0,0 +1,361 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A central FIFO sched_ext scheduler which demonstrates the followings:
+ *
+ * a. Making all scheduling decisions from one CPU:
+ *
+ *    The central CPU is the only one making scheduling decisions. All other
+ *    CPUs kick the central CPU when they run out of tasks to run.
+ *
+ *    There is one global BPF queue and the central CPU schedules all CPUs by
+ *    dispatching from the global queue to each CPU's local dsq from dispatch().
+ *    This isn't the most straightforward. e.g. It'd be easier to bounce
+ *    through per-CPU BPF queues. The current design is chosen to maximally
+ *    utilize and verify various SCX mechanisms such as LOCAL_ON dispatching.
+ *
+ * b. Tickless operation
+ *
+ *    All tasks are dispatched with the infinite slice which allows stopping the
+ *    ticks on CONFIG_NO_HZ_FULL kernels running with the proper nohz_full
+ *    parameter. The tickless operation can be observed through
+ *    /proc/interrupts.
+ *
+ *    Periodic switching is enforced by a periodic timer checking all CPUs and
+ *    preempting them as necessary. Unfortunately, BPF timer currently doesn't
+ *    have a way to pin to a specific CPU, so the periodic timer isn't pinned to
+ *    the central CPU.
+ *
+ * c. Preemption
+ *
+ *    Kthreads are unconditionally queued to the head of a matching local dsq
+ *    and dispatched with SCX_DSQ_PREEMPT. This ensures that a kthread is always
+ *    prioritized over user threads, which is required for ensuring forward
+ *    progress as e.g. the periodic timer may run on a ksoftirqd and if the
+ *    ksoftirqd gets starved by a user thread, there may not be anything else to
+ *    vacate that user thread.
+ *
+ *    SCX_KICK_PREEMPT is used to trigger scheduling and CPUs to move to the
+ *    next tasks.
+ *
+ * This scheduler is designed to maximize usage of various SCX mechanisms. A
+ * more practical implementation would likely put the scheduling loop outside
+ * the central CPU's dispatch() path and add some form of priority mechanism.
+ *
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+enum {
+	FALLBACK_DSQ_ID		= 0,
+	MS_TO_NS		= 1000LLU * 1000,
+	TIMER_INTERVAL_NS	= 1 * MS_TO_NS,
+};
+
+const volatile s32 central_cpu;
+const volatile u32 nr_cpu_ids = 1;	/* !0 for veristat, set during init */
+const volatile u64 slice_ns = SCX_SLICE_DFL;
+
+bool timer_pinned = true;
+u64 nr_total, nr_locals, nr_queued, nr_lost_pids;
+u64 nr_timers, nr_dispatches, nr_mismatches, nr_retries;
+u64 nr_overflows;
+
+UEI_DEFINE(uei);
+
+struct {
+	__uint(type, BPF_MAP_TYPE_QUEUE);
+	__uint(max_entries, 4096);
+	__type(value, s32);
+} central_q SEC(".maps");
+
+/* can't use percpu map due to bad lookups */
+bool RESIZABLE_ARRAY(data, cpu_gimme_task);
+u64 RESIZABLE_ARRAY(data, cpu_started_at);
+
+struct central_timer {
+	struct bpf_timer timer;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_ARRAY);
+	__uint(max_entries, 1);
+	__type(key, u32);
+	__type(value, struct central_timer);
+} central_timer SEC(".maps");
+
+static bool vtime_before(u64 a, u64 b)
+{
+	return (s64)(a - b) < 0;
+}
+
+s32 BPF_STRUCT_OPS(central_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	/*
+	 * Steer wakeups to the central CPU as much as possible to avoid
+	 * disturbing other CPUs. It's safe to blindly return the central cpu as
+	 * select_cpu() is a hint and if @p can't be on it, the kernel will
+	 * automatically pick a fallback CPU.
+	 */
+	return central_cpu;
+}
+
+void BPF_STRUCT_OPS(central_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	s32 pid = p->pid;
+
+	__sync_fetch_and_add(&nr_total, 1);
+
+	/*
+	 * Push per-cpu kthreads at the head of local dsq's and preempt the
+	 * corresponding CPU. This ensures that e.g. ksoftirqd isn't blocked
+	 * behind other threads which is necessary for forward progress
+	 * guarantee as we depend on the BPF timer which may run from ksoftirqd.
+	 */
+	if ((p->flags & PF_KTHREAD) && p->nr_cpus_allowed == 1) {
+		__sync_fetch_and_add(&nr_locals, 1);
+		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_INF,
+				 enq_flags | SCX_ENQ_PREEMPT);
+		return;
+	}
+
+	if (bpf_map_push_elem(&central_q, &pid, 0)) {
+		__sync_fetch_and_add(&nr_overflows, 1);
+		scx_bpf_dispatch(p, FALLBACK_DSQ_ID, SCX_SLICE_INF, enq_flags);
+		return;
+	}
+
+	__sync_fetch_and_add(&nr_queued, 1);
+
+	if (!scx_bpf_task_running(p))
+		scx_bpf_kick_cpu(central_cpu, SCX_KICK_PREEMPT);
+}
+
+static bool dispatch_to_cpu(s32 cpu)
+{
+	struct task_struct *p;
+	s32 pid;
+
+	bpf_repeat(BPF_MAX_LOOPS) {
+		if (bpf_map_pop_elem(&central_q, &pid))
+			break;
+
+		__sync_fetch_and_sub(&nr_queued, 1);
+
+		p = bpf_task_from_pid(pid);
+		if (!p) {
+			__sync_fetch_and_add(&nr_lost_pids, 1);
+			continue;
+		}
+
+		/*
+		 * If we can't run the task at the top, do the dumb thing and
+		 * bounce it to the fallback dsq.
+		 */
+		if (!bpf_cpumask_test_cpu(cpu, p->cpus_ptr)) {
+			__sync_fetch_and_add(&nr_mismatches, 1);
+			scx_bpf_dispatch(p, FALLBACK_DSQ_ID, SCX_SLICE_INF, 0);
+			bpf_task_release(p);
+			/*
+			 * We might run out of dispatch buffer slots if we continue dispatching
+			 * to the fallback DSQ, without dispatching to the local DSQ of the
+			 * target CPU. In such a case, break the loop now as will fail the
+			 * next dispatch operation.
+			 */
+			if (!scx_bpf_dispatch_nr_slots())
+				break;
+			continue;
+		}
+
+		/* dispatch to local and mark that @cpu doesn't need more */
+		scx_bpf_dispatch(p, SCX_DSQ_LOCAL_ON | cpu, SCX_SLICE_INF, 0);
+
+		if (cpu != central_cpu)
+			scx_bpf_kick_cpu(cpu, SCX_KICK_IDLE);
+
+		bpf_task_release(p);
+		return true;
+	}
+
+	return false;
+}
+
+void BPF_STRUCT_OPS(central_dispatch, s32 cpu, struct task_struct *prev)
+{
+	if (cpu == central_cpu) {
+		/* dispatch for all other CPUs first */
+		__sync_fetch_and_add(&nr_dispatches, 1);
+
+		bpf_for(cpu, 0, nr_cpu_ids) {
+			bool *gimme;
+
+			if (!scx_bpf_dispatch_nr_slots())
+				break;
+
+			/* central's gimme is never set */
+			gimme = ARRAY_ELEM_PTR(cpu_gimme_task, cpu, nr_cpu_ids);
+			if (!gimme || !*gimme)
+				continue;
+
+			if (dispatch_to_cpu(cpu))
+				*gimme = false;
+		}
+
+		/*
+		 * Retry if we ran out of dispatch buffer slots as we might have
+		 * skipped some CPUs and also need to dispatch for self. The ext
+		 * core automatically retries if the local dsq is empty but we
+		 * can't rely on that as we're dispatching for other CPUs too.
+		 * Kick self explicitly to retry.
+		 */
+		if (!scx_bpf_dispatch_nr_slots()) {
+			__sync_fetch_and_add(&nr_retries, 1);
+			scx_bpf_kick_cpu(central_cpu, SCX_KICK_PREEMPT);
+			return;
+		}
+
+		/* look for a task to run on the central CPU */
+		if (scx_bpf_consume(FALLBACK_DSQ_ID))
+			return;
+		dispatch_to_cpu(central_cpu);
+	} else {
+		bool *gimme;
+
+		if (scx_bpf_consume(FALLBACK_DSQ_ID))
+			return;
+
+		gimme = ARRAY_ELEM_PTR(cpu_gimme_task, cpu, nr_cpu_ids);
+		if (gimme)
+			*gimme = true;
+
+		/*
+		 * Force dispatch on the scheduling CPU so that it finds a task
+		 * to run for us.
+		 */
+		scx_bpf_kick_cpu(central_cpu, SCX_KICK_PREEMPT);
+	}
+}
+
+void BPF_STRUCT_OPS(central_running, struct task_struct *p)
+{
+	s32 cpu = scx_bpf_task_cpu(p);
+	u64 *started_at = ARRAY_ELEM_PTR(cpu_started_at, cpu, nr_cpu_ids);
+	if (started_at)
+		*started_at = bpf_ktime_get_ns() ?: 1;	/* 0 indicates idle */
+}
+
+void BPF_STRUCT_OPS(central_stopping, struct task_struct *p, bool runnable)
+{
+	s32 cpu = scx_bpf_task_cpu(p);
+	u64 *started_at = ARRAY_ELEM_PTR(cpu_started_at, cpu, nr_cpu_ids);
+	if (started_at)
+		*started_at = 0;
+}
+
+static int central_timerfn(void *map, int *key, struct bpf_timer *timer)
+{
+	u64 now = bpf_ktime_get_ns();
+	u64 nr_to_kick = nr_queued;
+	s32 i, curr_cpu;
+
+	curr_cpu = bpf_get_smp_processor_id();
+	if (timer_pinned && (curr_cpu != central_cpu)) {
+		scx_bpf_error("Central timer ran on CPU %d, not central CPU %d",
+			      curr_cpu, central_cpu);
+		return 0;
+	}
+
+	bpf_for(i, 0, nr_cpu_ids) {
+		s32 cpu = (nr_timers + i) % nr_cpu_ids;
+		u64 *started_at;
+
+		if (cpu == central_cpu)
+			continue;
+
+		/* kick iff the current one exhausted its slice */
+		started_at = ARRAY_ELEM_PTR(cpu_started_at, cpu, nr_cpu_ids);
+		if (started_at && *started_at &&
+		    vtime_before(now, *started_at + slice_ns))
+			continue;
+
+		/* and there's something pending */
+		if (scx_bpf_dsq_nr_queued(FALLBACK_DSQ_ID) ||
+		    scx_bpf_dsq_nr_queued(SCX_DSQ_LOCAL_ON | cpu))
+			;
+		else if (nr_to_kick)
+			nr_to_kick--;
+		else
+			continue;
+
+		scx_bpf_kick_cpu(cpu, SCX_KICK_PREEMPT);
+	}
+
+	bpf_timer_start(timer, TIMER_INTERVAL_NS, BPF_F_TIMER_CPU_PIN);
+	__sync_fetch_and_add(&nr_timers, 1);
+	return 0;
+}
+
+int BPF_STRUCT_OPS_SLEEPABLE(central_init)
+{
+	u32 key = 0;
+	struct bpf_timer *timer;
+	int ret;
+
+	ret = scx_bpf_create_dsq(FALLBACK_DSQ_ID, -1);
+	if (ret)
+		return ret;
+
+	timer = bpf_map_lookup_elem(&central_timer, &key);
+	if (!timer)
+		return -ESRCH;
+
+	if (bpf_get_smp_processor_id() != central_cpu) {
+		scx_bpf_error("init from non-central CPU");
+		return -EINVAL;
+	}
+
+	bpf_timer_init(timer, &central_timer, CLOCK_MONOTONIC);
+	bpf_timer_set_callback(timer, central_timerfn);
+
+	ret = bpf_timer_start(timer, TIMER_INTERVAL_NS, BPF_F_TIMER_CPU_PIN);
+	/*
+	 * BPF_F_TIMER_CPU_PIN is pretty new (>=6.7). If we're running in a
+	 * kernel which doesn't have it, bpf_timer_start() will return -EINVAL.
+	 * Retry without the PIN. This would be the perfect use case for
+	 * bpf_core_enum_value_exists() but the enum type doesn't have a name
+	 * and can't be used with bpf_core_enum_value_exists(). Oh well...
+	 */
+	if (ret == -EINVAL) {
+		timer_pinned = false;
+		ret = bpf_timer_start(timer, TIMER_INTERVAL_NS, 0);
+	}
+	if (ret)
+		scx_bpf_error("bpf_timer_start failed (%d)", ret);
+	return ret;
+}
+
+void BPF_STRUCT_OPS(central_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SCX_OPS_DEFINE(central_ops,
+	       /*
+		* We are offloading all scheduling decisions to the central CPU
+		* and thus being the last task on a given CPU doesn't mean
+		* anything special. Enqueue the last tasks like any other tasks.
+		*/
+	       .flags			= SCX_OPS_ENQ_LAST,
+
+	       .select_cpu		= (void *)central_select_cpu,
+	       .enqueue			= (void *)central_enqueue,
+	       .dispatch		= (void *)central_dispatch,
+	       .running			= (void *)central_running,
+	       .stopping		= (void *)central_stopping,
+	       .init			= (void *)central_init,
+	       .exit			= (void *)central_exit,
+	       .name			= "central");
diff --git a/tools/sched_ext/scx_central.c b/tools/sched_ext/scx_central.c
new file mode 100644
index 000000000000..21deea320bd7
--- /dev/null
+++ b/tools/sched_ext/scx_central.c
@@ -0,0 +1,135 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#define _GNU_SOURCE
+#include <sched.h>
+#include <stdio.h>
+#include <unistd.h>
+#include <inttypes.h>
+#include <signal.h>
+#include <libgen.h>
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include "scx_central.bpf.skel.h"
+
+const char help_fmt[] =
+"A central FIFO sched_ext scheduler.\n"
+"\n"
+"See the top-level comment in .bpf.c for more details.\n"
+"\n"
+"Usage: %s [-s SLICE_US] [-c CPU]\n"
+"\n"
+"  -s SLICE_US   Override slice duration\n"
+"  -c CPU        Override the central CPU (default: 0)\n"
+"  -v            Print libbpf debug messages\n"
+"  -h            Display this help and exit\n";
+
+static bool verbose;
+static volatile int exit_req;
+
+static int libbpf_print_fn(enum libbpf_print_level level, const char *format, va_list args)
+{
+	if (level == LIBBPF_DEBUG && !verbose)
+		return 0;
+	return vfprintf(stderr, format, args);
+}
+
+static void sigint_handler(int dummy)
+{
+	exit_req = 1;
+}
+
+int main(int argc, char **argv)
+{
+	struct scx_central *skel;
+	struct bpf_link *link;
+	__u64 seq = 0, ecode;
+	__s32 opt;
+	cpu_set_t *cpuset;
+
+	libbpf_set_print(libbpf_print_fn);
+	signal(SIGINT, sigint_handler);
+	signal(SIGTERM, sigint_handler);
+restart:
+	skel = SCX_OPS_OPEN(central_ops, scx_central);
+
+	skel->rodata->central_cpu = 0;
+	skel->rodata->nr_cpu_ids = libbpf_num_possible_cpus();
+
+	while ((opt = getopt(argc, argv, "s:c:pvh")) != -1) {
+		switch (opt) {
+		case 's':
+			skel->rodata->slice_ns = strtoull(optarg, NULL, 0) * 1000;
+			break;
+		case 'c':
+			skel->rodata->central_cpu = strtoul(optarg, NULL, 0);
+			break;
+		case 'v':
+			verbose = true;
+			break;
+		default:
+			fprintf(stderr, help_fmt, basename(argv[0]));
+			return opt != 'h';
+		}
+	}
+
+	/* Resize arrays so their element count is equal to cpu count. */
+	RESIZE_ARRAY(skel, data, cpu_gimme_task, skel->rodata->nr_cpu_ids);
+	RESIZE_ARRAY(skel, data, cpu_started_at, skel->rodata->nr_cpu_ids);
+
+	SCX_OPS_LOAD(skel, central_ops, scx_central, uei);
+
+	/*
+	 * Affinitize the loading thread to the central CPU, as:
+	 * - That's where the BPF timer is first invoked in the BPF program.
+	 * - We probably don't want this user space component to take up a core
+	 *   from a task that would benefit from avoiding preemption on one of
+	 *   the tickless cores.
+	 *
+	 * Until BPF supports pinning the timer, it's not guaranteed that it
+	 * will always be invoked on the central CPU. In practice, this
+	 * suffices the majority of the time.
+	 */
+	cpuset = CPU_ALLOC(skel->rodata->nr_cpu_ids);
+	SCX_BUG_ON(!cpuset, "Failed to allocate cpuset");
+	CPU_ZERO(cpuset);
+	CPU_SET(skel->rodata->central_cpu, cpuset);
+	SCX_BUG_ON(sched_setaffinity(0, sizeof(cpuset), cpuset),
+		   "Failed to affinitize to central CPU %d (max %d)",
+		   skel->rodata->central_cpu, skel->rodata->nr_cpu_ids - 1);
+	CPU_FREE(cpuset);
+
+	link = SCX_OPS_ATTACH(skel, central_ops, scx_central);
+
+	if (!skel->data->timer_pinned)
+		printf("WARNING : BPF_F_TIMER_CPU_PIN not available, timer not pinned to central\n");
+
+	while (!exit_req && !UEI_EXITED(skel, uei)) {
+		printf("[SEQ %llu]\n", seq++);
+		printf("total   :%10" PRIu64 "    local:%10" PRIu64 "   queued:%10" PRIu64 "  lost:%10" PRIu64 "\n",
+		       skel->bss->nr_total,
+		       skel->bss->nr_locals,
+		       skel->bss->nr_queued,
+		       skel->bss->nr_lost_pids);
+		printf("timer   :%10" PRIu64 " dispatch:%10" PRIu64 " mismatch:%10" PRIu64 " retry:%10" PRIu64 "\n",
+		       skel->bss->nr_timers,
+		       skel->bss->nr_dispatches,
+		       skel->bss->nr_mismatches,
+		       skel->bss->nr_retries);
+		printf("overflow:%10" PRIu64 "\n",
+		       skel->bss->nr_overflows);
+		fflush(stdout);
+		sleep(1);
+	}
+
+	bpf_link__destroy(link);
+	ecode = UEI_REPORT(skel, uei);
+	scx_central__destroy(skel);
+
+	if (UEI_ECODE_RESTART(ecode))
+		goto restart;
+	return 0;
+}
diff --git a/tools/sched_ext/scx_flatcg.bpf.c b/tools/sched_ext/scx_flatcg.bpf.c
new file mode 100644
index 000000000000..b722baf6da4b
--- /dev/null
+++ b/tools/sched_ext/scx_flatcg.bpf.c
@@ -0,0 +1,957 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A demo sched_ext flattened cgroup hierarchy scheduler. It implements
+ * hierarchical weight-based cgroup CPU control by flattening the cgroup
+ * hierarchy into a single layer by compounding the active weight share at each
+ * level. Consider the following hierarchy with weights in parentheses:
+ *
+ * R + A (100) + B (100)
+ *   |         \ C (100)
+ *   \ D (200)
+ *
+ * Ignoring the root and threaded cgroups, only B, C and D can contain tasks.
+ * Let's say all three have runnable tasks. The total share that each of these
+ * three cgroups is entitled to can be calculated by compounding its share at
+ * each level.
+ *
+ * For example, B is competing against C and in that competition its share is
+ * 100/(100+100) == 1/2. At its parent level, A is competing against D and A's
+ * share in that competition is 100/(200+100) == 1/3. B's eventual share in the
+ * system can be calculated by multiplying the two shares, 1/2 * 1/3 == 1/6. C's
+ * eventual shaer is the same at 1/6. D is only competing at the top level and
+ * its share is 200/(100+200) == 2/3.
+ *
+ * So, instead of hierarchically scheduling level-by-level, we can consider it
+ * as B, C and D competing each other with respective share of 1/6, 1/6 and 2/3
+ * and keep updating the eventual shares as the cgroups' runnable states change.
+ *
+ * This flattening of hierarchy can bring a substantial performance gain when
+ * the cgroup hierarchy is nested multiple levels. in a simple benchmark using
+ * wrk[8] on apache serving a CGI script calculating sha1sum of a small file, it
+ * outperforms CFS by ~3% with CPU controller disabled and by ~10% with two
+ * apache instances competing with 2:1 weight ratio nested four level deep.
+ *
+ * However, the gain comes at the cost of not being able to properly handle
+ * thundering herd of cgroups. For example, if many cgroups which are nested
+ * behind a low priority parent cgroup wake up around the same time, they may be
+ * able to consume more CPU cycles than they are entitled to. In many use cases,
+ * this isn't a real concern especially given the performance gain. Also, there
+ * are ways to mitigate the problem further by e.g. introducing an extra
+ * scheduling layer on cgroup delegation boundaries.
+ *
+ * The scheduler first picks the cgroup to run and then schedule the tasks
+ * within by using nested weighted vtime scheduling by default. The
+ * cgroup-internal scheduling can be switched to FIFO with the -f option.
+ */
+#include <scx/common.bpf.h>
+#include "scx_flatcg.h"
+
+/*
+ * Maximum amount of retries to find a valid cgroup.
+ */
+enum {
+	FALLBACK_DSQ		= 0,
+	CGROUP_MAX_RETRIES	= 1024,
+};
+
+char _license[] SEC("license") = "GPL";
+
+const volatile u32 nr_cpus = 32;	/* !0 for veristat, set during init */
+const volatile u64 cgrp_slice_ns = SCX_SLICE_DFL;
+const volatile bool fifo_sched;
+
+u64 cvtime_now;
+UEI_DEFINE(uei);
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
+	__type(key, u32);
+	__type(value, u64);
+	__uint(max_entries, FCG_NR_STATS);
+} stats SEC(".maps");
+
+static void stat_inc(enum fcg_stat_idx idx)
+{
+	u32 idx_v = idx;
+
+	u64 *cnt_p = bpf_map_lookup_elem(&stats, &idx_v);
+	if (cnt_p)
+		(*cnt_p)++;
+}
+
+struct fcg_cpu_ctx {
+	u64			cur_cgid;
+	u64			cur_at;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
+	__type(key, u32);
+	__type(value, struct fcg_cpu_ctx);
+	__uint(max_entries, 1);
+} cpu_ctx SEC(".maps");
+
+struct {
+	__uint(type, BPF_MAP_TYPE_CGRP_STORAGE);
+	__uint(map_flags, BPF_F_NO_PREALLOC);
+	__type(key, int);
+	__type(value, struct fcg_cgrp_ctx);
+} cgrp_ctx SEC(".maps");
+
+struct cgv_node {
+	struct bpf_rb_node	rb_node;
+	__u64			cvtime;
+	__u64			cgid;
+};
+
+private(CGV_TREE) struct bpf_spin_lock cgv_tree_lock;
+private(CGV_TREE) struct bpf_rb_root cgv_tree __contains(cgv_node, rb_node);
+
+struct cgv_node_stash {
+	struct cgv_node __kptr *node;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_HASH);
+	__uint(max_entries, 16384);
+	__type(key, __u64);
+	__type(value, struct cgv_node_stash);
+} cgv_node_stash SEC(".maps");
+
+struct fcg_task_ctx {
+	u64		bypassed_at;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
+	__uint(map_flags, BPF_F_NO_PREALLOC);
+	__type(key, int);
+	__type(value, struct fcg_task_ctx);
+} task_ctx SEC(".maps");
+
+/* gets inc'd on weight tree changes to expire the cached hweights */
+u64 hweight_gen = 1;
+
+static u64 div_round_up(u64 dividend, u64 divisor)
+{
+	return (dividend + divisor - 1) / divisor;
+}
+
+static bool vtime_before(u64 a, u64 b)
+{
+	return (s64)(a - b) < 0;
+}
+
+static bool cgv_node_less(struct bpf_rb_node *a, const struct bpf_rb_node *b)
+{
+	struct cgv_node *cgc_a, *cgc_b;
+
+	cgc_a = container_of(a, struct cgv_node, rb_node);
+	cgc_b = container_of(b, struct cgv_node, rb_node);
+
+	return cgc_a->cvtime < cgc_b->cvtime;
+}
+
+static struct fcg_cpu_ctx *find_cpu_ctx(void)
+{
+	struct fcg_cpu_ctx *cpuc;
+	u32 idx = 0;
+
+	cpuc = bpf_map_lookup_elem(&cpu_ctx, &idx);
+	if (!cpuc) {
+		scx_bpf_error("cpu_ctx lookup failed");
+		return NULL;
+	}
+	return cpuc;
+}
+
+static struct fcg_cgrp_ctx *find_cgrp_ctx(struct cgroup *cgrp)
+{
+	struct fcg_cgrp_ctx *cgc;
+
+	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0, 0);
+	if (!cgc) {
+		scx_bpf_error("cgrp_ctx lookup failed for cgid %llu", cgrp->kn->id);
+		return NULL;
+	}
+	return cgc;
+}
+
+static struct fcg_cgrp_ctx *find_ancestor_cgrp_ctx(struct cgroup *cgrp, int level)
+{
+	struct fcg_cgrp_ctx *cgc;
+
+	cgrp = bpf_cgroup_ancestor(cgrp, level);
+	if (!cgrp) {
+		scx_bpf_error("ancestor cgroup lookup failed");
+		return NULL;
+	}
+
+	cgc = find_cgrp_ctx(cgrp);
+	if (!cgc)
+		scx_bpf_error("ancestor cgrp_ctx lookup failed");
+	bpf_cgroup_release(cgrp);
+	return cgc;
+}
+
+static void cgrp_refresh_hweight(struct cgroup *cgrp, struct fcg_cgrp_ctx *cgc)
+{
+	int level;
+
+	if (!cgc->nr_active) {
+		stat_inc(FCG_STAT_HWT_SKIP);
+		return;
+	}
+
+	if (cgc->hweight_gen == hweight_gen) {
+		stat_inc(FCG_STAT_HWT_CACHE);
+		return;
+	}
+
+	stat_inc(FCG_STAT_HWT_UPDATES);
+	bpf_for(level, 0, cgrp->level + 1) {
+		struct fcg_cgrp_ctx *cgc;
+		bool is_active;
+
+		cgc = find_ancestor_cgrp_ctx(cgrp, level);
+		if (!cgc)
+			break;
+
+		if (!level) {
+			cgc->hweight = FCG_HWEIGHT_ONE;
+			cgc->hweight_gen = hweight_gen;
+		} else {
+			struct fcg_cgrp_ctx *pcgc;
+
+			pcgc = find_ancestor_cgrp_ctx(cgrp, level - 1);
+			if (!pcgc)
+				break;
+
+			/*
+			 * We can be opportunistic here and not grab the
+			 * cgv_tree_lock and deal with the occasional races.
+			 * However, hweight updates are already cached and
+			 * relatively low-frequency. Let's just do the
+			 * straightforward thing.
+			 */
+			bpf_spin_lock(&cgv_tree_lock);
+			is_active = cgc->nr_active;
+			if (is_active) {
+				cgc->hweight_gen = pcgc->hweight_gen;
+				cgc->hweight =
+					div_round_up(pcgc->hweight * cgc->weight,
+						     pcgc->child_weight_sum);
+			}
+			bpf_spin_unlock(&cgv_tree_lock);
+
+			if (!is_active) {
+				stat_inc(FCG_STAT_HWT_RACE);
+				break;
+			}
+		}
+	}
+}
+
+static void cgrp_cap_budget(struct cgv_node *cgv_node, struct fcg_cgrp_ctx *cgc)
+{
+	u64 delta, cvtime, max_budget;
+
+	/*
+	 * A node which is on the rbtree can't be pointed to from elsewhere yet
+	 * and thus can't be updated and repositioned. Instead, we collect the
+	 * vtime deltas separately and apply it asynchronously here.
+	 */
+	delta = __sync_fetch_and_sub(&cgc->cvtime_delta, cgc->cvtime_delta);
+	cvtime = cgv_node->cvtime + delta;
+
+	/*
+	 * Allow a cgroup to carry the maximum budget proportional to its
+	 * hweight such that a full-hweight cgroup can immediately take up half
+	 * of the CPUs at the most while staying at the front of the rbtree.
+	 */
+	max_budget = (cgrp_slice_ns * nr_cpus * cgc->hweight) /
+		(2 * FCG_HWEIGHT_ONE);
+	if (vtime_before(cvtime, cvtime_now - max_budget))
+		cvtime = cvtime_now - max_budget;
+
+	cgv_node->cvtime = cvtime;
+}
+
+static void cgrp_enqueued(struct cgroup *cgrp, struct fcg_cgrp_ctx *cgc)
+{
+	struct cgv_node_stash *stash;
+	struct cgv_node *cgv_node;
+	u64 cgid = cgrp->kn->id;
+
+	/* paired with cmpxchg in try_pick_next_cgroup() */
+	if (__sync_val_compare_and_swap(&cgc->queued, 0, 1)) {
+		stat_inc(FCG_STAT_ENQ_SKIP);
+		return;
+	}
+
+	stash = bpf_map_lookup_elem(&cgv_node_stash, &cgid);
+	if (!stash) {
+		scx_bpf_error("cgv_node lookup failed for cgid %llu", cgid);
+		return;
+	}
+
+	/* NULL if the node is already on the rbtree */
+	cgv_node = bpf_kptr_xchg(&stash->node, NULL);
+	if (!cgv_node) {
+		stat_inc(FCG_STAT_ENQ_RACE);
+		return;
+	}
+
+	bpf_spin_lock(&cgv_tree_lock);
+	cgrp_cap_budget(cgv_node, cgc);
+	bpf_rbtree_add(&cgv_tree, &cgv_node->rb_node, cgv_node_less);
+	bpf_spin_unlock(&cgv_tree_lock);
+}
+
+static void set_bypassed_at(struct task_struct *p, struct fcg_task_ctx *taskc)
+{
+	/*
+	 * Tell fcg_stopping() that this bypassed the regular scheduling path
+	 * and should be force charged to the cgroup. 0 is used to indicate that
+	 * the task isn't bypassing, so if the current runtime is 0, go back by
+	 * one nanosecond.
+	 */
+	taskc->bypassed_at = p->se.sum_exec_runtime ?: (u64)-1;
+}
+
+s32 BPF_STRUCT_OPS(fcg_select_cpu, struct task_struct *p, s32 prev_cpu, u64 wake_flags)
+{
+	struct fcg_task_ctx *taskc;
+	bool is_idle = false;
+	s32 cpu;
+
+	cpu = scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags, &is_idle);
+
+	taskc = bpf_task_storage_get(&task_ctx, p, 0, 0);
+	if (!taskc) {
+		scx_bpf_error("task_ctx lookup failed");
+		return cpu;
+	}
+
+	/*
+	 * If select_cpu_dfl() is recommending local enqueue, the target CPU is
+	 * idle. Follow it and charge the cgroup later in fcg_stopping() after
+	 * the fact.
+	 */
+	if (is_idle) {
+		set_bypassed_at(p, taskc);
+		stat_inc(FCG_STAT_LOCAL);
+		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, 0);
+	}
+
+	return cpu;
+}
+
+void BPF_STRUCT_OPS(fcg_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	struct fcg_task_ctx *taskc;
+	struct cgroup *cgrp;
+	struct fcg_cgrp_ctx *cgc;
+
+	taskc = bpf_task_storage_get(&task_ctx, p, 0, 0);
+	if (!taskc) {
+		scx_bpf_error("task_ctx lookup failed");
+		return;
+	}
+
+	/*
+	 * Use the direct dispatching and force charging to deal with tasks with
+	 * custom affinities so that we don't have to worry about per-cgroup
+	 * dq's containing tasks that can't be executed from some CPUs.
+	 */
+	if (p->nr_cpus_allowed != nr_cpus) {
+		set_bypassed_at(p, taskc);
+
+		/*
+		 * The global dq is deprioritized as we don't want to let tasks
+		 * to boost themselves by constraining its cpumask. The
+		 * deprioritization is rather severe, so let's not apply that to
+		 * per-cpu kernel threads. This is ham-fisted. We probably wanna
+		 * implement per-cgroup fallback dq's instead so that we have
+		 * more control over when tasks with custom cpumask get issued.
+		 */
+		if (p->nr_cpus_allowed == 1 && (p->flags & PF_KTHREAD)) {
+			stat_inc(FCG_STAT_LOCAL);
+			scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, enq_flags);
+		} else {
+			stat_inc(FCG_STAT_GLOBAL);
+			scx_bpf_dispatch(p, FALLBACK_DSQ, SCX_SLICE_DFL, enq_flags);
+		}
+		return;
+	}
+
+	cgrp = __COMPAT_scx_bpf_task_cgroup(p);
+	cgc = find_cgrp_ctx(cgrp);
+	if (!cgc)
+		goto out_release;
+
+	if (fifo_sched) {
+		scx_bpf_dispatch(p, cgrp->kn->id, SCX_SLICE_DFL, enq_flags);
+	} else {
+		u64 tvtime = p->scx.dsq_vtime;
+
+		/*
+		 * Limit the amount of budget that an idling task can accumulate
+		 * to one slice.
+		 */
+		if (vtime_before(tvtime, cgc->tvtime_now - SCX_SLICE_DFL))
+			tvtime = cgc->tvtime_now - SCX_SLICE_DFL;
+
+		scx_bpf_dispatch_vtime(p, cgrp->kn->id, SCX_SLICE_DFL,
+				       tvtime, enq_flags);
+	}
+
+	cgrp_enqueued(cgrp, cgc);
+out_release:
+	bpf_cgroup_release(cgrp);
+}
+
+/*
+ * Walk the cgroup tree to update the active weight sums as tasks wake up and
+ * sleep. The weight sums are used as the base when calculating the proportion a
+ * given cgroup or task is entitled to at each level.
+ */
+static void update_active_weight_sums(struct cgroup *cgrp, bool runnable)
+{
+	struct fcg_cgrp_ctx *cgc;
+	bool updated = false;
+	int idx;
+
+	cgc = find_cgrp_ctx(cgrp);
+	if (!cgc)
+		return;
+
+	/*
+	 * In most cases, a hot cgroup would have multiple threads going to
+	 * sleep and waking up while the whole cgroup stays active. In leaf
+	 * cgroups, ->nr_runnable which is updated with __sync operations gates
+	 * ->nr_active updates, so that we don't have to grab the cgv_tree_lock
+	 * repeatedly for a busy cgroup which is staying active.
+	 */
+	if (runnable) {
+		if (__sync_fetch_and_add(&cgc->nr_runnable, 1))
+			return;
+		stat_inc(FCG_STAT_ACT);
+	} else {
+		if (__sync_sub_and_fetch(&cgc->nr_runnable, 1))
+			return;
+		stat_inc(FCG_STAT_DEACT);
+	}
+
+	/*
+	 * If @cgrp is becoming runnable, its hweight should be refreshed after
+	 * it's added to the weight tree so that enqueue has the up-to-date
+	 * value. If @cgrp is becoming quiescent, the hweight should be
+	 * refreshed before it's removed from the weight tree so that the usage
+	 * charging which happens afterwards has access to the latest value.
+	 */
+	if (!runnable)
+		cgrp_refresh_hweight(cgrp, cgc);
+
+	/* propagate upwards */
+	bpf_for(idx, 0, cgrp->level) {
+		int level = cgrp->level - idx;
+		struct fcg_cgrp_ctx *cgc, *pcgc = NULL;
+		bool propagate = false;
+
+		cgc = find_ancestor_cgrp_ctx(cgrp, level);
+		if (!cgc)
+			break;
+		if (level) {
+			pcgc = find_ancestor_cgrp_ctx(cgrp, level - 1);
+			if (!pcgc)
+				break;
+		}
+
+		/*
+		 * We need the propagation protected by a lock to synchronize
+		 * against weight changes. There's no reason to drop the lock at
+		 * each level but bpf_spin_lock() doesn't want any function
+		 * calls while locked.
+		 */
+		bpf_spin_lock(&cgv_tree_lock);
+
+		if (runnable) {
+			if (!cgc->nr_active++) {
+				updated = true;
+				if (pcgc) {
+					propagate = true;
+					pcgc->child_weight_sum += cgc->weight;
+				}
+			}
+		} else {
+			if (!--cgc->nr_active) {
+				updated = true;
+				if (pcgc) {
+					propagate = true;
+					pcgc->child_weight_sum -= cgc->weight;
+				}
+			}
+		}
+
+		bpf_spin_unlock(&cgv_tree_lock);
+
+		if (!propagate)
+			break;
+	}
+
+	if (updated)
+		__sync_fetch_and_add(&hweight_gen, 1);
+
+	if (runnable)
+		cgrp_refresh_hweight(cgrp, cgc);
+}
+
+void BPF_STRUCT_OPS(fcg_runnable, struct task_struct *p, u64 enq_flags)
+{
+	struct cgroup *cgrp;
+
+	cgrp = __COMPAT_scx_bpf_task_cgroup(p);
+	update_active_weight_sums(cgrp, true);
+	bpf_cgroup_release(cgrp);
+}
+
+void BPF_STRUCT_OPS(fcg_running, struct task_struct *p)
+{
+	struct cgroup *cgrp;
+	struct fcg_cgrp_ctx *cgc;
+
+	if (fifo_sched)
+		return;
+
+	cgrp = __COMPAT_scx_bpf_task_cgroup(p);
+	cgc = find_cgrp_ctx(cgrp);
+	if (cgc) {
+		/*
+		 * @cgc->tvtime_now always progresses forward as tasks start
+		 * executing. The test and update can be performed concurrently
+		 * from multiple CPUs and thus racy. Any error should be
+		 * contained and temporary. Let's just live with it.
+		 */
+		if (vtime_before(cgc->tvtime_now, p->scx.dsq_vtime))
+			cgc->tvtime_now = p->scx.dsq_vtime;
+	}
+	bpf_cgroup_release(cgrp);
+}
+
+void BPF_STRUCT_OPS(fcg_stopping, struct task_struct *p, bool runnable)
+{
+	struct fcg_task_ctx *taskc;
+	struct cgroup *cgrp;
+	struct fcg_cgrp_ctx *cgc;
+
+	/*
+	 * Scale the execution time by the inverse of the weight and charge.
+	 *
+	 * Note that the default yield implementation yields by setting
+	 * @p->scx.slice to zero and the following would treat the yielding task
+	 * as if it has consumed all its slice. If this penalizes yielding tasks
+	 * too much, determine the execution time by taking explicit timestamps
+	 * instead of depending on @p->scx.slice.
+	 */
+	if (!fifo_sched)
+		p->scx.dsq_vtime +=
+			(SCX_SLICE_DFL - p->scx.slice) * 100 / p->scx.weight;
+
+	taskc = bpf_task_storage_get(&task_ctx, p, 0, 0);
+	if (!taskc) {
+		scx_bpf_error("task_ctx lookup failed");
+		return;
+	}
+
+	if (!taskc->bypassed_at)
+		return;
+
+	cgrp = __COMPAT_scx_bpf_task_cgroup(p);
+	cgc = find_cgrp_ctx(cgrp);
+	if (cgc) {
+		__sync_fetch_and_add(&cgc->cvtime_delta,
+				     p->se.sum_exec_runtime - taskc->bypassed_at);
+		taskc->bypassed_at = 0;
+	}
+	bpf_cgroup_release(cgrp);
+}
+
+void BPF_STRUCT_OPS(fcg_quiescent, struct task_struct *p, u64 deq_flags)
+{
+	struct cgroup *cgrp;
+
+	cgrp = __COMPAT_scx_bpf_task_cgroup(p);
+	update_active_weight_sums(cgrp, false);
+	bpf_cgroup_release(cgrp);
+}
+
+void BPF_STRUCT_OPS(fcg_cgroup_set_weight, struct cgroup *cgrp, u32 weight)
+{
+	struct fcg_cgrp_ctx *cgc, *pcgc = NULL;
+
+	cgc = find_cgrp_ctx(cgrp);
+	if (!cgc)
+		return;
+
+	if (cgrp->level) {
+		pcgc = find_ancestor_cgrp_ctx(cgrp, cgrp->level - 1);
+		if (!pcgc)
+			return;
+	}
+
+	bpf_spin_lock(&cgv_tree_lock);
+	if (pcgc && cgc->nr_active)
+		pcgc->child_weight_sum += (s64)weight - cgc->weight;
+	cgc->weight = weight;
+	bpf_spin_unlock(&cgv_tree_lock);
+}
+
+static bool try_pick_next_cgroup(u64 *cgidp)
+{
+	struct bpf_rb_node *rb_node;
+	struct cgv_node_stash *stash;
+	struct cgv_node *cgv_node;
+	struct fcg_cgrp_ctx *cgc;
+	struct cgroup *cgrp;
+	u64 cgid;
+
+	/* pop the front cgroup and wind cvtime_now accordingly */
+	bpf_spin_lock(&cgv_tree_lock);
+
+	rb_node = bpf_rbtree_first(&cgv_tree);
+	if (!rb_node) {
+		bpf_spin_unlock(&cgv_tree_lock);
+		stat_inc(FCG_STAT_PNC_NO_CGRP);
+		*cgidp = 0;
+		return true;
+	}
+
+	rb_node = bpf_rbtree_remove(&cgv_tree, rb_node);
+	bpf_spin_unlock(&cgv_tree_lock);
+
+	if (!rb_node) {
+		/*
+		 * This should never happen. bpf_rbtree_first() was called
+		 * above while the tree lock was held, so the node should
+		 * always be present.
+		 */
+		scx_bpf_error("node could not be removed");
+		return true;
+	}
+
+	cgv_node = container_of(rb_node, struct cgv_node, rb_node);
+	cgid = cgv_node->cgid;
+
+	if (vtime_before(cvtime_now, cgv_node->cvtime))
+		cvtime_now = cgv_node->cvtime;
+
+	/*
+	 * If lookup fails, the cgroup's gone. Free and move on. See
+	 * fcg_cgroup_exit().
+	 */
+	cgrp = bpf_cgroup_from_id(cgid);
+	if (!cgrp) {
+		stat_inc(FCG_STAT_PNC_GONE);
+		goto out_free;
+	}
+
+	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0, 0);
+	if (!cgc) {
+		bpf_cgroup_release(cgrp);
+		stat_inc(FCG_STAT_PNC_GONE);
+		goto out_free;
+	}
+
+	if (!scx_bpf_consume(cgid)) {
+		bpf_cgroup_release(cgrp);
+		stat_inc(FCG_STAT_PNC_EMPTY);
+		goto out_stash;
+	}
+
+	/*
+	 * Successfully consumed from the cgroup. This will be our current
+	 * cgroup for the new slice. Refresh its hweight.
+	 */
+	cgrp_refresh_hweight(cgrp, cgc);
+
+	bpf_cgroup_release(cgrp);
+
+	/*
+	 * As the cgroup may have more tasks, add it back to the rbtree. Note
+	 * that here we charge the full slice upfront and then exact later
+	 * according to the actual consumption. This prevents lowpri thundering
+	 * herd from saturating the machine.
+	 */
+	bpf_spin_lock(&cgv_tree_lock);
+	cgv_node->cvtime += cgrp_slice_ns * FCG_HWEIGHT_ONE / (cgc->hweight ?: 1);
+	cgrp_cap_budget(cgv_node, cgc);
+	bpf_rbtree_add(&cgv_tree, &cgv_node->rb_node, cgv_node_less);
+	bpf_spin_unlock(&cgv_tree_lock);
+
+	*cgidp = cgid;
+	stat_inc(FCG_STAT_PNC_NEXT);
+	return true;
+
+out_stash:
+	stash = bpf_map_lookup_elem(&cgv_node_stash, &cgid);
+	if (!stash) {
+		stat_inc(FCG_STAT_PNC_GONE);
+		goto out_free;
+	}
+
+	/*
+	 * Paired with cmpxchg in cgrp_enqueued(). If they see the following
+	 * transition, they'll enqueue the cgroup. If they are earlier, we'll
+	 * see their task in the dq below and requeue the cgroup.
+	 */
+	__sync_val_compare_and_swap(&cgc->queued, 1, 0);
+
+	if (scx_bpf_dsq_nr_queued(cgid)) {
+		bpf_spin_lock(&cgv_tree_lock);
+		bpf_rbtree_add(&cgv_tree, &cgv_node->rb_node, cgv_node_less);
+		bpf_spin_unlock(&cgv_tree_lock);
+		stat_inc(FCG_STAT_PNC_RACE);
+	} else {
+		cgv_node = bpf_kptr_xchg(&stash->node, cgv_node);
+		if (cgv_node) {
+			scx_bpf_error("unexpected !NULL cgv_node stash");
+			goto out_free;
+		}
+	}
+
+	return false;
+
+out_free:
+	bpf_obj_drop(cgv_node);
+	return false;
+}
+
+void BPF_STRUCT_OPS(fcg_dispatch, s32 cpu, struct task_struct *prev)
+{
+	struct fcg_cpu_ctx *cpuc;
+	struct fcg_cgrp_ctx *cgc;
+	struct cgroup *cgrp;
+	u64 now = bpf_ktime_get_ns();
+	bool picked_next = false;
+
+	cpuc = find_cpu_ctx();
+	if (!cpuc)
+		return;
+
+	if (!cpuc->cur_cgid)
+		goto pick_next_cgroup;
+
+	if (vtime_before(now, cpuc->cur_at + cgrp_slice_ns)) {
+		if (scx_bpf_consume(cpuc->cur_cgid)) {
+			stat_inc(FCG_STAT_CNS_KEEP);
+			return;
+		}
+		stat_inc(FCG_STAT_CNS_EMPTY);
+	} else {
+		stat_inc(FCG_STAT_CNS_EXPIRE);
+	}
+
+	/*
+	 * The current cgroup is expiring. It was already charged a full slice.
+	 * Calculate the actual usage and accumulate the delta.
+	 */
+	cgrp = bpf_cgroup_from_id(cpuc->cur_cgid);
+	if (!cgrp) {
+		stat_inc(FCG_STAT_CNS_GONE);
+		goto pick_next_cgroup;
+	}
+
+	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0, 0);
+	if (cgc) {
+		/*
+		 * We want to update the vtime delta and then look for the next
+		 * cgroup to execute but the latter needs to be done in a loop
+		 * and we can't keep the lock held. Oh well...
+		 */
+		bpf_spin_lock(&cgv_tree_lock);
+		__sync_fetch_and_add(&cgc->cvtime_delta,
+				     (cpuc->cur_at + cgrp_slice_ns - now) *
+				     FCG_HWEIGHT_ONE / (cgc->hweight ?: 1));
+		bpf_spin_unlock(&cgv_tree_lock);
+	} else {
+		stat_inc(FCG_STAT_CNS_GONE);
+	}
+
+	bpf_cgroup_release(cgrp);
+
+pick_next_cgroup:
+	cpuc->cur_at = now;
+
+	if (scx_bpf_consume(FALLBACK_DSQ)) {
+		cpuc->cur_cgid = 0;
+		return;
+	}
+
+	bpf_repeat(CGROUP_MAX_RETRIES) {
+		if (try_pick_next_cgroup(&cpuc->cur_cgid)) {
+			picked_next = true;
+			break;
+		}
+	}
+
+	/*
+	 * This only happens if try_pick_next_cgroup() races against enqueue
+	 * path for more than CGROUP_MAX_RETRIES times, which is extremely
+	 * unlikely and likely indicates an underlying bug. There shouldn't be
+	 * any stall risk as the race is against enqueue.
+	 */
+	if (!picked_next)
+		stat_inc(FCG_STAT_PNC_FAIL);
+}
+
+s32 BPF_STRUCT_OPS(fcg_init_task, struct task_struct *p,
+		   struct scx_init_task_args *args)
+{
+	struct fcg_task_ctx *taskc;
+	struct fcg_cgrp_ctx *cgc;
+
+	/*
+	 * @p is new. Let's ensure that its task_ctx is available. We can sleep
+	 * in this function and the following will automatically use GFP_KERNEL.
+	 */
+	taskc = bpf_task_storage_get(&task_ctx, p, 0,
+				     BPF_LOCAL_STORAGE_GET_F_CREATE);
+	if (!taskc)
+		return -ENOMEM;
+
+	taskc->bypassed_at = 0;
+
+	if (!(cgc = find_cgrp_ctx(args->cgroup)))
+		return -ENOENT;
+
+	p->scx.dsq_vtime = cgc->tvtime_now;
+
+	return 0;
+}
+
+int BPF_STRUCT_OPS_SLEEPABLE(fcg_cgroup_init, struct cgroup *cgrp,
+			     struct scx_cgroup_init_args *args)
+{
+	struct fcg_cgrp_ctx *cgc;
+	struct cgv_node *cgv_node;
+	struct cgv_node_stash empty_stash = {}, *stash;
+	u64 cgid = cgrp->kn->id;
+	int ret;
+
+	/*
+	 * Technically incorrect as cgroup ID is full 64bit while dsq ID is
+	 * 63bit. Should not be a problem in practice and easy to spot in the
+	 * unlikely case that it breaks.
+	 */
+	ret = scx_bpf_create_dsq(cgid, -1);
+	if (ret)
+		return ret;
+
+	cgc = bpf_cgrp_storage_get(&cgrp_ctx, cgrp, 0,
+				   BPF_LOCAL_STORAGE_GET_F_CREATE);
+	if (!cgc) {
+		ret = -ENOMEM;
+		goto err_destroy_dsq;
+	}
+
+	cgc->weight = args->weight;
+	cgc->hweight = FCG_HWEIGHT_ONE;
+
+	ret = bpf_map_update_elem(&cgv_node_stash, &cgid, &empty_stash,
+				  BPF_NOEXIST);
+	if (ret) {
+		if (ret != -ENOMEM)
+			scx_bpf_error("unexpected stash creation error (%d)",
+				      ret);
+		goto err_destroy_dsq;
+	}
+
+	stash = bpf_map_lookup_elem(&cgv_node_stash, &cgid);
+	if (!stash) {
+		scx_bpf_error("unexpected cgv_node stash lookup failure");
+		ret = -ENOENT;
+		goto err_destroy_dsq;
+	}
+
+	cgv_node = bpf_obj_new(struct cgv_node);
+	if (!cgv_node) {
+		ret = -ENOMEM;
+		goto err_del_cgv_node;
+	}
+
+	cgv_node->cgid = cgid;
+	cgv_node->cvtime = cvtime_now;
+
+	cgv_node = bpf_kptr_xchg(&stash->node, cgv_node);
+	if (cgv_node) {
+		scx_bpf_error("unexpected !NULL cgv_node stash");
+		ret = -EBUSY;
+		goto err_drop;
+	}
+
+	return 0;
+
+err_drop:
+	bpf_obj_drop(cgv_node);
+err_del_cgv_node:
+	bpf_map_delete_elem(&cgv_node_stash, &cgid);
+err_destroy_dsq:
+	scx_bpf_destroy_dsq(cgid);
+	return ret;
+}
+
+void BPF_STRUCT_OPS(fcg_cgroup_exit, struct cgroup *cgrp)
+{
+	u64 cgid = cgrp->kn->id;
+
+	/*
+	 * For now, there's no way find and remove the cgv_node if it's on the
+	 * cgv_tree. Let's drain them in the dispatch path as they get popped
+	 * off the front of the tree.
+	 */
+	bpf_map_delete_elem(&cgv_node_stash, &cgid);
+	scx_bpf_destroy_dsq(cgid);
+}
+
+void BPF_STRUCT_OPS(fcg_cgroup_move, struct task_struct *p,
+		    struct cgroup *from, struct cgroup *to)
+{
+	struct fcg_cgrp_ctx *from_cgc, *to_cgc;
+	s64 vtime_delta;
+
+	/* find_cgrp_ctx() triggers scx_ops_error() on lookup failures */
+	if (!(from_cgc = find_cgrp_ctx(from)) || !(to_cgc = find_cgrp_ctx(to)))
+		return;
+
+	vtime_delta = p->scx.dsq_vtime - from_cgc->tvtime_now;
+	p->scx.dsq_vtime = to_cgc->tvtime_now + vtime_delta;
+}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(fcg_init)
+{
+	return scx_bpf_create_dsq(FALLBACK_DSQ, -1);
+}
+
+void BPF_STRUCT_OPS(fcg_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SCX_OPS_DEFINE(flatcg_ops,
+	       .select_cpu		= (void *)fcg_select_cpu,
+	       .enqueue			= (void *)fcg_enqueue,
+	       .dispatch		= (void *)fcg_dispatch,
+	       .runnable		= (void *)fcg_runnable,
+	       .running			= (void *)fcg_running,
+	       .stopping		= (void *)fcg_stopping,
+	       .quiescent		= (void *)fcg_quiescent,
+	       .init_task		= (void *)fcg_init_task,
+	       .cgroup_set_weight	= (void *)fcg_cgroup_set_weight,
+	       .cgroup_init		= (void *)fcg_cgroup_init,
+	       .cgroup_exit		= (void *)fcg_cgroup_exit,
+	       .cgroup_move		= (void *)fcg_cgroup_move,
+	       .init			= (void *)fcg_init,
+	       .exit			= (void *)fcg_exit,
+	       .flags			= SCX_OPS_HAS_CGROUP_WEIGHT | SCX_OPS_ENQ_EXITING,
+	       .name			= "flatcg");
diff --git a/tools/sched_ext/scx_flatcg.c b/tools/sched_ext/scx_flatcg.c
new file mode 100644
index 000000000000..5d24ca9c29d9
--- /dev/null
+++ b/tools/sched_ext/scx_flatcg.c
@@ -0,0 +1,233 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ */
+#include <stdio.h>
+#include <signal.h>
+#include <unistd.h>
+#include <libgen.h>
+#include <limits.h>
+#include <inttypes.h>
+#include <fcntl.h>
+#include <time.h>
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include "scx_flatcg.h"
+#include "scx_flatcg.bpf.skel.h"
+
+#ifndef FILEID_KERNFS
+#define FILEID_KERNFS		0xfe
+#endif
+
+const char help_fmt[] =
+"A flattened cgroup hierarchy sched_ext scheduler.\n"
+"\n"
+"See the top-level comment in .bpf.c for more details.\n"
+"\n"
+"Usage: %s [-s SLICE_US] [-i INTERVAL] [-f] [-v]\n"
+"\n"
+"  -s SLICE_US   Override slice duration\n"
+"  -i INTERVAL   Report interval\n"
+"  -f            Use FIFO scheduling instead of weighted vtime scheduling\n"
+"  -v            Print libbpf debug messages\n"
+"  -h            Display this help and exit\n";
+
+static bool verbose;
+static volatile int exit_req;
+
+static int libbpf_print_fn(enum libbpf_print_level level, const char *format, va_list args)
+{
+	if (level == LIBBPF_DEBUG && !verbose)
+		return 0;
+	return vfprintf(stderr, format, args);
+}
+
+static void sigint_handler(int dummy)
+{
+	exit_req = 1;
+}
+
+static float read_cpu_util(__u64 *last_sum, __u64 *last_idle)
+{
+	FILE *fp;
+	char buf[4096];
+	char *line, *cur = NULL, *tok;
+	__u64 sum = 0, idle = 0;
+	__u64 delta_sum, delta_idle;
+	int idx;
+
+	fp = fopen("/proc/stat", "r");
+	if (!fp) {
+		perror("fopen(\"/proc/stat\")");
+		return 0.0;
+	}
+
+	if (!fgets(buf, sizeof(buf), fp)) {
+		perror("fgets(\"/proc/stat\")");
+		fclose(fp);
+		return 0.0;
+	}
+	fclose(fp);
+
+	line = buf;
+	for (idx = 0; (tok = strtok_r(line, " \n", &cur)); idx++) {
+		char *endp = NULL;
+		__u64 v;
+
+		if (idx == 0) {
+			line = NULL;
+			continue;
+		}
+		v = strtoull(tok, &endp, 0);
+		if (!endp || *endp != '\0') {
+			fprintf(stderr, "failed to parse %dth field of /proc/stat (\"%s\")\n",
+				idx, tok);
+			continue;
+		}
+		sum += v;
+		if (idx == 4)
+			idle = v;
+	}
+
+	delta_sum = sum - *last_sum;
+	delta_idle = idle - *last_idle;
+	*last_sum = sum;
+	*last_idle = idle;
+
+	return delta_sum ? (float)(delta_sum - delta_idle) / delta_sum : 0.0;
+}
+
+static void fcg_read_stats(struct scx_flatcg *skel, __u64 *stats)
+{
+	__u64 cnts[FCG_NR_STATS][skel->rodata->nr_cpus];
+	__u32 idx;
+
+	memset(stats, 0, sizeof(stats[0]) * FCG_NR_STATS);
+
+	for (idx = 0; idx < FCG_NR_STATS; idx++) {
+		int ret, cpu;
+
+		ret = bpf_map_lookup_elem(bpf_map__fd(skel->maps.stats),
+					  &idx, cnts[idx]);
+		if (ret < 0)
+			continue;
+		for (cpu = 0; cpu < skel->rodata->nr_cpus; cpu++)
+			stats[idx] += cnts[idx][cpu];
+	}
+}
+
+int main(int argc, char **argv)
+{
+	struct scx_flatcg *skel;
+	struct bpf_link *link;
+	struct timespec intv_ts = { .tv_sec = 2, .tv_nsec = 0 };
+	bool dump_cgrps = false;
+	__u64 last_cpu_sum = 0, last_cpu_idle = 0;
+	__u64 last_stats[FCG_NR_STATS] = {};
+	unsigned long seq = 0;
+	__s32 opt;
+	__u64 ecode;
+
+	libbpf_set_print(libbpf_print_fn);
+	signal(SIGINT, sigint_handler);
+	signal(SIGTERM, sigint_handler);
+restart:
+	skel = SCX_OPS_OPEN(flatcg_ops, scx_flatcg);
+
+	skel->rodata->nr_cpus = libbpf_num_possible_cpus();
+
+	while ((opt = getopt(argc, argv, "s:i:dfvh")) != -1) {
+		double v;
+
+		switch (opt) {
+		case 's':
+			v = strtod(optarg, NULL);
+			skel->rodata->cgrp_slice_ns = v * 1000;
+			break;
+		case 'i':
+			v = strtod(optarg, NULL);
+			intv_ts.tv_sec = v;
+			intv_ts.tv_nsec = (v - (float)intv_ts.tv_sec) * 1000000000;
+			break;
+		case 'd':
+			dump_cgrps = true;
+			break;
+		case 'f':
+			skel->rodata->fifo_sched = true;
+			break;
+		case 'v':
+			verbose = true;
+			break;
+		case 'h':
+		default:
+			fprintf(stderr, help_fmt, basename(argv[0]));
+			return opt != 'h';
+		}
+	}
+
+	printf("slice=%.1lfms intv=%.1lfs dump_cgrps=%d",
+	       (double)skel->rodata->cgrp_slice_ns / 1000000.0,
+	       (double)intv_ts.tv_sec + (double)intv_ts.tv_nsec / 1000000000.0,
+	       dump_cgrps);
+
+	SCX_OPS_LOAD(skel, flatcg_ops, scx_flatcg, uei);
+	link = SCX_OPS_ATTACH(skel, flatcg_ops, scx_flatcg);
+
+	while (!exit_req && !UEI_EXITED(skel, uei)) {
+		__u64 acc_stats[FCG_NR_STATS];
+		__u64 stats[FCG_NR_STATS];
+		float cpu_util;
+		int i;
+
+		cpu_util = read_cpu_util(&last_cpu_sum, &last_cpu_idle);
+
+		fcg_read_stats(skel, acc_stats);
+		for (i = 0; i < FCG_NR_STATS; i++)
+			stats[i] = acc_stats[i] - last_stats[i];
+
+		memcpy(last_stats, acc_stats, sizeof(acc_stats));
+
+		printf("\n[SEQ %6lu cpu=%5.1lf hweight_gen=%" PRIu64 "]\n",
+		       seq++, cpu_util * 100.0, skel->data->hweight_gen);
+		printf("       act:%6llu  deact:%6llu global:%6llu local:%6llu\n",
+		       stats[FCG_STAT_ACT],
+		       stats[FCG_STAT_DEACT],
+		       stats[FCG_STAT_GLOBAL],
+		       stats[FCG_STAT_LOCAL]);
+		printf("HWT  cache:%6llu update:%6llu   skip:%6llu  race:%6llu\n",
+		       stats[FCG_STAT_HWT_CACHE],
+		       stats[FCG_STAT_HWT_UPDATES],
+		       stats[FCG_STAT_HWT_SKIP],
+		       stats[FCG_STAT_HWT_RACE]);
+		printf("ENQ   skip:%6llu   race:%6llu\n",
+		       stats[FCG_STAT_ENQ_SKIP],
+		       stats[FCG_STAT_ENQ_RACE]);
+		printf("CNS   keep:%6llu expire:%6llu  empty:%6llu  gone:%6llu\n",
+		       stats[FCG_STAT_CNS_KEEP],
+		       stats[FCG_STAT_CNS_EXPIRE],
+		       stats[FCG_STAT_CNS_EMPTY],
+		       stats[FCG_STAT_CNS_GONE]);
+		printf("PNC   next:%6llu  empty:%6llu nocgrp:%6llu  gone:%6llu race:%6llu fail:%6llu\n",
+		       stats[FCG_STAT_PNC_NEXT],
+		       stats[FCG_STAT_PNC_EMPTY],
+		       stats[FCG_STAT_PNC_NO_CGRP],
+		       stats[FCG_STAT_PNC_GONE],
+		       stats[FCG_STAT_PNC_RACE],
+		       stats[FCG_STAT_PNC_FAIL]);
+		printf("BAD remove:%6llu\n",
+		       acc_stats[FCG_STAT_BAD_REMOVAL]);
+		fflush(stdout);
+
+		nanosleep(&intv_ts, NULL);
+	}
+
+	bpf_link__destroy(link);
+	ecode = UEI_REPORT(skel, uei);
+	scx_flatcg__destroy(skel);
+
+	if (UEI_ECODE_RESTART(ecode))
+		goto restart;
+	return 0;
+}
diff --git a/tools/sched_ext/scx_flatcg.h b/tools/sched_ext/scx_flatcg.h
new file mode 100644
index 000000000000..6f2ea50acb1c
--- /dev/null
+++ b/tools/sched_ext/scx_flatcg.h
@@ -0,0 +1,51 @@
+#ifndef __SCX_EXAMPLE_FLATCG_H
+#define __SCX_EXAMPLE_FLATCG_H
+
+enum {
+	FCG_HWEIGHT_ONE		= 1LLU << 16,
+};
+
+enum fcg_stat_idx {
+	FCG_STAT_ACT,
+	FCG_STAT_DEACT,
+	FCG_STAT_LOCAL,
+	FCG_STAT_GLOBAL,
+
+	FCG_STAT_HWT_UPDATES,
+	FCG_STAT_HWT_CACHE,
+	FCG_STAT_HWT_SKIP,
+	FCG_STAT_HWT_RACE,
+
+	FCG_STAT_ENQ_SKIP,
+	FCG_STAT_ENQ_RACE,
+
+	FCG_STAT_CNS_KEEP,
+	FCG_STAT_CNS_EXPIRE,
+	FCG_STAT_CNS_EMPTY,
+	FCG_STAT_CNS_GONE,
+
+	FCG_STAT_PNC_NO_CGRP,
+	FCG_STAT_PNC_NEXT,
+	FCG_STAT_PNC_EMPTY,
+	FCG_STAT_PNC_GONE,
+	FCG_STAT_PNC_RACE,
+	FCG_STAT_PNC_FAIL,
+
+	FCG_STAT_BAD_REMOVAL,
+
+	FCG_NR_STATS,
+};
+
+struct fcg_cgrp_ctx {
+	u32			nr_active;
+	u32			nr_runnable;
+	u32			queued;
+	u32			weight;
+	u32			hweight;
+	u64			child_weight_sum;
+	u64			hweight_gen;
+	s64			cvtime_delta;
+	u64			tvtime_now;
+};
+
+#endif /* __SCX_EXAMPLE_FLATCG_H */
diff --git a/tools/sched_ext/scx_qmap.bpf.c b/tools/sched_ext/scx_qmap.bpf.c
new file mode 100644
index 000000000000..5b39bee9eb23
--- /dev/null
+++ b/tools/sched_ext/scx_qmap.bpf.c
@@ -0,0 +1,813 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A simple five-level FIFO queue scheduler.
+ *
+ * There are five FIFOs implemented using BPF_MAP_TYPE_QUEUE. A task gets
+ * assigned to one depending on its compound weight. Each CPU round robins
+ * through the FIFOs and dispatches more from FIFOs with higher indices - 1 from
+ * queue0, 2 from queue1, 4 from queue2 and so on.
+ *
+ * This scheduler demonstrates:
+ *
+ * - BPF-side queueing using PIDs.
+ * - Sleepable per-task storage allocation using ops.prep_enable().
+ * - Using ops.cpu_release() to handle a higher priority scheduling class taking
+ *   the CPU away.
+ * - Core-sched support.
+ *
+ * This scheduler is primarily for demonstration and testing of sched_ext
+ * features and unlikely to be useful for actual workloads.
+ *
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#include <scx/common.bpf.h>
+
+enum consts {
+	ONE_SEC_IN_NS		= 1000000000,
+	SHARED_DSQ		= 0,
+	HIGHPRI_DSQ		= 1,
+	HIGHPRI_WEIGHT		= 8668,		/* this is what -20 maps to */
+};
+
+char _license[] SEC("license") = "GPL";
+
+const volatile u64 slice_ns = SCX_SLICE_DFL;
+const volatile u32 stall_user_nth;
+const volatile u32 stall_kernel_nth;
+const volatile u32 dsp_inf_loop_after;
+const volatile u32 dsp_batch;
+const volatile bool highpri_boosting;
+const volatile bool print_shared_dsq;
+const volatile s32 disallow_tgid;
+const volatile bool suppress_dump;
+
+u64 nr_highpri_queued;
+u32 test_error_cnt;
+
+UEI_DEFINE(uei);
+
+struct qmap {
+	__uint(type, BPF_MAP_TYPE_QUEUE);
+	__uint(max_entries, 4096);
+	__type(value, u32);
+} queue0 SEC(".maps"),
+  queue1 SEC(".maps"),
+  queue2 SEC(".maps"),
+  queue3 SEC(".maps"),
+  queue4 SEC(".maps");
+
+struct {
+	__uint(type, BPF_MAP_TYPE_ARRAY_OF_MAPS);
+	__uint(max_entries, 5);
+	__type(key, int);
+	__array(values, struct qmap);
+} queue_arr SEC(".maps") = {
+	.values = {
+		[0] = &queue0,
+		[1] = &queue1,
+		[2] = &queue2,
+		[3] = &queue3,
+		[4] = &queue4,
+	},
+};
+
+/*
+ * If enabled, CPU performance target is set according to the queue index
+ * according to the following table.
+ */
+static const u32 qidx_to_cpuperf_target[] = {
+	[0] = SCX_CPUPERF_ONE * 0 / 4,
+	[1] = SCX_CPUPERF_ONE * 1 / 4,
+	[2] = SCX_CPUPERF_ONE * 2 / 4,
+	[3] = SCX_CPUPERF_ONE * 3 / 4,
+	[4] = SCX_CPUPERF_ONE * 4 / 4,
+};
+
+/*
+ * Per-queue sequence numbers to implement core-sched ordering.
+ *
+ * Tail seq is assigned to each queued task and incremented. Head seq tracks the
+ * sequence number of the latest dispatched task. The distance between the a
+ * task's seq and the associated queue's head seq is called the queue distance
+ * and used when comparing two tasks for ordering. See qmap_core_sched_before().
+ */
+static u64 core_sched_head_seqs[5];
+static u64 core_sched_tail_seqs[5];
+
+/* Per-task scheduling context */
+struct task_ctx {
+	bool	force_local;	/* Dispatch directly to local_dsq */
+	bool	highpri;
+	u64	core_sched_seq;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
+	__uint(map_flags, BPF_F_NO_PREALLOC);
+	__type(key, int);
+	__type(value, struct task_ctx);
+} task_ctx_stor SEC(".maps");
+
+struct cpu_ctx {
+	u64	dsp_idx;	/* dispatch index */
+	u64	dsp_cnt;	/* remaining count */
+	u32	avg_weight;
+	u32	cpuperf_target;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
+	__uint(max_entries, 1);
+	__type(key, u32);
+	__type(value, struct cpu_ctx);
+} cpu_ctx_stor SEC(".maps");
+
+/* Statistics */
+u64 nr_enqueued, nr_dispatched, nr_reenqueued, nr_dequeued, nr_ddsp_from_enq;
+u64 nr_core_sched_execed;
+u64 nr_expedited_local, nr_expedited_remote, nr_expedited_lost, nr_expedited_from_timer;
+u32 cpuperf_min, cpuperf_avg, cpuperf_max;
+u32 cpuperf_target_min, cpuperf_target_avg, cpuperf_target_max;
+
+static s32 pick_direct_dispatch_cpu(struct task_struct *p, s32 prev_cpu)
+{
+	s32 cpu;
+
+	if (p->nr_cpus_allowed == 1 ||
+	    scx_bpf_test_and_clear_cpu_idle(prev_cpu))
+		return prev_cpu;
+
+	cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
+	if (cpu >= 0)
+		return cpu;
+
+	return -1;
+}
+
+static struct task_ctx *lookup_task_ctx(struct task_struct *p)
+{
+	struct task_ctx *tctx;
+
+	if (!(tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0))) {
+		scx_bpf_error("task_ctx lookup failed");
+		return NULL;
+	}
+	return tctx;
+}
+
+s32 BPF_STRUCT_OPS(qmap_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	struct task_ctx *tctx;
+	s32 cpu;
+
+	if (!(tctx = lookup_task_ctx(p)))
+		return -ESRCH;
+
+	cpu = pick_direct_dispatch_cpu(p, prev_cpu);
+
+	if (cpu >= 0) {
+		tctx->force_local = true;
+		return cpu;
+	} else {
+		return prev_cpu;
+	}
+}
+
+static int weight_to_idx(u32 weight)
+{
+	/* Coarsely map the compound weight to a FIFO. */
+	if (weight <= 25)
+		return 0;
+	else if (weight <= 50)
+		return 1;
+	else if (weight < 200)
+		return 2;
+	else if (weight < 400)
+		return 3;
+	else
+		return 4;
+}
+
+void BPF_STRUCT_OPS(qmap_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	static u32 user_cnt, kernel_cnt;
+	struct task_ctx *tctx;
+	u32 pid = p->pid;
+	int idx = weight_to_idx(p->scx.weight);
+	void *ring;
+	s32 cpu;
+
+	if (p->flags & PF_KTHREAD) {
+		if (stall_kernel_nth && !(++kernel_cnt % stall_kernel_nth))
+			return;
+	} else {
+		if (stall_user_nth && !(++user_cnt % stall_user_nth))
+			return;
+	}
+
+	if (test_error_cnt && !--test_error_cnt)
+		scx_bpf_error("test triggering error");
+
+	if (!(tctx = lookup_task_ctx(p)))
+		return;
+
+	/*
+	 * All enqueued tasks must have their core_sched_seq updated for correct
+	 * core-sched ordering, which is why %SCX_OPS_ENQ_LAST is specified in
+	 * qmap_ops.flags.
+	 */
+	tctx->core_sched_seq = core_sched_tail_seqs[idx]++;
+
+	/*
+	 * If qmap_select_cpu() is telling us to or this is the last runnable
+	 * task on the CPU, enqueue locally.
+	 */
+	if (tctx->force_local || (enq_flags & SCX_ENQ_LAST)) {
+		tctx->force_local = false;
+		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, slice_ns, enq_flags);
+		return;
+	}
+
+	/* if !WAKEUP, select_cpu() wasn't called, try direct dispatch */
+	if (!(enq_flags & SCX_ENQ_WAKEUP) &&
+	    (cpu = pick_direct_dispatch_cpu(p, scx_bpf_task_cpu(p))) >= 0) {
+		__sync_fetch_and_add(&nr_ddsp_from_enq, 1);
+		scx_bpf_dispatch(p, SCX_DSQ_LOCAL_ON | cpu, slice_ns, enq_flags);
+		return;
+	}
+
+	/*
+	 * If the task was re-enqueued due to the CPU being preempted by a
+	 * higher priority scheduling class, just re-enqueue the task directly
+	 * on the global DSQ. As we want another CPU to pick it up, find and
+	 * kick an idle CPU.
+	 */
+	if (enq_flags & SCX_ENQ_REENQ) {
+		s32 cpu;
+
+		scx_bpf_dispatch(p, SHARED_DSQ, 0, enq_flags);
+		cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
+		if (cpu >= 0)
+			scx_bpf_kick_cpu(cpu, SCX_KICK_IDLE);
+		return;
+	}
+
+	ring = bpf_map_lookup_elem(&queue_arr, &idx);
+	if (!ring) {
+		scx_bpf_error("failed to find ring %d", idx);
+		return;
+	}
+
+	/* Queue on the selected FIFO. If the FIFO overflows, punt to global. */
+	if (bpf_map_push_elem(ring, &pid, 0)) {
+		scx_bpf_dispatch(p, SHARED_DSQ, slice_ns, enq_flags);
+		return;
+	}
+
+	if (highpri_boosting && p->scx.weight >= HIGHPRI_WEIGHT) {
+		tctx->highpri = true;
+		__sync_fetch_and_add(&nr_highpri_queued, 1);
+	}
+	__sync_fetch_and_add(&nr_enqueued, 1);
+}
+
+/*
+ * The BPF queue map doesn't support removal and sched_ext can handle spurious
+ * dispatches. qmap_dequeue() is only used to collect statistics.
+ */
+void BPF_STRUCT_OPS(qmap_dequeue, struct task_struct *p, u64 deq_flags)
+{
+	__sync_fetch_and_add(&nr_dequeued, 1);
+	if (deq_flags & SCX_DEQ_CORE_SCHED_EXEC)
+		__sync_fetch_and_add(&nr_core_sched_execed, 1);
+}
+
+static void update_core_sched_head_seq(struct task_struct *p)
+{
+	int idx = weight_to_idx(p->scx.weight);
+	struct task_ctx *tctx;
+
+	if ((tctx = lookup_task_ctx(p)))
+		core_sched_head_seqs[idx] = tctx->core_sched_seq;
+}
+
+/*
+ * To demonstrate the use of scx_bpf_dispatch_from_dsq(), implement silly
+ * selective priority boosting mechanism by scanning SHARED_DSQ looking for
+ * highpri tasks, moving them to HIGHPRI_DSQ and then consuming them first. This
+ * makes minor difference only when dsp_batch is larger than 1.
+ *
+ * scx_bpf_dispatch[_vtime]_from_dsq() are allowed both from ops.dispatch() and
+ * non-rq-lock holding BPF programs. As demonstration, this function is called
+ * from qmap_dispatch() and monitor_timerfn().
+ */
+static bool dispatch_highpri(bool from_timer)
+{
+	struct task_struct *p;
+	s32 this_cpu = bpf_get_smp_processor_id();
+
+	/* scan SHARED_DSQ and move highpri tasks to HIGHPRI_DSQ */
+	bpf_for_each(scx_dsq, p, SHARED_DSQ, 0) {
+		static u64 highpri_seq;
+		struct task_ctx *tctx;
+
+		if (!(tctx = lookup_task_ctx(p)))
+			return false;
+
+		if (tctx->highpri) {
+			/* exercise the set_*() and vtime interface too */
+			__COMPAT_scx_bpf_dispatch_from_dsq_set_slice(
+				BPF_FOR_EACH_ITER, slice_ns * 2);
+			__COMPAT_scx_bpf_dispatch_from_dsq_set_vtime(
+				BPF_FOR_EACH_ITER, highpri_seq++);
+			__COMPAT_scx_bpf_dispatch_vtime_from_dsq(
+				BPF_FOR_EACH_ITER, p, HIGHPRI_DSQ, 0);
+		}
+	}
+
+	/*
+	 * Scan HIGHPRI_DSQ and dispatch until a task that can run on this CPU
+	 * is found.
+	 */
+	bpf_for_each(scx_dsq, p, HIGHPRI_DSQ, 0) {
+		bool dispatched = false;
+		s32 cpu;
+
+		if (bpf_cpumask_test_cpu(this_cpu, p->cpus_ptr))
+			cpu = this_cpu;
+		else
+			cpu = scx_bpf_pick_any_cpu(p->cpus_ptr, 0);
+
+		if (__COMPAT_scx_bpf_dispatch_from_dsq(BPF_FOR_EACH_ITER, p,
+						       SCX_DSQ_LOCAL_ON | cpu,
+						       SCX_ENQ_PREEMPT)) {
+			if (cpu == this_cpu) {
+				dispatched = true;
+				__sync_fetch_and_add(&nr_expedited_local, 1);
+			} else {
+				__sync_fetch_and_add(&nr_expedited_remote, 1);
+			}
+			if (from_timer)
+				__sync_fetch_and_add(&nr_expedited_from_timer, 1);
+		} else {
+			__sync_fetch_and_add(&nr_expedited_lost, 1);
+		}
+
+		if (dispatched)
+			return true;
+	}
+
+	return false;
+}
+
+void BPF_STRUCT_OPS(qmap_dispatch, s32 cpu, struct task_struct *prev)
+{
+	struct task_struct *p;
+	struct cpu_ctx *cpuc;
+	u32 zero = 0, batch = dsp_batch ?: 1;
+	void *fifo;
+	s32 i, pid;
+
+	if (dispatch_highpri(false))
+		return;
+
+	if (!nr_highpri_queued && scx_bpf_consume(SHARED_DSQ))
+		return;
+
+	if (dsp_inf_loop_after && nr_dispatched > dsp_inf_loop_after) {
+		/*
+		 * PID 2 should be kthreadd which should mostly be idle and off
+		 * the scheduler. Let's keep dispatching it to force the kernel
+		 * to call this function over and over again.
+		 */
+		p = bpf_task_from_pid(2);
+		if (p) {
+			scx_bpf_dispatch(p, SCX_DSQ_LOCAL, slice_ns, 0);
+			bpf_task_release(p);
+			return;
+		}
+	}
+
+	if (!(cpuc = bpf_map_lookup_elem(&cpu_ctx_stor, &zero))) {
+		scx_bpf_error("failed to look up cpu_ctx");
+		return;
+	}
+
+	for (i = 0; i < 5; i++) {
+		/* Advance the dispatch cursor and pick the fifo. */
+		if (!cpuc->dsp_cnt) {
+			cpuc->dsp_idx = (cpuc->dsp_idx + 1) % 5;
+			cpuc->dsp_cnt = 1 << cpuc->dsp_idx;
+		}
+
+		fifo = bpf_map_lookup_elem(&queue_arr, &cpuc->dsp_idx);
+		if (!fifo) {
+			scx_bpf_error("failed to find ring %llu", cpuc->dsp_idx);
+			return;
+		}
+
+		/* Dispatch or advance. */
+		bpf_repeat(BPF_MAX_LOOPS) {
+			struct task_ctx *tctx;
+
+			if (bpf_map_pop_elem(fifo, &pid))
+				break;
+
+			p = bpf_task_from_pid(pid);
+			if (!p)
+				continue;
+
+			if (!(tctx = lookup_task_ctx(p))) {
+				bpf_task_release(p);
+				return;
+			}
+
+			if (tctx->highpri)
+				__sync_fetch_and_sub(&nr_highpri_queued, 1);
+
+			update_core_sched_head_seq(p);
+			__sync_fetch_and_add(&nr_dispatched, 1);
+
+			scx_bpf_dispatch(p, SHARED_DSQ, slice_ns, 0);
+			bpf_task_release(p);
+
+			batch--;
+			cpuc->dsp_cnt--;
+			if (!batch || !scx_bpf_dispatch_nr_slots()) {
+				if (dispatch_highpri(false))
+					return;
+				scx_bpf_consume(SHARED_DSQ);
+				return;
+			}
+			if (!cpuc->dsp_cnt)
+				break;
+		}
+
+		cpuc->dsp_cnt = 0;
+	}
+}
+
+void BPF_STRUCT_OPS(qmap_tick, struct task_struct *p)
+{
+	struct cpu_ctx *cpuc;
+	u32 zero = 0;
+	int idx;
+
+	if (!(cpuc = bpf_map_lookup_elem(&cpu_ctx_stor, &zero))) {
+		scx_bpf_error("failed to look up cpu_ctx");
+		return;
+	}
+
+	/*
+	 * Use the running avg of weights to select the target cpuperf level.
+	 * This is a demonstration of the cpuperf feature rather than a
+	 * practical strategy to regulate CPU frequency.
+	 */
+	cpuc->avg_weight = cpuc->avg_weight * 3 / 4 + p->scx.weight / 4;
+	idx = weight_to_idx(cpuc->avg_weight);
+	cpuc->cpuperf_target = qidx_to_cpuperf_target[idx];
+
+	scx_bpf_cpuperf_set(scx_bpf_task_cpu(p), cpuc->cpuperf_target);
+}
+
+/*
+ * The distance from the head of the queue scaled by the weight of the queue.
+ * The lower the number, the older the task and the higher the priority.
+ */
+static s64 task_qdist(struct task_struct *p)
+{
+	int idx = weight_to_idx(p->scx.weight);
+	struct task_ctx *tctx;
+	s64 qdist;
+
+	tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0);
+	if (!tctx) {
+		scx_bpf_error("task_ctx lookup failed");
+		return 0;
+	}
+
+	qdist = tctx->core_sched_seq - core_sched_head_seqs[idx];
+
+	/*
+	 * As queue index increments, the priority doubles. The queue w/ index 3
+	 * is dispatched twice more frequently than 2. Reflect the difference by
+	 * scaling qdists accordingly. Note that the shift amount needs to be
+	 * flipped depending on the sign to avoid flipping priority direction.
+	 */
+	if (qdist >= 0)
+		return qdist << (4 - idx);
+	else
+		return qdist << idx;
+}
+
+/*
+ * This is called to determine the task ordering when core-sched is picking
+ * tasks to execute on SMT siblings and should encode about the same ordering as
+ * the regular scheduling path. Use the priority-scaled distances from the head
+ * of the queues to compare the two tasks which should be consistent with the
+ * dispatch path behavior.
+ */
+bool BPF_STRUCT_OPS(qmap_core_sched_before,
+		    struct task_struct *a, struct task_struct *b)
+{
+	return task_qdist(a) > task_qdist(b);
+}
+
+void BPF_STRUCT_OPS(qmap_cpu_release, s32 cpu, struct scx_cpu_release_args *args)
+{
+	u32 cnt;
+
+	/*
+	 * Called when @cpu is taken by a higher priority scheduling class. This
+	 * makes @cpu no longer available for executing sched_ext tasks. As we
+	 * don't want the tasks in @cpu's local dsq to sit there until @cpu
+	 * becomes available again, re-enqueue them into the global dsq. See
+	 * %SCX_ENQ_REENQ handling in qmap_enqueue().
+	 */
+	cnt = scx_bpf_reenqueue_local();
+	if (cnt)
+		__sync_fetch_and_add(&nr_reenqueued, cnt);
+}
+
+s32 BPF_STRUCT_OPS(qmap_init_task, struct task_struct *p,
+		   struct scx_init_task_args *args)
+{
+	if (p->tgid == disallow_tgid)
+		p->scx.disallow = true;
+
+	/*
+	 * @p is new. Let's ensure that its task_ctx is available. We can sleep
+	 * in this function and the following will automatically use GFP_KERNEL.
+	 */
+	if (bpf_task_storage_get(&task_ctx_stor, p, 0,
+				 BPF_LOCAL_STORAGE_GET_F_CREATE))
+		return 0;
+	else
+		return -ENOMEM;
+}
+
+void BPF_STRUCT_OPS(qmap_dump, struct scx_dump_ctx *dctx)
+{
+	s32 i, pid;
+
+	if (suppress_dump)
+		return;
+
+	bpf_for(i, 0, 5) {
+		void *fifo;
+
+		if (!(fifo = bpf_map_lookup_elem(&queue_arr, &i)))
+			return;
+
+		scx_bpf_dump("QMAP FIFO[%d]:", i);
+		bpf_repeat(4096) {
+			if (bpf_map_pop_elem(fifo, &pid))
+				break;
+			scx_bpf_dump(" %d", pid);
+		}
+		scx_bpf_dump("\n");
+	}
+}
+
+void BPF_STRUCT_OPS(qmap_dump_cpu, struct scx_dump_ctx *dctx, s32 cpu, bool idle)
+{
+	u32 zero = 0;
+	struct cpu_ctx *cpuc;
+
+	if (suppress_dump || idle)
+		return;
+	if (!(cpuc = bpf_map_lookup_percpu_elem(&cpu_ctx_stor, &zero, cpu)))
+		return;
+
+	scx_bpf_dump("QMAP: dsp_idx=%llu dsp_cnt=%llu avg_weight=%u cpuperf_target=%u",
+		     cpuc->dsp_idx, cpuc->dsp_cnt, cpuc->avg_weight,
+		     cpuc->cpuperf_target);
+}
+
+void BPF_STRUCT_OPS(qmap_dump_task, struct scx_dump_ctx *dctx, struct task_struct *p)
+{
+	struct task_ctx *taskc;
+
+	if (suppress_dump)
+		return;
+	if (!(taskc = bpf_task_storage_get(&task_ctx_stor, p, 0, 0)))
+		return;
+
+	scx_bpf_dump("QMAP: force_local=%d core_sched_seq=%llu",
+		     taskc->force_local, taskc->core_sched_seq);
+}
+
+/*
+ * Print out the online and possible CPU map using bpf_printk() as a
+ * demonstration of using the cpumask kfuncs and ops.cpu_on/offline().
+ */
+static void print_cpus(void)
+{
+	const struct cpumask *possible, *online;
+	s32 cpu;
+	char buf[128] = "", *p;
+	int idx;
+
+	possible = scx_bpf_get_possible_cpumask();
+	online = scx_bpf_get_online_cpumask();
+
+	idx = 0;
+	bpf_for(cpu, 0, scx_bpf_nr_cpu_ids()) {
+		if (!(p = MEMBER_VPTR(buf, [idx++])))
+			break;
+		if (bpf_cpumask_test_cpu(cpu, online))
+			*p++ = 'O';
+		else if (bpf_cpumask_test_cpu(cpu, possible))
+			*p++ = 'X';
+		else
+			*p++ = ' ';
+
+		if ((cpu & 7) == 7) {
+			if (!(p = MEMBER_VPTR(buf, [idx++])))
+				break;
+			*p++ = '|';
+		}
+	}
+	buf[sizeof(buf) - 1] = '\0';
+
+	scx_bpf_put_cpumask(online);
+	scx_bpf_put_cpumask(possible);
+
+	bpf_printk("CPUS: |%s", buf);
+}
+
+void BPF_STRUCT_OPS(qmap_cpu_online, s32 cpu)
+{
+	bpf_printk("CPU %d coming online", cpu);
+	/* @cpu is already online at this point */
+	print_cpus();
+}
+
+void BPF_STRUCT_OPS(qmap_cpu_offline, s32 cpu)
+{
+	bpf_printk("CPU %d going offline", cpu);
+	/* @cpu is still online at this point */
+	print_cpus();
+}
+
+struct monitor_timer {
+	struct bpf_timer timer;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_ARRAY);
+	__uint(max_entries, 1);
+	__type(key, u32);
+	__type(value, struct monitor_timer);
+} monitor_timer SEC(".maps");
+
+/*
+ * Print out the min, avg and max performance levels of CPUs every second to
+ * demonstrate the cpuperf interface.
+ */
+static void monitor_cpuperf(void)
+{
+	u32 zero = 0, nr_cpu_ids;
+	u64 cap_sum = 0, cur_sum = 0, cur_min = SCX_CPUPERF_ONE, cur_max = 0;
+	u64 target_sum = 0, target_min = SCX_CPUPERF_ONE, target_max = 0;
+	const struct cpumask *online;
+	int i, nr_online_cpus = 0;
+
+	nr_cpu_ids = scx_bpf_nr_cpu_ids();
+	online = scx_bpf_get_online_cpumask();
+
+	bpf_for(i, 0, nr_cpu_ids) {
+		struct cpu_ctx *cpuc;
+		u32 cap, cur;
+
+		if (!bpf_cpumask_test_cpu(i, online))
+			continue;
+		nr_online_cpus++;
+
+		/* collect the capacity and current cpuperf */
+		cap = scx_bpf_cpuperf_cap(i);
+		cur = scx_bpf_cpuperf_cur(i);
+
+		cur_min = cur < cur_min ? cur : cur_min;
+		cur_max = cur > cur_max ? cur : cur_max;
+
+		/*
+		 * $cur is relative to $cap. Scale it down accordingly so that
+		 * it's in the same scale as other CPUs and $cur_sum/$cap_sum
+		 * makes sense.
+		 */
+		cur_sum += cur * cap / SCX_CPUPERF_ONE;
+		cap_sum += cap;
+
+		if (!(cpuc = bpf_map_lookup_percpu_elem(&cpu_ctx_stor, &zero, i))) {
+			scx_bpf_error("failed to look up cpu_ctx");
+			goto out;
+		}
+
+		/* collect target */
+		cur = cpuc->cpuperf_target;
+		target_sum += cur;
+		target_min = cur < target_min ? cur : target_min;
+		target_max = cur > target_max ? cur : target_max;
+	}
+
+	cpuperf_min = cur_min;
+	cpuperf_avg = cur_sum * SCX_CPUPERF_ONE / cap_sum;
+	cpuperf_max = cur_max;
+
+	cpuperf_target_min = target_min;
+	cpuperf_target_avg = target_sum / nr_online_cpus;
+	cpuperf_target_max = target_max;
+out:
+	scx_bpf_put_cpumask(online);
+}
+
+/*
+ * Dump the currently queued tasks in the shared DSQ to demonstrate the usage of
+ * scx_bpf_dsq_nr_queued() and DSQ iterator. Raise the dispatch batch count to
+ * see meaningful dumps in the trace pipe.
+ */
+static void dump_shared_dsq(void)
+{
+	struct task_struct *p;
+	s32 nr;
+
+	if (!(nr = scx_bpf_dsq_nr_queued(SHARED_DSQ)))
+		return;
+
+	bpf_printk("Dumping %d tasks in SHARED_DSQ in reverse order", nr);
+
+	bpf_rcu_read_lock();
+	bpf_for_each(scx_dsq, p, SHARED_DSQ, SCX_DSQ_ITER_REV)
+		bpf_printk("%s[%d]", p->comm, p->pid);
+	bpf_rcu_read_unlock();
+}
+
+static int monitor_timerfn(void *map, int *key, struct bpf_timer *timer)
+{
+	bpf_rcu_read_lock();
+	dispatch_highpri(true);
+	bpf_rcu_read_unlock();
+
+	monitor_cpuperf();
+
+	if (print_shared_dsq)
+		dump_shared_dsq();
+
+	bpf_timer_start(timer, ONE_SEC_IN_NS, 0);
+	return 0;
+}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(qmap_init)
+{
+	u32 key = 0;
+	struct bpf_timer *timer;
+	s32 ret;
+
+	print_cpus();
+
+	ret = scx_bpf_create_dsq(SHARED_DSQ, -1);
+	if (ret)
+		return ret;
+
+	ret = scx_bpf_create_dsq(HIGHPRI_DSQ, -1);
+	if (ret)
+		return ret;
+
+	timer = bpf_map_lookup_elem(&monitor_timer, &key);
+	if (!timer)
+		return -ESRCH;
+
+	bpf_timer_init(timer, &monitor_timer, CLOCK_MONOTONIC);
+	bpf_timer_set_callback(timer, monitor_timerfn);
+
+	return bpf_timer_start(timer, ONE_SEC_IN_NS, 0);
+}
+
+void BPF_STRUCT_OPS(qmap_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SCX_OPS_DEFINE(qmap_ops,
+	       .select_cpu		= (void *)qmap_select_cpu,
+	       .enqueue			= (void *)qmap_enqueue,
+	       .dequeue			= (void *)qmap_dequeue,
+	       .dispatch		= (void *)qmap_dispatch,
+	       .tick			= (void *)qmap_tick,
+	       .core_sched_before	= (void *)qmap_core_sched_before,
+	       .cpu_release		= (void *)qmap_cpu_release,
+	       .init_task		= (void *)qmap_init_task,
+	       .dump			= (void *)qmap_dump,
+	       .dump_cpu		= (void *)qmap_dump_cpu,
+	       .dump_task		= (void *)qmap_dump_task,
+	       .cpu_online		= (void *)qmap_cpu_online,
+	       .cpu_offline		= (void *)qmap_cpu_offline,
+	       .init			= (void *)qmap_init,
+	       .exit			= (void *)qmap_exit,
+	       .flags			= SCX_OPS_ENQ_LAST,
+	       .timeout_ms		= 5000U,
+	       .name			= "qmap");
diff --git a/tools/sched_ext/scx_qmap.c b/tools/sched_ext/scx_qmap.c
new file mode 100644
index 000000000000..ac45a02b4055
--- /dev/null
+++ b/tools/sched_ext/scx_qmap.c
@@ -0,0 +1,153 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <inttypes.h>
+#include <signal.h>
+#include <libgen.h>
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include "scx_qmap.bpf.skel.h"
+
+const char help_fmt[] =
+"A simple five-level FIFO queue sched_ext scheduler.\n"
+"\n"
+"See the top-level comment in .bpf.c for more details.\n"
+"\n"
+"Usage: %s [-s SLICE_US] [-e COUNT] [-t COUNT] [-T COUNT] [-l COUNT] [-b COUNT]\n"
+"       [-P] [-d PID] [-D LEN] [-p] [-v]\n"
+"\n"
+"  -s SLICE_US   Override slice duration\n"
+"  -e COUNT      Trigger scx_bpf_error() after COUNT enqueues\n"
+"  -t COUNT      Stall every COUNT'th user thread\n"
+"  -T COUNT      Stall every COUNT'th kernel thread\n"
+"  -l COUNT      Trigger dispatch infinite looping after COUNT dispatches\n"
+"  -b COUNT      Dispatch upto COUNT tasks together\n"
+"  -P            Print out DSQ content to trace_pipe every second, use with -b\n"
+"  -H            Boost nice -20 tasks in SHARED_DSQ, use with -b\n"
+"  -d PID        Disallow a process from switching into SCHED_EXT (-1 for self)\n"
+"  -D LEN        Set scx_exit_info.dump buffer length\n"
+"  -S            Suppress qmap-specific debug dump\n"
+"  -p            Switch only tasks on SCHED_EXT policy instead of all\n"
+"  -v            Print libbpf debug messages\n"
+"  -h            Display this help and exit\n";
+
+static bool verbose;
+static volatile int exit_req;
+
+static int libbpf_print_fn(enum libbpf_print_level level, const char *format, va_list args)
+{
+	if (level == LIBBPF_DEBUG && !verbose)
+		return 0;
+	return vfprintf(stderr, format, args);
+}
+
+static void sigint_handler(int dummy)
+{
+	exit_req = 1;
+}
+
+int main(int argc, char **argv)
+{
+	struct scx_qmap *skel;
+	struct bpf_link *link;
+	int opt;
+
+	libbpf_set_print(libbpf_print_fn);
+	signal(SIGINT, sigint_handler);
+	signal(SIGTERM, sigint_handler);
+
+	skel = SCX_OPS_OPEN(qmap_ops, scx_qmap);
+
+	while ((opt = getopt(argc, argv, "s:e:t:T:l:b:PHd:D:Spvh")) != -1) {
+		switch (opt) {
+		case 's':
+			skel->rodata->slice_ns = strtoull(optarg, NULL, 0) * 1000;
+			break;
+		case 'e':
+			skel->bss->test_error_cnt = strtoul(optarg, NULL, 0);
+			break;
+		case 't':
+			skel->rodata->stall_user_nth = strtoul(optarg, NULL, 0);
+			break;
+		case 'T':
+			skel->rodata->stall_kernel_nth = strtoul(optarg, NULL, 0);
+			break;
+		case 'l':
+			skel->rodata->dsp_inf_loop_after = strtoul(optarg, NULL, 0);
+			break;
+		case 'b':
+			skel->rodata->dsp_batch = strtoul(optarg, NULL, 0);
+			break;
+		case 'P':
+			skel->rodata->print_shared_dsq = true;
+			break;
+		case 'H':
+			skel->rodata->highpri_boosting = true;
+			break;
+		case 'd':
+			skel->rodata->disallow_tgid = strtol(optarg, NULL, 0);
+			if (skel->rodata->disallow_tgid < 0)
+				skel->rodata->disallow_tgid = getpid();
+			break;
+		case 'D':
+			skel->struct_ops.qmap_ops->exit_dump_len = strtoul(optarg, NULL, 0);
+			break;
+		case 'S':
+			skel->rodata->suppress_dump = true;
+			break;
+		case 'p':
+			skel->struct_ops.qmap_ops->flags |= SCX_OPS_SWITCH_PARTIAL;
+			break;
+		case 'v':
+			verbose = true;
+			break;
+		default:
+			fprintf(stderr, help_fmt, basename(argv[0]));
+			return opt != 'h';
+		}
+	}
+
+	SCX_OPS_LOAD(skel, qmap_ops, scx_qmap, uei);
+	link = SCX_OPS_ATTACH(skel, qmap_ops, scx_qmap);
+
+	while (!exit_req && !UEI_EXITED(skel, uei)) {
+		long nr_enqueued = skel->bss->nr_enqueued;
+		long nr_dispatched = skel->bss->nr_dispatched;
+
+		printf("stats  : enq=%lu dsp=%lu delta=%ld reenq=%"PRIu64" deq=%"PRIu64" core=%"PRIu64" enq_ddsp=%"PRIu64"\n",
+		       nr_enqueued, nr_dispatched, nr_enqueued - nr_dispatched,
+		       skel->bss->nr_reenqueued, skel->bss->nr_dequeued,
+		       skel->bss->nr_core_sched_execed,
+		       skel->bss->nr_ddsp_from_enq);
+		printf("         exp_local=%"PRIu64" exp_remote=%"PRIu64" exp_timer=%"PRIu64" exp_lost=%"PRIu64"\n",
+		       skel->bss->nr_expedited_local,
+		       skel->bss->nr_expedited_remote,
+		       skel->bss->nr_expedited_from_timer,
+		       skel->bss->nr_expedited_lost);
+		if (__COMPAT_has_ksym("scx_bpf_cpuperf_cur"))
+			printf("cpuperf: cur min/avg/max=%u/%u/%u target min/avg/max=%u/%u/%u\n",
+			       skel->bss->cpuperf_min,
+			       skel->bss->cpuperf_avg,
+			       skel->bss->cpuperf_max,
+			       skel->bss->cpuperf_target_min,
+			       skel->bss->cpuperf_target_avg,
+			       skel->bss->cpuperf_target_max);
+		fflush(stdout);
+		sleep(1);
+	}
+
+	bpf_link__destroy(link);
+	UEI_REPORT(skel, uei);
+	scx_qmap__destroy(skel);
+	/*
+	 * scx_qmap implements ops.cpu_on/offline() and doesn't need to restart
+	 * on CPU hotplug events.
+	 */
+	return 0;
+}
diff --git a/tools/sched_ext/scx_show_state.py b/tools/sched_ext/scx_show_state.py
new file mode 100644
index 000000000000..8bc626ede1c4
--- /dev/null
+++ b/tools/sched_ext/scx_show_state.py
@@ -0,0 +1,40 @@
+#!/usr/bin/env drgn
+#
+# Copyright (C) 2024 Tejun Heo <tj@kernel.org>
+# Copyright (C) 2024 Meta Platforms, Inc. and affiliates.
+
+desc = """
+This is a drgn script to show the current sched_ext state.
+For more info on drgn, visit https://github.com/osandov/drgn.
+"""
+
+import drgn
+import sys
+
+def err(s):
+    print(s, file=sys.stderr, flush=True)
+    sys.exit(1)
+
+def read_int(name):
+    return int(prog[name].value_())
+
+def read_atomic(name):
+    return prog[name].counter.value_()
+
+def read_static_key(name):
+    return prog[name].key.enabled.counter.value_()
+
+def ops_state_str(state):
+    return prog['scx_ops_enable_state_str'][state].string_().decode()
+
+ops = prog['scx_ops']
+enable_state = read_atomic("scx_ops_enable_state_var")
+
+print(f'ops           : {ops.name.string_().decode()}')
+print(f'enabled       : {read_static_key("__scx_ops_enabled")}')
+print(f'switching_all : {read_int("scx_switching_all")}')
+print(f'switched_all  : {read_static_key("__scx_switched_all")}')
+print(f'enable_state  : {ops_state_str(enable_state)} ({enable_state})')
+print(f'bypass_depth  : {read_atomic("scx_ops_bypass_depth")}')
+print(f'nr_rejected   : {read_atomic("scx_nr_rejected")}')
+print(f'enable_seq    : {read_atomic("scx_enable_seq")}')
diff --git a/tools/sched_ext/scx_simple.bpf.c b/tools/sched_ext/scx_simple.bpf.c
new file mode 100644
index 000000000000..ed7e8d535fc5
--- /dev/null
+++ b/tools/sched_ext/scx_simple.bpf.c
@@ -0,0 +1,156 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A simple scheduler.
+ *
+ * By default, it operates as a simple global weighted vtime scheduler and can
+ * be switched to FIFO scheduling. It also demonstrates the following niceties.
+ *
+ * - Statistics tracking how many tasks are queued to local and global dsq's.
+ * - Termination notification for userspace.
+ *
+ * While very simple, this scheduler should work reasonably well on CPUs with a
+ * uniform L3 cache topology. While preemption is not implemented, the fact that
+ * the scheduling queue is shared across all CPUs means that whatever is at the
+ * front of the queue is likely to be executed fairly quickly given enough
+ * number of CPUs. The FIFO scheduling mode may be beneficial to some workloads
+ * but comes with the usual problems with FIFO scheduling where saturating
+ * threads can easily drown out interactive ones.
+ *
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+const volatile bool fifo_sched;
+
+static u64 vtime_now;
+UEI_DEFINE(uei);
+
+/*
+ * Built-in DSQs such as SCX_DSQ_GLOBAL cannot be used as priority queues
+ * (meaning, cannot be dispatched to with scx_bpf_dispatch_vtime()). We
+ * therefore create a separate DSQ with ID 0 that we dispatch to and consume
+ * from. If scx_simple only supported global FIFO scheduling, then we could
+ * just use SCX_DSQ_GLOBAL.
+ */
+#define SHARED_DSQ 0
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
+	__uint(key_size, sizeof(u32));
+	__uint(value_size, sizeof(u64));
+	__uint(max_entries, 2);			/* [local, global] */
+} stats SEC(".maps");
+
+static void stat_inc(u32 idx)
+{
+	u64 *cnt_p = bpf_map_lookup_elem(&stats, &idx);
+	if (cnt_p)
+		(*cnt_p)++;
+}
+
+static inline bool vtime_before(u64 a, u64 b)
+{
+	return (s64)(a - b) < 0;
+}
+
+s32 BPF_STRUCT_OPS(simple_select_cpu, struct task_struct *p, s32 prev_cpu, u64 wake_flags)
+{
+	bool is_idle = false;
+	s32 cpu;
+
+	cpu = scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags, &is_idle);
+	if (is_idle) {
+		stat_inc(0);	/* count local queueing */
+		scx_bpf_dispatch(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL, 0);
+	}
+
+	return cpu;
+}
+
+void BPF_STRUCT_OPS(simple_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	stat_inc(1);	/* count global queueing */
+
+	if (fifo_sched) {
+		scx_bpf_dispatch(p, SHARED_DSQ, SCX_SLICE_DFL, enq_flags);
+	} else {
+		u64 vtime = p->scx.dsq_vtime;
+
+		/*
+		 * Limit the amount of budget that an idling task can accumulate
+		 * to one slice.
+		 */
+		if (vtime_before(vtime, vtime_now - SCX_SLICE_DFL))
+			vtime = vtime_now - SCX_SLICE_DFL;
+
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ, SCX_SLICE_DFL, vtime,
+				       enq_flags);
+	}
+}
+
+void BPF_STRUCT_OPS(simple_dispatch, s32 cpu, struct task_struct *prev)
+{
+	scx_bpf_consume(SHARED_DSQ);
+}
+
+void BPF_STRUCT_OPS(simple_running, struct task_struct *p)
+{
+	if (fifo_sched)
+		return;
+
+	/*
+	 * Global vtime always progresses forward as tasks start executing. The
+	 * test and update can be performed concurrently from multiple CPUs and
+	 * thus racy. Any error should be contained and temporary. Let's just
+	 * live with it.
+	 */
+	if (vtime_before(vtime_now, p->scx.dsq_vtime))
+		vtime_now = p->scx.dsq_vtime;
+}
+
+void BPF_STRUCT_OPS(simple_stopping, struct task_struct *p, bool runnable)
+{
+	if (fifo_sched)
+		return;
+
+	/*
+	 * Scale the execution time by the inverse of the weight and charge.
+	 *
+	 * Note that the default yield implementation yields by setting
+	 * @p->scx.slice to zero and the following would treat the yielding task
+	 * as if it has consumed all its slice. If this penalizes yielding tasks
+	 * too much, determine the execution time by taking explicit timestamps
+	 * instead of depending on @p->scx.slice.
+	 */
+	p->scx.dsq_vtime += (SCX_SLICE_DFL - p->scx.slice) * 100 / p->scx.weight;
+}
+
+void BPF_STRUCT_OPS(simple_enable, struct task_struct *p)
+{
+	p->scx.dsq_vtime = vtime_now;
+}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(simple_init)
+{
+	return scx_bpf_create_dsq(SHARED_DSQ, -1);
+}
+
+void BPF_STRUCT_OPS(simple_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SCX_OPS_DEFINE(simple_ops,
+	       .select_cpu		= (void *)simple_select_cpu,
+	       .enqueue			= (void *)simple_enqueue,
+	       .dispatch		= (void *)simple_dispatch,
+	       .running			= (void *)simple_running,
+	       .stopping		= (void *)simple_stopping,
+	       .enable			= (void *)simple_enable,
+	       .init			= (void *)simple_init,
+	       .exit			= (void *)simple_exit,
+	       .name			= "simple");
diff --git a/tools/sched_ext/scx_simple.c b/tools/sched_ext/scx_simple.c
new file mode 100644
index 000000000000..76d83199545c
--- /dev/null
+++ b/tools/sched_ext/scx_simple.c
@@ -0,0 +1,107 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2022 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2022 David Vernet <dvernet@meta.com>
+ */
+#include <stdio.h>
+#include <unistd.h>
+#include <signal.h>
+#include <libgen.h>
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include "scx_simple.bpf.skel.h"
+
+const char help_fmt[] =
+"A simple sched_ext scheduler.\n"
+"\n"
+"See the top-level comment in .bpf.c for more details.\n"
+"\n"
+"Usage: %s [-f] [-v]\n"
+"\n"
+"  -f            Use FIFO scheduling instead of weighted vtime scheduling\n"
+"  -v            Print libbpf debug messages\n"
+"  -h            Display this help and exit\n";
+
+static bool verbose;
+static volatile int exit_req;
+
+static int libbpf_print_fn(enum libbpf_print_level level, const char *format, va_list args)
+{
+	if (level == LIBBPF_DEBUG && !verbose)
+		return 0;
+	return vfprintf(stderr, format, args);
+}
+
+static void sigint_handler(int simple)
+{
+	exit_req = 1;
+}
+
+static void read_stats(struct scx_simple *skel, __u64 *stats)
+{
+	int nr_cpus = libbpf_num_possible_cpus();
+	__u64 cnts[2][nr_cpus];
+	__u32 idx;
+
+	memset(stats, 0, sizeof(stats[0]) * 2);
+
+	for (idx = 0; idx < 2; idx++) {
+		int ret, cpu;
+
+		ret = bpf_map_lookup_elem(bpf_map__fd(skel->maps.stats),
+					  &idx, cnts[idx]);
+		if (ret < 0)
+			continue;
+		for (cpu = 0; cpu < nr_cpus; cpu++)
+			stats[idx] += cnts[idx][cpu];
+	}
+}
+
+int main(int argc, char **argv)
+{
+	struct scx_simple *skel;
+	struct bpf_link *link;
+	__u32 opt;
+	__u64 ecode;
+
+	libbpf_set_print(libbpf_print_fn);
+	signal(SIGINT, sigint_handler);
+	signal(SIGTERM, sigint_handler);
+restart:
+	skel = SCX_OPS_OPEN(simple_ops, scx_simple);
+
+	while ((opt = getopt(argc, argv, "fvh")) != -1) {
+		switch (opt) {
+		case 'f':
+			skel->rodata->fifo_sched = true;
+			break;
+		case 'v':
+			verbose = true;
+			break;
+		default:
+			fprintf(stderr, help_fmt, basename(argv[0]));
+			return opt != 'h';
+		}
+	}
+
+	SCX_OPS_LOAD(skel, simple_ops, scx_simple, uei);
+	link = SCX_OPS_ATTACH(skel, simple_ops, scx_simple);
+
+	while (!exit_req && !UEI_EXITED(skel, uei)) {
+		__u64 stats[2];
+
+		read_stats(skel, stats);
+		printf("local=%llu global=%llu\n", stats[0], stats[1]);
+		fflush(stdout);
+		sleep(1);
+	}
+
+	bpf_link__destroy(link);
+	ecode = UEI_REPORT(skel, uei);
+	scx_simple__destroy(skel);
+
+	if (UEI_ECODE_RESTART(ecode))
+		goto restart;
+	return 0;
+}
diff --git a/tools/testing/selftests/sched_ext/.gitignore b/tools/testing/selftests/sched_ext/.gitignore
new file mode 100644
index 000000000000..ae5491a114c0
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/.gitignore
@@ -0,0 +1,6 @@
+*
+!*.c
+!*.h
+!Makefile
+!.gitignore
+!config
diff --git a/tools/testing/selftests/sched_ext/Makefile b/tools/testing/selftests/sched_ext/Makefile
new file mode 100644
index 000000000000..0754a2c110a1
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/Makefile
@@ -0,0 +1,218 @@
+# SPDX-License-Identifier: GPL-2.0
+# Copyright (c) 2022 Meta Platforms, Inc. and affiliates.
+include ../../../build/Build.include
+include ../../../scripts/Makefile.arch
+include ../../../scripts/Makefile.include
+include ../lib.mk
+
+ifneq ($(LLVM),)
+ifneq ($(filter %/,$(LLVM)),)
+LLVM_PREFIX := $(LLVM)
+else ifneq ($(filter -%,$(LLVM)),)
+LLVM_SUFFIX := $(LLVM)
+endif
+
+CC := $(LLVM_PREFIX)clang$(LLVM_SUFFIX) $(CLANG_FLAGS) -fintegrated-as
+else
+CC := gcc
+endif # LLVM
+
+ifneq ($(CROSS_COMPILE),)
+$(error CROSS_COMPILE not supported for scx selftests)
+endif # CROSS_COMPILE
+
+CURDIR := $(abspath .)
+REPOROOT := $(abspath ../../../..)
+TOOLSDIR := $(REPOROOT)/tools
+LIBDIR := $(TOOLSDIR)/lib
+BPFDIR := $(LIBDIR)/bpf
+TOOLSINCDIR := $(TOOLSDIR)/include
+BPFTOOLDIR := $(TOOLSDIR)/bpf/bpftool
+APIDIR := $(TOOLSINCDIR)/uapi
+GENDIR := $(REPOROOT)/include/generated
+GENHDR := $(GENDIR)/autoconf.h
+SCXTOOLSDIR := $(TOOLSDIR)/sched_ext
+SCXTOOLSINCDIR := $(TOOLSDIR)/sched_ext/include
+
+OUTPUT_DIR := $(CURDIR)/build
+OBJ_DIR := $(OUTPUT_DIR)/obj
+INCLUDE_DIR := $(OUTPUT_DIR)/include
+BPFOBJ_DIR := $(OBJ_DIR)/libbpf
+SCXOBJ_DIR := $(OBJ_DIR)/sched_ext
+BPFOBJ := $(BPFOBJ_DIR)/libbpf.a
+LIBBPF_OUTPUT := $(OBJ_DIR)/libbpf/libbpf.a
+DEFAULT_BPFTOOL := $(OUTPUT_DIR)/sbin/bpftool
+HOST_BUILD_DIR := $(OBJ_DIR)
+HOST_OUTPUT_DIR := $(OUTPUT_DIR)
+
+VMLINUX_BTF_PATHS ?= ../../../../vmlinux					\
+		     /sys/kernel/btf/vmlinux					\
+		     /boot/vmlinux-$(shell uname -r)
+VMLINUX_BTF ?= $(abspath $(firstword $(wildcard $(VMLINUX_BTF_PATHS))))
+ifeq ($(VMLINUX_BTF),)
+$(error Cannot find a vmlinux for VMLINUX_BTF at any of "$(VMLINUX_BTF_PATHS)")
+endif
+
+BPFTOOL ?= $(DEFAULT_BPFTOOL)
+
+ifneq ($(wildcard $(GENHDR)),)
+  GENFLAGS := -DHAVE_GENHDR
+endif
+
+CFLAGS += -g -O2 -rdynamic -pthread -Wall -Werror $(GENFLAGS)			\
+	  -I$(INCLUDE_DIR) -I$(GENDIR) -I$(LIBDIR)				\
+	  -I$(TOOLSINCDIR) -I$(APIDIR) -I$(CURDIR)/include -I$(SCXTOOLSINCDIR)
+
+# Silence some warnings when compiled with clang
+ifneq ($(LLVM),)
+CFLAGS += -Wno-unused-command-line-argument
+endif
+
+LDFLAGS = -lelf -lz -lpthread -lzstd
+
+IS_LITTLE_ENDIAN = $(shell $(CC) -dM -E - </dev/null |				\
+			grep 'define __BYTE_ORDER__ __ORDER_LITTLE_ENDIAN__')
+
+# Get Clang's default includes on this system, as opposed to those seen by
+# '-target bpf'. This fixes "missing" files on some architectures/distros,
+# such as asm/byteorder.h, asm/socket.h, asm/sockios.h, sys/cdefs.h etc.
+#
+# Use '-idirafter': Don't interfere with include mechanics except where the
+# build would have failed anyways.
+define get_sys_includes
+$(shell $(1) -v -E - </dev/null 2>&1 \
+	| sed -n '/<...> search starts here:/,/End of search list./{ s| \(/.*\)|-idirafter \1|p }') \
+$(shell $(1) -dM -E - </dev/null | grep '__riscv_xlen ' | awk '{printf("-D__riscv_xlen=%d -D__BITS_PER_LONG=%d", $$3, $$3)}')
+endef
+
+BPF_CFLAGS = -g -D__TARGET_ARCH_$(SRCARCH)					\
+	     $(if $(IS_LITTLE_ENDIAN),-mlittle-endian,-mbig-endian)		\
+	     -I$(CURDIR)/include -I$(CURDIR)/include/bpf-compat			\
+	     -I$(INCLUDE_DIR) -I$(APIDIR) -I$(SCXTOOLSINCDIR)			\
+	     -I$(REPOROOT)/include						\
+	     $(call get_sys_includes,$(CLANG))					\
+	     -Wall -Wno-compare-distinct-pointer-types				\
+	     -Wno-incompatible-function-pointer-types				\
+	     -O2 -mcpu=v3
+
+# sort removes libbpf duplicates when not cross-building
+MAKE_DIRS := $(sort $(OBJ_DIR)/libbpf $(OBJ_DIR)/libbpf				\
+	       $(OBJ_DIR)/bpftool $(OBJ_DIR)/resolve_btfids			\
+	       $(INCLUDE_DIR) $(SCXOBJ_DIR))
+
+$(MAKE_DIRS):
+	$(call msg,MKDIR,,$@)
+	$(Q)mkdir -p $@
+
+$(BPFOBJ): $(wildcard $(BPFDIR)/*.[ch] $(BPFDIR)/Makefile)			\
+	   $(APIDIR)/linux/bpf.h						\
+	   | $(OBJ_DIR)/libbpf
+	$(Q)$(MAKE) $(submake_extras) -C $(BPFDIR) OUTPUT=$(OBJ_DIR)/libbpf/	\
+		    EXTRA_CFLAGS='-g -O0 -fPIC'					\
+		    DESTDIR=$(OUTPUT_DIR) prefix= all install_headers
+
+$(DEFAULT_BPFTOOL): $(wildcard $(BPFTOOLDIR)/*.[ch] $(BPFTOOLDIR)/Makefile)	\
+		    $(LIBBPF_OUTPUT) | $(OBJ_DIR)/bpftool
+	$(Q)$(MAKE) $(submake_extras)  -C $(BPFTOOLDIR)				\
+		    ARCH= CROSS_COMPILE= CC=$(HOSTCC) LD=$(HOSTLD)		\
+		    EXTRA_CFLAGS='-g -O0'					\
+		    OUTPUT=$(OBJ_DIR)/bpftool/					\
+		    LIBBPF_OUTPUT=$(OBJ_DIR)/libbpf/				\
+		    LIBBPF_DESTDIR=$(OUTPUT_DIR)/				\
+		    prefix= DESTDIR=$(OUTPUT_DIR)/ install-bin
+
+$(INCLUDE_DIR)/vmlinux.h: $(VMLINUX_BTF) $(BPFTOOL) | $(INCLUDE_DIR)
+ifeq ($(VMLINUX_H),)
+	$(call msg,GEN,,$@)
+	$(Q)$(BPFTOOL) btf dump file $(VMLINUX_BTF) format c > $@
+else
+	$(call msg,CP,,$@)
+	$(Q)cp "$(VMLINUX_H)" $@
+endif
+
+$(SCXOBJ_DIR)/%.bpf.o: %.bpf.c $(INCLUDE_DIR)/vmlinux.h	| $(BPFOBJ) $(SCXOBJ_DIR)
+	$(call msg,CLNG-BPF,,$(notdir $@))
+	$(Q)$(CLANG) $(BPF_CFLAGS) -target bpf -c $< -o $@
+
+$(INCLUDE_DIR)/%.bpf.skel.h: $(SCXOBJ_DIR)/%.bpf.o $(INCLUDE_DIR)/vmlinux.h $(BPFTOOL) | $(INCLUDE_DIR)
+	$(eval sched=$(notdir $@))
+	$(call msg,GEN-SKEL,,$(sched))
+	$(Q)$(BPFTOOL) gen object $(<:.o=.linked1.o) $<
+	$(Q)$(BPFTOOL) gen object $(<:.o=.linked2.o) $(<:.o=.linked1.o)
+	$(Q)$(BPFTOOL) gen object $(<:.o=.linked3.o) $(<:.o=.linked2.o)
+	$(Q)diff $(<:.o=.linked2.o) $(<:.o=.linked3.o)
+	$(Q)$(BPFTOOL) gen skeleton $(<:.o=.linked3.o) name $(subst .bpf.skel.h,,$(sched)) > $@
+	$(Q)$(BPFTOOL) gen subskeleton $(<:.o=.linked3.o) name $(subst .bpf.skel.h,,$(sched)) > $(@:.skel.h=.subskel.h)
+
+################
+# C schedulers #
+################
+
+override define CLEAN
+	rm -rf $(OUTPUT_DIR)
+	rm -f *.o *.bpf.o *.bpf.skel.h *.bpf.subskel.h
+	rm -f $(TEST_GEN_PROGS)
+	rm -f runner
+endef
+
+# Every testcase takes all of the BPF progs are dependencies by default. This
+# allows testcases to load any BPF scheduler, which is useful for testcases
+# that don't need their own prog to run their test.
+all_test_bpfprogs := $(foreach prog,$(wildcard *.bpf.c),$(INCLUDE_DIR)/$(patsubst %.c,%.skel.h,$(prog)))
+
+auto-test-targets :=			\
+	create_dsq			\
+	enq_last_no_enq_fails		\
+	enq_select_cpu_fails		\
+	ddsp_bogus_dsq_fail		\
+	ddsp_vtimelocal_fail		\
+	dsp_local_on			\
+	exit				\
+	hotplug				\
+	init_enable_count		\
+	maximal				\
+	maybe_null			\
+	minimal				\
+	prog_run			\
+	reload_loop			\
+	select_cpu_dfl			\
+	select_cpu_dfl_nodispatch	\
+	select_cpu_dispatch		\
+	select_cpu_dispatch_bad_dsq	\
+	select_cpu_dispatch_dbl_dsp	\
+	select_cpu_vtime		\
+	test_example			\
+
+testcase-targets := $(addsuffix .o,$(addprefix $(SCXOBJ_DIR)/,$(auto-test-targets)))
+
+$(SCXOBJ_DIR)/runner.o: runner.c | $(SCXOBJ_DIR)
+	$(CC) $(CFLAGS) -c $< -o $@
+
+# Create all of the test targets object files, whose testcase objects will be
+# registered into the runner in ELF constructors.
+#
+# Note that we must do double expansion here in order to support conditionally
+# compiling BPF object files only if one is present, as the wildcard Make
+# function doesn't support using implicit rules otherwise.
+$(testcase-targets): $(SCXOBJ_DIR)/%.o: %.c $(SCXOBJ_DIR)/runner.o $(all_test_bpfprogs) | $(SCXOBJ_DIR)
+	$(eval test=$(patsubst %.o,%.c,$(notdir $@)))
+	$(CC) $(CFLAGS) -c $< -o $@ $(SCXOBJ_DIR)/runner.o
+
+$(SCXOBJ_DIR)/util.o: util.c | $(SCXOBJ_DIR)
+	$(CC) $(CFLAGS) -c $< -o $@
+
+runner: $(SCXOBJ_DIR)/runner.o $(SCXOBJ_DIR)/util.o $(BPFOBJ) $(testcase-targets)
+	@echo "$(testcase-targets)"
+	$(CC) $(CFLAGS) -o $@ $^ $(LDFLAGS)
+
+TEST_GEN_PROGS := runner
+
+all: runner
+
+.PHONY: all clean help
+
+.DEFAULT_GOAL := all
+
+.DELETE_ON_ERROR:
+
+.SECONDARY:
diff --git a/tools/testing/selftests/sched_ext/config b/tools/testing/selftests/sched_ext/config
new file mode 100644
index 000000000000..0de9b4ee249d
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/config
@@ -0,0 +1,9 @@
+CONFIG_SCHED_DEBUG=y
+CONFIG_SCHED_CLASS_EXT=y
+CONFIG_CGROUPS=y
+CONFIG_CGROUP_SCHED=y
+CONFIG_EXT_GROUP_SCHED=y
+CONFIG_BPF=y
+CONFIG_BPF_SYSCALL=y
+CONFIG_DEBUG_INFO=y
+CONFIG_DEBUG_INFO_BTF=y
diff --git a/tools/testing/selftests/sched_ext/create_dsq.bpf.c b/tools/testing/selftests/sched_ext/create_dsq.bpf.c
new file mode 100644
index 000000000000..23f79ed343f0
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/create_dsq.bpf.c
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Create and destroy DSQs in a loop.
+ *
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+void BPF_STRUCT_OPS(create_dsq_exit_task, struct task_struct *p,
+		    struct scx_exit_task_args *args)
+{
+	scx_bpf_destroy_dsq(p->pid);
+}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(create_dsq_init_task, struct task_struct *p,
+			     struct scx_init_task_args *args)
+{
+	s32 err;
+
+	err = scx_bpf_create_dsq(p->pid, -1);
+	if (err)
+		scx_bpf_error("Failed to create DSQ for %s[%d]",
+			      p->comm, p->pid);
+
+	return err;
+}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(create_dsq_init)
+{
+	u32 i;
+	s32 err;
+
+	bpf_for(i, 0, 1024) {
+		err = scx_bpf_create_dsq(i, -1);
+		if (err) {
+			scx_bpf_error("Failed to create DSQ %d", i);
+			return 0;
+		}
+	}
+
+	bpf_for(i, 0, 1024) {
+		scx_bpf_destroy_dsq(i);
+	}
+
+	return 0;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops create_dsq_ops = {
+	.init_task		= create_dsq_init_task,
+	.exit_task		= create_dsq_exit_task,
+	.init			= create_dsq_init,
+	.name			= "create_dsq",
+};
diff --git a/tools/testing/selftests/sched_ext/create_dsq.c b/tools/testing/selftests/sched_ext/create_dsq.c
new file mode 100644
index 000000000000..fa946d9146d4
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/create_dsq.c
@@ -0,0 +1,57 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "create_dsq.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct create_dsq *skel;
+
+	skel = create_dsq__open_and_load();
+	if (!skel) {
+		SCX_ERR("Failed to open and load skel");
+		return SCX_TEST_FAIL;
+	}
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct create_dsq *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.create_dsq_ops);
+	if (!link) {
+		SCX_ERR("Failed to attach scheduler");
+		return SCX_TEST_FAIL;
+	}
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct create_dsq *skel = ctx;
+
+	create_dsq__destroy(skel);
+}
+
+struct scx_test create_dsq = {
+	.name = "create_dsq",
+	.description = "Create and destroy a dsq in a loop",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&create_dsq)
diff --git a/tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.bpf.c b/tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.bpf.c
new file mode 100644
index 000000000000..e97ad41d354a
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.bpf.c
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ */
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+UEI_DEFINE(uei);
+
+s32 BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	s32 cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
+
+	if (cpu >= 0) {
+		/*
+		 * If we dispatch to a bogus DSQ that will fall back to the
+		 * builtin global DSQ, we fail gracefully.
+		 */
+		scx_bpf_dispatch_vtime(p, 0xcafef00d, SCX_SLICE_DFL,
+				       p->scx.dsq_vtime, 0);
+		return cpu;
+	}
+
+	return prev_cpu;
+}
+
+void BPF_STRUCT_OPS(ddsp_bogus_dsq_fail_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops ddsp_bogus_dsq_fail_ops = {
+	.select_cpu		= ddsp_bogus_dsq_fail_select_cpu,
+	.exit			= ddsp_bogus_dsq_fail_exit,
+	.name			= "ddsp_bogus_dsq_fail",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.c b/tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.c
new file mode 100644
index 000000000000..e65d22f23f3b
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/ddsp_bogus_dsq_fail.c
@@ -0,0 +1,57 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "ddsp_bogus_dsq_fail.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct ddsp_bogus_dsq_fail *skel;
+
+	skel = ddsp_bogus_dsq_fail__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct ddsp_bogus_dsq_fail *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.ddsp_bogus_dsq_fail_ops);
+	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+
+	sleep(1);
+
+	SCX_EQ(skel->data->uei.kind, EXIT_KIND(SCX_EXIT_ERROR));
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct ddsp_bogus_dsq_fail *skel = ctx;
+
+	ddsp_bogus_dsq_fail__destroy(skel);
+}
+
+struct scx_test ddsp_bogus_dsq_fail = {
+	.name = "ddsp_bogus_dsq_fail",
+	.description = "Verify we gracefully fail, and fall back to using a "
+		       "built-in DSQ, if we do a direct dispatch to an invalid"
+		       " DSQ in ops.select_cpu()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&ddsp_bogus_dsq_fail)
diff --git a/tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.bpf.c b/tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.bpf.c
new file mode 100644
index 000000000000..dde7e7dafbfb
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.bpf.c
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ */
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+UEI_DEFINE(uei);
+
+s32 BPF_STRUCT_OPS(ddsp_vtimelocal_fail_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	s32 cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
+
+	if (cpu >= 0) {
+		/* Shouldn't be allowed to vtime dispatch to a builtin DSQ. */
+		scx_bpf_dispatch_vtime(p, SCX_DSQ_LOCAL, SCX_SLICE_DFL,
+				       p->scx.dsq_vtime, 0);
+		return cpu;
+	}
+
+	return prev_cpu;
+}
+
+void BPF_STRUCT_OPS(ddsp_vtimelocal_fail_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops ddsp_vtimelocal_fail_ops = {
+	.select_cpu		= ddsp_vtimelocal_fail_select_cpu,
+	.exit			= ddsp_vtimelocal_fail_exit,
+	.name			= "ddsp_vtimelocal_fail",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.c b/tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.c
new file mode 100644
index 000000000000..abafee587cd6
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/ddsp_vtimelocal_fail.c
@@ -0,0 +1,56 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <unistd.h>
+#include "ddsp_vtimelocal_fail.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct ddsp_vtimelocal_fail *skel;
+
+	skel = ddsp_vtimelocal_fail__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct ddsp_vtimelocal_fail *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.ddsp_vtimelocal_fail_ops);
+	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+
+	sleep(1);
+
+	SCX_EQ(skel->data->uei.kind, EXIT_KIND(SCX_EXIT_ERROR));
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct ddsp_vtimelocal_fail *skel = ctx;
+
+	ddsp_vtimelocal_fail__destroy(skel);
+}
+
+struct scx_test ddsp_vtimelocal_fail = {
+	.name = "ddsp_vtimelocal_fail",
+	.description = "Verify we gracefully fail, and fall back to using a "
+		       "built-in DSQ, if we do a direct vtime dispatch to a "
+		       "built-in DSQ from DSQ in ops.select_cpu()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&ddsp_vtimelocal_fail)
diff --git a/tools/testing/selftests/sched_ext/dsp_local_on.bpf.c b/tools/testing/selftests/sched_ext/dsp_local_on.bpf.c
new file mode 100644
index 000000000000..efb4672decb4
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/dsp_local_on.bpf.c
@@ -0,0 +1,65 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+const volatile s32 nr_cpus;
+
+UEI_DEFINE(uei);
+
+struct {
+	__uint(type, BPF_MAP_TYPE_QUEUE);
+	__uint(max_entries, 8192);
+	__type(value, s32);
+} queue SEC(".maps");
+
+s32 BPF_STRUCT_OPS(dsp_local_on_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	return prev_cpu;
+}
+
+void BPF_STRUCT_OPS(dsp_local_on_enqueue, struct task_struct *p,
+		    u64 enq_flags)
+{
+	s32 pid = p->pid;
+
+	if (bpf_map_push_elem(&queue, &pid, 0))
+		scx_bpf_error("Failed to enqueue %s[%d]", p->comm, p->pid);
+}
+
+void BPF_STRUCT_OPS(dsp_local_on_dispatch, s32 cpu, struct task_struct *prev)
+{
+	s32 pid, target;
+	struct task_struct *p;
+
+	if (bpf_map_pop_elem(&queue, &pid))
+		return;
+
+	p = bpf_task_from_pid(pid);
+	if (!p)
+		return;
+
+	target = bpf_get_prandom_u32() % nr_cpus;
+
+	scx_bpf_dispatch(p, SCX_DSQ_LOCAL_ON | target, SCX_SLICE_DFL, 0);
+	bpf_task_release(p);
+}
+
+void BPF_STRUCT_OPS(dsp_local_on_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops dsp_local_on_ops = {
+	.select_cpu		= dsp_local_on_select_cpu,
+	.enqueue		= dsp_local_on_enqueue,
+	.dispatch		= dsp_local_on_dispatch,
+	.exit			= dsp_local_on_exit,
+	.name			= "dsp_local_on",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/dsp_local_on.c b/tools/testing/selftests/sched_ext/dsp_local_on.c
new file mode 100644
index 000000000000..472851b56854
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/dsp_local_on.c
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <unistd.h>
+#include "dsp_local_on.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct dsp_local_on *skel;
+
+	skel = dsp_local_on__open();
+	SCX_FAIL_IF(!skel, "Failed to open");
+
+	skel->rodata->nr_cpus = libbpf_num_possible_cpus();
+	SCX_FAIL_IF(dsp_local_on__load(skel), "Failed to load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct dsp_local_on *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.dsp_local_on_ops);
+	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+
+	/* Just sleeping is fine, plenty of scheduling events happening */
+	sleep(1);
+
+	SCX_EQ(skel->data->uei.kind, EXIT_KIND(SCX_EXIT_ERROR));
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct dsp_local_on *skel = ctx;
+
+	dsp_local_on__destroy(skel);
+}
+
+struct scx_test dsp_local_on = {
+	.name = "dsp_local_on",
+	.description = "Verify we can directly dispatch tasks to a local DSQs "
+		       "from osp.dispatch()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&dsp_local_on)
diff --git a/tools/testing/selftests/sched_ext/enq_last_no_enq_fails.bpf.c b/tools/testing/selftests/sched_ext/enq_last_no_enq_fails.bpf.c
new file mode 100644
index 000000000000..b0b99531d5d5
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/enq_last_no_enq_fails.bpf.c
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates the behavior of direct dispatching with a default
+ * select_cpu implementation.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+SEC(".struct_ops.link")
+struct sched_ext_ops enq_last_no_enq_fails_ops = {
+	.name			= "enq_last_no_enq_fails",
+	/* Need to define ops.enqueue() with SCX_OPS_ENQ_LAST */
+	.flags			= SCX_OPS_ENQ_LAST,
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/enq_last_no_enq_fails.c b/tools/testing/selftests/sched_ext/enq_last_no_enq_fails.c
new file mode 100644
index 000000000000..2a3eda5e2c0b
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/enq_last_no_enq_fails.c
@@ -0,0 +1,60 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "enq_last_no_enq_fails.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct enq_last_no_enq_fails *skel;
+
+	skel = enq_last_no_enq_fails__open_and_load();
+	if (!skel) {
+		SCX_ERR("Failed to open and load skel");
+		return SCX_TEST_FAIL;
+	}
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct enq_last_no_enq_fails *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.enq_last_no_enq_fails_ops);
+	if (link) {
+		SCX_ERR("Incorrectly succeeded in to attaching scheduler");
+		return SCX_TEST_FAIL;
+	}
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct enq_last_no_enq_fails *skel = ctx;
+
+	enq_last_no_enq_fails__destroy(skel);
+}
+
+struct scx_test enq_last_no_enq_fails = {
+	.name = "enq_last_no_enq_fails",
+	.description = "Verify we fail to load a scheduler if we specify "
+		       "the SCX_OPS_ENQ_LAST flag without defining "
+		       "ops.enqueue()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&enq_last_no_enq_fails)
diff --git a/tools/testing/selftests/sched_ext/enq_select_cpu_fails.bpf.c b/tools/testing/selftests/sched_ext/enq_select_cpu_fails.bpf.c
new file mode 100644
index 000000000000..b3dfc1033cd6
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/enq_select_cpu_fails.bpf.c
@@ -0,0 +1,43 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+/* Manually specify the signature until the kfunc is added to the scx repo. */
+s32 scx_bpf_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flags,
+			   bool *found) __ksym;
+
+s32 BPF_STRUCT_OPS(enq_select_cpu_fails_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	return prev_cpu;
+}
+
+void BPF_STRUCT_OPS(enq_select_cpu_fails_enqueue, struct task_struct *p,
+		    u64 enq_flags)
+{
+	/*
+	 * Need to initialize the variable or the verifier will fail to load.
+	 * Improving these semantics is actively being worked on.
+	 */
+	bool found = false;
+
+	/* Can only call from ops.select_cpu() */
+	scx_bpf_select_cpu_dfl(p, 0, 0, &found);
+
+	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops enq_select_cpu_fails_ops = {
+	.select_cpu		= enq_select_cpu_fails_select_cpu,
+	.enqueue		= enq_select_cpu_fails_enqueue,
+	.name			= "enq_select_cpu_fails",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/enq_select_cpu_fails.c b/tools/testing/selftests/sched_ext/enq_select_cpu_fails.c
new file mode 100644
index 000000000000..dd1350e5f002
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/enq_select_cpu_fails.c
@@ -0,0 +1,61 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "enq_select_cpu_fails.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct enq_select_cpu_fails *skel;
+
+	skel = enq_select_cpu_fails__open_and_load();
+	if (!skel) {
+		SCX_ERR("Failed to open and load skel");
+		return SCX_TEST_FAIL;
+	}
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct enq_select_cpu_fails *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.enq_select_cpu_fails_ops);
+	if (!link) {
+		SCX_ERR("Failed to attach scheduler");
+		return SCX_TEST_FAIL;
+	}
+
+	sleep(1);
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct enq_select_cpu_fails *skel = ctx;
+
+	enq_select_cpu_fails__destroy(skel);
+}
+
+struct scx_test enq_select_cpu_fails = {
+	.name = "enq_select_cpu_fails",
+	.description = "Verify we fail to call scx_bpf_select_cpu_dfl() "
+		       "from ops.enqueue()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&enq_select_cpu_fails)
diff --git a/tools/testing/selftests/sched_ext/exit.bpf.c b/tools/testing/selftests/sched_ext/exit.bpf.c
new file mode 100644
index 000000000000..ae12ddaac921
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/exit.bpf.c
@@ -0,0 +1,84 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+#include "exit_test.h"
+
+const volatile int exit_point;
+UEI_DEFINE(uei);
+
+#define EXIT_CLEANLY() scx_bpf_exit(exit_point, "%d", exit_point)
+
+s32 BPF_STRUCT_OPS(exit_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	bool found;
+
+	if (exit_point == EXIT_SELECT_CPU)
+		EXIT_CLEANLY();
+
+	return scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags, &found);
+}
+
+void BPF_STRUCT_OPS(exit_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	if (exit_point == EXIT_ENQUEUE)
+		EXIT_CLEANLY();
+
+	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
+}
+
+void BPF_STRUCT_OPS(exit_dispatch, s32 cpu, struct task_struct *p)
+{
+	if (exit_point == EXIT_DISPATCH)
+		EXIT_CLEANLY();
+
+	scx_bpf_consume(SCX_DSQ_GLOBAL);
+}
+
+void BPF_STRUCT_OPS(exit_enable, struct task_struct *p)
+{
+	if (exit_point == EXIT_ENABLE)
+		EXIT_CLEANLY();
+}
+
+s32 BPF_STRUCT_OPS(exit_init_task, struct task_struct *p,
+		    struct scx_init_task_args *args)
+{
+	if (exit_point == EXIT_INIT_TASK)
+		EXIT_CLEANLY();
+
+	return 0;
+}
+
+void BPF_STRUCT_OPS(exit_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(exit_init)
+{
+	if (exit_point == EXIT_INIT)
+		EXIT_CLEANLY();
+
+	return 0;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops exit_ops = {
+	.select_cpu		= exit_select_cpu,
+	.enqueue		= exit_enqueue,
+	.dispatch		= exit_dispatch,
+	.init_task		= exit_init_task,
+	.enable			= exit_enable,
+	.exit			= exit_exit,
+	.init			= exit_init,
+	.name			= "exit",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/exit.c b/tools/testing/selftests/sched_ext/exit.c
new file mode 100644
index 000000000000..31bcd06e21cd
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/exit.c
@@ -0,0 +1,55 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <sched.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "exit.bpf.skel.h"
+#include "scx_test.h"
+
+#include "exit_test.h"
+
+static enum scx_test_status run(void *ctx)
+{
+	enum exit_test_case tc;
+
+	for (tc = 0; tc < NUM_EXITS; tc++) {
+		struct exit *skel;
+		struct bpf_link *link;
+		char buf[16];
+
+		skel = exit__open();
+		skel->rodata->exit_point = tc;
+		exit__load(skel);
+		link = bpf_map__attach_struct_ops(skel->maps.exit_ops);
+		if (!link) {
+			SCX_ERR("Failed to attach scheduler");
+			exit__destroy(skel);
+			return SCX_TEST_FAIL;
+		}
+
+		/* Assumes uei.kind is written last */
+		while (skel->data->uei.kind == EXIT_KIND(SCX_EXIT_NONE))
+			sched_yield();
+
+		SCX_EQ(skel->data->uei.kind, EXIT_KIND(SCX_EXIT_UNREG_BPF));
+		SCX_EQ(skel->data->uei.exit_code, tc);
+		sprintf(buf, "%d", tc);
+		SCX_ASSERT(!strcmp(skel->data->uei.msg, buf));
+		bpf_link__destroy(link);
+		exit__destroy(skel);
+	}
+
+	return SCX_TEST_PASS;
+}
+
+struct scx_test exit_test = {
+	.name = "exit",
+	.description = "Verify we can cleanly exit a scheduler in multiple places",
+	.run = run,
+};
+REGISTER_SCX_TEST(&exit_test)
diff --git a/tools/testing/selftests/sched_ext/exit_test.h b/tools/testing/selftests/sched_ext/exit_test.h
new file mode 100644
index 000000000000..94f0268b9cb8
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/exit_test.h
@@ -0,0 +1,20 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+
+#ifndef __EXIT_TEST_H__
+#define __EXIT_TEST_H__
+
+enum exit_test_case {
+	EXIT_SELECT_CPU,
+	EXIT_ENQUEUE,
+	EXIT_DISPATCH,
+	EXIT_ENABLE,
+	EXIT_INIT_TASK,
+	EXIT_INIT,
+	NUM_EXITS,
+};
+
+#endif  // # __EXIT_TEST_H__
diff --git a/tools/testing/selftests/sched_ext/hotplug.bpf.c b/tools/testing/selftests/sched_ext/hotplug.bpf.c
new file mode 100644
index 000000000000..8f2601db39f3
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/hotplug.bpf.c
@@ -0,0 +1,61 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+#include "hotplug_test.h"
+
+UEI_DEFINE(uei);
+
+void BPF_STRUCT_OPS(hotplug_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+static void exit_from_hotplug(s32 cpu, bool onlining)
+{
+	/*
+	 * Ignored, just used to verify that we can invoke blocking kfuncs
+	 * from the hotplug path.
+	 */
+	scx_bpf_create_dsq(0, -1);
+
+	s64 code = SCX_ECODE_ACT_RESTART | HOTPLUG_EXIT_RSN;
+
+	if (onlining)
+		code |= HOTPLUG_ONLINING;
+
+	scx_bpf_exit(code, "hotplug event detected (%d going %s)", cpu,
+		     onlining ? "online" : "offline");
+}
+
+void BPF_STRUCT_OPS_SLEEPABLE(hotplug_cpu_online, s32 cpu)
+{
+	exit_from_hotplug(cpu, true);
+}
+
+void BPF_STRUCT_OPS_SLEEPABLE(hotplug_cpu_offline, s32 cpu)
+{
+	exit_from_hotplug(cpu, false);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops hotplug_cb_ops = {
+	.cpu_online		= hotplug_cpu_online,
+	.cpu_offline		= hotplug_cpu_offline,
+	.exit			= hotplug_exit,
+	.name			= "hotplug_cbs",
+	.timeout_ms		= 1000U,
+};
+
+SEC(".struct_ops.link")
+struct sched_ext_ops hotplug_nocb_ops = {
+	.exit			= hotplug_exit,
+	.name			= "hotplug_nocbs",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/hotplug.c b/tools/testing/selftests/sched_ext/hotplug.c
new file mode 100644
index 000000000000..87bf220b1bce
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/hotplug.c
@@ -0,0 +1,168 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <sched.h>
+#include <scx/common.h>
+#include <sched.h>
+#include <sys/wait.h>
+#include <unistd.h>
+
+#include "hotplug_test.h"
+#include "hotplug.bpf.skel.h"
+#include "scx_test.h"
+#include "util.h"
+
+const char *online_path = "/sys/devices/system/cpu/cpu1/online";
+
+static bool is_cpu_online(void)
+{
+	return file_read_long(online_path) > 0;
+}
+
+static void toggle_online_status(bool online)
+{
+	long val = online ? 1 : 0;
+	int ret;
+
+	ret = file_write_long(online_path, val);
+	if (ret != 0)
+		fprintf(stderr, "Failed to bring CPU %s (%s)",
+			online ? "online" : "offline", strerror(errno));
+}
+
+static enum scx_test_status setup(void **ctx)
+{
+	if (!is_cpu_online())
+		return SCX_TEST_SKIP;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status test_hotplug(bool onlining, bool cbs_defined)
+{
+	struct hotplug *skel;
+	struct bpf_link *link;
+	long kind, code;
+
+	SCX_ASSERT(is_cpu_online());
+
+	skel = hotplug__open_and_load();
+	SCX_ASSERT(skel);
+
+	/* Testing the offline -> online path, so go offline before starting */
+	if (onlining)
+		toggle_online_status(0);
+
+	if (cbs_defined) {
+		kind = SCX_KIND_VAL(SCX_EXIT_UNREG_BPF);
+		code = SCX_ECODE_VAL(SCX_ECODE_ACT_RESTART) | HOTPLUG_EXIT_RSN;
+		if (onlining)
+			code |= HOTPLUG_ONLINING;
+	} else {
+		kind = SCX_KIND_VAL(SCX_EXIT_UNREG_KERN);
+		code = SCX_ECODE_VAL(SCX_ECODE_ACT_RESTART) |
+		       SCX_ECODE_VAL(SCX_ECODE_RSN_HOTPLUG);
+	}
+
+	if (cbs_defined)
+		link = bpf_map__attach_struct_ops(skel->maps.hotplug_cb_ops);
+	else
+		link = bpf_map__attach_struct_ops(skel->maps.hotplug_nocb_ops);
+
+	if (!link) {
+		SCX_ERR("Failed to attach scheduler");
+		hotplug__destroy(skel);
+		return SCX_TEST_FAIL;
+	}
+
+	toggle_online_status(onlining ? 1 : 0);
+
+	while (!UEI_EXITED(skel, uei))
+		sched_yield();
+
+	SCX_EQ(skel->data->uei.kind, kind);
+	SCX_EQ(UEI_REPORT(skel, uei), code);
+
+	if (!onlining)
+		toggle_online_status(1);
+
+	bpf_link__destroy(link);
+	hotplug__destroy(skel);
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status test_hotplug_attach(void)
+{
+	struct hotplug *skel;
+	struct bpf_link *link;
+	enum scx_test_status status = SCX_TEST_PASS;
+	long kind, code;
+
+	SCX_ASSERT(is_cpu_online());
+	SCX_ASSERT(scx_hotplug_seq() > 0);
+
+	skel = SCX_OPS_OPEN(hotplug_nocb_ops, hotplug);
+	SCX_ASSERT(skel);
+
+	SCX_OPS_LOAD(skel, hotplug_nocb_ops, hotplug, uei);
+
+	/*
+	 * Take the CPU offline to increment the global hotplug seq, which
+	 * should cause attach to fail due to us setting the hotplug seq above
+	 */
+	toggle_online_status(0);
+	link = bpf_map__attach_struct_ops(skel->maps.hotplug_nocb_ops);
+
+	toggle_online_status(1);
+
+	SCX_ASSERT(link);
+	while (!UEI_EXITED(skel, uei))
+		sched_yield();
+
+	kind = SCX_KIND_VAL(SCX_EXIT_UNREG_KERN);
+	code = SCX_ECODE_VAL(SCX_ECODE_ACT_RESTART) |
+	       SCX_ECODE_VAL(SCX_ECODE_RSN_HOTPLUG);
+	SCX_EQ(skel->data->uei.kind, kind);
+	SCX_EQ(UEI_REPORT(skel, uei), code);
+
+	bpf_link__destroy(link);
+	hotplug__destroy(skel);
+
+	return status;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+
+#define HP_TEST(__onlining, __cbs_defined) ({				\
+	if (test_hotplug(__onlining, __cbs_defined) != SCX_TEST_PASS)	\
+		return SCX_TEST_FAIL;					\
+})
+
+	HP_TEST(true, true);
+	HP_TEST(false, true);
+	HP_TEST(true, false);
+	HP_TEST(false, false);
+
+#undef HP_TEST
+
+	return test_hotplug_attach();
+}
+
+static void cleanup(void *ctx)
+{
+	toggle_online_status(1);
+}
+
+struct scx_test hotplug_test = {
+	.name = "hotplug",
+	.description = "Verify hotplug behavior",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&hotplug_test)
diff --git a/tools/testing/selftests/sched_ext/hotplug_test.h b/tools/testing/selftests/sched_ext/hotplug_test.h
new file mode 100644
index 000000000000..73d236f90787
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/hotplug_test.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+
+#ifndef __HOTPLUG_TEST_H__
+#define __HOTPLUG_TEST_H__
+
+enum hotplug_test_flags {
+	HOTPLUG_EXIT_RSN = 1LLU << 0,
+	HOTPLUG_ONLINING = 1LLU << 1,
+};
+
+#endif  // # __HOTPLUG_TEST_H__
diff --git a/tools/testing/selftests/sched_ext/init_enable_count.bpf.c b/tools/testing/selftests/sched_ext/init_enable_count.bpf.c
new file mode 100644
index 000000000000..47ea89a626c3
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/init_enable_count.bpf.c
@@ -0,0 +1,53 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that verifies that we do proper counting of init, enable, etc
+ * callbacks.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+u64 init_task_cnt, exit_task_cnt, enable_cnt, disable_cnt;
+u64 init_fork_cnt, init_transition_cnt;
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(cnt_init_task, struct task_struct *p,
+			     struct scx_init_task_args *args)
+{
+	__sync_fetch_and_add(&init_task_cnt, 1);
+
+	if (args->fork)
+		__sync_fetch_and_add(&init_fork_cnt, 1);
+	else
+		__sync_fetch_and_add(&init_transition_cnt, 1);
+
+	return 0;
+}
+
+void BPF_STRUCT_OPS(cnt_exit_task, struct task_struct *p)
+{
+	__sync_fetch_and_add(&exit_task_cnt, 1);
+}
+
+void BPF_STRUCT_OPS(cnt_enable, struct task_struct *p)
+{
+	__sync_fetch_and_add(&enable_cnt, 1);
+}
+
+void BPF_STRUCT_OPS(cnt_disable, struct task_struct *p)
+{
+	__sync_fetch_and_add(&disable_cnt, 1);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops init_enable_count_ops = {
+	.init_task	= cnt_init_task,
+	.exit_task	= cnt_exit_task,
+	.enable		= cnt_enable,
+	.disable	= cnt_disable,
+	.name		= "init_enable_count",
+};
diff --git a/tools/testing/selftests/sched_ext/init_enable_count.c b/tools/testing/selftests/sched_ext/init_enable_count.c
new file mode 100644
index 000000000000..97d45f1e5597
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/init_enable_count.c
@@ -0,0 +1,166 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <stdio.h>
+#include <unistd.h>
+#include <sched.h>
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include "scx_test.h"
+#include "init_enable_count.bpf.skel.h"
+
+#define SCHED_EXT 7
+
+static struct init_enable_count *
+open_load_prog(bool global)
+{
+	struct init_enable_count *skel;
+
+	skel = init_enable_count__open();
+	SCX_BUG_ON(!skel, "Failed to open skel");
+
+	if (!global)
+		skel->struct_ops.init_enable_count_ops->flags |= SCX_OPS_SWITCH_PARTIAL;
+
+	SCX_BUG_ON(init_enable_count__load(skel), "Failed to load skel");
+
+	return skel;
+}
+
+static enum scx_test_status run_test(bool global)
+{
+	struct init_enable_count *skel;
+	struct bpf_link *link;
+	const u32 num_children = 5, num_pre_forks = 1024;
+	int ret, i, status;
+	struct sched_param param = {};
+	pid_t pids[num_pre_forks];
+
+	skel = open_load_prog(global);
+
+	/*
+	 * Fork a bunch of children before we attach the scheduler so that we
+	 * ensure (at least in practical terms) that there are more tasks that
+	 * transition from SCHED_OTHER -> SCHED_EXT than there are tasks that
+	 * take the fork() path either below or in other processes.
+	 */
+	for (i = 0; i < num_pre_forks; i++) {
+		pids[i] = fork();
+		SCX_FAIL_IF(pids[i] < 0, "Failed to fork child");
+		if (pids[i] == 0) {
+			sleep(1);
+			exit(0);
+		}
+	}
+
+	link = bpf_map__attach_struct_ops(skel->maps.init_enable_count_ops);
+	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+
+	for (i = 0; i < num_pre_forks; i++) {
+		SCX_FAIL_IF(waitpid(pids[i], &status, 0) != pids[i],
+			    "Failed to wait for pre-forked child\n");
+
+		SCX_FAIL_IF(status != 0, "Pre-forked child %d exited with status %d\n", i,
+			    status);
+	}
+
+	bpf_link__destroy(link);
+	SCX_GE(skel->bss->init_task_cnt, num_pre_forks);
+	SCX_GE(skel->bss->exit_task_cnt, num_pre_forks);
+
+	link = bpf_map__attach_struct_ops(skel->maps.init_enable_count_ops);
+	SCX_FAIL_IF(!link, "Failed to attach struct_ops");
+
+	/* SCHED_EXT children */
+	for (i = 0; i < num_children; i++) {
+		pids[i] = fork();
+		SCX_FAIL_IF(pids[i] < 0, "Failed to fork child");
+
+		if (pids[i] == 0) {
+			ret = sched_setscheduler(0, SCHED_EXT, &param);
+			SCX_BUG_ON(ret, "Failed to set sched to sched_ext");
+
+			/*
+			 * Reset to SCHED_OTHER for half of them. Counts for
+			 * everything should still be the same regardless, as
+			 * ops.disable() is invoked even if a task is still on
+			 * SCHED_EXT before it exits.
+			 */
+			if (i % 2 == 0) {
+				ret = sched_setscheduler(0, SCHED_OTHER, &param);
+				SCX_BUG_ON(ret, "Failed to reset sched to normal");
+			}
+			exit(0);
+		}
+	}
+	for (i = 0; i < num_children; i++) {
+		SCX_FAIL_IF(waitpid(pids[i], &status, 0) != pids[i],
+			    "Failed to wait for SCX child\n");
+
+		SCX_FAIL_IF(status != 0, "SCX child %d exited with status %d\n", i,
+			    status);
+	}
+
+	/* SCHED_OTHER children */
+	for (i = 0; i < num_children; i++) {
+		pids[i] = fork();
+		if (pids[i] == 0)
+			exit(0);
+	}
+
+	for (i = 0; i < num_children; i++) {
+		SCX_FAIL_IF(waitpid(pids[i], &status, 0) != pids[i],
+			    "Failed to wait for normal child\n");
+
+		SCX_FAIL_IF(status != 0, "Normal child %d exited with status %d\n", i,
+			    status);
+	}
+
+	bpf_link__destroy(link);
+
+	SCX_GE(skel->bss->init_task_cnt, 2 * num_children);
+	SCX_GE(skel->bss->exit_task_cnt, 2 * num_children);
+
+	if (global) {
+		SCX_GE(skel->bss->enable_cnt, 2 * num_children);
+		SCX_GE(skel->bss->disable_cnt, 2 * num_children);
+	} else {
+		SCX_EQ(skel->bss->enable_cnt, num_children);
+		SCX_EQ(skel->bss->disable_cnt, num_children);
+	}
+	/*
+	 * We forked a ton of tasks before we attached the scheduler above, so
+	 * this should be fine. Technically it could be flaky if a ton of forks
+	 * are happening at the same time in other processes, but that should
+	 * be exceedingly unlikely.
+	 */
+	SCX_GT(skel->bss->init_transition_cnt, skel->bss->init_fork_cnt);
+	SCX_GE(skel->bss->init_fork_cnt, 2 * num_children);
+
+	init_enable_count__destroy(skel);
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	enum scx_test_status status;
+
+	status = run_test(true);
+	if (status != SCX_TEST_PASS)
+		return status;
+
+	return run_test(false);
+}
+
+struct scx_test init_enable_count = {
+	.name = "init_enable_count",
+	.description = "Verify we do the correct amount of counting of init, "
+		       "enable, etc callbacks.",
+	.run = run,
+};
+REGISTER_SCX_TEST(&init_enable_count)
diff --git a/tools/testing/selftests/sched_ext/maximal.bpf.c b/tools/testing/selftests/sched_ext/maximal.bpf.c
new file mode 100644
index 000000000000..00bfa9cb95d3
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/maximal.bpf.c
@@ -0,0 +1,164 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler with every callback defined.
+ *
+ * This scheduler defines every callback.
+ *
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+s32 BPF_STRUCT_OPS(maximal_select_cpu, struct task_struct *p, s32 prev_cpu,
+		   u64 wake_flags)
+{
+	return prev_cpu;
+}
+
+void BPF_STRUCT_OPS(maximal_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
+}
+
+void BPF_STRUCT_OPS(maximal_dequeue, struct task_struct *p, u64 deq_flags)
+{}
+
+void BPF_STRUCT_OPS(maximal_dispatch, s32 cpu, struct task_struct *prev)
+{
+	scx_bpf_consume(SCX_DSQ_GLOBAL);
+}
+
+void BPF_STRUCT_OPS(maximal_runnable, struct task_struct *p, u64 enq_flags)
+{}
+
+void BPF_STRUCT_OPS(maximal_running, struct task_struct *p)
+{}
+
+void BPF_STRUCT_OPS(maximal_stopping, struct task_struct *p, bool runnable)
+{}
+
+void BPF_STRUCT_OPS(maximal_quiescent, struct task_struct *p, u64 deq_flags)
+{}
+
+bool BPF_STRUCT_OPS(maximal_yield, struct task_struct *from,
+		    struct task_struct *to)
+{
+	return false;
+}
+
+bool BPF_STRUCT_OPS(maximal_core_sched_before, struct task_struct *a,
+		    struct task_struct *b)
+{
+	return false;
+}
+
+void BPF_STRUCT_OPS(maximal_set_weight, struct task_struct *p, u32 weight)
+{}
+
+void BPF_STRUCT_OPS(maximal_set_cpumask, struct task_struct *p,
+		    const struct cpumask *cpumask)
+{}
+
+void BPF_STRUCT_OPS(maximal_update_idle, s32 cpu, bool idle)
+{}
+
+void BPF_STRUCT_OPS(maximal_cpu_acquire, s32 cpu,
+		    struct scx_cpu_acquire_args *args)
+{}
+
+void BPF_STRUCT_OPS(maximal_cpu_release, s32 cpu,
+		    struct scx_cpu_release_args *args)
+{}
+
+void BPF_STRUCT_OPS(maximal_cpu_online, s32 cpu)
+{}
+
+void BPF_STRUCT_OPS(maximal_cpu_offline, s32 cpu)
+{}
+
+s32 BPF_STRUCT_OPS(maximal_init_task, struct task_struct *p,
+		   struct scx_init_task_args *args)
+{
+	return 0;
+}
+
+void BPF_STRUCT_OPS(maximal_enable, struct task_struct *p)
+{}
+
+void BPF_STRUCT_OPS(maximal_exit_task, struct task_struct *p,
+		    struct scx_exit_task_args *args)
+{}
+
+void BPF_STRUCT_OPS(maximal_disable, struct task_struct *p)
+{}
+
+s32 BPF_STRUCT_OPS(maximal_cgroup_init, struct cgroup *cgrp,
+		   struct scx_cgroup_init_args *args)
+{
+	return 0;
+}
+
+void BPF_STRUCT_OPS(maximal_cgroup_exit, struct cgroup *cgrp)
+{}
+
+s32 BPF_STRUCT_OPS(maximal_cgroup_prep_move, struct task_struct *p,
+		   struct cgroup *from, struct cgroup *to)
+{
+	return 0;
+}
+
+void BPF_STRUCT_OPS(maximal_cgroup_move, struct task_struct *p,
+		    struct cgroup *from, struct cgroup *to)
+{}
+
+void BPF_STRUCT_OPS(maximal_cgroup_cancel_move, struct task_struct *p,
+	       struct cgroup *from, struct cgroup *to)
+{}
+
+void BPF_STRUCT_OPS(maximal_cgroup_set_weight, struct cgroup *cgrp, u32 weight)
+{}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(maximal_init)
+{
+	return 0;
+}
+
+void BPF_STRUCT_OPS(maximal_exit, struct scx_exit_info *info)
+{}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops maximal_ops = {
+	.select_cpu		= maximal_select_cpu,
+	.enqueue		= maximal_enqueue,
+	.dequeue		= maximal_dequeue,
+	.dispatch		= maximal_dispatch,
+	.runnable		= maximal_runnable,
+	.running		= maximal_running,
+	.stopping		= maximal_stopping,
+	.quiescent		= maximal_quiescent,
+	.yield			= maximal_yield,
+	.core_sched_before	= maximal_core_sched_before,
+	.set_weight		= maximal_set_weight,
+	.set_cpumask		= maximal_set_cpumask,
+	.update_idle		= maximal_update_idle,
+	.cpu_acquire		= maximal_cpu_acquire,
+	.cpu_release		= maximal_cpu_release,
+	.cpu_online		= maximal_cpu_online,
+	.cpu_offline		= maximal_cpu_offline,
+	.init_task		= maximal_init_task,
+	.enable			= maximal_enable,
+	.exit_task		= maximal_exit_task,
+	.disable		= maximal_disable,
+	.cgroup_init		= maximal_cgroup_init,
+	.cgroup_exit		= maximal_cgroup_exit,
+	.cgroup_prep_move	= maximal_cgroup_prep_move,
+	.cgroup_move		= maximal_cgroup_move,
+	.cgroup_cancel_move	= maximal_cgroup_cancel_move,
+	.cgroup_set_weight	= maximal_cgroup_set_weight,
+	.init			= maximal_init,
+	.exit			= maximal_exit,
+	.name			= "maximal",
+};
diff --git a/tools/testing/selftests/sched_ext/maximal.c b/tools/testing/selftests/sched_ext/maximal.c
new file mode 100644
index 000000000000..f38fc973c380
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/maximal.c
@@ -0,0 +1,51 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "maximal.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct maximal *skel;
+
+	skel = maximal__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct maximal *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.maximal_ops);
+	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct maximal *skel = ctx;
+
+	maximal__destroy(skel);
+}
+
+struct scx_test maximal = {
+	.name = "maximal",
+	.description = "Verify we can load a scheduler with every callback defined",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&maximal)
diff --git a/tools/testing/selftests/sched_ext/maybe_null.bpf.c b/tools/testing/selftests/sched_ext/maybe_null.bpf.c
new file mode 100644
index 000000000000..27d0f386acfb
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/maybe_null.bpf.c
@@ -0,0 +1,36 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+u64 vtime_test;
+
+void BPF_STRUCT_OPS(maybe_null_running, struct task_struct *p)
+{}
+
+void BPF_STRUCT_OPS(maybe_null_success_dispatch, s32 cpu, struct task_struct *p)
+{
+	if (p != NULL)
+		vtime_test = p->scx.dsq_vtime;
+}
+
+bool BPF_STRUCT_OPS(maybe_null_success_yield, struct task_struct *from,
+		    struct task_struct *to)
+{
+	if (to)
+		bpf_printk("Yielding to %s[%d]", to->comm, to->pid);
+
+	return false;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops maybe_null_success = {
+	.dispatch               = maybe_null_success_dispatch,
+	.yield			= maybe_null_success_yield,
+	.enable			= maybe_null_running,
+	.name			= "minimal",
+};
diff --git a/tools/testing/selftests/sched_ext/maybe_null.c b/tools/testing/selftests/sched_ext/maybe_null.c
new file mode 100644
index 000000000000..31cfafb0cf65
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/maybe_null.c
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "maybe_null.bpf.skel.h"
+#include "maybe_null_fail_dsp.bpf.skel.h"
+#include "maybe_null_fail_yld.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status run(void *ctx)
+{
+	struct maybe_null *skel;
+	struct maybe_null_fail_dsp *fail_dsp;
+	struct maybe_null_fail_yld *fail_yld;
+
+	skel = maybe_null__open_and_load();
+	if (!skel) {
+		SCX_ERR("Failed to open and load maybe_null skel");
+		return SCX_TEST_FAIL;
+	}
+	maybe_null__destroy(skel);
+
+	fail_dsp = maybe_null_fail_dsp__open_and_load();
+	if (fail_dsp) {
+		maybe_null_fail_dsp__destroy(fail_dsp);
+		SCX_ERR("Should failed to open and load maybe_null_fail_dsp skel");
+		return SCX_TEST_FAIL;
+	}
+
+	fail_yld = maybe_null_fail_yld__open_and_load();
+	if (fail_yld) {
+		maybe_null_fail_yld__destroy(fail_yld);
+		SCX_ERR("Should failed to open and load maybe_null_fail_yld skel");
+		return SCX_TEST_FAIL;
+	}
+
+	return SCX_TEST_PASS;
+}
+
+struct scx_test maybe_null = {
+	.name = "maybe_null",
+	.description = "Verify if PTR_MAYBE_NULL work for .dispatch",
+	.run = run,
+};
+REGISTER_SCX_TEST(&maybe_null)
diff --git a/tools/testing/selftests/sched_ext/maybe_null_fail_dsp.bpf.c b/tools/testing/selftests/sched_ext/maybe_null_fail_dsp.bpf.c
new file mode 100644
index 000000000000..c0641050271d
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/maybe_null_fail_dsp.bpf.c
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+u64 vtime_test;
+
+void BPF_STRUCT_OPS(maybe_null_running, struct task_struct *p)
+{}
+
+void BPF_STRUCT_OPS(maybe_null_fail_dispatch, s32 cpu, struct task_struct *p)
+{
+	vtime_test = p->scx.dsq_vtime;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops maybe_null_fail = {
+	.dispatch               = maybe_null_fail_dispatch,
+	.enable			= maybe_null_running,
+	.name			= "maybe_null_fail_dispatch",
+};
diff --git a/tools/testing/selftests/sched_ext/maybe_null_fail_yld.bpf.c b/tools/testing/selftests/sched_ext/maybe_null_fail_yld.bpf.c
new file mode 100644
index 000000000000..3c1740028e3b
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/maybe_null_fail_yld.bpf.c
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+u64 vtime_test;
+
+void BPF_STRUCT_OPS(maybe_null_running, struct task_struct *p)
+{}
+
+bool BPF_STRUCT_OPS(maybe_null_fail_yield, struct task_struct *from,
+		    struct task_struct *to)
+{
+	bpf_printk("Yielding to %s[%d]", to->comm, to->pid);
+
+	return false;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops maybe_null_fail = {
+	.yield			= maybe_null_fail_yield,
+	.enable			= maybe_null_running,
+	.name			= "maybe_null_fail_yield",
+};
diff --git a/tools/testing/selftests/sched_ext/minimal.bpf.c b/tools/testing/selftests/sched_ext/minimal.bpf.c
new file mode 100644
index 000000000000..6a7eccef0104
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/minimal.bpf.c
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A completely minimal scheduler.
+ *
+ * This scheduler defines the absolute minimal set of struct sched_ext_ops
+ * fields: its name. It should _not_ fail to be loaded, and can be used to
+ * exercise the default scheduling paths in ext.c.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+SEC(".struct_ops.link")
+struct sched_ext_ops minimal_ops = {
+	.name			= "minimal",
+};
diff --git a/tools/testing/selftests/sched_ext/minimal.c b/tools/testing/selftests/sched_ext/minimal.c
new file mode 100644
index 000000000000..6c5db8ebbf8a
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/minimal.c
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "minimal.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct minimal *skel;
+
+	skel = minimal__open_and_load();
+	if (!skel) {
+		SCX_ERR("Failed to open and load skel");
+		return SCX_TEST_FAIL;
+	}
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct minimal *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.minimal_ops);
+	if (!link) {
+		SCX_ERR("Failed to attach scheduler");
+		return SCX_TEST_FAIL;
+	}
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct minimal *skel = ctx;
+
+	minimal__destroy(skel);
+}
+
+struct scx_test minimal = {
+	.name = "minimal",
+	.description = "Verify we can load a fully minimal scheduler",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&minimal)
diff --git a/tools/testing/selftests/sched_ext/prog_run.bpf.c b/tools/testing/selftests/sched_ext/prog_run.bpf.c
new file mode 100644
index 000000000000..6a4d7c48e3f2
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/prog_run.bpf.c
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates that we can invoke sched_ext kfuncs in
+ * BPF_PROG_TYPE_SYSCALL programs.
+ *
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+
+#include <scx/common.bpf.h>
+
+UEI_DEFINE(uei);
+
+char _license[] SEC("license") = "GPL";
+
+SEC("syscall")
+int BPF_PROG(prog_run_syscall)
+{
+	scx_bpf_create_dsq(0, -1);
+	scx_bpf_exit(0xdeadbeef, "Exited from PROG_RUN");
+	return 0;
+}
+
+void BPF_STRUCT_OPS(prog_run_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops prog_run_ops = {
+	.exit			= prog_run_exit,
+	.name			= "prog_run",
+};
diff --git a/tools/testing/selftests/sched_ext/prog_run.c b/tools/testing/selftests/sched_ext/prog_run.c
new file mode 100644
index 000000000000..3cd57ef8daaa
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/prog_run.c
@@ -0,0 +1,78 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <sched.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "prog_run.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct prog_run *skel;
+
+	skel = prog_run__open_and_load();
+	if (!skel) {
+		SCX_ERR("Failed to open and load skel");
+		return SCX_TEST_FAIL;
+	}
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct prog_run *skel = ctx;
+	struct bpf_link *link;
+	int prog_fd, err = 0;
+
+	prog_fd = bpf_program__fd(skel->progs.prog_run_syscall);
+	if (prog_fd < 0) {
+		SCX_ERR("Failed to get BPF_PROG_RUN prog");
+		return SCX_TEST_FAIL;
+	}
+
+	LIBBPF_OPTS(bpf_test_run_opts, topts);
+
+	link = bpf_map__attach_struct_ops(skel->maps.prog_run_ops);
+	if (!link) {
+		SCX_ERR("Failed to attach scheduler");
+		close(prog_fd);
+		return SCX_TEST_FAIL;
+	}
+
+	err = bpf_prog_test_run_opts(prog_fd, &topts);
+	SCX_EQ(err, 0);
+
+	/* Assumes uei.kind is written last */
+	while (skel->data->uei.kind == EXIT_KIND(SCX_EXIT_NONE))
+		sched_yield();
+
+	SCX_EQ(skel->data->uei.kind, EXIT_KIND(SCX_EXIT_UNREG_BPF));
+	SCX_EQ(skel->data->uei.exit_code, 0xdeadbeef);
+	close(prog_fd);
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct prog_run *skel = ctx;
+
+	prog_run__destroy(skel);
+}
+
+struct scx_test prog_run = {
+	.name = "prog_run",
+	.description = "Verify we can call into a scheduler with BPF_PROG_RUN, and invoke kfuncs",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&prog_run)
diff --git a/tools/testing/selftests/sched_ext/reload_loop.c b/tools/testing/selftests/sched_ext/reload_loop.c
new file mode 100644
index 000000000000..5cfba2d6e056
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/reload_loop.c
@@ -0,0 +1,75 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <pthread.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "maximal.bpf.skel.h"
+#include "scx_test.h"
+
+static struct maximal *skel;
+static pthread_t threads[2];
+
+bool force_exit = false;
+
+static enum scx_test_status setup(void **ctx)
+{
+	skel = maximal__open_and_load();
+	if (!skel) {
+		SCX_ERR("Failed to open and load skel");
+		return SCX_TEST_FAIL;
+	}
+
+	return SCX_TEST_PASS;
+}
+
+static void *do_reload_loop(void *arg)
+{
+	u32 i;
+
+	for (i = 0; i < 1024 && !force_exit; i++) {
+		struct bpf_link *link;
+
+		link = bpf_map__attach_struct_ops(skel->maps.maximal_ops);
+		if (link)
+			bpf_link__destroy(link);
+	}
+
+	return NULL;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	int err;
+	void *ret;
+
+	err = pthread_create(&threads[0], NULL, do_reload_loop, NULL);
+	SCX_FAIL_IF(err, "Failed to create thread 0");
+
+	err = pthread_create(&threads[1], NULL, do_reload_loop, NULL);
+	SCX_FAIL_IF(err, "Failed to create thread 1");
+
+	SCX_FAIL_IF(pthread_join(threads[0], &ret), "thread 0 failed");
+	SCX_FAIL_IF(pthread_join(threads[1], &ret), "thread 1 failed");
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	force_exit = true;
+	maximal__destroy(skel);
+}
+
+struct scx_test reload_loop = {
+	.name = "reload_loop",
+	.description = "Stress test loading and unloading schedulers repeatedly in a tight loop",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&reload_loop)
diff --git a/tools/testing/selftests/sched_ext/runner.c b/tools/testing/selftests/sched_ext/runner.c
new file mode 100644
index 000000000000..eab48c7ff309
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/runner.c
@@ -0,0 +1,201 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ */
+#include <stdio.h>
+#include <unistd.h>
+#include <signal.h>
+#include <libgen.h>
+#include <bpf/bpf.h>
+#include "scx_test.h"
+
+const char help_fmt[] =
+"The runner for sched_ext tests.\n"
+"\n"
+"The runner is statically linked against all testcases, and runs them all serially.\n"
+"It's required for the testcases to be serial, as only a single host-wide sched_ext\n"
+"scheduler may be loaded at any given time."
+"\n"
+"Usage: %s [-t TEST] [-h]\n"
+"\n"
+"  -t TEST       Only run tests whose name includes this string\n"
+"  -s            Include print output for skipped tests\n"
+"  -q            Don't print the test descriptions during run\n"
+"  -h            Display this help and exit\n";
+
+static volatile int exit_req;
+static bool quiet, print_skipped;
+
+#define MAX_SCX_TESTS 2048
+
+static struct scx_test __scx_tests[MAX_SCX_TESTS];
+static unsigned __scx_num_tests = 0;
+
+static void sigint_handler(int simple)
+{
+	exit_req = 1;
+}
+
+static void print_test_preamble(const struct scx_test *test, bool quiet)
+{
+	printf("===== START =====\n");
+	printf("TEST: %s\n", test->name);
+	if (!quiet)
+		printf("DESCRIPTION: %s\n", test->description);
+	printf("OUTPUT:\n");
+}
+
+static const char *status_to_result(enum scx_test_status status)
+{
+	switch (status) {
+	case SCX_TEST_PASS:
+	case SCX_TEST_SKIP:
+		return "ok";
+	case SCX_TEST_FAIL:
+		return "not ok";
+	default:
+		return "<UNKNOWN>";
+	}
+}
+
+static void print_test_result(const struct scx_test *test,
+			      enum scx_test_status status,
+			      unsigned int testnum)
+{
+	const char *result = status_to_result(status);
+	const char *directive = status == SCX_TEST_SKIP ? "SKIP " : "";
+
+	printf("%s %u %s # %s\n", result, testnum, test->name, directive);
+	printf("=====  END  =====\n");
+}
+
+static bool should_skip_test(const struct scx_test *test, const char * filter)
+{
+	return !strstr(test->name, filter);
+}
+
+static enum scx_test_status run_test(const struct scx_test *test)
+{
+	enum scx_test_status status;
+	void *context = NULL;
+
+	if (test->setup) {
+		status = test->setup(&context);
+		if (status != SCX_TEST_PASS)
+			return status;
+	}
+
+	status = test->run(context);
+
+	if (test->cleanup)
+		test->cleanup(context);
+
+	return status;
+}
+
+static bool test_valid(const struct scx_test *test)
+{
+	if (!test) {
+		fprintf(stderr, "NULL test detected\n");
+		return false;
+	}
+
+	if (!test->name) {
+		fprintf(stderr,
+			"Test with no name found. Must specify test name.\n");
+		return false;
+	}
+
+	if (!test->description) {
+		fprintf(stderr, "Test %s requires description.\n", test->name);
+		return false;
+	}
+
+	if (!test->run) {
+		fprintf(stderr, "Test %s has no run() callback\n", test->name);
+		return false;
+	}
+
+	return true;
+}
+
+int main(int argc, char **argv)
+{
+	const char *filter = NULL;
+	unsigned testnum = 0, i;
+	unsigned passed = 0, skipped = 0, failed = 0;
+	int opt;
+
+	signal(SIGINT, sigint_handler);
+	signal(SIGTERM, sigint_handler);
+
+	libbpf_set_strict_mode(LIBBPF_STRICT_ALL);
+
+	while ((opt = getopt(argc, argv, "qst:h")) != -1) {
+		switch (opt) {
+		case 'q':
+			quiet = true;
+			break;
+		case 's':
+			print_skipped = true;
+			break;
+		case 't':
+			filter = optarg;
+			break;
+		default:
+			fprintf(stderr, help_fmt, basename(argv[0]));
+			return opt != 'h';
+		}
+	}
+
+	for (i = 0; i < __scx_num_tests; i++) {
+		enum scx_test_status status;
+		struct scx_test *test = &__scx_tests[i];
+
+		if (filter && should_skip_test(test, filter)) {
+			/*
+			 * Printing the skipped tests and their preambles can
+			 * add a lot of noise to the runner output. Printing
+			 * this is only really useful for CI, so let's skip it
+			 * by default.
+			 */
+			if (print_skipped) {
+				print_test_preamble(test, quiet);
+				print_test_result(test, SCX_TEST_SKIP, ++testnum);
+			}
+			continue;
+		}
+
+		print_test_preamble(test, quiet);
+		status = run_test(test);
+		print_test_result(test, status, ++testnum);
+		switch (status) {
+		case SCX_TEST_PASS:
+			passed++;
+			break;
+		case SCX_TEST_SKIP:
+			skipped++;
+			break;
+		case SCX_TEST_FAIL:
+			failed++;
+			break;
+		}
+	}
+	printf("\n\n=============================\n\n");
+	printf("RESULTS:\n\n");
+	printf("PASSED:  %u\n", passed);
+	printf("SKIPPED: %u\n", skipped);
+	printf("FAILED:  %u\n", failed);
+
+	return 0;
+}
+
+void scx_test_register(struct scx_test *test)
+{
+	SCX_BUG_ON(!test_valid(test), "Invalid test found");
+	SCX_BUG_ON(__scx_num_tests >= MAX_SCX_TESTS, "Maximum tests exceeded");
+
+	__scx_tests[__scx_num_tests++] = *test;
+}
diff --git a/tools/testing/selftests/sched_ext/scx_test.h b/tools/testing/selftests/sched_ext/scx_test.h
new file mode 100644
index 000000000000..90b8d6915bb7
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/scx_test.h
@@ -0,0 +1,131 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ */
+
+#ifndef __SCX_TEST_H__
+#define __SCX_TEST_H__
+
+#include <errno.h>
+#include <scx/common.h>
+#include <scx/compat.h>
+
+enum scx_test_status {
+	SCX_TEST_PASS = 0,
+	SCX_TEST_SKIP,
+	SCX_TEST_FAIL,
+};
+
+#define EXIT_KIND(__ent) __COMPAT_ENUM_OR_ZERO("scx_exit_kind", #__ent)
+
+struct scx_test {
+	/**
+	 * name - The name of the testcase.
+	 */
+	const char *name;
+
+	/**
+	 * description - A description of your testcase: what it tests and is
+	 * meant to validate.
+	 */
+	const char *description;
+
+	/*
+	 * setup - Setup the test.
+	 * @ctx: A pointer to a context object that will be passed to run and
+	 *	 cleanup.
+	 *
+	 * An optional callback that allows a testcase to perform setup for its
+	 * run. A test may return SCX_TEST_SKIP to skip the run.
+	 */
+	enum scx_test_status (*setup)(void **ctx);
+
+	/*
+	 * run - Run the test.
+	 * @ctx: Context set in the setup() callback. If @ctx was not set in
+	 *	 setup(), it is NULL.
+	 *
+	 * The main test. Callers should return one of:
+	 *
+	 * - SCX_TEST_PASS: Test passed
+	 * - SCX_TEST_SKIP: Test should be skipped
+	 * - SCX_TEST_FAIL: Test failed
+	 *
+	 * This callback must be defined.
+	 */
+	enum scx_test_status (*run)(void *ctx);
+
+	/*
+	 * cleanup - Perform cleanup following the test
+	 * @ctx: Context set in the setup() callback. If @ctx was not set in
+	 *	 setup(), it is NULL.
+	 *
+	 * An optional callback that allows a test to perform cleanup after
+	 * being run. This callback is run even if the run() callback returns
+	 * SCX_TEST_SKIP or SCX_TEST_FAIL. It is not run if setup() returns
+	 * SCX_TEST_SKIP or SCX_TEST_FAIL.
+	 */
+	void (*cleanup)(void *ctx);
+};
+
+void scx_test_register(struct scx_test *test);
+
+#define REGISTER_SCX_TEST(__test)			\
+	__attribute__((constructor))			\
+	static void ___scxregister##__LINE__(void)	\
+	{						\
+		scx_test_register(__test);		\
+	}
+
+#define SCX_ERR(__fmt, ...)						\
+	do {								\
+		fprintf(stderr, "ERR: %s:%d\n", __FILE__, __LINE__);	\
+		fprintf(stderr, __fmt"\n", ##__VA_ARGS__);			\
+	} while (0)
+
+#define SCX_FAIL(__fmt, ...)						\
+	do {								\
+		SCX_ERR(__fmt, ##__VA_ARGS__);				\
+		return SCX_TEST_FAIL;					\
+	} while (0)
+
+#define SCX_FAIL_IF(__cond, __fmt, ...)					\
+	do {								\
+		if (__cond)						\
+			SCX_FAIL(__fmt, ##__VA_ARGS__);			\
+	} while (0)
+
+#define SCX_GT(_x, _y) SCX_FAIL_IF((_x) <= (_y), "Expected %s > %s (%lu > %lu)",	\
+				   #_x, #_y, (u64)(_x), (u64)(_y))
+#define SCX_GE(_x, _y) SCX_FAIL_IF((_x) < (_y), "Expected %s >= %s (%lu >= %lu)",	\
+				   #_x, #_y, (u64)(_x), (u64)(_y))
+#define SCX_LT(_x, _y) SCX_FAIL_IF((_x) >= (_y), "Expected %s < %s (%lu < %lu)",	\
+				   #_x, #_y, (u64)(_x), (u64)(_y))
+#define SCX_LE(_x, _y) SCX_FAIL_IF((_x) > (_y), "Expected %s <= %s (%lu <= %lu)",	\
+				   #_x, #_y, (u64)(_x), (u64)(_y))
+#define SCX_EQ(_x, _y) SCX_FAIL_IF((_x) != (_y), "Expected %s == %s (%lu == %lu)",	\
+				   #_x, #_y, (u64)(_x), (u64)(_y))
+#define SCX_ASSERT(_x) SCX_FAIL_IF(!(_x), "Expected %s to be true (%lu)",		\
+				   #_x, (u64)(_x))
+
+#define SCX_ECODE_VAL(__ecode) ({						\
+        u64 __val = 0;								\
+	bool __found = false;							\
+										\
+	__found = __COMPAT_read_enum("scx_exit_code", #__ecode, &__val);	\
+	SCX_ASSERT(__found);							\
+	(s64)__val;								\
+})
+
+#define SCX_KIND_VAL(__kind) ({							\
+        u64 __val = 0;								\
+	bool __found = false;							\
+										\
+	__found = __COMPAT_read_enum("scx_exit_kind", #__kind, &__val);		\
+	SCX_ASSERT(__found);							\
+	__val;									\
+})
+
+#endif  // # __SCX_TEST_H__
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dfl.bpf.c b/tools/testing/selftests/sched_ext/select_cpu_dfl.bpf.c
new file mode 100644
index 000000000000..2ed2991afafe
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dfl.bpf.c
@@ -0,0 +1,40 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates the behavior of direct dispatching with a default
+ * select_cpu implementation.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+bool saw_local = false;
+
+static bool task_is_test(const struct task_struct *p)
+{
+	return !bpf_strncmp(p->comm, 9, "select_cpu");
+}
+
+void BPF_STRUCT_OPS(select_cpu_dfl_enqueue, struct task_struct *p,
+		    u64 enq_flags)
+{
+	const struct cpumask *idle_mask = scx_bpf_get_idle_cpumask();
+
+	if (task_is_test(p) &&
+	    bpf_cpumask_test_cpu(scx_bpf_task_cpu(p), idle_mask)) {
+		saw_local = true;
+	}
+	scx_bpf_put_idle_cpumask(idle_mask);
+
+	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, enq_flags);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops select_cpu_dfl_ops = {
+	.enqueue		= select_cpu_dfl_enqueue,
+	.name			= "select_cpu_dfl",
+};
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dfl.c b/tools/testing/selftests/sched_ext/select_cpu_dfl.c
new file mode 100644
index 000000000000..a53a40c2d2f0
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dfl.c
@@ -0,0 +1,72 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "select_cpu_dfl.bpf.skel.h"
+#include "scx_test.h"
+
+#define NUM_CHILDREN 1028
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct select_cpu_dfl *skel;
+
+	skel = select_cpu_dfl__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct select_cpu_dfl *skel = ctx;
+	struct bpf_link *link;
+	pid_t pids[NUM_CHILDREN];
+	int i, status;
+
+	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dfl_ops);
+	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+
+	for (i = 0; i < NUM_CHILDREN; i++) {
+		pids[i] = fork();
+		if (pids[i] == 0) {
+			sleep(1);
+			exit(0);
+		}
+	}
+
+	for (i = 0; i < NUM_CHILDREN; i++) {
+		SCX_EQ(waitpid(pids[i], &status, 0), pids[i]);
+		SCX_EQ(status, 0);
+	}
+
+	SCX_ASSERT(!skel->bss->saw_local);
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct select_cpu_dfl *skel = ctx;
+
+	select_cpu_dfl__destroy(skel);
+}
+
+struct scx_test select_cpu_dfl = {
+	.name = "select_cpu_dfl",
+	.description = "Verify the default ops.select_cpu() dispatches tasks "
+		       "when idles cores are found, and skips ops.enqueue()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&select_cpu_dfl)
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.bpf.c b/tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.bpf.c
new file mode 100644
index 000000000000..4bb5abb2d369
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.bpf.c
@@ -0,0 +1,89 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates the behavior of direct dispatching with a default
+ * select_cpu implementation, and with the SCX_OPS_ENQ_DFL_NO_DISPATCH ops flag
+ * specified.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+bool saw_local = false;
+
+/* Per-task scheduling context */
+struct task_ctx {
+	bool	force_local;	/* CPU changed by ops.select_cpu() */
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
+	__uint(map_flags, BPF_F_NO_PREALLOC);
+	__type(key, int);
+	__type(value, struct task_ctx);
+} task_ctx_stor SEC(".maps");
+
+/* Manually specify the signature until the kfunc is added to the scx repo. */
+s32 scx_bpf_select_cpu_dfl(struct task_struct *p, s32 prev_cpu, u64 wake_flags,
+			   bool *found) __ksym;
+
+s32 BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	struct task_ctx *tctx;
+	s32 cpu;
+
+	tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0);
+	if (!tctx) {
+		scx_bpf_error("task_ctx lookup failed");
+		return -ESRCH;
+	}
+
+	cpu = scx_bpf_select_cpu_dfl(p, prev_cpu, wake_flags,
+				     &tctx->force_local);
+
+	return cpu;
+}
+
+void BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_enqueue, struct task_struct *p,
+		    u64 enq_flags)
+{
+	u64 dsq_id = SCX_DSQ_GLOBAL;
+	struct task_ctx *tctx;
+
+	tctx = bpf_task_storage_get(&task_ctx_stor, p, 0, 0);
+	if (!tctx) {
+		scx_bpf_error("task_ctx lookup failed");
+		return;
+	}
+
+	if (tctx->force_local) {
+		dsq_id = SCX_DSQ_LOCAL;
+		tctx->force_local = false;
+		saw_local = true;
+	}
+
+	scx_bpf_dispatch(p, dsq_id, SCX_SLICE_DFL, enq_flags);
+}
+
+s32 BPF_STRUCT_OPS(select_cpu_dfl_nodispatch_init_task,
+		   struct task_struct *p, struct scx_init_task_args *args)
+{
+	if (bpf_task_storage_get(&task_ctx_stor, p, 0,
+				 BPF_LOCAL_STORAGE_GET_F_CREATE))
+		return 0;
+	else
+		return -ENOMEM;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops select_cpu_dfl_nodispatch_ops = {
+	.select_cpu		= select_cpu_dfl_nodispatch_select_cpu,
+	.enqueue		= select_cpu_dfl_nodispatch_enqueue,
+	.init_task		= select_cpu_dfl_nodispatch_init_task,
+	.name			= "select_cpu_dfl_nodispatch",
+};
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.c b/tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.c
new file mode 100644
index 000000000000..1d85bf4bf3a3
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dfl_nodispatch.c
@@ -0,0 +1,72 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "select_cpu_dfl_nodispatch.bpf.skel.h"
+#include "scx_test.h"
+
+#define NUM_CHILDREN 1028
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct select_cpu_dfl_nodispatch *skel;
+
+	skel = select_cpu_dfl_nodispatch__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct select_cpu_dfl_nodispatch *skel = ctx;
+	struct bpf_link *link;
+	pid_t pids[NUM_CHILDREN];
+	int i, status;
+
+	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dfl_nodispatch_ops);
+	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+
+	for (i = 0; i < NUM_CHILDREN; i++) {
+		pids[i] = fork();
+		if (pids[i] == 0) {
+			sleep(1);
+			exit(0);
+		}
+	}
+
+	for (i = 0; i < NUM_CHILDREN; i++) {
+		SCX_EQ(waitpid(pids[i], &status, 0), pids[i]);
+		SCX_EQ(status, 0);
+	}
+
+	SCX_ASSERT(skel->bss->saw_local);
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct select_cpu_dfl_nodispatch *skel = ctx;
+
+	select_cpu_dfl_nodispatch__destroy(skel);
+}
+
+struct scx_test select_cpu_dfl_nodispatch = {
+	.name = "select_cpu_dfl_nodispatch",
+	.description = "Verify behavior of scx_bpf_select_cpu_dfl() in "
+		       "ops.select_cpu()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&select_cpu_dfl_nodispatch)
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dispatch.bpf.c b/tools/testing/selftests/sched_ext/select_cpu_dispatch.bpf.c
new file mode 100644
index 000000000000..f0b96a4a04b2
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dispatch.bpf.c
@@ -0,0 +1,41 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates the behavior of direct dispatching with a default
+ * select_cpu implementation.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+s32 BPF_STRUCT_OPS(select_cpu_dispatch_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	u64 dsq_id = SCX_DSQ_LOCAL;
+	s32 cpu = prev_cpu;
+
+	if (scx_bpf_test_and_clear_cpu_idle(cpu))
+		goto dispatch;
+
+	cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
+	if (cpu >= 0)
+		goto dispatch;
+
+	dsq_id = SCX_DSQ_GLOBAL;
+	cpu = prev_cpu;
+
+dispatch:
+	scx_bpf_dispatch(p, dsq_id, SCX_SLICE_DFL, 0);
+	return cpu;
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops select_cpu_dispatch_ops = {
+	.select_cpu		= select_cpu_dispatch_select_cpu,
+	.name			= "select_cpu_dispatch",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dispatch.c b/tools/testing/selftests/sched_ext/select_cpu_dispatch.c
new file mode 100644
index 000000000000..0309ca8785b3
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dispatch.c
@@ -0,0 +1,70 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "select_cpu_dispatch.bpf.skel.h"
+#include "scx_test.h"
+
+#define NUM_CHILDREN 1028
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct select_cpu_dispatch *skel;
+
+	skel = select_cpu_dispatch__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct select_cpu_dispatch *skel = ctx;
+	struct bpf_link *link;
+	pid_t pids[NUM_CHILDREN];
+	int i, status;
+
+	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_ops);
+	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+
+	for (i = 0; i < NUM_CHILDREN; i++) {
+		pids[i] = fork();
+		if (pids[i] == 0) {
+			sleep(1);
+			exit(0);
+		}
+	}
+
+	for (i = 0; i < NUM_CHILDREN; i++) {
+		SCX_EQ(waitpid(pids[i], &status, 0), pids[i]);
+		SCX_EQ(status, 0);
+	}
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct select_cpu_dispatch *skel = ctx;
+
+	select_cpu_dispatch__destroy(skel);
+}
+
+struct scx_test select_cpu_dispatch = {
+	.name = "select_cpu_dispatch",
+	.description = "Test direct dispatching to built-in DSQs from "
+		       "ops.select_cpu()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&select_cpu_dispatch)
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.bpf.c b/tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.bpf.c
new file mode 100644
index 000000000000..7b42ddce0f56
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.bpf.c
@@ -0,0 +1,37 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates the behavior of direct dispatching with a default
+ * select_cpu implementation.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+UEI_DEFINE(uei);
+
+s32 BPF_STRUCT_OPS(select_cpu_dispatch_bad_dsq_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	/* Dispatching to a random DSQ should fail. */
+	scx_bpf_dispatch(p, 0xcafef00d, SCX_SLICE_DFL, 0);
+
+	return prev_cpu;
+}
+
+void BPF_STRUCT_OPS(select_cpu_dispatch_bad_dsq_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops select_cpu_dispatch_bad_dsq_ops = {
+	.select_cpu		= select_cpu_dispatch_bad_dsq_select_cpu,
+	.exit			= select_cpu_dispatch_bad_dsq_exit,
+	.name			= "select_cpu_dispatch_bad_dsq",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.c b/tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.c
new file mode 100644
index 000000000000..47eb6ed7627d
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dispatch_bad_dsq.c
@@ -0,0 +1,56 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "select_cpu_dispatch_bad_dsq.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct select_cpu_dispatch_bad_dsq *skel;
+
+	skel = select_cpu_dispatch_bad_dsq__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct select_cpu_dispatch_bad_dsq *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_bad_dsq_ops);
+	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+
+	sleep(1);
+
+	SCX_EQ(skel->data->uei.kind, EXIT_KIND(SCX_EXIT_ERROR));
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct select_cpu_dispatch_bad_dsq *skel = ctx;
+
+	select_cpu_dispatch_bad_dsq__destroy(skel);
+}
+
+struct scx_test select_cpu_dispatch_bad_dsq = {
+	.name = "select_cpu_dispatch_bad_dsq",
+	.description = "Verify graceful failure if we direct-dispatch to a "
+		       "bogus DSQ in ops.select_cpu()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&select_cpu_dispatch_bad_dsq)
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.bpf.c b/tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.bpf.c
new file mode 100644
index 000000000000..653e3dc0b4dc
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.bpf.c
@@ -0,0 +1,38 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates the behavior of direct dispatching with a default
+ * select_cpu implementation.
+ *
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+UEI_DEFINE(uei);
+
+s32 BPF_STRUCT_OPS(select_cpu_dispatch_dbl_dsp_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	/* Dispatching twice in a row is disallowed. */
+	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, 0);
+	scx_bpf_dispatch(p, SCX_DSQ_GLOBAL, SCX_SLICE_DFL, 0);
+
+	return prev_cpu;
+}
+
+void BPF_STRUCT_OPS(select_cpu_dispatch_dbl_dsp_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops select_cpu_dispatch_dbl_dsp_ops = {
+	.select_cpu		= select_cpu_dispatch_dbl_dsp_select_cpu,
+	.exit			= select_cpu_dispatch_dbl_dsp_exit,
+	.name			= "select_cpu_dispatch_dbl_dsp",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.c b/tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.c
new file mode 100644
index 000000000000..48ff028a3c46
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_dispatch_dbl_dsp.c
@@ -0,0 +1,56 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2023 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2023 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2023 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "select_cpu_dispatch_dbl_dsp.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct select_cpu_dispatch_dbl_dsp *skel;
+
+	skel = select_cpu_dispatch_dbl_dsp__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct select_cpu_dispatch_dbl_dsp *skel = ctx;
+	struct bpf_link *link;
+
+	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_dispatch_dbl_dsp_ops);
+	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+
+	sleep(1);
+
+	SCX_EQ(skel->data->uei.kind, EXIT_KIND(SCX_EXIT_ERROR));
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct select_cpu_dispatch_dbl_dsp *skel = ctx;
+
+	select_cpu_dispatch_dbl_dsp__destroy(skel);
+}
+
+struct scx_test select_cpu_dispatch_dbl_dsp = {
+	.name = "select_cpu_dispatch_dbl_dsp",
+	.description = "Verify graceful failure if we dispatch twice to a "
+		       "DSQ in ops.select_cpu()",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&select_cpu_dispatch_dbl_dsp)
diff --git a/tools/testing/selftests/sched_ext/select_cpu_vtime.bpf.c b/tools/testing/selftests/sched_ext/select_cpu_vtime.bpf.c
new file mode 100644
index 000000000000..7f3ebf4fc2ea
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_vtime.bpf.c
@@ -0,0 +1,92 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * A scheduler that validates that enqueue flags are properly stored and
+ * applied at dispatch time when a task is directly dispatched from
+ * ops.select_cpu(). We validate this by using scx_bpf_dispatch_vtime(), and
+ * making the test a very basic vtime scheduler.
+ *
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ */
+
+#include <scx/common.bpf.h>
+
+char _license[] SEC("license") = "GPL";
+
+volatile bool consumed;
+
+static u64 vtime_now;
+
+#define VTIME_DSQ 0
+
+static inline bool vtime_before(u64 a, u64 b)
+{
+	return (s64)(a - b) < 0;
+}
+
+static inline u64 task_vtime(const struct task_struct *p)
+{
+	u64 vtime = p->scx.dsq_vtime;
+
+	if (vtime_before(vtime, vtime_now - SCX_SLICE_DFL))
+		return vtime_now - SCX_SLICE_DFL;
+	else
+		return vtime;
+}
+
+s32 BPF_STRUCT_OPS(select_cpu_vtime_select_cpu, struct task_struct *p,
+		   s32 prev_cpu, u64 wake_flags)
+{
+	s32 cpu;
+
+	cpu = scx_bpf_pick_idle_cpu(p->cpus_ptr, 0);
+	if (cpu >= 0)
+		goto ddsp;
+
+	cpu = prev_cpu;
+	scx_bpf_test_and_clear_cpu_idle(cpu);
+ddsp:
+	scx_bpf_dispatch_vtime(p, VTIME_DSQ, SCX_SLICE_DFL, task_vtime(p), 0);
+	return cpu;
+}
+
+void BPF_STRUCT_OPS(select_cpu_vtime_dispatch, s32 cpu, struct task_struct *p)
+{
+	if (scx_bpf_consume(VTIME_DSQ))
+		consumed = true;
+}
+
+void BPF_STRUCT_OPS(select_cpu_vtime_running, struct task_struct *p)
+{
+	if (vtime_before(vtime_now, p->scx.dsq_vtime))
+		vtime_now = p->scx.dsq_vtime;
+}
+
+void BPF_STRUCT_OPS(select_cpu_vtime_stopping, struct task_struct *p,
+		    bool runnable)
+{
+	p->scx.dsq_vtime += (SCX_SLICE_DFL - p->scx.slice) * 100 / p->scx.weight;
+}
+
+void BPF_STRUCT_OPS(select_cpu_vtime_enable, struct task_struct *p)
+{
+	p->scx.dsq_vtime = vtime_now;
+}
+
+s32 BPF_STRUCT_OPS_SLEEPABLE(select_cpu_vtime_init)
+{
+	return scx_bpf_create_dsq(VTIME_DSQ, -1);
+}
+
+SEC(".struct_ops.link")
+struct sched_ext_ops select_cpu_vtime_ops = {
+	.select_cpu		= select_cpu_vtime_select_cpu,
+	.dispatch		= select_cpu_vtime_dispatch,
+	.running		= select_cpu_vtime_running,
+	.stopping		= select_cpu_vtime_stopping,
+	.enable			= select_cpu_vtime_enable,
+	.init			= select_cpu_vtime_init,
+	.name			= "select_cpu_vtime",
+	.timeout_ms		= 1000U,
+};
diff --git a/tools/testing/selftests/sched_ext/select_cpu_vtime.c b/tools/testing/selftests/sched_ext/select_cpu_vtime.c
new file mode 100644
index 000000000000..b4629c2364f5
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/select_cpu_vtime.c
@@ -0,0 +1,59 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include <sys/wait.h>
+#include <unistd.h>
+#include "select_cpu_vtime.bpf.skel.h"
+#include "scx_test.h"
+
+static enum scx_test_status setup(void **ctx)
+{
+	struct select_cpu_vtime *skel;
+
+	skel = select_cpu_vtime__open_and_load();
+	SCX_FAIL_IF(!skel, "Failed to open and load skel");
+	*ctx = skel;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	struct select_cpu_vtime *skel = ctx;
+	struct bpf_link *link;
+
+	SCX_ASSERT(!skel->bss->consumed);
+
+	link = bpf_map__attach_struct_ops(skel->maps.select_cpu_vtime_ops);
+	SCX_FAIL_IF(!link, "Failed to attach scheduler");
+
+	sleep(1);
+
+	SCX_ASSERT(skel->bss->consumed);
+
+	bpf_link__destroy(link);
+
+	return SCX_TEST_PASS;
+}
+
+static void cleanup(void *ctx)
+{
+	struct select_cpu_vtime *skel = ctx;
+
+	select_cpu_vtime__destroy(skel);
+}
+
+struct scx_test select_cpu_vtime = {
+	.name = "select_cpu_vtime",
+	.description = "Test doing direct vtime-dispatching from "
+		       "ops.select_cpu(), to a non-built-in DSQ",
+	.setup = setup,
+	.run = run,
+	.cleanup = cleanup,
+};
+REGISTER_SCX_TEST(&select_cpu_vtime)
diff --git a/tools/testing/selftests/sched_ext/test_example.c b/tools/testing/selftests/sched_ext/test_example.c
new file mode 100644
index 000000000000..ce36cdf03cdc
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/test_example.c
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 Tejun Heo <tj@kernel.org>
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <bpf/bpf.h>
+#include <scx/common.h>
+#include "scx_test.h"
+
+static bool setup_called = false;
+static bool run_called = false;
+static bool cleanup_called = false;
+
+static int context = 10;
+
+static enum scx_test_status setup(void **ctx)
+{
+	setup_called = true;
+	*ctx = &context;
+
+	return SCX_TEST_PASS;
+}
+
+static enum scx_test_status run(void *ctx)
+{
+	int *arg = ctx;
+
+	SCX_ASSERT(setup_called);
+	SCX_ASSERT(!run_called && !cleanup_called);
+	SCX_EQ(*arg, context);
+
+	run_called = true;
+	return SCX_TEST_PASS;
+}
+
+static void cleanup (void *ctx)
+{
+	SCX_BUG_ON(!run_called || cleanup_called, "Wrong callbacks invoked");
+}
+
+struct scx_test example = {
+	.name		= "example",
+	.description	= "Validate the basic function of the test suite itself",
+	.setup		= setup,
+	.run		= run,
+	.cleanup	= cleanup,
+};
+REGISTER_SCX_TEST(&example)
diff --git a/tools/testing/selftests/sched_ext/util.c b/tools/testing/selftests/sched_ext/util.c
new file mode 100644
index 000000000000..e47769c91918
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/util.c
@@ -0,0 +1,71 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <dvernet@meta.com>
+ */
+#include <errno.h>
+#include <fcntl.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+
+/* Returns read len on success, or -errno on failure. */
+static ssize_t read_text(const char *path, char *buf, size_t max_len)
+{
+	ssize_t len;
+	int fd;
+
+	fd = open(path, O_RDONLY);
+	if (fd < 0)
+		return -errno;
+
+	len = read(fd, buf, max_len - 1);
+
+	if (len >= 0)
+		buf[len] = 0;
+
+	close(fd);
+	return len < 0 ? -errno : len;
+}
+
+/* Returns written len on success, or -errno on failure. */
+static ssize_t write_text(const char *path, char *buf, ssize_t len)
+{
+	int fd;
+	ssize_t written;
+
+	fd = open(path, O_WRONLY | O_APPEND);
+	if (fd < 0)
+		return -errno;
+
+	written = write(fd, buf, len);
+	close(fd);
+	return written < 0 ? -errno : written;
+}
+
+long file_read_long(const char *path)
+{
+	char buf[128];
+
+
+	if (read_text(path, buf, sizeof(buf)) <= 0)
+		return -1;
+
+	return atol(buf);
+}
+
+int file_write_long(const char *path, long val)
+{
+	char buf[64];
+	int ret;
+
+	ret = sprintf(buf, "%lu", val);
+	if (ret < 0)
+		return ret;
+
+	if (write_text(path, buf, sizeof(buf)) <= 0)
+		return -1;
+
+	return 0;
+}
diff --git a/tools/testing/selftests/sched_ext/util.h b/tools/testing/selftests/sched_ext/util.h
new file mode 100644
index 000000000000..bc13dfec1267
--- /dev/null
+++ b/tools/testing/selftests/sched_ext/util.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2024 Meta Platforms, Inc. and affiliates.
+ * Copyright (c) 2024 David Vernet <void@manifault.com>
+ */
+
+#ifndef __SCX_TEST_UTIL_H__
+#define __SCX_TEST_UTIL_H__
+
+long file_read_long(const char *path);
+int file_write_long(const char *path, long val);
+
+#endif // __SCX_TEST_H__
-- 
2.47.0


From 0a5a89e291e0463c20d59c09325b7821bf273172 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 20:36:44 +0200
Subject: [PATCH v1.4 107/120] add bore scheduler (linux6.11.y-bore5.6.1)

---
 include/linux/sched.h      |  20 +-
 include/linux/sched/bore.h |  37 ++++
 init/Kconfig               |  17 ++
 kernel/Kconfig.hz          |  17 ++
 kernel/fork.c              |   5 +
 kernel/sched/Makefile      |   1 +
 kernel/sched/bore.c        | 381 +++++++++++++++++++++++++++++++++++++
 kernel/sched/core.c        |   7 +
 kernel/sched/debug.c       |  60 +++++-
 kernel/sched/fair.c        |  89 ++++++++-
 kernel/sched/features.h    |   4 +
 kernel/sched/sched.h       |   7 +
 12 files changed, 640 insertions(+), 5 deletions(-)
 create mode 100644 include/linux/sched/bore.h
 create mode 100644 kernel/sched/bore.c

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c5a7901b2580..bab2d659b667 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -537,6 +537,14 @@ struct sched_statistics {
 #endif /* CONFIG_SCHEDSTATS */
 } ____cacheline_aligned;
 
+#ifdef CONFIG_SCHED_BORE
+struct sched_burst_cache {
+	u8				score;
+	u32				count;
+	u64				timestamp;
+};
+#endif // CONFIG_SCHED_BORE
+
 struct sched_entity {
 	/* For load-balancing: */
 	struct load_weight		load;
@@ -545,12 +553,22 @@ struct sched_entity {
 	u64				min_vruntime;
 
 	struct list_head		group_node;
-	unsigned int			on_rq;
+	unsigned char			on_rq;
+	unsigned char			rel_deadline;
 
 	u64				exec_start;
 	u64				sum_exec_runtime;
 	u64				prev_sum_exec_runtime;
 	u64				vruntime;
+#ifdef CONFIG_SCHED_BORE
+	u64				burst_time;
+	u8				prev_burst_penalty;
+	u8				curr_burst_penalty;
+	u8				burst_penalty;
+	u8				burst_score;
+	struct sched_burst_cache child_burst;
+	struct sched_burst_cache group_burst;
+#endif // CONFIG_SCHED_BORE
 	s64				vlag;
 	u64				slice;
 
diff --git a/include/linux/sched/bore.h b/include/linux/sched/bore.h
new file mode 100644
index 000000000000..12a613a94ff0
--- /dev/null
+++ b/include/linux/sched/bore.h
@@ -0,0 +1,37 @@
+
+#include <linux/sched.h>
+#include <linux/sched/cputime.h>
+
+#ifndef _LINUX_SCHED_BORE_H
+#define _LINUX_SCHED_BORE_H
+
+#ifdef CONFIG_SCHED_BORE
+extern u8   __read_mostly sched_bore;
+extern u8   __read_mostly sched_burst_exclude_kthreads;
+extern u8   __read_mostly sched_burst_smoothness_long;
+extern u8   __read_mostly sched_burst_smoothness_short;
+extern u8   __read_mostly sched_burst_fork_atavistic;
+extern u8   __read_mostly sched_burst_parity_threshold;
+extern u8   __read_mostly sched_burst_penalty_offset;
+extern uint __read_mostly sched_burst_penalty_scale;
+extern uint __read_mostly sched_burst_cache_lifetime;
+extern uint __read_mostly sched_deadline_boost_mask;
+
+extern void update_burst_score(struct sched_entity *se);
+extern void update_burst_penalty(struct sched_entity *se);
+
+extern void restart_burst(struct sched_entity *se);
+extern void restart_burst_rescale_deadline(struct sched_entity *se);
+
+extern int sched_bore_update_handler(const struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp, loff_t *ppos);
+
+extern void sched_clone_bore(
+	struct task_struct *p, struct task_struct *parent, u64 clone_flags);
+
+extern void init_task_bore(struct task_struct *p);
+
+extern void reweight_entity(
+	struct cfs_rq *cfs_rq, struct sched_entity *se, unsigned long weight);
+#endif // CONFIG_SCHED_BORE
+#endif // _LINUX_SCHED_BORE_H
diff --git a/init/Kconfig b/init/Kconfig
index 34cfb0d41b26..d67aa0b27163 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1307,6 +1307,23 @@ config CHECKPOINT_RESTORE
 
 	  If unsure, say N here.
 
+config SCHED_BORE
+	bool "Burst-Oriented Response Enhancer"
+	default y
+	help
+	  In Desktop and Mobile computing, one might prefer interactive
+	  tasks to keep responsive no matter what they run in the background.
+
+	  Enabling this kernel feature modifies the scheduler to discriminate
+	  tasks by their burst time (runtime since it last went sleeping or
+	  yielding state) and prioritize those that run less bursty.
+	  Such tasks usually include window compositor, widgets backend,
+	  terminal emulator, video playback, games and so on.
+	  With a little impact to scheduling fairness, it may improve
+	  responsiveness especially under heavy background workload.
+
+	  If unsure, say Y here.
+
 config SCHED_AUTOGROUP
 	bool "Automatic process group scheduling"
 	select CGROUPS
diff --git a/kernel/Kconfig.hz b/kernel/Kconfig.hz
index 38ef6d06888e..253c566b5946 100644
--- a/kernel/Kconfig.hz
+++ b/kernel/Kconfig.hz
@@ -55,5 +55,22 @@ config HZ
 	default 300 if HZ_300
 	default 1000 if HZ_1000
 
+config MIN_BASE_SLICE_NS
+	int "Default value for min_base_slice_ns"
+	default 2000000
+	help
+	 The BORE Scheduler automatically calculates the optimal base
+	 slice for the configured HZ using the following equation:
+	 
+	 base_slice_ns =
+	 	1000000000/HZ * DIV_ROUNDUP(min_base_slice_ns, 1000000000/HZ)
+	 
+	 This option sets the default lower bound limit of the base slice
+	 to prevent the loss of task throughput due to overscheduling.
+	 
+	 Setting this value too high can cause the system to boot with
+	 an unnecessarily large base slice, resulting in high scheduling
+	 latency and poor system responsiveness.
+
 config SCHED_HRTICK
 	def_bool HIGH_RES_TIMERS
diff --git a/kernel/fork.c b/kernel/fork.c
index 0844b59dc082..cd74836fbe1f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -112,6 +112,8 @@
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
 
+#include <linux/sched/bore.h>
+
 #include <trace/events/sched.h>
 
 #define CREATE_TRACE_POINTS
@@ -2343,6 +2345,9 @@ __latent_entropy struct task_struct *copy_process(
 	retval = sched_fork(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_policy;
+#ifdef CONFIG_SCHED_BORE
+	sched_clone_bore(p, current, clone_flags);
+#endif // CONFIG_SCHED_BORE
 
 	retval = perf_event_init_task(p, clone_flags);
 	if (retval)
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 976092b7bd45..293aad675444 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -32,3 +32,4 @@ obj-y += core.o
 obj-y += fair.o
 obj-y += build_policy.o
 obj-y += build_utility.o
+obj-y += bore.o
diff --git a/kernel/sched/bore.c b/kernel/sched/bore.c
new file mode 100644
index 000000000000..cd7e8a8d6075
--- /dev/null
+++ b/kernel/sched/bore.c
@@ -0,0 +1,381 @@
+/*
+ *  Burst-Oriented Response Enhancer (BORE) CPU Scheduler
+ *  Copyright (C) 2021-2024 Masahito Suzuki <firelzrd@gmail.com>
+ */
+#include <linux/cpuset.h>
+#include <linux/sched/bore.h>
+#include "sched.h"
+
+#ifdef CONFIG_SCHED_BORE
+u8   __read_mostly sched_bore                   = 1;
+u8   __read_mostly sched_burst_exclude_kthreads = 1;
+u8   __read_mostly sched_burst_smoothness_long  = 1;
+u8   __read_mostly sched_burst_smoothness_short = 0;
+u8   __read_mostly sched_burst_fork_atavistic   = 2;
+u8   __read_mostly sched_burst_parity_threshold = 2;
+u8   __read_mostly sched_burst_penalty_offset   = 24;
+uint __read_mostly sched_burst_penalty_scale    = 1280;
+uint __read_mostly sched_burst_cache_lifetime   = 60000000;
+uint __read_mostly sched_deadline_boost_mask    = ENQUEUE_INITIAL
+                                                | ENQUEUE_WAKEUP;
+static int __maybe_unused sixty_four     = 64;
+static int __maybe_unused maxval_u8      = 255;
+static int __maybe_unused maxval_12_bits = 4095;
+
+#define MAX_BURST_PENALTY (39U <<2)
+
+static inline u32 log2plus1_u64_u32f8(u64 v) {
+	u32 integral = fls64(v);
+	u8  fractional = v << (64 - integral) >> 55;
+	return integral << 8 | fractional;
+}
+
+static inline u32 calc_burst_penalty(u64 burst_time) {
+	u32 greed, tolerance, penalty, scaled_penalty;
+	
+	greed = log2plus1_u64_u32f8(burst_time);
+	tolerance = sched_burst_penalty_offset << 8;
+	penalty = max(0, (s32)(greed - tolerance));
+	scaled_penalty = penalty * sched_burst_penalty_scale >> 16;
+
+	return min(MAX_BURST_PENALTY, scaled_penalty);
+}
+
+static inline u64 __scale_slice(u64 delta, u8 score)
+{return mul_u64_u32_shr(delta, sched_prio_to_wmult[score], 22);}
+
+static inline u64 __unscale_slice(u64 delta, u8 score)
+{return mul_u64_u32_shr(delta, sched_prio_to_weight[score], 10);}
+
+static void reweight_task_by_prio(struct task_struct *p, int prio) {
+	struct sched_entity *se = &p->se;
+	unsigned long weight = scale_load(sched_prio_to_weight[prio]);
+
+	reweight_entity(cfs_rq_of(se), se, weight);
+	se->load.inv_weight = sched_prio_to_wmult[prio];
+}
+
+static inline u8 effective_prio(struct task_struct *p) {
+	u8 prio = p->static_prio - MAX_RT_PRIO;
+	if (likely(sched_bore))
+		prio += p->se.burst_score;
+	return min(39, prio);
+}
+
+void update_burst_score(struct sched_entity *se) {
+	if (!entity_is_task(se)) return;
+	struct task_struct *p = task_of(se);
+	u8 prev_prio = effective_prio(p);
+
+	u8 burst_score = 0;
+	if (!((p->flags & PF_KTHREAD) && likely(sched_burst_exclude_kthreads)))
+		burst_score = se->burst_penalty >> 2;
+	se->burst_score = burst_score;
+
+	u8 new_prio = effective_prio(p);
+	if (new_prio != prev_prio)
+		reweight_task_by_prio(p, new_prio);
+}
+
+void update_burst_penalty(struct sched_entity *se) {
+	se->curr_burst_penalty = calc_burst_penalty(se->burst_time);
+	se->burst_penalty = max(se->prev_burst_penalty, se->curr_burst_penalty);
+	update_burst_score(se);
+}
+
+static inline u32 binary_smooth(u32 new, u32 old) {
+	int increment = new - old;
+	return (0 <= increment)?
+		old + ( increment >> (int)sched_burst_smoothness_long):
+		old - (-increment >> (int)sched_burst_smoothness_short);
+}
+
+static void revolve_burst_penalty(struct sched_entity *se) {
+	se->prev_burst_penalty =
+		binary_smooth(se->curr_burst_penalty, se->prev_burst_penalty);
+	se->burst_time = 0;
+	se->curr_burst_penalty = 0;
+}
+
+inline void restart_burst(struct sched_entity *se) {
+	revolve_burst_penalty(se);
+	se->burst_penalty = se->prev_burst_penalty;
+	update_burst_score(se);
+}
+
+void restart_burst_rescale_deadline(struct sched_entity *se) {
+	s64 vscaled, wremain, vremain = se->deadline - se->vruntime;
+	struct task_struct *p = task_of(se);
+	u8 prev_prio = effective_prio(p);
+	restart_burst(se);
+	u8 new_prio = effective_prio(p);
+	if (prev_prio > new_prio) {
+		wremain = __unscale_slice(abs(vremain), prev_prio);
+		vscaled = __scale_slice(wremain, new_prio);
+		if (unlikely(vremain < 0))
+			vscaled = -vscaled;
+		se->deadline = se->vruntime + vscaled;
+	}
+}
+
+static inline bool task_is_bore_eligible(struct task_struct *p)
+{return p->sched_class == &fair_sched_class;}
+
+static void reset_task_weights_bore(void) {
+	struct task_struct *task;
+	struct rq *rq;
+	struct rq_flags rf;
+
+	write_lock_irq(&tasklist_lock);
+	for_each_process(task) {
+		if (!task_is_bore_eligible(task)) continue;
+		rq = task_rq(task);
+		rq_lock_irqsave(rq, &rf);
+		reweight_task_by_prio(task, effective_prio(task));
+		rq_unlock_irqrestore(rq, &rf);
+	}
+	write_unlock_irq(&tasklist_lock);
+}
+
+int sched_bore_update_handler(const struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp, loff_t *ppos) {
+	int ret = proc_dou8vec_minmax(table, write, buffer, lenp, ppos);
+	if (ret || !write)
+		return ret;
+
+	reset_task_weights_bore();
+
+	return 0;
+}
+
+static u32 count_child_tasks(struct task_struct *p) {
+	struct task_struct *child;
+	u32 cnt = 0;
+	list_for_each_entry(child, &p->children, sibling) {cnt++;}
+	return cnt;
+}
+
+static inline bool burst_cache_expired(struct sched_burst_cache *bc, u64 now)
+{return (s64)(bc->timestamp + sched_burst_cache_lifetime - now) < 0;}
+
+static void update_burst_cache(struct sched_burst_cache *bc,
+		struct task_struct *p, u32 cnt, u32 sum, u64 now) {
+	u8 avg = cnt ? sum / cnt : 0;
+	bc->score = max(avg, p->se.burst_penalty);
+	bc->count = cnt;
+	bc->timestamp = now;
+}
+
+static inline void update_child_burst_direct(struct task_struct *p, u64 now) {
+	u32 cnt = 0, sum = 0;
+	struct task_struct *child;
+
+	list_for_each_entry(child, &p->children, sibling) {
+		if (!task_is_bore_eligible(child)) continue;
+		cnt++;
+		sum += child->se.burst_penalty;
+	}
+
+	update_burst_cache(&p->se.child_burst, p, cnt, sum, now);
+}
+
+static inline u8 inherit_burst_direct(struct task_struct *p, u64 now) {
+	struct task_struct *parent = p;
+	if (burst_cache_expired(&parent->se.child_burst, now))
+		update_child_burst_direct(parent, now);
+
+	return parent->se.child_burst.score;
+}
+
+static void update_child_burst_topological(
+	struct task_struct *p, u64 now, u32 depth, u32 *acnt, u32 *asum) {
+	u32 cnt = 0, dcnt = 0, sum = 0;
+	struct task_struct *child, *dec;
+
+	list_for_each_entry(child, &p->children, sibling) {
+		dec = child;
+		while ((dcnt = count_child_tasks(dec)) == 1)
+			dec = list_first_entry(&dec->children, struct task_struct, sibling);
+		
+		if (!dcnt || !depth) {
+			if (!task_is_bore_eligible(dec)) continue;
+			cnt++;
+			sum += dec->se.burst_penalty;
+			continue;
+		}
+		if (!burst_cache_expired(&dec->se.child_burst, now)) {
+			cnt += dec->se.child_burst.count;
+			sum += (u32)dec->se.child_burst.score * dec->se.child_burst.count;
+			continue;
+		}
+		update_child_burst_topological(dec, now, depth - 1, &cnt, &sum);
+	}
+
+	update_burst_cache(&p->se.child_burst, p, cnt, sum, now);
+	*acnt += cnt;
+	*asum += sum;
+}
+
+static inline u8 inherit_burst_topological(struct task_struct *p, u64 now) {
+	struct task_struct *anc = p;
+	u32 cnt = 0, sum = 0;
+
+	while (anc->real_parent != anc && count_child_tasks(anc) == 1)
+		anc = anc->real_parent;
+
+	if (burst_cache_expired(&anc->se.child_burst, now))
+		update_child_burst_topological(
+			anc, now, sched_burst_fork_atavistic - 1, &cnt, &sum);
+
+	return anc->se.child_burst.score;
+}
+
+static inline void update_tg_burst(struct task_struct *p, u64 now) {
+	struct task_struct *task;
+	u32 cnt = 0, sum = 0;
+
+	for_each_thread(p, task) {
+		if (!task_is_bore_eligible(task)) continue;
+		cnt++;
+		sum += task->se.burst_penalty;
+	}
+
+	update_burst_cache(&p->se.group_burst, p, cnt, sum, now);
+}
+
+static inline u8 inherit_burst_tg(struct task_struct *p, u64 now) {
+	struct task_struct *parent = p->group_leader;
+	if (burst_cache_expired(&parent->se.group_burst, now))
+		update_tg_burst(parent, now);
+
+	return parent->se.group_burst.score;
+}
+
+void sched_clone_bore(
+	struct task_struct *p, struct task_struct *parent, u64 clone_flags) {
+	if (!task_is_bore_eligible(p)) return;
+
+	u64 now = ktime_get_ns();
+	read_lock(&tasklist_lock);
+	u8 penalty = (clone_flags & CLONE_THREAD) ?
+		inherit_burst_tg(parent, now) :
+		likely(sched_burst_fork_atavistic) ?
+			inherit_burst_topological(parent, now):
+			inherit_burst_direct(parent, now);
+	read_unlock(&tasklist_lock);
+
+	struct sched_entity *se = &p->se;
+	revolve_burst_penalty(se);
+	se->burst_penalty = se->prev_burst_penalty =
+		max(se->prev_burst_penalty, penalty);
+	se->child_burst.timestamp = 0;
+	se->group_burst.timestamp = 0;
+}
+
+void init_task_bore(struct task_struct *p) {
+	p->se.burst_time = 0;
+	p->se.prev_burst_penalty = 0;
+	p->se.curr_burst_penalty = 0;
+	p->se.burst_penalty = 0;
+	p->se.burst_score = 0;
+	memset(&p->se.child_burst, 0, sizeof(struct sched_burst_cache));
+	memset(&p->se.group_burst, 0, sizeof(struct sched_burst_cache));
+}
+
+#ifdef CONFIG_SYSCTL
+static struct ctl_table sched_bore_sysctls[] = {
+	{
+		.procname	= "sched_bore",
+		.data		= &sched_bore,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler = sched_bore_update_handler,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+	{
+		.procname	= "sched_burst_exclude_kthreads",
+		.data		= &sched_burst_exclude_kthreads,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler = proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+	{
+		.procname	= "sched_burst_smoothness_long",
+		.data		= &sched_burst_smoothness_long,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler = proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+	{
+		.procname	= "sched_burst_smoothness_short",
+		.data		= &sched_burst_smoothness_short,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler = proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+	{
+		.procname	= "sched_burst_fork_atavistic",
+		.data		= &sched_burst_fork_atavistic,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler = proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_THREE,
+	},
+	{
+		.procname	= "sched_burst_parity_threshold",
+		.data		= &sched_burst_parity_threshold,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler = proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &maxval_u8,
+	},
+	{
+		.procname	= "sched_burst_penalty_offset",
+		.data		= &sched_burst_penalty_offset,
+		.maxlen		= sizeof(u8),
+		.mode		= 0644,
+		.proc_handler = proc_dou8vec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &sixty_four,
+	},
+	{
+		.procname	= "sched_burst_penalty_scale",
+		.data		= &sched_burst_penalty_scale,
+		.maxlen		= sizeof(uint),
+		.mode		= 0644,
+		.proc_handler = proc_douintvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &maxval_12_bits,
+	},
+	{
+		.procname	= "sched_burst_cache_lifetime",
+		.data		= &sched_burst_cache_lifetime,
+		.maxlen		= sizeof(uint),
+		.mode		= 0644,
+		.proc_handler = proc_douintvec,
+	},
+	{
+		.procname	= "sched_deadline_boost_mask",
+		.data		= &sched_deadline_boost_mask,
+		.maxlen		= sizeof(uint),
+		.mode		= 0644,
+		.proc_handler = proc_douintvec,
+	},
+};
+
+static int __init sched_bore_sysctl_init(void) {
+	register_sysctl_init("kernel", sched_bore_sysctls);
+	return 0;
+}
+late_initcall(sched_bore_sysctl_init);
+#endif // CONFIG_SYSCTL
+#endif // CONFIG_SCHED_BORE
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index b6a3fbccf5d6..61c016831485 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -97,6 +97,8 @@
 #include "../../io_uring/io-wq.h"
 #include "../smpboot.h"
 
+#include <linux/sched/bore.h>
+
 EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpu);
 EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpumask);
 
@@ -8298,6 +8300,11 @@ void __init sched_init(void)
 	BUG_ON(!sched_class_above(&ext_sched_class, &idle_sched_class));
 #endif
 
+#ifdef CONFIG_SCHED_BORE
+	printk(KERN_INFO "BORE (Burst-Oriented Response Enhancer) CPU Scheduler modification 5.6.1 by Masahito Suzuki");
+	init_task_bore(&init_task);
+#endif // CONFIG_SCHED_BORE
+
 	wait_bit_init();
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index c057ef46c5f8..3cab39e34824 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -167,7 +167,52 @@ static const struct file_operations sched_feat_fops = {
 };
 
 #ifdef CONFIG_SMP
+#ifdef CONFIG_SCHED_BORE
+static ssize_t sched_min_base_slice_write(struct file *filp, const char __user *ubuf,
+				   size_t cnt, loff_t *ppos)
+{
+	char buf[16];
+	unsigned int value;
+
+	if (cnt > 15)
+		cnt = 15;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+	buf[cnt] = '\0';
+
+	if (kstrtouint(buf, 10, &value))
+		return -EINVAL;
 
+	if (!value)
+		return -EINVAL;
+
+	sysctl_sched_min_base_slice = value;
+	sched_update_min_base_slice();
+
+	*ppos += cnt;
+	return cnt;
+}
+
+static int sched_min_base_slice_show(struct seq_file *m, void *v)
+{
+	seq_printf(m, "%d\n", sysctl_sched_min_base_slice);
+	return 0;
+}
+
+static int sched_min_base_slice_open(struct inode *inode, struct file *filp)
+{
+	return single_open(filp, sched_min_base_slice_show, NULL);
+}
+
+static const struct file_operations sched_min_base_slice_fops = {
+	.open		= sched_min_base_slice_open,
+	.write		= sched_min_base_slice_write,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+#else // !CONFIG_SCHED_BORE
 static ssize_t sched_scaling_write(struct file *filp, const char __user *ubuf,
 				   size_t cnt, loff_t *ppos)
 {
@@ -213,7 +258,7 @@ static const struct file_operations sched_scaling_fops = {
 	.llseek		= seq_lseek,
 	.release	= single_release,
 };
-
+#endif // CONFIG_SCHED_BORE
 #endif /* SMP */
 
 #ifdef CONFIG_PREEMPT_DYNAMIC
@@ -347,13 +392,20 @@ static __init int sched_init_debug(void)
 	debugfs_create_file("preempt", 0644, debugfs_sched, NULL, &sched_dynamic_fops);
 #endif
 
+#ifdef CONFIG_SCHED_BORE
+	debugfs_create_file("min_base_slice_ns", 0644, debugfs_sched, NULL, &sched_min_base_slice_fops);
+	debugfs_create_u32("base_slice_ns", 0400, debugfs_sched, &sysctl_sched_base_slice);
+#else // !CONFIG_SCHED_BORE
 	debugfs_create_u32("base_slice_ns", 0644, debugfs_sched, &sysctl_sched_base_slice);
+#endif // CONFIG_SCHED_BORE
 
 	debugfs_create_u32("latency_warn_ms", 0644, debugfs_sched, &sysctl_resched_latency_warn_ms);
 	debugfs_create_u32("latency_warn_once", 0644, debugfs_sched, &sysctl_resched_latency_warn_once);
 
 #ifdef CONFIG_SMP
+#if !defined(CONFIG_SCHED_BORE)
 	debugfs_create_file("tunable_scaling", 0644, debugfs_sched, NULL, &sched_scaling_fops);
+#endif // CONFIG_SCHED_BORE
 	debugfs_create_u32("migration_cost_ns", 0644, debugfs_sched, &sysctl_sched_migration_cost);
 	debugfs_create_u32("nr_migrate", 0644, debugfs_sched, &sysctl_sched_nr_migrate);
 
@@ -596,6 +648,9 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		SPLIT_NS(schedstat_val_or_zero(p->stats.sum_sleep_runtime)),
 		SPLIT_NS(schedstat_val_or_zero(p->stats.sum_block_runtime)));
 
+#ifdef CONFIG_SCHED_BORE
+	SEQ_printf(m, " %2d", p->se.burst_score);
+#endif // CONFIG_SCHED_BORE
 #ifdef CONFIG_NUMA_BALANCING
 	SEQ_printf(m, " %d %d", task_node(p), task_numa_group_id(p));
 #endif
@@ -1069,6 +1124,9 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 
 	P(se.load.weight);
 #ifdef CONFIG_SMP
+#ifdef CONFIG_SCHED_BORE
+	P(se.burst_score);
+#endif // CONFIG_SCHED_BORE
 	P(se.avg.load_sum);
 	P(se.avg.runnable_sum);
 	P(se.avg.util_sum);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 718c92335979..603d72b9e6e8 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -55,6 +55,8 @@
 #include "stats.h"
 #include "autogroup.h"
 
+#include <linux/sched/bore.h>
+
 /*
  * The initial- and re-scaling of tunables is configurable
  *
@@ -64,17 +66,29 @@
  *   SCHED_TUNABLESCALING_LOG - scaled logarithmically, *1+ilog(ncpus)
  *   SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus
  *
- * (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus))
+ * (BORE  default SCHED_TUNABLESCALING_NONE = *1 constant)
+ * (EEVDF default SCHED_TUNABLESCALING_LOG  = *(1+ilog(ncpus))
  */
+#ifdef CONFIG_SCHED_BORE
+unsigned int sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_NONE;
+#else // !CONFIG_SCHED_BORE
 unsigned int sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;
+#endif // CONFIG_SCHED_BORE
 
 /*
  * Minimal preemption granularity for CPU-bound tasks:
  *
- * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)
+ * (BORE  default: max(1 sec / HZ, min_base_slice) constant, units: nanoseconds)
+ * (EEVDF default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)
  */
+#ifdef CONFIG_SCHED_BORE
+unsigned int            sysctl_sched_base_slice = 1000000000ULL / HZ;
+static unsigned int configured_sched_base_slice = 1000000000ULL / HZ;
+unsigned int        sysctl_sched_min_base_slice = CONFIG_MIN_BASE_SLICE_NS;
+#else // !CONFIG_SCHED_BORE
 unsigned int sysctl_sched_base_slice			= 750000ULL;
 static unsigned int normalized_sysctl_sched_base_slice	= 750000ULL;
+#endif // CONFIG_SCHED_BORE
 
 const_debug unsigned int sysctl_sched_migration_cost	= 500000UL;
 
@@ -188,6 +202,18 @@ static inline void update_load_set(struct load_weight *lw, unsigned long w)
  *
  * This idea comes from the SD scheduler of Con Kolivas:
  */
+#ifdef CONFIG_SCHED_BORE
+static void update_sysctl(void) {
+	unsigned int base_slice = configured_sched_base_slice;
+	unsigned int min_base_slice = sysctl_sched_min_base_slice;
+
+	if (min_base_slice)
+		base_slice *= DIV_ROUND_UP(min_base_slice, base_slice);
+
+	sysctl_sched_base_slice = base_slice;
+}
+void sched_update_min_base_slice(void) { update_sysctl(); }
+#else // !CONFIG_SCHED_BORE
 static unsigned int get_update_sysctl_factor(void)
 {
 	unsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);
@@ -218,6 +244,7 @@ static void update_sysctl(void)
 	SET_SYSCTL(sched_base_slice);
 #undef SET_SYSCTL
 }
+#endif // CONFIG_SCHED_BORE
 
 void __init sched_init_granularity(void)
 {
@@ -695,6 +722,9 @@ static s64 entity_lag(u64 avruntime, struct sched_entity *se)
 
 	vlag = avruntime - se->vruntime;
 	limit = calc_delta_fair(max_t(u64, 2*se->slice, TICK_NSEC), se);
+#ifdef CONFIG_SCHED_BORE
+	limit >>= !!sched_bore;
+#endif // CONFIG_SCHED_BORE
 
 	return clamp(vlag, -limit, limit);
 }
@@ -896,6 +926,10 @@ static struct sched_entity *pick_eevdf(struct cfs_rq *cfs_rq)
 	 * until it gets a new slice. See the HACK in set_next_entity().
 	 */
 	if (sched_feat(RUN_TO_PARITY) && curr && curr->vlag == curr->deadline)
+#ifdef CONFIG_SCHED_BORE
+		if (!(likely(sched_bore) && likely(sched_burst_parity_threshold) &&
+			sched_burst_parity_threshold < cfs_rq->nr_running))
+#endif // CONFIG_SCHED_BORE
 		return curr;
 
 	/* Pick the leftmost entity if it's eligible */
@@ -954,6 +988,7 @@ struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)
  * Scheduling class statistics methods:
  */
 #ifdef CONFIG_SMP
+#if !defined(CONFIG_SCHED_BORE)
 int sched_update_scaling(void)
 {
 	unsigned int factor = get_update_sysctl_factor();
@@ -965,6 +1000,7 @@ int sched_update_scaling(void)
 
 	return 0;
 }
+#endif // CONFIG_SCHED_BORE
 #endif
 #endif
 
@@ -1165,6 +1201,10 @@ static void update_curr(struct cfs_rq *cfs_rq)
 	if (unlikely(delta_exec <= 0))
 		return;
 
+#ifdef CONFIG_SCHED_BORE
+	curr->burst_time += delta_exec;
+	update_burst_penalty(curr);
+#endif // CONFIG_SCHED_BORE
 	curr->vruntime += calc_delta_fair(delta_exec, curr);
 	update_deadline(cfs_rq, curr);
 	update_min_vruntime(cfs_rq);
@@ -3791,7 +3831,7 @@ static void reweight_eevdf(struct sched_entity *se, u64 avruntime,
 	se->deadline = avruntime + vslice;
 }
 
-static void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,
+void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,
 			    unsigned long weight)
 {
 	bool curr = cfs_rq->curr == se;
@@ -5199,6 +5239,9 @@ place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 	 *
 	 * EEVDF: placement strategy #1 / #2
 	 */
+#ifdef CONFIG_SCHED_BORE
+	if (se->vlag)
+#endif // CONFIG_SCHED_BORE
 	if (sched_feat(PLACE_LAG) && cfs_rq->nr_running) {
 		struct sched_entity *curr = cfs_rq->curr;
 		unsigned long load;
@@ -5269,6 +5312,16 @@ place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 
 	se->vruntime = vruntime - lag;
 
+	if (sched_feat(PLACE_REL_DEADLINE) && se->rel_deadline) {
+		se->deadline += se->vruntime;
+		se->rel_deadline = 0;
+		return;
+	}
+#ifdef CONFIG_SCHED_BORE
+	else if (likely(sched_bore))
+		vslice >>= !!(flags & sched_deadline_boost_mask);
+	else
+#endif // CONFIG_SCHED_BORE
 	/*
 	 * When joining the competition; the existing tasks will be,
 	 * on average, halfway through their slice, as such start tasks
@@ -5378,6 +5431,7 @@ static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);
 static void
 dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 {
+	bool sleep = flags & DEQUEUE_SLEEP;
 	int action = UPDATE_TG;
 
 	if (entity_is_task(se) && task_on_rq_migrating(task_of(se)))
@@ -5405,6 +5459,11 @@ dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 	clear_buddies(cfs_rq, se);
 
 	update_entity_lag(cfs_rq, se);
+	if (sched_feat(PLACE_REL_DEADLINE) && !sleep) {
+		se->deadline -= se->vruntime;
+		se->rel_deadline = 1;
+	}
+
 	if (se != cfs_rq->curr)
 		__dequeue_entity(cfs_rq, se);
 	se->on_rq = 0;
@@ -6856,6 +6915,14 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 	bool was_sched_idle = sched_idle_rq(rq);
 
 	util_est_dequeue(&rq->cfs, p);
+#ifdef CONFIG_SCHED_BORE
+	if (task_sleep) {
+		cfs_rq = cfs_rq_of(se);
+		if (cfs_rq->curr == se)
+			update_curr(cfs_rq);
+		restart_burst(se);
+	}
+#endif // CONFIG_SCHED_BORE
 
 	for_each_sched_entity(se) {
 		cfs_rq = cfs_rq_of(se);
@@ -8638,16 +8705,25 @@ static void yield_task_fair(struct rq *rq)
 	/*
 	 * Are we the only task in the tree?
 	 */
+#if !defined(CONFIG_SCHED_BORE)
 	if (unlikely(rq->nr_running == 1))
 		return;
 
 	clear_buddies(cfs_rq, se);
+#endif // CONFIG_SCHED_BORE
 
 	update_rq_clock(rq);
 	/*
 	 * Update run-time statistics of the 'current'.
 	 */
 	update_curr(cfs_rq);
+#ifdef CONFIG_SCHED_BORE
+	restart_burst_rescale_deadline(se);
+	if (unlikely(rq->nr_running == 1))
+		return;
+
+	clear_buddies(cfs_rq, se);
+#endif // CONFIG_SCHED_BORE
 	/*
 	 * Tell update_rq_clock() that we've just updated,
 	 * so we don't do microscopic update in schedule()
@@ -12712,6 +12788,9 @@ static void task_fork_fair(struct task_struct *p)
 	curr = cfs_rq->curr;
 	if (curr)
 		update_curr(cfs_rq);
+#ifdef CONFIG_SCHED_BORE
+	update_burst_score(se);
+#endif // CONFIG_SCHED_BORE
 	place_entity(cfs_rq, se, ENQUEUE_INITIAL);
 	rq_unlock(rq, &rf);
 }
@@ -12824,6 +12903,10 @@ static void attach_task_cfs_rq(struct task_struct *p)
 
 static void switched_from_fair(struct rq *rq, struct task_struct *p)
 {
+	p->se.rel_deadline = 0;
+#ifdef CONFIG_SCHED_BORE
+	init_task_bore(p);
+#endif // CONFIG_SCHED_BORE
 	detach_task_cfs_rq(p);
 }
 
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 143f55df890b..e97b7b68bdd3 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -6,6 +6,10 @@
  */
 SCHED_FEAT(PLACE_LAG, true)
 SCHED_FEAT(PLACE_DEADLINE_INITIAL, true)
+/*
+ * Preserve relative virtual deadline on 'migration'.
+ */
+SCHED_FEAT(PLACE_REL_DEADLINE, true)
 SCHED_FEAT(RUN_TO_PARITY, true)
 
 /*
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4965853277e2..f2256886a700 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2065,7 +2065,11 @@ static inline void update_sched_domain_debugfs(void) { }
 static inline void dirty_sched_domain_sysctl(int cpu) { }
 #endif
 
+#ifdef CONFIG_SCHED_BORE
+extern void sched_update_min_base_slice(void);
+#else // !CONFIG_SCHED_BORE
 extern int sched_update_scaling(void);
+#endif // CONFIG_SCHED_BORE
 
 static inline const struct cpumask *task_user_cpus(struct task_struct *p)
 {
@@ -2738,6 +2742,9 @@ extern const_debug unsigned int sysctl_sched_nr_migrate;
 extern const_debug unsigned int sysctl_sched_migration_cost;
 
 extern unsigned int sysctl_sched_base_slice;
+#ifdef CONFIG_SCHED_BORE
+extern unsigned int sysctl_sched_min_base_slice;
+#endif // CONFIG_SCHED_BORE
 
 #ifdef CONFIG_SCHED_DEBUG
 extern int sysctl_resched_latency_warn_ms;
-- 
2.47.0


From ba83def98ed5cedd1ad55163766586bdccb4a78a Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:23:29 +0200
Subject: [PATCH v1.4 108/120] [BEGIN] General and Temporary Patches

-- 
2.47.0


From 7cbe6d3844d4d111c0822dd47025ef609b7e917e Mon Sep 17 00:00:00 2001
From: Denis <benato.denis96@gmail.com>
Date: Thu, 28 Sep 2023 03:40:53 +0200
Subject: [PATCH v1.4 109/120] [NOT FOR UPSTREAM] add acpi_call module

---
 drivers/platform/x86/Kconfig     |   5 +
 drivers/platform/x86/Makefile    |   4 +
 drivers/platform/x86/acpi_call.c | 449 +++++++++++++++++++++++++++++++
 3 files changed, 458 insertions(+)
 create mode 100644 drivers/platform/x86/acpi_call.c

diff --git a/drivers/platform/x86/Kconfig b/drivers/platform/x86/Kconfig
index ddfccc226751..10af8a252f88 100644
--- a/drivers/platform/x86/Kconfig
+++ b/drivers/platform/x86/Kconfig
@@ -180,6 +180,11 @@ config ACER_WIRELESS
           If you choose to compile this driver as a module the module will be
           called acer-wireless.
 
+config ACPI_CALL
+	tristate "acpi_call module"
+	help
+	  This embeds acpi_call module into the kernel
+
 config ACER_WMI
 	tristate "Acer WMI Laptop Extras"
 	depends on BACKLIGHT_CLASS_DEVICE
diff --git a/drivers/platform/x86/Makefile b/drivers/platform/x86/Makefile
index e1b142947067..870d0157dbc7 100644
--- a/drivers/platform/x86/Makefile
+++ b/drivers/platform/x86/Makefile
@@ -4,10 +4,14 @@
 # x86 Platform-Specific Drivers
 #
 
+# ACPI calls
+
 # Windows Management Interface
 obj-$(CONFIG_ACPI_WMI)		+= wmi.o
 obj-$(CONFIG_WMI_BMOF)		+= wmi-bmof.o
 
+obj-$(CONFIG_ACPI_CALL)		+= acpi_call.o
+
 # WMI drivers
 obj-$(CONFIG_HUAWEI_WMI)		+= huawei-wmi.o
 obj-$(CONFIG_MXM_WMI)			+= mxm-wmi.o
diff --git a/drivers/platform/x86/acpi_call.c b/drivers/platform/x86/acpi_call.c
new file mode 100644
index 000000000000..d7bc238e16da
--- /dev/null
+++ b/drivers/platform/x86/acpi_call.c
@@ -0,0 +1,449 @@
+/* Copyright (c) 2010: Michal Kottman */
+
+#define BUILDING_ACPICA
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/proc_fs.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
+#include <asm/uaccess.h>
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 17, 0)
+#include <linux/acpi.h>
+#else
+#include <acpi/acpi.h>
+#endif
+
+MODULE_LICENSE("GPL");
+
+/* Uncomment the following line to enable debug messages */
+/*
+#define DEBUG
+*/
+
+#define BUFFER_SIZE 4096
+#define INPUT_BUFFER_SIZE (2 * BUFFER_SIZE)
+#define MAX_ACPI_ARGS 16
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 10, 0)
+#define HAVE_PROC_CREATE
+#endif
+
+extern struct proc_dir_entry *acpi_root_dir;
+
+static char input_buffer[INPUT_BUFFER_SIZE];
+static char result_buffer[BUFFER_SIZE];
+static char not_called_message[11] = "not called";
+
+static u8 temporary_buffer[BUFFER_SIZE];
+
+static size_t get_avail_bytes(void) {
+    return BUFFER_SIZE - strlen(result_buffer);
+}
+static char *get_buffer_end(void) {
+    return result_buffer + strlen(result_buffer);
+}
+
+/** Appends the contents of an acpi_object to the result buffer
+@param result   An acpi object holding result data
+@returns        0 if the result could fully be saved, a higher value otherwise
+*/
+static int acpi_result_to_string(union acpi_object *result) {
+    if (result->type == ACPI_TYPE_INTEGER) {
+        snprintf(get_buffer_end(), get_avail_bytes(),
+            "0x%x", (int)result->integer.value);
+    } else if (result->type == ACPI_TYPE_STRING) {
+        snprintf(get_buffer_end(), get_avail_bytes(),
+            "\"%*s\"", result->string.length, result->string.pointer);
+    } else if (result->type == ACPI_TYPE_BUFFER) {
+        int i;
+        // do not store more than data if it does not fit. The first element is
+        // just 4 chars, but there is also two bytes from the curly brackets
+        int show_values = min((size_t)result->buffer.length, get_avail_bytes() / 6);
+
+        snprintf(get_buffer_end(), get_avail_bytes(), "{");
+        for (i = 0; i < show_values; i++)
+            sprintf(get_buffer_end(),
+                i == 0 ? "0x%02x" : ", 0x%02x", result->buffer.pointer[i]);
+
+        if (result->buffer.length > show_values) {
+            // if data was truncated, show a trailing comma if there is space
+            snprintf(get_buffer_end(), get_avail_bytes(), ",");
+            return 1;
+        } else {
+            // in case show_values == 0, but the buffer is too small to hold
+            // more values (i.e. the buffer cannot have anything more than "{")
+            snprintf(get_buffer_end(), get_avail_bytes(), "}");
+        }
+    } else if (result->type == ACPI_TYPE_PACKAGE) {
+        int i;
+        snprintf(get_buffer_end(), get_avail_bytes(), "[");
+        for (i=0; i<result->package.count; i++) {
+            if (i > 0)
+                snprintf(get_buffer_end(), get_avail_bytes(), ", ");
+
+            // abort if there is no more space available
+            if (!get_avail_bytes() || acpi_result_to_string(&result->package.elements[i]))
+                return 1;
+        }
+        snprintf(get_buffer_end(), get_avail_bytes(), "]");
+    } else {
+        snprintf(get_buffer_end(), get_avail_bytes(),
+            "Object type 0x%x\n", result->type);
+    }
+
+    // return 0 if there are still bytes available, 1 otherwise
+    return !get_avail_bytes();
+}
+
+/**
+@param method   The full name of ACPI method to call
+@param argc     The number of parameters
+@param argv     A pre-allocated array of arguments of type acpi_object
+*/
+static void do_acpi_call(const char * method, int argc, union acpi_object *argv)
+{
+    acpi_status status;
+    acpi_handle handle;
+    struct acpi_object_list arg;
+    struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };
+
+#ifdef DEBUG
+    printk(KERN_INFO "acpi_call: Calling %s\n", method);
+#endif
+
+    // get the handle of the method, must be a fully qualified path
+    status = acpi_get_handle(NULL, (acpi_string) method, &handle);
+
+    if (ACPI_FAILURE(status))
+    {
+        snprintf(result_buffer, BUFFER_SIZE, "Error: %s", acpi_format_exception(status));
+        printk(KERN_ERR "acpi_call: Cannot get handle: %s\n", result_buffer);
+        return;
+    }
+
+    // prepare parameters
+    arg.count = argc;
+    arg.pointer = argv;
+
+    // call the method
+    status = acpi_evaluate_object(handle, NULL, &arg, &buffer);
+    if (ACPI_FAILURE(status))
+    {
+        snprintf(result_buffer, BUFFER_SIZE, "Error: %s", acpi_format_exception(status));
+        printk(KERN_ERR "acpi_call: Method call failed: %s\n", result_buffer);
+        return;
+    }
+
+    // reset the result buffer
+    *result_buffer = '\0';
+    acpi_result_to_string(buffer.pointer);
+    kfree(buffer.pointer);
+
+#ifdef DEBUG
+    printk(KERN_INFO "acpi_call: Call successful: %s\n", result_buffer);
+#endif
+}
+
+/** Decodes 2 hex characters to an u8 int
+*/
+u8 decodeHex(char *hex) {
+    char buf[3] = { hex[0], hex[1], 0};
+    return (u8) simple_strtoul(buf, NULL, 16);
+}
+
+/** Parses method name and arguments
+@param input Input string to be parsed. Modified in the process.
+@param nargs Set to number of arguments parsed (output)
+@param args
+*/
+static char *parse_acpi_args(char *input, int *nargs, union acpi_object **args)
+{
+    char *s = input;
+    int i;
+
+    *nargs = 0;
+    *args = NULL;
+
+    // the method name is separated from the arguments by a space
+    while (*s && *s != ' ')
+        s++;
+    // if no space is found, return 0 arguments
+    if (*s == 0)
+        return input;
+
+    *args = (union acpi_object *) kmalloc(MAX_ACPI_ARGS * sizeof(union acpi_object), GFP_KERNEL);
+    if (!*args) {
+        printk(KERN_ERR "acpi_call: unable to allocate buffer\n");
+        return NULL;
+    }
+
+    while (*s) {
+        if (*s == ' ') {
+            if (*nargs == 0)
+                *s = 0; // change first space to nul
+            ++ *nargs;
+            ++ s;
+        } else {
+            union acpi_object *arg = (*args) + (*nargs - 1);
+            if (*s == '"') {
+                // decode string
+                arg->type = ACPI_TYPE_STRING;
+                arg->string.pointer = ++s;
+                arg->string.length = 0;
+                while (*s && *s++ != '"')
+                    arg->string.length ++;
+                // skip the last "
+                if (*s == '"')
+                    ++s;
+            } else if (*s == 'b') {
+                // decode buffer - bXXXX
+                char *p = ++s;
+                int len = 0, i;
+                u8 *buf = NULL;
+
+                while (*p && *p!=' ')
+                    p++;
+
+                len = p - s;
+                if (len % 2 == 1) {
+                    printk(KERN_ERR "acpi_call: buffer arg%d is not multiple of 8 bits\n", *nargs);
+                    --*nargs;
+                    goto err;
+                }
+                len /= 2;
+
+                buf = (u8*) kmalloc(len, GFP_KERNEL);
+                if (!buf) {
+                    printk(KERN_ERR "acpi_call: unable to allocate buffer\n");
+                    --*nargs;
+                    goto err;
+                }
+                for (i=0; i<len; i++) {
+                    buf[i] = decodeHex(s + i*2);
+                }
+                s = p;
+
+                arg->type = ACPI_TYPE_BUFFER;
+                arg->buffer.pointer = buf;
+                arg->buffer.length = len;
+            } else if (*s == '{') {
+                // decode buffer - { b1, b2 ...}
+                u8 *buf = temporary_buffer;
+                arg->type = ACPI_TYPE_BUFFER;
+                arg->buffer.pointer = buf;
+                arg->buffer.length = 0;
+                while (*s && *s++ != '}') {
+                    if (buf >= temporary_buffer + sizeof(temporary_buffer)) {
+                        printk(KERN_ERR "acpi_call: buffer arg%d is truncated because the buffer is full\n", *nargs);
+                        // clear remaining arguments
+                        while (*s && *s != '}')
+                            ++s;
+                        break;
+                    }
+                    else if (*s >= '0' && *s <= '9') {
+                        // decode integer into buffer
+                        arg->buffer.length ++;
+                        if (s[0] == '0' && s[1] == 'x')
+                            *buf++ = simple_strtol(s+2, 0, 16);
+                        else
+                            *buf++ = simple_strtol(s, 0, 10);
+                    }
+                    // skip until space or comma or '}'
+                    while (*s && *s != ' ' && *s != ',' && *s != '}')
+                        ++s;
+                }
+                // store the result in new allocated buffer
+                buf = (u8*) kmalloc(arg->buffer.length, GFP_KERNEL);
+                if (!buf) {
+                    printk(KERN_ERR "acpi_call: unable to allocate buffer\n");
+                    --*nargs;
+                    goto err;
+                }
+                memcpy(buf, temporary_buffer, arg->buffer.length);
+                arg->buffer.pointer = buf;
+            } else {
+                // decode integer, N or 0xN
+                arg->type = ACPI_TYPE_INTEGER;
+                if (s[0] == '0' && s[1] == 'x') {
+                    arg->integer.value = simple_strtol(s+2, 0, 16);
+                } else {
+                    arg->integer.value = simple_strtol(s, 0, 10);
+                }
+                while (*s && *s != ' ') {
+                    ++s;
+                }
+            }
+        }
+    }
+
+    return input;
+
+err:
+    for (i=0; i<*nargs; i++)
+        if ((*args)[i].type == ACPI_TYPE_BUFFER && (*args)[i].buffer.pointer)
+            kfree((*args)[i].buffer.pointer);
+    kfree(*args);
+    return NULL;
+}
+
+/** procfs write callback. Called when writing into /proc/acpi/call.
+*/
+#ifdef HAVE_PROC_CREATE
+static ssize_t acpi_proc_write( struct file *filp, const char __user *buff,
+    size_t len, loff_t *data )
+#else
+static int acpi_proc_write( struct file *filp, const char __user *buff,
+    unsigned long len, void *data )
+#endif
+{
+    union acpi_object *args;
+    int nargs, i;
+    char *method;
+
+    memset(input_buffer, 0, INPUT_BUFFER_SIZE);
+    if (len > sizeof(input_buffer) - 1) {
+#ifdef HAVE_PROC_CREATE
+        printk(KERN_ERR "acpi_call: Input too long! (%zu)\n", len);
+#else
+        printk(KERN_ERR "acpi_call: Input too long! (%lu)\n", len);
+#endif
+        return -ENOSPC;
+    }
+
+    if (copy_from_user( input_buffer, buff, len )) {
+        return -EFAULT;
+    }
+    input_buffer[len] = '\0';
+    if (input_buffer[len-1] == '\n')
+        input_buffer[len-1] = '\0';
+
+    method = parse_acpi_args(input_buffer, &nargs, &args);
+    if (method) {
+        do_acpi_call(method, nargs, args);
+        if (args) {
+            for (i=0; i<nargs; i++)
+                if (args[i].type == ACPI_TYPE_BUFFER)
+                    kfree(args[i].buffer.pointer);
+        }
+    }
+    if (args)
+        kfree(args);
+
+    return len;
+}
+
+/** procfs 'call' read callback. Called when reading the content of /proc/acpi/call.
+Returns the last call status:
+- "not called" when no call was previously issued
+- "failed" if the call failed
+- "ok" if the call succeeded
+*/
+#ifdef HAVE_PROC_CREATE
+static ssize_t acpi_proc_read( struct file *filp, char __user *buff,
+            size_t count, loff_t *off )
+{
+    ssize_t ret;
+    int len = strlen(result_buffer);
+
+    if(len == 0) {
+        ret = simple_read_from_buffer(buff, count, off, not_called_message, strlen(not_called_message) + 1);
+    } else if(len + 1 > count) {
+        // user buffer is too small
+        ret = 0;
+    } else if(*off == len + 1) {
+        // we're done
+        ret = 0;
+        result_buffer[0] = '\0';
+    } else {
+        // output the current result buffer
+        ret = simple_read_from_buffer(buff, count, off, result_buffer, len + 1);
+        *off = ret;
+    }
+
+    return ret;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0)
+static struct proc_ops proc_acpi_operations = {
+	.proc_read = acpi_proc_read,
+	.proc_write = acpi_proc_write,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 13, 0)
+	.proc_lseek = default_llseek,
+#endif
+};
+#else
+static struct file_operations proc_acpi_operations = {
+        .owner    = THIS_MODULE,
+        .read     = acpi_proc_read,
+        .write    = acpi_proc_write,
+};
+#endif
+
+#else
+static int acpi_proc_read(char *page, char **start, off_t off,
+    int count, int *eof, void *data)
+{
+    int len = 0;
+
+    if (off > 0) {
+        *eof = 1;
+        return 0;
+    }
+
+    // output the current result buffer
+    len = strlen(result_buffer);
+    memcpy(page, result_buffer, len + 1);
+
+    // initialize the result buffer for later
+    strcpy(result_buffer, "not called");
+
+    return len;
+}
+#endif
+
+/** module initialization function */
+static int __init init_acpi_call(void)
+{
+#ifdef HAVE_PROC_CREATE
+    struct proc_dir_entry *acpi_entry = proc_create("call",
+                                                    0660,
+                                                    acpi_root_dir,
+                                                    &proc_acpi_operations);
+#else
+    struct proc_dir_entry *acpi_entry = create_proc_entry("call", 0660, acpi_root_dir);
+#endif
+
+    strcpy(result_buffer, "not called");
+
+    if (acpi_entry == NULL) {
+      printk(KERN_ERR "acpi_call: Couldn't create proc entry\n");
+      return -ENOMEM;
+    }
+
+#ifndef HAVE_PROC_CREATE
+    acpi_entry->write_proc = acpi_proc_write;
+    acpi_entry->read_proc = acpi_proc_read;
+#endif
+
+#ifdef DEBUG
+    printk(KERN_INFO "acpi_call: Module loaded successfully\n");
+#endif
+
+    return 0;
+}
+
+static void __exit unload_acpi_call(void)
+{
+    remove_proc_entry("call", acpi_root_dir);
+
+#ifdef DEBUG
+    printk(KERN_INFO "acpi_call: Module unloaded successfully\n");
+#endif
+}
+
+module_init(init_acpi_call);
+module_exit(unload_acpi_call);
\ No newline at end of file
-- 
2.47.0


From 5faa9510f31657daf5d19db9212d85e5d03676ad Mon Sep 17 00:00:00 2001
From: Abhishek Pandit-Subedi <abhishekpandit@chromium.org>
Date: Wed, 4 Dec 2019 18:30:54 -0800
Subject: [PATCH v1.4 110/120] Input: uinput - Add UI_SET_PHYS_STR and
 UI_SET_UNIQ_STR

The ioctl definition for UI_SET_PHYS is ambiguous because it is defined
with size = sizeof(char*) but is expected to be given a variable length
string. Add a deprecation notice for UI_SET_PHYS and provide
UI_SET_PHYS_STR(len) which expects a size from the user.

Also support setting the uniq attribute of the input device. The uniq
attribute is used as a unique identifier for the connected device.

For example, uinput devices created by BlueZ will store the address of
the connected device as the uniq property.

Signed-off-by: Abhishek Pandit-Subedi <abhishekpandit@chromium.org>
---
 drivers/input/misc/uinput.c | 48 +++++++++++++++++++++++++------------
 include/uapi/linux/uinput.h |  5 ++++
 2 files changed, 38 insertions(+), 15 deletions(-)

diff --git a/drivers/input/misc/uinput.c b/drivers/input/misc/uinput.c
index 445856c9127a..4514f7d65ddb 100644
--- a/drivers/input/misc/uinput.c
+++ b/drivers/input/misc/uinput.c
@@ -20,6 +20,7 @@
  */
 #include <uapi/linux/uinput.h>
 #include <linux/poll.h>
+#include <linux/printk.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/module.h>
@@ -285,7 +286,7 @@ static int uinput_dev_flush(struct input_dev *dev, struct file *file)
 
 static void uinput_destroy_device(struct uinput_device *udev)
 {
-	const char *name, *phys;
+	const char *name, *phys, *uniq;
 	struct input_dev *dev = udev->dev;
 	enum uinput_state old_state = udev->state;
 
@@ -294,6 +295,7 @@ static void uinput_destroy_device(struct uinput_device *udev)
 	if (dev) {
 		name = dev->name;
 		phys = dev->phys;
+		uniq = dev->uniq;
 		if (old_state == UIST_CREATED) {
 			uinput_flush_requests(udev);
 			input_unregister_device(dev);
@@ -302,6 +304,7 @@ static void uinput_destroy_device(struct uinput_device *udev)
 		}
 		kfree(name);
 		kfree(phys);
+		kfree(uniq);
 		udev->dev = NULL;
 	}
 }
@@ -884,6 +887,24 @@ static int uinput_str_to_user(void __user *dest, const char *str,
 	return ret ? -EFAULT : len;
 }
 
+static int uinput_get_user_str(struct uinput_device *udev, const char **kptr,
+			       const char *uptr, unsigned int size)
+{
+	char *tmp;
+
+	if (udev->state == UIST_CREATED)
+		return -EINVAL;
+
+	tmp = strndup_user(uptr, size);
+	if (IS_ERR(tmp))
+		return PTR_ERR(tmp);
+
+	kfree(*kptr);
+	*kptr = tmp;
+
+	return 0;
+}
+
 static long uinput_ioctl_handler(struct file *file, unsigned int cmd,
 				 unsigned long arg, void __user *p)
 {
@@ -892,7 +913,6 @@ static long uinput_ioctl_handler(struct file *file, unsigned int cmd,
 	struct uinput_ff_upload ff_up;
 	struct uinput_ff_erase  ff_erase;
 	struct uinput_request   *req;
-	char			*phys;
 	const char		*name;
 	unsigned int		size;
 
@@ -969,19 +989,8 @@ static long uinput_ioctl_handler(struct file *file, unsigned int cmd,
 		goto out;
 
 	case UI_SET_PHYS:
-		if (udev->state == UIST_CREATED) {
-			retval = -EINVAL;
-			goto out;
-		}
-
-		phys = strndup_user(p, 1024);
-		if (IS_ERR(phys)) {
-			retval = PTR_ERR(phys);
-			goto out;
-		}
-
-		kfree(udev->dev->phys);
-		udev->dev->phys = phys;
+		pr_warn_once("uinput: UI_SET_PHYS is deprecated. Use UI_SET_PHYS_STR");
+		retval = uinput_get_user_str(udev, &udev->dev->phys, p, 1024);
 		goto out;
 
 	case UI_BEGIN_FF_UPLOAD:
@@ -1076,6 +1085,15 @@ static long uinput_ioctl_handler(struct file *file, unsigned int cmd,
 	case UI_ABS_SETUP & ~IOCSIZE_MASK:
 		retval = uinput_abs_setup(udev, p, size);
 		goto out;
+
+	case UI_SET_PHYS_STR(0):
+		retval = uinput_get_user_str(udev, &udev->dev->phys, p, size);
+		goto out;
+
+	case UI_SET_UNIQ_STR(0):
+		retval = uinput_get_user_str(udev, &udev->dev->uniq, p, size);
+		goto out;
+
 	}
 
 	retval = -EINVAL;
diff --git a/include/uapi/linux/uinput.h b/include/uapi/linux/uinput.h
index c9e677e3af1d..84d4fa142830 100644
--- a/include/uapi/linux/uinput.h
+++ b/include/uapi/linux/uinput.h
@@ -142,9 +142,14 @@ struct uinput_abs_setup {
 #define UI_SET_LEDBIT		_IOW(UINPUT_IOCTL_BASE, 105, int)
 #define UI_SET_SNDBIT		_IOW(UINPUT_IOCTL_BASE, 106, int)
 #define UI_SET_FFBIT		_IOW(UINPUT_IOCTL_BASE, 107, int)
+
+/* DEPRECATED: Data size is ambiguous. Use UI_SET_PHYS_STR instead. */
 #define UI_SET_PHYS		_IOW(UINPUT_IOCTL_BASE, 108, char*)
+
 #define UI_SET_SWBIT		_IOW(UINPUT_IOCTL_BASE, 109, int)
 #define UI_SET_PROPBIT		_IOW(UINPUT_IOCTL_BASE, 110, int)
+#define UI_SET_PHYS_STR(len)	_IOC(_IOC_WRITE, UINPUT_IOCTL_BASE, 111, len)
+#define UI_SET_UNIQ_STR(len)	_IOC(_IOC_WRITE, UINPUT_IOCTL_BASE, 112, len)
 
 #define UI_BEGIN_FF_UPLOAD	_IOWR(UINPUT_IOCTL_BASE, 200, struct uinput_ff_upload)
 #define UI_END_FF_UPLOAD	_IOW(UINPUT_IOCTL_BASE, 201, struct uinput_ff_upload)
-- 
2.47.0


From b14406b121f34611ffbaf2617cef96157cad8592 Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Sat, 12 Oct 2024 19:24:08 +0200
Subject: [PATCH v1.4 111/120] Fix HAINAN amdgpu card not being bootable

Link:
---
 drivers/gpu/drm/amd/pm/legacy-dpm/si_dpm.c | 4 +++-
 drivers/gpu/drm/radeon/si_dpm.c            | 4 +++-
 2 files changed, 6 insertions(+), 2 deletions(-)

diff --git a/drivers/gpu/drm/amd/pm/legacy-dpm/si_dpm.c b/drivers/gpu/drm/amd/pm/legacy-dpm/si_dpm.c
index a1baa13ab2c2..d95e80d87e2e 100644
--- a/drivers/gpu/drm/amd/pm/legacy-dpm/si_dpm.c
+++ b/drivers/gpu/drm/amd/pm/legacy-dpm/si_dpm.c
@@ -3435,9 +3435,11 @@ static void si_apply_state_adjust_rules(struct amdgpu_device *adev,
 	if (adev->asic_type == CHIP_HAINAN) {
 		if ((adev->pdev->revision == 0x81) ||
 		    (adev->pdev->revision == 0xC3) ||
+		    (adev->pdev->device == 0x6660) ||
 		    (adev->pdev->device == 0x6664) ||
 		    (adev->pdev->device == 0x6665) ||
-		    (adev->pdev->device == 0x6667)) {
+		    (adev->pdev->device == 0x6667) ||
+		    (adev->pdev->device == 0x666F)) {
 			max_sclk = 75000;
 		}
 		if ((adev->pdev->revision == 0xC3) ||
diff --git a/drivers/gpu/drm/radeon/si_dpm.c b/drivers/gpu/drm/radeon/si_dpm.c
index 9deb91970d4d..5db16c20bd17 100644
--- a/drivers/gpu/drm/radeon/si_dpm.c
+++ b/drivers/gpu/drm/radeon/si_dpm.c
@@ -2915,9 +2915,11 @@ static void si_apply_state_adjust_rules(struct radeon_device *rdev,
 	if (rdev->family == CHIP_HAINAN) {
 		if ((rdev->pdev->revision == 0x81) ||
 		    (rdev->pdev->revision == 0xC3) ||
+		    (rdev->pdev->device == 0x6660) ||
 		    (rdev->pdev->device == 0x6664) ||
 		    (rdev->pdev->device == 0x6665) ||
-		    (rdev->pdev->device == 0x6667)) {
+		    (rdev->pdev->device == 0x6667) ||
+		    (rdev->pdev->device == 0x666F)) {
 			max_sclk = 75000;
 		}
 		if ((rdev->pdev->revision == 0xC3) ||
-- 
2.47.0


From 6a247b44656f3950901f610636796de908aeefa8 Mon Sep 17 00:00:00 2001
From: Simon May <simon.may@protonmail.ch>
Date: Sun, 19 Sep 2021 23:45:59 +0200
Subject: [PATCH v1.4 112/120] Revert "PCI: Add a REBAR size quirk for Sapphire
 RX 5600 XT Pulse"

This reverts commit 907830b0fc9e374d00f3c83de5e426157b482c01.
---
 drivers/pci/pci.c | 8 +-------
 1 file changed, 1 insertion(+), 7 deletions(-)

diff --git a/drivers/pci/pci.c b/drivers/pci/pci.c
index 85ced6958d6d..a74c31e032c8 100644
--- a/drivers/pci/pci.c
+++ b/drivers/pci/pci.c
@@ -3743,14 +3743,8 @@ u32 pci_rebar_get_possible_sizes(struct pci_dev *pdev, int bar)
 		return 0;
 
 	pci_read_config_dword(pdev, pos + PCI_REBAR_CAP, &cap);
-	cap = FIELD_GET(PCI_REBAR_CAP_SIZES, cap);
 
-	/* Sapphire RX 5600 XT Pulse has an invalid cap dword for BAR 0 */
-	if (pdev->vendor == PCI_VENDOR_ID_ATI && pdev->device == 0x731f &&
-	    bar == 0 && cap == 0x700)
-		return 0x3f00;
-
-	return cap;
+	return (cap & PCI_REBAR_CAP_SIZES) >> 4;
 }
 EXPORT_SYMBOL(pci_rebar_get_possible_sizes);
 
-- 
2.47.0


From 77a92a12bfcc31f78278fca35c3e19842b457761 Mon Sep 17 00:00:00 2001
From: K Prateek Nayak <kprateek.nayak@amd.com>
Date: Wed, 21 Sep 2022 12:06:38 +0530
Subject: [PATCH v1.4 113/120] ACPI: processor_idle: Skip dummy wait for
 processors based on the Zen microarchitecture
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Processors based on the Zen microarchitecture support IOPORT based deeper
C-states. The idle driver reads the acpi_gbl_FADT.xpm_timer_block.address
in the IOPORT based C-state exit path which is claimed to be a
"Dummy wait op" and has been around since ACPI introduction to Linux
dating back to Andy Grover's Mar 14, 2002 posting [1].
The comment above the dummy operation was elaborated by Andreas Mohr back
in 2006 in commit b488f02156d3d ("ACPI: restore comment justifying 'extra'
P_LVLx access") [2] where the commit log claims:
"this dummy read was about: STPCLK# doesn't get asserted in time on
(some) chipsets, which is why we need to have a dummy I/O read to delay
further instruction processing until the CPU is fully stopped."

However, sampling certain workloads with IBS on AMD Zen3 system shows
that a significant amount of time is spent in the dummy op, which
incorrectly gets accounted as C-State residency. A large C-State
residency value can prime the cpuidle governor to recommend a deeper
C-State during the subsequent idle instances, starting a vicious cycle,
leading to performance degradation on workloads that rapidly switch
between busy and idle phases.

One such workload is tbench where a massive performance degradation can
be observed during certain runs. Following are some statistics gathered
by running tbench with 128 clients, on a dual socket (2 x 64C/128T) Zen3
system with the baseline kernel, baseline kernel keeping C2 disabled,
and baseline kernel with this patch applied keeping C2 enabled:

baseline kernel was tip:sched/core at
commit f3dd3f674555 ("sched: Remove the limitation of WF_ON_CPU on
wakelist if wakee cpu is idle")

Kernel        : baseline      baseline + C2 disabled   baseline + patch

Min (MB/s)    : 2215.06       33072.10 (+1393.05%)     33016.10 (+1390.52%)
Max (MB/s)    : 32938.80      34399.10                 34774.50
Median (MB/s) : 32191.80      33476.60                 33805.70
AMean (MB/s)  : 22448.55      33649.27 (+49.89%)       33865.43 (+50.85%)
AMean Stddev  : 17526.70      680.14                   880.72
AMean CoefVar : 78.07%        2.02%                    2.60%

The data shows there are edge cases that can cause massive regressions
in case of tbench. Profiling the bad runs with IBS shows a significant
amount of time being spent in acpi_idle_do_entry method:

Overhead  Command          Shared Object             Symbol
  74.76%  swapper          [kernel.kallsyms]         [k] acpi_idle_do_entry
   0.71%  tbench           [kernel.kallsyms]         [k] update_sd_lb_stats.constprop.0
   0.69%  tbench_srv       [kernel.kallsyms]         [k] update_sd_lb_stats.constprop.0
   0.49%  swapper          [kernel.kallsyms]         [k] psi_group_change
   ...

Annotation of acpi_idle_do_entry method reveals almost all the time in
acpi_idle_do_entry is spent on the port I/O in wait_for_freeze():

  0.14 │      in     (%dx),%al       # <------ First "in" corresponding to inb(cx->address)
  0.51 │      mov    0x144d64d(%rip),%rax
  0.00 │      test   $0x80000000,%eax
       │    ↓ jne    62 	     # <------ Skip if running in guest
  0.00 │      mov    0x19800c3(%rip),%rdx
 99.33 │      in     (%dx),%eax      # <------ Second "in" corresponding to inl(acpi_gbl_FADT.xpm_timer_block.address)
  0.00 │62:   mov    -0x8(%rbp),%r12
  0.00 │      leave
  0.00 │    ← ret

This overhead is reflected in the C2 residency on the test system where
C2 is an IOPORT based C-State. The total C-state residency reported by
"cpupower idle-info" on CPU0 for good and bad case over the 80s tbench
run is as follows (all numbers are in microseconds):

			    Good Run 		Bad Run
			   (Baseline)

POLL: 			       43338		   6231  (-85.62%)
C1 (MWAIT Based): 	    23576156 		 363861  (-98.45%)
C2 (IOPORT Based): 	    10781218 	       77027280  (+614.45%)

The larger residency value in bad case leads to the system recommending
C2 state again for subsequent idle instances. The pattern lasts till the
end of the tbench run. Following is the breakdown of "entry_method"
passed to acpi_idle_do_entry during good run and bad run:

                                        			Good Run    Bad Run
							       (Baseline)

Number of times acpi_idle_do_entry was called:             	6149573     6149050  (-0.01%)
 |-> Number of times entry_method was "ACPI_CSTATE_FFH":        6141494       88144  (-98.56%)
 |-> Number of times entry_method was "ACPI_CSTATE_HALT":             0           0  (+0.00%)
 |-> Number of times entry_method was "ACPI_CSTATE_SYSTEMIO":      8079     6060906  (+74920.49%)

For processors based on the Zen microarchitecture, this dummy wait op is
unnecessary and can be skipped when choosing IOPORT based C-States to
avoid polluting the C-state residency information.

Link: https://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux-fullhistory.git/commit/?id=972c16130d9dc182cedcdd408408d9eacc7d6a2d [1]
Link: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=b488f02156d3deb08f5ad7816d565c370a8cc6f1 [2]

Suggested-by: Calvin Ong <calvin.ong@amd.com>
Cc: stable@vger.kernel.org
Cc: regressions@lists.linux.dev
Signed-off-by: K Prateek Nayak <kprateek.nayak@amd.com>
Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
Link: https://lore.kernel.org/all/20220921063638.2489-1-kprateek.nayak@amd.com/
---
 drivers/acpi/processor_idle.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/drivers/acpi/processor_idle.c b/drivers/acpi/processor_idle.c
index 831fa4a12159..aec5c5ab9e64 100644
--- a/drivers/acpi/processor_idle.c
+++ b/drivers/acpi/processor_idle.c
@@ -524,8 +524,11 @@ static __cpuidle void io_idle(unsigned long addr)
 	inb(addr);
 
 #ifdef	CONFIG_X86
-	/* No delay is needed if we are in guest */
-	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+	/*
+	 * No delay is needed if we are in guest or on a processor
+	 * based on the Zen microarchitecture.
+	 */
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR) || boot_cpu_has(X86_FEATURE_ZEN))
 		return;
 	/*
 	 * Modern (>=Nehalem) Intel systems use ACPI via intel_idle,
-- 
2.47.0


From cc3fabfeb86a56ff3c4bafe23749d4ff55843003 Mon Sep 17 00:00:00 2001
From: GloriousEggroll <gloriouseggroll@gmail.com>
Date: Mon, 30 Oct 2023 22:36:19 -0600
Subject: [PATCH v1.4 114/120] Fix the steam deck not coming back from
 hibernation with revert

Revert "nvme-pci: drop redundant pci_enable_pcie_error_reporting()"

This reverts commits:
1ad11eafc63ac16e667853bee4273879226d2d1b
7ec4b34be4234599cf1241ef807cdb7c3636f6fe
69b264df8a412820e98867dbab871c6526c5e5aa
---
 drivers/nvme/host/pci.c |  6 +++++-
 drivers/pci/pcie/aer.c  | 15 ++++++++++++++-
 include/linux/aer.h     | 11 +++++++++++
 3 files changed, 30 insertions(+), 2 deletions(-)

diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 7990c3f22ecf..3c1b70a93ff1 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -5,6 +5,7 @@
  */
 
 #include <linux/acpi.h>
+#include <linux/aer.h>
 #include <linux/async.h>
 #include <linux/blkdev.h>
 #include <linux/blk-mq.h>
@@ -2587,6 +2588,7 @@ static int nvme_pci_enable(struct nvme_dev *dev)
 
 	nvme_map_cmb(dev);
 
+	pci_enable_pcie_error_reporting(pdev);
 	pci_save_state(pdev);
 
 	result = nvme_pci_configure_admin_queue(dev);
@@ -2651,8 +2653,10 @@ static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 	nvme_suspend_io_queues(dev);
 	nvme_suspend_queue(dev, 0);
 	pci_free_irq_vectors(pdev);
-	if (pci_is_enabled(pdev))
+	if (pci_is_enabled(pdev)) {
+		pci_disable_pcie_error_reporting(pdev);
 		pci_disable_device(pdev);
+	}
 	nvme_reap_pending_cqes(dev);
 
 	nvme_cancel_tagset(&dev->ctrl);
diff --git a/drivers/pci/pcie/aer.c b/drivers/pci/pcie/aer.c
index 13b8586924ea..12ba10cbc174 100644
--- a/drivers/pci/pcie/aer.c
+++ b/drivers/pci/pcie/aer.c
@@ -232,7 +232,7 @@ int pcie_aer_is_native(struct pci_dev *dev)
 }
 EXPORT_SYMBOL_NS_GPL(pcie_aer_is_native, CXL);
 
-static int pci_enable_pcie_error_reporting(struct pci_dev *dev)
+int pci_enable_pcie_error_reporting(struct pci_dev *dev)
 {
 	int rc;
 
@@ -242,6 +242,19 @@ static int pci_enable_pcie_error_reporting(struct pci_dev *dev)
 	rc = pcie_capability_set_word(dev, PCI_EXP_DEVCTL, PCI_EXP_AER_FLAGS);
 	return pcibios_err_to_errno(rc);
 }
+EXPORT_SYMBOL_GPL(pci_enable_pcie_error_reporting);
+
+int pci_disable_pcie_error_reporting(struct pci_dev *dev)
+{
+	int rc;
+
+	if (!pcie_aer_is_native(dev))
+		return -EIO;
+
+	rc = pcie_capability_clear_word(dev, PCI_EXP_DEVCTL, PCI_EXP_AER_FLAGS);
+	return pcibios_err_to_errno(rc);
+}
+EXPORT_SYMBOL_GPL(pci_disable_pcie_error_reporting);
 
 int pci_aer_clear_nonfatal_status(struct pci_dev *dev)
 {
diff --git a/include/linux/aer.h b/include/linux/aer.h
index 4b97f38f3fcf..3b338b556ecb 100644
--- a/include/linux/aer.h
+++ b/include/linux/aer.h
@@ -40,9 +40,20 @@ struct aer_capability_regs {
 int pcie_read_tlp_log(struct pci_dev *dev, int where, struct pcie_tlp_log *log);
 
 #if defined(CONFIG_PCIEAER)
+/* PCIe port driver needs this function to enable AER */
+int pci_enable_pcie_error_reporting(struct pci_dev *dev);
+int pci_disable_pcie_error_reporting(struct pci_dev *dev);
 int pci_aer_clear_nonfatal_status(struct pci_dev *dev);
 int pcie_aer_is_native(struct pci_dev *dev);
 #else
+static inline int pci_enable_pcie_error_reporting(struct pci_dev *dev)
+{
+	return -EINVAL;
+}
+static inline int pci_disable_pcie_error_reporting(struct pci_dev *dev)
+{
+	return -EINVAL;
+}
 static inline int pci_aer_clear_nonfatal_status(struct pci_dev *dev)
 {
 	return -EINVAL;
-- 
2.47.0


From 23e3cd08919e447f1e4f09a1a4610d0940da26c6 Mon Sep 17 00:00:00 2001
From: Steven Barrett <steven@liquorix.net>
Date: Fri, 15 Mar 2024 12:36:51 -0500
Subject: [PATCH v1.4 115/120] Cachy: drm/amdgpu/pm: Allow override of
 min_power_limit with ignore_min_pcap

---
 drivers/gpu/drm/amd/amdgpu/amdgpu.h       |  1 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c   | 10 ++++++++++
 drivers/gpu/drm/amd/pm/amdgpu_pm.c        |  3 +++
 drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c | 14 ++++++++++++--
 4 files changed, 26 insertions(+), 2 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu.h b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
index 137a88b8de45..233c17537492 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu.h
@@ -162,6 +162,7 @@ struct amdgpu_watchdog_timer {
  */
 extern int amdgpu_modeset;
 extern unsigned int amdgpu_vram_limit;
+extern int amdgpu_ignore_min_pcap;
 extern int amdgpu_vis_vram_limit;
 extern int amdgpu_gart_size;
 extern int amdgpu_gtt_size;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
index e2382566af44..7514399024d7 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
@@ -135,6 +135,7 @@ enum AMDGPU_DEBUG_MASK {
 };
 
 unsigned int amdgpu_vram_limit = UINT_MAX;
+int amdgpu_ignore_min_pcap = 1; /* do not ignore by default */
 int amdgpu_vis_vram_limit;
 int amdgpu_gart_size = -1; /* auto */
 int amdgpu_gtt_size = -1; /* auto */
@@ -248,6 +249,15 @@ struct amdgpu_watchdog_timer amdgpu_watchdog_timer = {
 	.period = 0x0, /* default to 0x0 (timeout disable) */
 };
 
+/**
+ * DOC: ignore_min_pcap (int)
+ * Ignore the minimum power cap.
+ * Useful on graphics cards where the minimum power cap is very high.
+ * The default is 0 (Do not ignore).
+ */
+MODULE_PARM_DESC(ignore_min_pcap, "Ignore the minimum power cap");
+module_param_named(ignore_min_pcap, amdgpu_ignore_min_pcap, int, 0600);
+
 /**
  * DOC: vramlimit (int)
  * Restrict the total amount of VRAM in MiB for testing.  The default is 0 (Use full VRAM).
diff --git a/drivers/gpu/drm/amd/pm/amdgpu_pm.c b/drivers/gpu/drm/amd/pm/amdgpu_pm.c
index d5d6ab484e5a..dccba7bcdf97 100644
--- a/drivers/gpu/drm/amd/pm/amdgpu_pm.c
+++ b/drivers/gpu/drm/amd/pm/amdgpu_pm.c
@@ -3272,6 +3272,9 @@ static ssize_t amdgpu_hwmon_show_power_cap_min(struct device *dev,
 					 struct device_attribute *attr,
 					 char *buf)
 {
+	if (amdgpu_ignore_min_pcap)
+		return sysfs_emit(buf, "%i\n", 0);
+
 	return amdgpu_hwmon_show_power_cap_generic(dev, attr, buf, PP_PWR_LIMIT_MIN);
 }
 
diff --git a/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c b/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c
index d3ab0f9b5a97..2ad827bcd8f1 100644
--- a/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c
+++ b/drivers/gpu/drm/amd/pm/swsmu/amdgpu_smu.c
@@ -2761,7 +2761,10 @@ int smu_get_power_limit(void *handle,
 			*limit = smu->max_power_limit;
 			break;
 		case SMU_PPT_LIMIT_MIN:
-			*limit = smu->min_power_limit;
+			if (amdgpu_ignore_min_pcap)
+				*limit = 0;
+			else
+				*limit = smu->min_power_limit;
 			break;
 		default:
 			return -EINVAL;
@@ -2785,7 +2788,14 @@ static int smu_set_power_limit(void *handle, uint32_t limit)
 		if (smu->ppt_funcs->set_power_limit)
 			return smu->ppt_funcs->set_power_limit(smu, limit_type, limit);
 
-	if ((limit > smu->max_power_limit) || (limit < smu->min_power_limit)) {
+	if (amdgpu_ignore_min_pcap) {
+		if ((limit > smu->max_power_limit)) {
+			dev_err(smu->adev->dev,
+				"New power limit (%d) is over the max allowed %d\n",
+				limit, smu->max_power_limit);
+			return -EINVAL;
+		}
+	} else if ((limit > smu->max_power_limit) || (limit < smu->min_power_limit)) {
 		dev_err(smu->adev->dev,
 			"New power limit (%d) is out of range [%d,%d]\n",
 			limit, smu->min_power_limit, smu->max_power_limit);
-- 
2.47.0


From b03a578e1848fa24ff7486f12ff690d800d192c8 Mon Sep 17 00:00:00 2001
From: GloriousEggroll <gloriouseggroll@gmail.com>
Date: Tue, 17 Jan 2023 12:08:46 -0700
Subject: [PATCH v1.4 116/120] Allow to set custom USB pollrate for specific
 devices like so: usbcore.interrupt_interval_override=045e:00db:16,1bcf:0005:1

---
 .../admin-guide/kernel-parameters.txt         |   8 +
 drivers/usb/core/config.c                     | 170 +++++++++++++++++-
 drivers/usb/core/usb.c                        |   1 +
 drivers/usb/core/usb.h                        |   1 +
 4 files changed, 179 insertions(+), 1 deletion(-)

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index cc6e7ae5786e..3fc1ebd881a8 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -7091,6 +7091,14 @@
 					request from 5000 ms to 500 ms);
 			Example: quirks=0781:5580:bk,0a5c:5834:gij
 
+	usbcore.interrupt_interval_override=
+			[USB] A list of USB devices for which a different polling
+			interval than the default shall be used on all interrupt-type
+			endpoints. The format is VendorID:ProductID:interval, with
+			the vendor and product ids specified hexadecimally, and the
+			interval decimally in milliseconds.
+			Example: interrupt_interval_override=045e:00db:16,1bcf:0005:2
+
 	usbhid.mousepoll=
 			[USBHID] The interval which mice are to be polled at.
 
diff --git a/drivers/usb/core/config.c b/drivers/usb/core/config.c
index 880d52c0949d..9f2b7e9aad96 100644
--- a/drivers/usb/core/config.c
+++ b/drivers/usb/core/config.c
@@ -19,6 +19,149 @@
 #define USB_MAXCONFIG			8	/* Arbitrary limit */
 
 
+/* A struct associated with the interrupt_interval_override module parameter, representing
+   an user's choice to force a specific interrupt interval upon all interrupt endpoints of
+   a certain device. */
+struct interrupt_interval_override {
+	/* The vendor ID of the device of which the interrupt interval shall be overridden */
+	u16 vendor;
+	/* The product ID of the device of which the interrupt interval shall be overridden */
+	u16 product;
+	/* The new interval measured in milliseconds that shall be given to all endpoints of type interrupt on said device */
+	unsigned int interval;
+};
+
+static DEFINE_MUTEX(interrupt_interval_override_mutex);
+static char interrupt_interval_override_param[128];
+static struct interrupt_interval_override *interrupt_interval_override_list = NULL;
+static size_t interrupt_interval_override_count = 0;
+
+static int interrupt_interval_override_param_set(const char *value, const struct kernel_param *kp)
+{
+	const char *p;
+	unsigned short vendor, product;
+	unsigned int interval;
+	struct interrupt_interval_override* list;
+	struct interrupt_interval_override param;
+	size_t count, max_count, i, len;
+	int err, res;
+
+	mutex_lock(&interrupt_interval_override_mutex);
+
+	if (!value || !*value) {
+		/* Unset the current variable. */
+		kfree(interrupt_interval_override_list);
+		interrupt_interval_override_list = NULL;
+		interrupt_interval_override_count = 0;
+		param_set_copystring(value, kp);  /* Does not fail: the empty string is short enough to fit. */
+		mutex_unlock(&interrupt_interval_override_mutex);
+		return 0;
+	}
+
+	/* Compute an upper bound on the amount of entries we need. */
+	for (max_count = 1, i = 0; value[i]; i++) {
+		if (value[i] == ',')
+			max_count++;
+	}
+
+	/* Ensure we can allocate enough memory before overwriting the global variables. */
+	list = kcalloc(max_count,
+		sizeof(struct interrupt_interval_override),
+		GFP_KERNEL);
+
+	if (!list) {
+		mutex_unlock(&interrupt_interval_override_mutex);
+		return -ENOMEM;
+	}
+
+	err = param_set_copystring(value, kp);
+	if (err) {
+		kfree(list);
+		mutex_unlock(&interrupt_interval_override_mutex);
+		return err;
+	}
+
+	/* Parse the parameter. Example of a valid parameter: 045e:00db:16,1bcf:0005:2 */
+	for (count = 0, p = (const char*)value; p && *p;) {
+		res = sscanf(p, "%hx:%hx:%d%zn", &vendor, &product, &interval, &len);
+
+		/* Check whether all variables (vendor, product, interval, len) were assigned.
+		   %zn does not increase the assignment count, so we need to check for value 3 instead of 4.
+		   %zn does not consume input either, so setting len shouldn't fail if interval has been properly set. */
+		if (res != 3) {
+			pr_warn("Error while parsing USB interrupt interval override parameter %s.\n", value);
+			break;
+		}
+
+		param.vendor = (u16)vendor;
+		param.product = (u16)product;
+		param.interval = interval;
+		list[count++] = param;
+
+		p += len;
+		if (*p == ',' && *(p+1) != '\0') {
+			p++;
+			continue;
+		} else if(*p == '\0' || (*p == '\n' && *(p+1) == '\0')) {
+			break;
+		} else {
+			pr_warn("Error while parsing USB interrupt interval override parameter %s.\n", value);
+			break;
+		}
+	}
+
+	/* Overwrite the global variables with the local ones. */
+	kfree(interrupt_interval_override_list);
+	interrupt_interval_override_list = list;
+	interrupt_interval_override_count = count;
+	mutex_unlock(&interrupt_interval_override_mutex);
+	return 0;
+}
+
+static const struct kernel_param_ops interrupt_interval_override_param_ops = {
+	.set = interrupt_interval_override_param_set,
+	.get = param_get_string,
+};
+
+static struct kparam_string interrupt_interval_override_param_string = {
+	.maxlen = sizeof(interrupt_interval_override_param),
+	.string = interrupt_interval_override_param,
+};
+
+device_param_cb(interrupt_interval_override,
+	&interrupt_interval_override_param_ops,
+	&interrupt_interval_override_param_string,
+	0644);
+MODULE_PARM_DESC(interrupt_interval_override,
+	"Override the polling interval of all interrupt-type endpoints of a specific USB"
+	" device by specifying interrupt_interval_override=vendorID:productID:interval.");
+
+/* Given an USB device, this checks whether the user has specified they want to override the interrupt
+   polling interval on all interrupt-type endpoints of said device.
+
+   This function returns the user-desired amount of milliseconds between interrupts on said endpoint.
+   If this function returns zero, the device-requested interrupt interval should be used. */
+static unsigned int usb_check_interrupt_interval_override(struct usb_device* udev)
+{
+	size_t i;
+	unsigned int res;
+	u16 vendor = le16_to_cpu(udev->descriptor.idVendor);
+	u16 product = le16_to_cpu(udev->descriptor.idProduct);
+
+	mutex_lock(&interrupt_interval_override_mutex);
+	for (i = 0; i < interrupt_interval_override_count; i++) {
+		if (interrupt_interval_override_list[i].vendor == vendor
+				&& interrupt_interval_override_list[i].product == product) {
+
+			res = interrupt_interval_override_list[i].interval;
+			mutex_unlock(&interrupt_interval_override_mutex);
+			return res;
+		}
+	}
+	mutex_unlock(&interrupt_interval_override_mutex);
+	return 0;
+}
+
 static inline const char *plural(int n)
 {
 	return (n == 1 ? "" : "s");
@@ -261,7 +404,7 @@ static int usb_parse_endpoint(struct device *ddev, int cfgno,
 	struct usb_endpoint_descriptor *d;
 	struct usb_host_endpoint *endpoint;
 	int n, i, j, retval;
-	unsigned int maxp;
+	unsigned int maxp, ival;
 	const unsigned short *maxpacket_maxes;
 
 	d = (struct usb_endpoint_descriptor *) buffer;
@@ -398,6 +541,23 @@ static int usb_parse_endpoint(struct device *ddev, int cfgno,
 		endpoint->desc.bInterval = n;
 	}
 
+	/* Override the interrupt polling interval if a module parameter tells us to do so. */
+	if (usb_endpoint_xfer_int(d)) {
+		ival = usb_check_interrupt_interval_override(udev);
+		if (ival > 0) {
+			switch (udev->speed) {
+			case USB_SPEED_SUPER_PLUS:
+			case USB_SPEED_SUPER:
+			case USB_SPEED_HIGH:
+				endpoint->desc.bInterval = fls(ival) + 3;
+				break;
+			default:  /* USB_SPEED_FULL or _LOW */
+				endpoint->desc.bInterval = ival;
+				break;
+			}
+		}
+	}
+
 	/* Some buggy low-speed devices have Bulk endpoints, which is
 	 * explicitly forbidden by the USB spec.  In an attempt to make
 	 * them usable, we will try treating them as Interrupt endpoints.
@@ -1102,3 +1262,11 @@ int usb_get_bos_descriptor(struct usb_device *dev)
 	usb_release_bos_descriptor(dev);
 	return ret;
 }
+
+void usb_release_interrupt_interval_override_list(void)
+{
+	mutex_lock(&interrupt_interval_override_mutex);
+	kfree(interrupt_interval_override_list);
+	interrupt_interval_override_list = NULL;
+	mutex_unlock(&interrupt_interval_override_mutex);
+}
diff --git a/drivers/usb/core/usb.c b/drivers/usb/core/usb.c
index 0b4685aad2d5..730331689a42 100644
--- a/drivers/usb/core/usb.c
+++ b/drivers/usb/core/usb.c
@@ -1135,6 +1135,7 @@ static void __exit usb_exit(void)
 		return;
 
 	usb_release_quirk_list();
+	usb_release_interrupt_interval_override_list();
 	usb_deregister_device_driver(&usb_generic_driver);
 	usb_major_cleanup();
 	usb_deregister(&usbfs_driver);
diff --git a/drivers/usb/core/usb.h b/drivers/usb/core/usb.h
index b8324ea05b20..f6f57d7d73d4 100644
--- a/drivers/usb/core/usb.h
+++ b/drivers/usb/core/usb.h
@@ -38,6 +38,7 @@ extern void usb_authorize_interface(struct usb_interface *);
 extern void usb_detect_quirks(struct usb_device *udev);
 extern void usb_detect_interface_quirks(struct usb_device *udev);
 extern void usb_release_quirk_list(void);
+extern void usb_release_interrupt_interval_override_list(void);
 extern bool usb_endpoint_is_ignored(struct usb_device *udev,
 		struct usb_host_interface *intf,
 		struct usb_endpoint_descriptor *epd);
-- 
2.47.0


From e7d2aa1a0b491c533c6057706e78711b8ad270ac Mon Sep 17 00:00:00 2001
From: GloriousEggroll <gloriouseggroll@gmail.com>
Date: Mon, 29 May 2023 17:15:14 -0600
Subject: [PATCH v1.4 117/120] set ds controller bluetooth pollrate to 1 ms

---
 drivers/hid/hid-playstation.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/hid/hid-playstation.c b/drivers/hid/hid-playstation.c
index e7c309cfe3a0..f3ea20af54ab 100644
--- a/drivers/hid/hid-playstation.c
+++ b/drivers/hid/hid-playstation.c
@@ -337,8 +337,8 @@ struct dualsense_output_report {
  * 0x3F - disabled
  */
 #define DS4_OUTPUT_HWCTL_BT_POLL_MASK	0x3F
-/* Default to 4ms poll interval, which is same as USB (not adjustable). */
-#define DS4_BT_DEFAULT_POLL_INTERVAL_MS	4
+/* Default to 1ms poll interval (1000Hz, lower latency). */
+#define DS4_BT_DEFAULT_POLL_INTERVAL_MS	1
 #define DS4_OUTPUT_HWCTL_CRC32		0x40
 #define DS4_OUTPUT_HWCTL_HID		0x80
 
-- 
2.47.0


From 6e4a01c75fc560b24ebab6cc1855bcc941dfc17c Mon Sep 17 00:00:00 2001
From: Jan200101 <sentrycraft123@gmail.com>
Date: Mon, 27 Nov 2023 09:53:59 +0100
Subject: [PATCH v1.4 118/120] drm/amdgpu: enable SI and CIK support by default

Signed-off-by: Jan200101 <sentrycraft123@gmail.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c | 10 ----------
 drivers/gpu/drm/radeon/radeon_drv.c     | 10 ++++++++++
 2 files changed, 10 insertions(+), 10 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
index 7514399024d7..75c33701fa7f 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
@@ -610,13 +610,8 @@ module_param_named(timeout_period, amdgpu_watchdog_timer.period, uint, 0644);
  */
 #ifdef CONFIG_DRM_AMDGPU_SI
 
-#if IS_ENABLED(CONFIG_DRM_RADEON) || IS_ENABLED(CONFIG_DRM_RADEON_MODULE)
-int amdgpu_si_support;
-MODULE_PARM_DESC(si_support, "SI support (1 = enabled, 0 = disabled (default))");
-#else
 int amdgpu_si_support = 1;
 MODULE_PARM_DESC(si_support, "SI support (1 = enabled (default), 0 = disabled)");
-#endif
 
 module_param_named(si_support, amdgpu_si_support, int, 0444);
 #endif
@@ -629,13 +624,8 @@ module_param_named(si_support, amdgpu_si_support, int, 0444);
  */
 #ifdef CONFIG_DRM_AMDGPU_CIK
 
-#if IS_ENABLED(CONFIG_DRM_RADEON) || IS_ENABLED(CONFIG_DRM_RADEON_MODULE)
-int amdgpu_cik_support;
-MODULE_PARM_DESC(cik_support, "CIK support (1 = enabled, 0 = disabled (default))");
-#else
 int amdgpu_cik_support = 1;
 MODULE_PARM_DESC(cik_support, "CIK support (1 = enabled (default), 0 = disabled)");
-#endif
 
 module_param_named(cik_support, amdgpu_cik_support, int, 0444);
 #endif
diff --git a/drivers/gpu/drm/radeon/radeon_drv.c b/drivers/gpu/drm/radeon/radeon_drv.c
index 7bf08164140e..865f186f48c4 100644
--- a/drivers/gpu/drm/radeon/radeon_drv.c
+++ b/drivers/gpu/drm/radeon/radeon_drv.c
@@ -239,12 +239,22 @@ module_param_named(uvd, radeon_uvd, int, 0444);
 MODULE_PARM_DESC(vce, "vce enable/disable vce support (1 = enable, 0 = disable)");
 module_param_named(vce, radeon_vce, int, 0444);
 
+#ifdef CONFIG_DRM_AMDGPU_SI
+int radeon_si_support = 0;
+MODULE_PARM_DESC(si_support, "SI support (1 = enabled, 0 = disabled (default))");
+#else
 int radeon_si_support = 1;
 MODULE_PARM_DESC(si_support, "SI support (1 = enabled (default), 0 = disabled)");
+#endif
 module_param_named(si_support, radeon_si_support, int, 0444);
 
+#ifdef CONFIG_DRM_AMDGPU_CIK
+int radeon_cik_support = 0;
+MODULE_PARM_DESC(cik_support, "CIK support (1 = enabled, 0 = disabled (default))");
+#else
 int radeon_cik_support = 1;
 MODULE_PARM_DESC(cik_support, "CIK support (1 = enabled (default), 0 = disabled)");
+#endif
 module_param_named(cik_support, radeon_cik_support, int, 0444);
 
 static struct pci_device_id pciidlist[] = {
-- 
2.47.0


From 76b85aacf99e29864dc8b048ed129caab4f323b6 Mon Sep 17 00:00:00 2001
From: John Martens <john.martens4@proton.me>
Date: Fri, 29 Mar 2024 20:18:47 +0000
Subject: [PATCH v1.4 119/120] add Lenovo Legion Laptop kernel driver

Add extra support for Lenovo Legion laptops.
---
 drivers/platform/x86/Kconfig         |   10 +
 drivers/platform/x86/Makefile        |    1 +
 drivers/platform/x86/legion-laptop.c | 6088 ++++++++++++++++++++++++++
 3 files changed, 6099 insertions(+)
 create mode 100644 drivers/platform/x86/legion-laptop.c

diff --git a/drivers/platform/x86/Kconfig b/drivers/platform/x86/Kconfig
index 10af8a252f88..e1499b15fafc 100644
--- a/drivers/platform/x86/Kconfig
+++ b/drivers/platform/x86/Kconfig
@@ -659,6 +659,16 @@ config THINKPAD_LMI
 	  To compile this driver as a module, choose M here: the module will
 	  be called think-lmi.
 
+config LEGION_LAPTOP
+	tristate "Lenovo Legion Laptop Extras"
+	depends on ACPI
+	depends on ACPI_WMI || ACPI_WMI = n
+	depends on HWMON || HWMON = n
+	select ACPI_PLATFORM_PROFILE
+	help
+	  This is a driver for Lenovo Legion laptops and contains drivers for
+	  hotkey, fan control, and power mode.
+
 source "drivers/platform/x86/intel/Kconfig"
 
 config ACPI_QUICKSTART
diff --git a/drivers/platform/x86/Makefile b/drivers/platform/x86/Makefile
index 870d0157dbc7..ecab298e500c 100644
--- a/drivers/platform/x86/Makefile
+++ b/drivers/platform/x86/Makefile
@@ -69,6 +69,7 @@ obj-$(CONFIG_LENOVO_YMC)	+= lenovo-ymc.o
 obj-$(CONFIG_SENSORS_HDAPS)	+= hdaps.o
 obj-$(CONFIG_THINKPAD_ACPI)	+= thinkpad_acpi.o
 obj-$(CONFIG_THINKPAD_LMI)	+= think-lmi.o
+obj-$(CONFIG_LEGION_LAPTOP)	+= legion-laptop.o
 obj-$(CONFIG_YOGABOOK)		+= lenovo-yogabook.o
 obj-$(CONFIG_YT2_1380)		+= lenovo-yoga-tab2-pro-1380-fastcharger.o
 obj-$(CONFIG_LENOVO_WMI_CAMERA)	+= lenovo-wmi-camera.o
diff --git a/drivers/platform/x86/legion-laptop.c b/drivers/platform/x86/legion-laptop.c
new file mode 100644
index 000000000000..cbdc716e9dd0
--- /dev/null
+++ b/drivers/platform/x86/legion-laptop.c
@@ -0,0 +1,6088 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ *  legion-laptop.c - Extra Lenovo Legion laptop support, in
+ *   particular for fan curve control and power mode.
+ *
+ *  Copyright (C) 2022 johnfan <johnfan (at) example (dot) com>
+ *
+ *
+ *  This driver might work on other Lenovo Legion models. If you
+ *  want to try it you can pass force=1 as argument
+ *  to the module which will force it to load even when the DMI
+ *  data doesn't match the model AND FIRMWARE.
+ *
+ *  Support for other hardware of this model is already partially
+ *  provided by the module ideapad-laptop.
+ *
+ *  The development page for this driver is located at
+ *  https://github.com/johnfanv2/LenovoLegionLinux
+ *
+ *  This driver exports the files:
+ *    - /sys/kernel/debug/legion/fancurve (ro)
+ *        The fan curve stored in the firmware in the form of a
+ *        human readable table.
+ *
+ *    - /sys/module/legion_laptop/drivers/platform\:legion/PNP0C09\:00/powermode (rw)
+ *       0: balanced mode (white)
+ *       1: performance mode (red)
+ *       2: quiet mode (blue)
+ *       ?: custom mode (pink)
+ *
+ *  NOTE: Writing to this will load the default fan curve from
+ *        the firmware for this mode, so the fan curve might
+ *        have to be reconfigured if needed.
+ *
+ *  It implements the usual hwmon interface to monitor fan speed and temmperature
+ *  and allows to set the fan curve inside the firware.
+ *
+ *    - /sys/class/hwmon/X/fan1_input or /sys/class/hwmon/X/fan2_input  (ro)
+ *        Current fan speed of fan1/fan2.
+ *    - /sys/class/hwmon/X/temp1_input (ro)
+ *    - /sys/class/hwmon/X/temp2_input (ro)
+ *    - /sys/class/hwmon/X/temp3_input (ro)
+ *        Temperature (Celsius) of CPU, GPU, and IC used for fan control.
+ *    - /sys/class/hwmon/X/pwmY_auto_pointZ_pwm (rw)
+ *          PWM (0-255) of the fan at the Y-level in the fan curve
+ *    - /sys/class/hwmon/X/pwmY_auto_pointZ_temp (rw)
+ *          upper temperature of tempZ (CPU, GPU, or IC) at the Y-level in the fan curve
+ *    - /sys/class/hwmon/X/pwmY_auto_pointZ_temp_hyst (rw)
+ *          hysteris (CPU, GPU, or IC) at the Y-level in the fan curve. The lower
+ *          temperatue of the level is the upper temperature minus the hysteris
+ *
+ *
+ *  Credits for reverse engineering the firmware to:
+ *      - David Woodhouse: heavily inspired by lenovo_laptop.c
+ *      - Luke Cama: Windows version "LegionFanControl"
+ *      - SmokelessCPU: reverse engineering of custom registers in EC
+ *                      and commincation method with EC via ports
+ *      - 0x1F9F1: additional reverse engineering for complete fan curve
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/acpi.h>
+#include <asm/io.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/dmi.h>
+#include <linux/leds.h>
+#include <linux/hwmon.h>
+#include <linux/hwmon-sysfs.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/platform_device.h>
+#include <linux/platform_profile.h>
+#include <linux/types.h>
+#include <linux/wmi.h>
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("johnfan");
+MODULE_DESCRIPTION("Lenovo Legion laptop extras");
+
+static bool force;
+module_param(force, bool, 0440);
+MODULE_PARM_DESC(
+	force,
+	"Force loading this module even if model or BIOS does not match.");
+
+static bool ec_readonly;
+module_param(ec_readonly, bool, 0440);
+MODULE_PARM_DESC(
+	ec_readonly,
+	"Only read from embedded controller but do not write or change settings.");
+
+static bool enable_platformprofile = true;
+module_param(enable_platformprofile, bool, 0440);
+MODULE_PARM_DESC(
+	enable_platformprofile,
+	"Enable the platform profile sysfs API to read and write the power mode.");
+
+#define LEGIONFEATURES \
+	"fancurve powermode platformprofile platformprofilenotify minifancurve"
+
+//Size of fancurve stored in embedded controller
+#define MAXFANCURVESIZE 10
+
+#define LEGION_DRVR_SHORTNAME "legion"
+#define LEGION_HWMON_NAME LEGION_DRVR_SHORTNAME "_hwmon"
+
+struct legion_private;
+
+/* =============================== */
+/* Embedded Controller Description */
+/* =============================== */
+
+/* The configuration and registers to access the embedded controller
+ * depending on different the version of the software on the
+ * embedded controller or and the BIOS/UEFI firmware.
+ *
+ * To control fan curve in the embedded controller (EC) one has to
+ * write to its "RAM". There are different possibilities:
+ *  - EC RAM is memory mapped (write to it with ioremap)
+ *  - access EC RAM via ported mapped IO (outb/inb)
+ *  - access EC RAM via ACPI methods. It is only possible to write
+ *    to part of it (first 0xFF bytes?)
+ *
+ * In later models the firmware directly exposes ACPI methods to
+ * set the fan curve directly, without writing to EC RAM. This
+ * is done inside the ACPI method.
+ */
+
+/**
+ * Offsets for interesting values inside the EC RAM  (0 = start of
+ * EC RAM) These might change depending on the software inside of
+ * the EC, which can be updated by a BIOS update from Lenovo.
+ */
+// TODO: same order as in initialization
+struct ec_register_offsets {
+	// Super I/O Configuration Registers
+	// 7.15 General Control (GCTRL)
+	// General Control (GCTRL)
+	// (see EC Interface Registers  and 6.2 Plug and Play Configuration (PNPCFG)) in datasheet
+	// note: these are in two places saved
+	// in EC Interface Registers  and in super io configuration registers
+	// Chip ID
+	u16 ECHIPID1;
+	u16 ECHIPID2;
+	// Chip Version
+	u16 ECHIPVER;
+	u16 ECDEBUG;
+
+	// Lenovo Custom OEM extension
+	// Firmware of ITE can be extended by
+	// custom program using its own "variables"
+	// These are the offsets to these "variables"
+	u16 EXT_FAN_CUR_POINT;
+	u16 EXT_FAN_POINTS_SIZE;
+	u16 EXT_FAN1_BASE;
+	u16 EXT_FAN2_BASE;
+	u16 EXT_FAN_ACC_BASE;
+	u16 EXT_FAN_DEC_BASE;
+	u16 EXT_CPU_TEMP;
+	u16 EXT_CPU_TEMP_HYST;
+	u16 EXT_GPU_TEMP;
+	u16 EXT_GPU_TEMP_HYST;
+	u16 EXT_VRM_TEMP;
+	u16 EXT_VRM_TEMP_HYST;
+	u16 EXT_FAN1_RPM_LSB;
+	u16 EXT_FAN1_RPM_MSB;
+	u16 EXT_FAN2_RPM_LSB;
+	u16 EXT_FAN2_RPM_MSB;
+	u16 EXT_FAN1_TARGET_RPM;
+	u16 EXT_FAN2_TARGET_RPM;
+	u16 EXT_POWERMODE;
+	u16 EXT_MINIFANCURVE_ON_COOL;
+	// values
+	// 0x04: enable mini fan curve if left for too long on cool level
+	//      - this might be due to potential temp failure
+	//      - or just because of really cool temps
+	// 0xA0: disable it
+	u16 EXT_LOCKFANCONTROLLER;
+	u16 EXT_MAXIMUMFANSPEED;
+	u16 EXT_WHITE_KEYBOARD_BACKLIGHT;
+	u16 EXT_IC_TEMP_INPUT;
+	u16 EXT_CPU_TEMP_INPUT;
+	u16 EXT_GPU_TEMP_INPUT;
+};
+
+enum access_method {
+	ACCESS_METHOD_NO_ACCESS = 0,
+	ACCESS_METHOD_EC = 1,
+	ACCESS_METHOD_ACPI = 2,
+	ACCESS_METHOD_WMI = 3,
+	ACCESS_METHOD_WMI2 = 4,
+	ACCESS_METHOD_WMI3 = 5,
+	ACCESS_METHOD_EC2 = 10, // ideapad fancurve method
+	ACCESS_METHOD_EC3 = 11, // loq
+};
+
+struct model_config {
+	const struct ec_register_offsets *registers;
+	bool check_embedded_controller_id;
+	u16 embedded_controller_id;
+
+	// first addr in EC we access/scan
+	phys_addr_t memoryio_physical_ec_start;
+	size_t memoryio_size;
+
+	// TODO: maybe use bitfield
+	bool has_minifancurve;
+	bool has_custom_powermode;
+	enum access_method access_method_powermode;
+
+	enum access_method access_method_keyboard;
+	enum access_method access_method_temperature;
+	enum access_method access_method_fanspeed;
+	enum access_method access_method_fancurve;
+	enum access_method access_method_fanfullspeed;
+	bool three_state_keyboard;
+
+	bool acpi_check_dev;
+
+	phys_addr_t ramio_physical_start;
+	size_t ramio_size;
+};
+
+/* =================================== */
+/* Configuration for different models */
+/* =================================== */
+
+// Idea by SmokelesssCPU (modified)
+// - all default names and register addresses are supported by datasheet
+// - register addresses for custom firmware by SmokelesssCPU
+static const struct ec_register_offsets ec_register_offsets_v0 = {
+	.ECHIPID1 = 0x2000,
+	.ECHIPID2 = 0x2001,
+	.ECHIPVER = 0x2002,
+	.ECDEBUG = 0x2003,
+	.EXT_FAN_CUR_POINT = 0xC534,
+	.EXT_FAN_POINTS_SIZE = 0xC535,
+	.EXT_FAN1_BASE = 0xC540,
+	.EXT_FAN2_BASE = 0xC550,
+	.EXT_FAN_ACC_BASE = 0xC560,
+	.EXT_FAN_DEC_BASE = 0xC570,
+	.EXT_CPU_TEMP = 0xC580,
+	.EXT_CPU_TEMP_HYST = 0xC590,
+	.EXT_GPU_TEMP = 0xC5A0,
+	.EXT_GPU_TEMP_HYST = 0xC5B0,
+	.EXT_VRM_TEMP = 0xC5C0,
+	.EXT_VRM_TEMP_HYST = 0xC5D0,
+	.EXT_FAN1_RPM_LSB = 0xC5E0,
+	.EXT_FAN1_RPM_MSB = 0xC5E1,
+	.EXT_FAN2_RPM_LSB = 0xC5E2,
+	.EXT_FAN2_RPM_MSB = 0xC5E3,
+	.EXT_MINIFANCURVE_ON_COOL = 0xC536,
+	.EXT_LOCKFANCONTROLLER = 0xc4AB,
+	.EXT_CPU_TEMP_INPUT = 0xc538,
+	.EXT_GPU_TEMP_INPUT = 0xc539,
+	.EXT_IC_TEMP_INPUT = 0xC5E8,
+	.EXT_POWERMODE = 0xc420,
+	.EXT_FAN1_TARGET_RPM = 0xc600,
+	.EXT_FAN2_TARGET_RPM = 0xc601,
+	.EXT_MAXIMUMFANSPEED = 0xBD,
+	.EXT_WHITE_KEYBOARD_BACKLIGHT = (0x3B + 0xC400)
+};
+
+static const struct ec_register_offsets ec_register_offsets_v1 = {
+	.ECHIPID1 = 0x2000,
+	.ECHIPID2 = 0x2001,
+	.ECHIPVER = 0x2002,
+	.ECDEBUG = 0x2003,
+	.EXT_FAN_CUR_POINT = 0xC534,
+	.EXT_FAN_POINTS_SIZE = 0xC535,
+	.EXT_FAN1_BASE = 0xC540,
+	.EXT_FAN2_BASE = 0xC550,
+	.EXT_FAN_ACC_BASE = 0xC560,
+	.EXT_FAN_DEC_BASE = 0xC570,
+	.EXT_CPU_TEMP = 0xC580,
+	.EXT_CPU_TEMP_HYST = 0xC590,
+	.EXT_GPU_TEMP = 0xC5A0,
+	.EXT_GPU_TEMP_HYST = 0xC5B0,
+	.EXT_VRM_TEMP = 0xC5C0,
+	.EXT_VRM_TEMP_HYST = 0xC5D0,
+	.EXT_FAN1_RPM_LSB = 0xC5E0,
+	.EXT_FAN1_RPM_MSB = 0xC5E1,
+	.EXT_FAN2_RPM_LSB = 0xC5E2,
+	.EXT_FAN2_RPM_MSB = 0xC5E3,
+	.EXT_MINIFANCURVE_ON_COOL = 0xC536,
+	.EXT_LOCKFANCONTROLLER = 0xc4AB,
+	.EXT_CPU_TEMP_INPUT = 0xc538,
+	.EXT_GPU_TEMP_INPUT = 0xc539,
+	.EXT_IC_TEMP_INPUT = 0xC5E8,
+	.EXT_POWERMODE = 0xc41D,
+	.EXT_FAN1_TARGET_RPM = 0xc600,
+	.EXT_FAN2_TARGET_RPM = 0xc601,
+	.EXT_MAXIMUMFANSPEED = 0xBD,
+	.EXT_WHITE_KEYBOARD_BACKLIGHT = (0x3B + 0xC400)
+};
+
+static const struct ec_register_offsets ec_register_offsets_ideapad_v0 = {
+	.ECHIPID1 = 0x2000,
+	.ECHIPID2 = 0x2001,
+	.ECHIPVER = 0x2002,
+	.ECDEBUG = 0x2003,
+	.EXT_FAN_CUR_POINT = 0xC5a0, // not found yet
+	.EXT_FAN_POINTS_SIZE = 0xC5a0, // constant 0
+	.EXT_FAN1_BASE = 0xC5a0,
+	.EXT_FAN2_BASE = 0xC5a8,
+	.EXT_FAN_ACC_BASE = 0xC5a0, // not found yet
+	.EXT_FAN_DEC_BASE = 0xC5a0, // not found yet
+	.EXT_CPU_TEMP = 0xC550, // and repeated after 8 bytes
+	.EXT_CPU_TEMP_HYST = 0xC590, // and repeated after 8 bytes
+	.EXT_GPU_TEMP = 0xC5C0, // and repeated after 8 bytes
+	.EXT_GPU_TEMP_HYST = 0xC5D0, // and repeated after 8 bytes
+	.EXT_VRM_TEMP = 0xC5a0, // does not exists or not found
+	.EXT_VRM_TEMP_HYST = 0xC5a0, // does not exists ot not found yet
+	.EXT_FAN1_RPM_LSB = 0xC5a0, // not found yet
+	.EXT_FAN1_RPM_MSB = 0xC5a0, // not found yet
+	.EXT_FAN2_RPM_LSB = 0xC5a0, // not found yet
+	.EXT_FAN2_RPM_MSB = 0xC5a0, // not found yet
+	.EXT_MINIFANCURVE_ON_COOL = 0xC5a0, // does not exists or not found
+	.EXT_LOCKFANCONTROLLER = 0xC5a0, // does not exists or not found
+	.EXT_CPU_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_GPU_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_IC_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_POWERMODE = 0xC5a0, // not found yet
+	.EXT_FAN1_TARGET_RPM = 0xC5a0, // not found yet
+	.EXT_FAN2_TARGET_RPM = 0xC5a0, // not found yet
+	.EXT_MAXIMUMFANSPEED = 0xC5a0, // not found yet
+	.EXT_WHITE_KEYBOARD_BACKLIGHT = 0xC5a0 // not found yet
+};
+
+static const struct ec_register_offsets ec_register_offsets_ideapad_v1 = {
+	.ECHIPID1 = 0x2000,
+	.ECHIPID2 = 0x2001,
+	.ECHIPVER = 0x2002,
+	.ECDEBUG = 0x2003,
+	.EXT_FAN_CUR_POINT = 0xC5a0, // not found yet
+	.EXT_FAN_POINTS_SIZE = 0xC5a0, // constant 0
+	.EXT_FAN1_BASE = 0xC5a0,
+	.EXT_FAN2_BASE = 0xC5a8,
+	.EXT_FAN_ACC_BASE = 0xC5a0, // not found yet
+	.EXT_FAN_DEC_BASE = 0xC5a0, // not found yet
+	.EXT_CPU_TEMP = 0xC550, // and repeated after 8 bytes
+	.EXT_CPU_TEMP_HYST = 0xC590, // and repeated after 8 bytes
+	.EXT_GPU_TEMP = 0xC5C0, // and repeated after 8 bytes
+	.EXT_GPU_TEMP_HYST = 0xC5D0, // and repeated after 8 bytes
+	.EXT_VRM_TEMP = 0xC5a0, // does not exists or not found
+	.EXT_VRM_TEMP_HYST = 0xC5a0, // does not exists ot not found yet
+	.EXT_FAN1_RPM_LSB = 0xC5a0, // not found yet
+	.EXT_FAN1_RPM_MSB = 0xC5a0, // not found yet
+	.EXT_FAN2_RPM_LSB = 0xC5a0, // not found yet
+	.EXT_FAN2_RPM_MSB = 0xC5a0, // not found yet
+	.EXT_MINIFANCURVE_ON_COOL = 0xC5a0, // does not exists or not found
+	.EXT_LOCKFANCONTROLLER = 0xC5a0, // does not exists or not found
+	.EXT_CPU_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_GPU_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_IC_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_POWERMODE = 0xC5a0, // not found yet
+	.EXT_FAN1_TARGET_RPM = 0xC5a0, // not found yet
+	.EXT_FAN2_TARGET_RPM = 0xC5a0, // not found yet
+	.EXT_MAXIMUMFANSPEED = 0xC5a0, // not found yet
+	.EXT_WHITE_KEYBOARD_BACKLIGHT = 0xC5a0 // not found yet
+};
+
+static const struct ec_register_offsets ec_register_offsets_loq_v0 = {
+	.ECHIPID1 = 0x2000,
+	.ECHIPID2 = 0x2001,
+	.ECHIPVER = 0x2002,
+	.ECDEBUG = 0x2003,
+	.EXT_FAN_CUR_POINT = 0xC5a0,
+	.EXT_FAN_POINTS_SIZE = 0xC5a0, // constant 0
+	.EXT_FAN1_BASE = 0xC530,
+	.EXT_FAN2_BASE = 0xC530, // same rpm as cpu
+	.EXT_FAN_ACC_BASE = 0xC5a0, // not found yet
+	.EXT_FAN_DEC_BASE = 0xC5a0, // not found yet
+	.EXT_CPU_TEMP = 0xC52F,
+	.EXT_CPU_TEMP_HYST = 0xC5a0, // not found yet
+	.EXT_GPU_TEMP = 0xC531,
+	.EXT_GPU_TEMP_HYST = 0xC5a0, // not found yet
+	.EXT_VRM_TEMP = 0xC5a0, // not found yet
+	.EXT_VRM_TEMP_HYST = 0xC5a0, // not found yet
+	.EXT_FAN1_RPM_LSB = 0xC5a0, // not found yet
+	.EXT_FAN1_RPM_MSB = 0xC5a0, // not found yet
+	.EXT_FAN2_RPM_LSB = 0xC5a0, // not found yet
+	.EXT_FAN2_RPM_MSB = 0xC5a0, // not found yet
+	.EXT_MINIFANCURVE_ON_COOL = 0xC5a0, // not found yet
+	.EXT_LOCKFANCONTROLLER = 0xC5a0, // not found yet
+	.EXT_CPU_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_GPU_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_IC_TEMP_INPUT = 0xC5a0, // not found yet
+	.EXT_POWERMODE = 0xc41D,
+	.EXT_FAN1_TARGET_RPM = 0xC5a0, // not found yet
+	.EXT_FAN2_TARGET_RPM = 0xC5a0, // not found yet
+	.EXT_MAXIMUMFANSPEED = 0xC5a0, // not found yet
+	.EXT_WHITE_KEYBOARD_BACKLIGHT = 0xC5a0 // not found yet
+};
+
+static const struct model_config model_v0 = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_j2cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_9vcn = {
+	.registers = &ec_register_offsets_ideapad_v1,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8226,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI,
+	.access_method_temperature = ACCESS_METHOD_WMI,
+	.access_method_fancurve = ACCESS_METHOD_EC2,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_v2022 = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_4gcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8226,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_bvcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = false,
+	.embedded_controller_id = 0x8226,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI,
+	.access_method_temperature = ACCESS_METHOD_WMI,
+	.access_method_fancurve = ACCESS_METHOD_NO_ACCESS,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFC7E0800,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_bhcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8226,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = false,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_ACPI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI,
+	.access_method_temperature = ACCESS_METHOD_ACPI,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFF00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_kwcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x5507,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_WMI3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_m0cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x5507,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_WMI3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_m1cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x5507,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_WMI3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_m2cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_WMI3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_m6cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_WMI3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_k1cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x5263,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_WMI3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_lpcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x5507,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_WMI3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_kfcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_hacn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = false,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_k9cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = false,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400, // or replace 0xC400 by 0x0400  ?
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_eucn = {
+	.registers = &ec_register_offsets_v1,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_fccn = {
+	.registers = &ec_register_offsets_ideapad_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI,
+	.access_method_temperature = ACCESS_METHOD_ACPI,
+	.access_method_fancurve = ACCESS_METHOD_EC2,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_h3cn = {
+	//0xFE0B0800
+	.registers = &ec_register_offsets_v1,
+	.check_embedded_controller_id = false,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = false,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	// not implemented (properly) in WMI, RGB conrolled by USB
+	.access_method_keyboard = ACCESS_METHOD_NO_ACCESS,
+	// accessing fan speed is not implemented in ACPI
+	// a variable in the operation region (or not found)
+	// and not per WMI (methods returns constant 0)
+	.access_method_fanspeed = ACCESS_METHOD_NO_ACCESS,
+	.access_method_temperature = ACCESS_METHOD_WMI,
+	.access_method_fancurve = ACCESS_METHOD_NO_ACCESS,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE0B0800,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_e9cn = {
+	//0xFE0B0800
+	.registers = &ec_register_offsets_v1,
+	.check_embedded_controller_id = false,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400, //0xFC7E0800
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = false,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	// not implemented (properly) in WMI, RGB conrolled by USB
+	.access_method_keyboard = ACCESS_METHOD_NO_ACCESS,
+	// accessing fan speed is not implemented in ACPI
+	// a variable in the operation region (or not found)
+	// and not per WMI (methods returns constant 0)
+	.access_method_fanspeed = ACCESS_METHOD_WMI,
+	.access_method_temperature = ACCESS_METHOD_WMI,
+	.access_method_fancurve = ACCESS_METHOD_NO_ACCESS,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFC7E0800,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_8jcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8226,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_WMI,
+	.access_method_temperature = ACCESS_METHOD_WMI,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE00D400,
+	.ramio_size = 0x600
+};
+
+static const struct model_config model_jncn = {
+	.registers = &ec_register_offsets_v1,
+	.check_embedded_controller_id = false,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = false,
+	.has_custom_powermode = false,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_NO_ACCESS,
+	.access_method_fanspeed = ACCESS_METHOD_WMI,
+	.access_method_temperature = ACCESS_METHOD_WMI,
+	.access_method_fancurve = ACCESS_METHOD_NO_ACCESS,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFC7E0800,
+	.ramio_size = 0x600
+};
+
+// Yoga Model!
+static const struct model_config model_j1cn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+// Yoga Model!
+static const struct model_config model_dmcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = true,
+	.ramio_physical_start = 0xFE700D00,
+	.ramio_size = 0x600
+};
+
+// Yoga Model!
+static const struct model_config model_khcn = {
+	.registers = &ec_register_offsets_v0,
+	.check_embedded_controller_id = false,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_EC,
+	.access_method_keyboard = ACCESS_METHOD_WMI,
+	.access_method_fanspeed = ACCESS_METHOD_EC,
+	.access_method_temperature = ACCESS_METHOD_EC,
+	.access_method_fancurve = ACCESS_METHOD_EC,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+// LOQ Model
+static const struct model_config model_lzcn = {
+	.registers = &ec_register_offsets_loq_v0,
+	.check_embedded_controller_id = true,
+	.embedded_controller_id = 0x8227,
+	.memoryio_physical_ec_start = 0xC400,
+	.memoryio_size = 0x300,
+	.has_minifancurve = true,
+	.has_custom_powermode = true,
+	.access_method_powermode = ACCESS_METHOD_WMI,
+	.access_method_keyboard = ACCESS_METHOD_WMI2,
+	.access_method_fanspeed = ACCESS_METHOD_WMI3,
+	.access_method_temperature = ACCESS_METHOD_WMI3,
+	.access_method_fancurve = ACCESS_METHOD_EC3,
+	.access_method_fanfullspeed = ACCESS_METHOD_WMI3,
+	.acpi_check_dev = false,
+	.ramio_physical_start = 0xFE0B0400,
+	.ramio_size = 0x600
+};
+
+static const struct dmi_system_id denylist[] = { {} };
+
+static const struct dmi_system_id optimistic_allowlist[] = {
+	{
+		// Release year: 2021
+		// Generation: 6
+		// Name: Legion 5, Legion 5 pro, Legion 7
+		// Family: Legion 5 15ACH6H, ...
+		.ident = "GKCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "GKCN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2020
+		.ident = "EUCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "EUCN"),
+		},
+		.driver_data = (void *)&model_eucn
+	},
+	{
+		// Release year: 2020
+		.ident = "EFCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "EFCN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2020
+		.ident = "FSCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "FSCN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2021
+		.ident = "HHCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "HHCN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2022
+		.ident = "H1CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "H1CN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2022
+		.ident = "J2CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "J2CN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2022
+		.ident = "JUCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "JUCN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2022
+		.ident = "KFCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "KFCN"),
+		},
+		.driver_data = (void *)&model_kfcn
+	},
+	{
+		// Release year: 2021
+		.ident = "HACN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "HACN"),
+		},
+		.driver_data = (void *)&model_hacn
+	},
+	{
+		// Release year: 2021
+		.ident = "G9CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "G9CN"),
+		},
+		.driver_data = (void *)&model_v0
+	},
+	{
+		// Release year: 2022
+		.ident = "K9CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "K9CN"),
+		},
+		.driver_data = (void *)&model_k9cn
+	},
+	{
+		// e.g. IdeaPad Gaming 3 15ARH05
+		.ident = "FCCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "FCCN"),
+		},
+		.driver_data = (void *)&model_fccn
+	},
+	{
+		// e.g. IdeaPad Gaming 3 15ARH05 (8K21)
+		.ident = "H4CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "H4CN"),
+		},
+		.driver_data = (void *)&model_fccn
+	},
+	{
+		// e.g. Ideapad Gaming 3 15ACH6
+		.ident = "H3CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "H3CN"),
+		},
+		.driver_data = (void *)&model_h3cn
+	},
+	{
+		// e.g. IdeaPad Gaming 3 15ARH7 (2022)
+		.ident = "JNCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "JNCN"),
+		},
+		.driver_data = (void *)&model_jncn
+	},
+	{
+		// 2020, seems very different in ACPI dissassembly
+		.ident = "E9CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "E9CN"),
+		},
+		.driver_data = (void *)&model_e9cn
+	},
+	{
+		// e.g. Legion Y7000 (older version)
+		.ident = "8JCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "8JCN"),
+		},
+		.driver_data = (void *)&model_8jcn
+	},
+	{
+		// e.g. Legion 7i Pro 2023
+		.ident = "KWCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "KWCN"),
+		},
+		.driver_data = (void *)&model_kwcn
+	},
+	{
+		// e.g. Legion Pro 5 2023 or R9000P
+		.ident = "LPCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "LPCN"),
+		},
+		.driver_data = (void *)&model_lpcn
+	},
+	{
+		// e.g. Lenovo Legion 5i/Y7000 2019 PG0
+		.ident = "BHCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "BHCN"),
+		},
+		.driver_data = (void *)&model_bhcn
+	},
+	{
+		// e.g. Lenovo 7 16IAX7
+		.ident = "K1CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "K1CN"),
+		},
+		.driver_data = (void *)&model_k1cn
+	},
+	{
+		// e.g. Legion Y720
+		.ident = "4GCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "4GCN"),
+		},
+		.driver_data = (void *)&model_4gcn
+	},
+	{
+		// e.g. Legion Slim 5 16APH8 2023
+		.ident = "M3CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "M3CN"),
+		},
+		.driver_data = (void *)&model_lpcn
+	},
+	{
+		// e.g. Legion Y7000p-1060
+		.ident = "9VCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "9VCN"),
+		},
+		.driver_data = (void *)&model_9vcn
+	},
+	{
+		// e.g. Legion Y9000X
+		.ident = "JYCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "JYCN"),
+		},
+		.driver_data = (void *)&model_v2022
+	},
+	{
+		// e.g. Legion Y740-15IRH, older model e.g. with GTX 1660
+		.ident = "BVCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "BVCN"),
+		},
+		.driver_data = (void *)&model_bvcn
+	},
+	{
+		// e.g. Legion 5 Pro 16IAH7H with a RTX 3070 Ti
+		.ident = "J2CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "J2CN"),
+		},
+		.driver_data = (void *)&model_j2cn
+	},
+	{
+		// e.g. Lenovo Yoga 7 16IAH7 with GPU Intel DG2 Arc A370M
+		.ident = "J1CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "J1CN"),
+		},
+		.driver_data = (void *)&model_j1cn
+	},
+	{
+		// e.g. Legion Slim 7 16IRH8 (2023) with RTX 4070
+		.ident = "M0CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "M0CN"),
+		},
+		.driver_data = (void *)&model_m0cn
+	},
+	{
+		// e.g. Legion Slim 7 16IRH8 (2023) AMD Ryzen 7 7840HS with RTX 4060
+		.ident = "M1CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "M1CN"),
+		},
+		.driver_data = (void *)&model_m1cn
+	},
+	{
+		// e.g. Legion Slim 5 16IRH8 (2023) with RTX 4070
+		.ident = "M2CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "M2CN"),
+		},
+		.driver_data = (void *)&model_m2cn
+	},
+	{
+		// e.g. Lenovo Yoga Slim 7 gen 8 (2023)
+		.ident = "M6CN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "M6CN"),
+		},
+		.driver_data = (void *)&model_m6cn
+	},
+	{
+		// e.g. Yoga Slim 7-14ARE05
+		.ident = "DMCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "DMCN"),
+		},
+		.driver_data = (void *)&model_dmcn
+	},
+	{
+		// e.g. Yoga Slim 7 Pro 14ARH7
+		.ident = "KHCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "KHCN"),
+		},
+		.driver_data = (void *)&model_khcn
+	},
+	{
+		// e.g. LOQ 15IRH8
+		.ident = "LZCN",
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "LENOVO"),
+			DMI_MATCH(DMI_BIOS_VERSION, "LZCN"),
+		},
+		.driver_data = (void *)&model_lzcn
+	},
+	{}
+};
+
+/* ================================= */
+/* ACPI and WMI access               */
+/* ================================= */
+
+// function from ideapad-laptop.c
+static int eval_int(acpi_handle handle, const char *name, unsigned long *res)
+{
+	unsigned long long result;
+	acpi_status status;
+
+	status = acpi_evaluate_integer(handle, (char *)name, NULL, &result);
+	if (ACPI_FAILURE(status))
+		return -EIO;
+
+	*res = result;
+
+	return 0;
+}
+
+// function from ideapad-laptop.c
+static int exec_simple_method(acpi_handle handle, const char *name,
+			      unsigned long arg)
+{
+	acpi_status status =
+		acpi_execute_simple_method(handle, (char *)name, arg);
+
+	return ACPI_FAILURE(status) ? -EIO : 0;
+}
+
+// function from ideapad-laptop.c
+static int exec_sbmc(acpi_handle handle, unsigned long arg)
+{
+	// \_SB.PCI0.LPC0.EC0.VPC0.SBMC
+	return exec_simple_method(handle, "VPC0.SBMC", arg);
+}
+
+//static int eval_qcho(acpi_handle handle, unsigned long *res)
+//{
+//	// \_SB.PCI0.LPC0.EC0.QCHO
+//	return eval_int(handle, "QCHO", res);
+//}
+
+static int eval_gbmd(acpi_handle handle, unsigned long *res)
+{
+	return eval_int(handle, "VPC0.GBMD", res);
+}
+
+static int eval_spmo(acpi_handle handle, unsigned long *res)
+{
+	// \_SB.PCI0.LPC0.EC0.QCHO
+	return eval_int(handle, "VPC0.BTSM", res);
+}
+
+static int acpi_process_buffer_to_ints(const char *id_name, int id_nr,
+				       acpi_status status,
+				       struct acpi_buffer *out_buffer, u8 *res,
+				       size_t ressize)
+{
+	// seto to NULL call kfree on NULL if next function call fails
+	union acpi_object *out = NULL;
+	size_t i;
+	int error = 0;
+
+	if (ACPI_FAILURE(status)) {
+		pr_info("ACPI evaluation error for: %s:%d\n", id_name, id_nr);
+		error = -EFAULT;
+		goto err;
+	}
+
+	out = out_buffer->pointer;
+	if (!out) {
+		pr_info("Unexpected ACPI result for %s:%d\n", id_name, id_nr);
+		error = -AE_ERROR;
+		goto err;
+	}
+
+	if (out->type != ACPI_TYPE_BUFFER || out->buffer.length != ressize) {
+		pr_info("Unexpected ACPI result for %s:%d: expected type %d but got %d; expected length %lu but got %u;\n",
+			id_name, id_nr, ACPI_TYPE_BUFFER, out->type, ressize,
+			out->buffer.length);
+		error = -AE_ERROR;
+		goto err;
+	}
+
+// Reduced verbosity (only printing when ACPI result have bad parameters)
+//	pr_info("ACPI result for %s:%d: ACPI buffer length: %u\n", id_name,
+//		id_nr, out->buffer.length);
+
+	for (i = 0; i < ressize; ++i)
+		res[i] = out->buffer.pointer[i];
+	error = 0;
+
+err:
+	kfree(out);
+	return error;
+}
+
+//static int exec_ints(acpi_handle handle, const char *method_name,
+//		     struct acpi_object_list *params, u8 *res, size_t ressize)
+//{
+//	acpi_status status;
+//	struct acpi_buffer out_buffer = { ACPI_ALLOCATE_BUFFER, NULL };
+
+//	status = acpi_evaluate_object(handle, (acpi_string)method_name, params,
+//				      &out_buffer);
+
+//	return acpi_process_buffer_to_ints(method_name, 0, status, &out_buffer,
+//					   res, ressize);
+//}
+
+static int wmi_exec_ints(const char *guid, u8 instance, u32 method_id,
+			 const struct acpi_buffer *params, u8 *res,
+			 size_t ressize)
+{
+	acpi_status status;
+	struct acpi_buffer out_buffer = { ACPI_ALLOCATE_BUFFER, NULL };
+
+	status = wmi_evaluate_method(guid, instance, method_id, params,
+				     &out_buffer);
+	return acpi_process_buffer_to_ints(guid, method_id, status, &out_buffer,
+					   res, ressize);
+}
+
+static int wmi_exec_int(const char *guid, u8 instance, u32 method_id,
+			const struct acpi_buffer *params, unsigned long *res)
+{
+	acpi_status status;
+	struct acpi_buffer out_buffer = { ACPI_ALLOCATE_BUFFER, NULL };
+	// set to NULL and call kfree on NULL if next function call fails
+	union acpi_object *out = NULL;
+	int error = 0;
+
+	status = wmi_evaluate_method(guid, instance, method_id, params,
+				     &out_buffer);
+
+	if (ACPI_FAILURE(status)) {
+		pr_info("WMI evaluation error for: %s:%d\n", guid, method_id);
+		error = -EFAULT;
+		goto err;
+	}
+
+	out = out_buffer.pointer;
+	if (!out) {
+		pr_info("Unexpected ACPI result for %s:%d", guid, method_id);
+		error = -AE_ERROR;
+		goto err;
+	}
+
+	if (out->type != ACPI_TYPE_INTEGER) {
+		pr_info("Unexpected ACPI result for %s:%d: expected type %d but got %d\n",
+			guid, method_id, ACPI_TYPE_INTEGER, out->type);
+		error = -AE_ERROR;
+		goto err;
+	}
+
+	*res = out->integer.value;
+	error = 0;
+
+err:
+	kfree(out);
+	return error;
+}
+
+static int wmi_exec_noarg_int(const char *guid, u8 instance, u32 method_id,
+			      unsigned long *res)
+{
+	struct acpi_buffer params;
+
+	params.length = 0;
+	params.pointer = NULL;
+	return wmi_exec_int(guid, instance, method_id, &params, res);
+}
+
+static int wmi_exec_noarg_ints(const char *guid, u8 instance, u32 method_id,
+			       u8 *res, size_t ressize)
+{
+	struct acpi_buffer params;
+
+	params.length = 0;
+	params.pointer = NULL;
+	return wmi_exec_ints(guid, instance, method_id, &params, res, ressize);
+}
+
+static int wmi_exec_arg(const char *guid, u8 instance, u32 method_id, void *arg,
+			size_t arg_size)
+{
+	struct acpi_buffer params;
+	acpi_status status;
+
+	params.length = arg_size;
+	params.pointer = arg;
+	status = wmi_evaluate_method(guid, instance, method_id, &params, NULL);
+
+	if (ACPI_FAILURE(status))
+		return -EIO;
+	return 0;
+}
+
+/* ================================= */
+/* Lenovo WMI config                 */
+/* ================================= */
+#define LEGION_WMI_GAMEZONE_GUID "887B54E3-DDDC-4B2C-8B88-68A26A8835D0"
+// GPU over clock
+#define WMI_METHOD_ID_ISSUPPORTGPUOC 4
+
+// Fan speed
+// only fully implemented for some models here
+// often implemented in other classes and methods too
+// below
+#define WMI_METHOD_ID_GETFAN1SPEED 8
+#define WMI_METHOD_ID_GETFAN2SPEED 9
+
+// Version of ACPI
+#define WMI_METHOD_ID_GETVERSION 11
+// Does it support CPU overclock?
+#define WMI_METHOD_ID_ISSUPPORTCPUOC 14
+// Temperatures
+// only fully implemented for some models here
+// often implemented in other classes and methods too
+// below
+#define WMI_METHOD_ID_GETCPUTEMP 18
+#define WMI_METHOD_ID_GETGPUTEMP 19
+
+// two state keyboard light
+#define WMI_METHOD_ID_GETKEYBOARDLIGHT 37
+#define WMI_METHOD_ID_SETKEYBOARDLIGHT 36
+// toggle win key
+// 0 = win key enabled; 1 = win key disabled
+#define WMI_METHOD_ID_ISSUPPORTDISABLEWINKEY 21
+#define WMI_METHOD_ID_GETWINKEYSTATUS 23
+#define WMI_METHOD_ID_SETWINKEYSTATUS 22
+// toggle touchpad
+//0 = touchpad enabled; 1 = touchpad disabled
+#define WMI_METHOD_ID_ISSUPPORTDISABLETP 24
+#define WMI_METHOD_ID_GETTPSTATUS 26
+#define WMI_METHOD_ID_SETTPSTATUS 25
+// GSync
+#define WMI_METHOD_ID_ISSUPPORTGSYNC 40
+#define WMI_METHOD_ID_GETGSYNCSTATUS 41
+#define WMI_METHOD_ID_SETGSYNCSTATUS 42
+//smartFanMode = powermode
+#define WMI_METHOD_ID_ISSUPPORTSMARTFAN 49
+#define WMI_METHOD_ID_GETSMARTFANMODE 45
+#define WMI_METHOD_ID_SETSMARTFANMODE 44
+// power charge mode
+#define WMI_METHOD_ID_GETPOWERCHARGEMODE 47
+// overdrive of display to reduce latency
+// 0=off, 1=on
+#define WMI_METHOD_ID_ISSUPPORTOD 49
+#define WMI_METHOD_ID_GETODSTATUS 50
+#define WMI_METHOD_ID_SETODSTATUS 51
+// thermal mode = power mode used for cooling
+#define WMI_METHOD_ID_GETTHERMALMODE 55
+// get max frequency of core 0
+#define WMI_METHOD_ID_GETCPUMAXFREQUENCY 60
+// check if AC adapter has enough power to overclock
+#define WMI_METHOD_ID_ISACFITFOROC 62
+// set iGPU (GPU packaged with CPU) state
+#define WMI_METHOD_ID_ISSUPPORTIGPUMODE 63
+#define WMI_METHOD_ID_GETIGPUMODESTATUS 64
+#define WMI_METHOD_ID_SETIGPUMODESTATUS 65
+#define WMI_METHOD_ID_NOTIFYDGPUSTATUS 66
+enum IGPUState {
+	IGPUState_default = 0,
+	IGPUState_iGPUOnly = 1,
+	IGPUState_auto = 2
+};
+
+#define WMI_GUID_LENOVO_CPU_METHOD "14afd777-106f-4c9b-b334-d388dc7809be"
+#define WMI_METHOD_ID_CPU_GET_SUPPORT_OC_STATUS 15
+#define WMI_METHOD_ID_CPU_GET_OC_STATUS 1
+#define WMI_METHOD_ID_CPU_SET_OC_STATUS 2
+
+// ppt limit slow
+#define WMI_METHOD_ID_CPU_GET_SHORTTERM_POWERLIMIT 3
+#define WMI_METHOD_ID_CPU_SET_SHORTTERM_POWERLIMIT 4
+// ppt stapm
+#define WMI_METHOD_ID_CPU_GET_LONGTERM_POWERLIMIT 5
+#define WMI_METHOD_ID_CPU_SET_LONGTERM_POWERLIMIT 6
+// default power limit
+#define WMI_METHOD_ID_CPU_GET_DEFAULT_POWERLIMIT 7
+// peak power limit
+#define WMI_METHOD_ID_CPU_GET_PEAK_POWERLIMIT 8
+#define WMI_METHOD_ID_CPU_SET_PEAK_POWERLIMIT 9
+// apu sppt powerlimit
+#define WMI_METHOD_ID_CPU_GET_APU_SPPT_POWERLIMIT 12
+#define WMI_METHOD_ID_CPU_SET_APU_SPPT_POWERLIMIT 13
+// cross loading powerlimit
+#define WMI_METHOD_ID_CPU_GET_CROSS_LOADING_POWERLIMIT 16
+#define WMI_METHOD_ID_CPU_SET_CROSS_LOADING_POWERLIMIT 17
+
+#define WMI_GUID_LENOVO_GPU_METHOD "da7547f1-824d-405f-be79-d9903e29ced7"
+// overclock GPU possible
+#define WMI_METHOD_ID_GPU_GET_OC_STATUS 1
+#define WMI_METHOD_ID_GPU_SET_OC_STATUS 2
+// dynamic boost power
+#define WMI_METHOD_ID_GPU_GET_PPAB_POWERLIMIT 3
+#define WMI_METHOD_ID_GPU_SET_PPAB_POWERLIMIT 4
+// configurable TGP (power)
+#define WMI_METHOD_ID_GPU_GET_CTGP_POWERLIMIT 5
+#define WMI_METHOD_ID_GPU_SET_CTGP_POWERLIMIT 6
+// ppab/ctgp powerlimit
+#define WMI_METHOD_ID_GPU_GET_DEFAULT_PPAB_CTGP_POWERLIMIT 7
+// temperature limit
+#define WMI_METHOD_ID_GPU_GET_TEMPERATURE_LIMIT 8
+#define WMI_METHOD_ID_GPU_SET_TEMPERATURE_LIMIT 9
+// boost clock
+#define WMI_METHOD_ID_GPU_GET_BOOST_CLOCK 10
+
+#define WMI_GUID_LENOVO_FAN_METHOD "92549549-4bde-4f06-ac04-ce8bf898dbaa"
+// set fan to maximal speed; dust cleaning mode
+// only works in custom power mode
+#define WMI_METHOD_ID_FAN_GET_FULLSPEED 1
+#define WMI_METHOD_ID_FAN_SET_FULLSPEED 2
+// max speed of fan
+#define WMI_METHOD_ID_FAN_GET_MAXSPEED 3
+#define WMI_METHOD_ID_FAN_SET_MAXSPEED 4
+// fan table in custom mode
+#define WMI_METHOD_ID_FAN_GET_TABLE 5
+#define WMI_METHOD_ID_FAN_SET_TABLE 6
+// get speed of fans
+#define WMI_METHOD_ID_FAN_GETCURRENTFANSPEED 7
+// get temperatures of CPU and GPU used for controlling cooling
+#define WMI_METHOD_ID_FAN_GETCURRENTSENSORTEMPERATURE 8
+
+// do not implement following
+// #define WMI_METHOD_ID_Fan_SetCurrentFanSpeed 9
+
+#define LEGION_WMI_KBBACKLIGHT_GUID "8C5B9127-ECD4-4657-980F-851019F99CA5"
+// access the keyboard backlight with 3 states
+#define WMI_METHOD_ID_KBBACKLIGHTGET 0x1
+#define WMI_METHOD_ID_KBBACKLIGHTSET 0x2
+
+// new method in newer methods to get or set most of the values
+// with the two methods GetFeatureValue or SetFeatureValue.
+// They are called like GetFeatureValue(feature_id) where
+// feature_id is a id for the feature
+#define LEGION_WMI_LENOVO_OTHER_METHOD_GUID \
+	"dc2a8805-3a8c-41ba-a6f7-092e0089cd3b"
+#define WMI_METHOD_ID_GET_FEATURE_VALUE 17
+#define WMI_METHOD_ID_SET_FEATURE_VALUE 18
+
+enum OtherMethodFeature {
+	OtherMethodFeature_U1 = 0x010000, //->PC00.LPCB.EC0.REJF
+	OtherMethodFeature_U2 = 0x0F0000, //->C00.PEG1.PXP._STA?
+	OtherMethodFeature_U3 = 0x030000, //->PC00.LPCB.EC0.FLBT?
+	OtherMethodFeature_CPU_SHORT_TERM_POWER_LIMIT = 0x01010000,
+	OtherMethodFeature_CPU_LONG_TERM_POWER_LIMIT = 0x01020000,
+	OtherMethodFeature_CPU_PEAK_POWER_LIMIT = 0x01030000,
+	OtherMethodFeature_CPU_TEMPERATURE_LIMIT = 0x01040000,
+
+	OtherMethodFeature_APU_PPT_POWER_LIMIT = 0x01050000,
+
+	OtherMethodFeature_CPU_CROSS_LOAD_POWER_LIMIT = 0x01060000,
+	OtherMethodFeature_CPU_L1_TAU = 0x01070000,
+
+	OtherMethodFeature_GPU_POWER_BOOST = 0x02010000,
+	OtherMethodFeature_GPU_cTGP = 0x02020000,
+	OtherMethodFeature_GPU_TEMPERATURE_LIMIT = 0x02030000,
+	OtherMethodFeature_GPU_POWER_TARGET_ON_AC_OFFSET_FROM_BASELINE =
+		0x02040000,
+
+	OtherMethodFeature_FAN_SPEED_1 = 0x04030001,
+	OtherMethodFeature_FAN_SPEED_2 = 0x04030002,
+
+	OtherMethodFeature_C_U1 = 0x05010000,
+	OtherMethodFeature_TEMP_CPU = 0x05040000,
+	OtherMethodFeature_TEMP_GPU = 0x05050000,
+};
+
+static ssize_t wmi_other_method_get_value(enum OtherMethodFeature feature_id,
+					  int *value)
+{
+	struct acpi_buffer params;
+	int error;
+	unsigned long res;
+	u32 param1 = feature_id;
+
+	params.length = sizeof(param1);
+	params.pointer = &param1;
+	error = wmi_exec_int(LEGION_WMI_LENOVO_OTHER_METHOD_GUID, 0,
+			     WMI_METHOD_ID_GET_FEATURE_VALUE, &params, &res);
+	if (!error)
+		*value = res;
+	return error;
+}
+
+/* =================================== */
+/* EC RAM Access with memory mapped IO */
+/* =================================== */
+
+struct ecram_memoryio {
+	// TODO: start of remapped memory in EC RAM is assumed to be 0
+	// u16 ecram_start;
+
+	// physical address of remapped IO, depends on model and firmware
+	phys_addr_t physical_start;
+	// start adress of region in ec memory
+	phys_addr_t physical_ec_start;
+	// virtual address of remapped IO
+	u8 *virtual_start;
+	// size of remapped access
+	size_t size;
+};
+
+/**
+ * physical_start : corresponds to EC RAM 0 inside EC
+ * size: size of remapped region
+ *
+ * strong exception safety
+ */
+static ssize_t ecram_memoryio_init(struct ecram_memoryio *ec_memoryio,
+				   phys_addr_t physical_start,
+				   phys_addr_t physical_ec_start, size_t size)
+{
+	void *virtual_start = ioremap(physical_start, size);
+
+	if (!IS_ERR_OR_NULL(virtual_start)) {
+		ec_memoryio->virtual_start = virtual_start;
+		ec_memoryio->physical_start = physical_start;
+		ec_memoryio->physical_ec_start = physical_ec_start;
+		ec_memoryio->size = size;
+		pr_info("Successfully mapped embedded controller: 0x%llx (in RAM)/0x%llx (in EC) to virtual 0x%p\n",
+			ec_memoryio->physical_start,
+			ec_memoryio->physical_ec_start,
+			ec_memoryio->virtual_start);
+	} else {
+		pr_info("Error mapping embedded controller memory at 0x%llx\n",
+			physical_start);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+static void ecram_memoryio_exit(struct ecram_memoryio *ec_memoryio)
+{
+	if (ec_memoryio->virtual_start != NULL) {
+		pr_info("Unmapping embedded controller memory at 0x%llx (in RAM)/0x%llx (in EC) at virtual 0x%p\n",
+			ec_memoryio->physical_start,
+			ec_memoryio->physical_ec_start,
+			ec_memoryio->virtual_start);
+		iounmap(ec_memoryio->virtual_start);
+		ec_memoryio->virtual_start = NULL;
+	}
+}
+
+/* Read a byte from the EC RAM.
+ *
+ * Return status because of commong signature for alle
+ * methods to access EC RAM.
+ */
+static ssize_t ecram_memoryio_read(const struct ecram_memoryio *ec_memoryio,
+				   u16 ec_offset, u8 *value)
+{
+	if (ec_offset < ec_memoryio->physical_ec_start) {
+		pr_info("Unexpected read at offset %d into EC RAM\n",
+			ec_offset);
+		return -1;
+	}
+	*value = *(ec_memoryio->virtual_start +
+		   (ec_offset - ec_memoryio->physical_ec_start));
+	return 0;
+}
+
+/* Write a byte to the EC RAM.
+ *
+ * Return status because of commong signature for alle
+ * methods to access EC RAM.
+ */
+ssize_t ecram_memoryio_write(const struct ecram_memoryio *ec_memoryio,
+			     u16 ec_offset, u8 value)
+{
+	if (ec_offset < ec_memoryio->physical_ec_start) {
+		pr_info("Unexpected write at offset %d into EC RAM\n",
+			ec_offset);
+		return -1;
+	}
+	*(ec_memoryio->virtual_start +
+	  (ec_offset - ec_memoryio->physical_ec_start)) = value;
+	return 0;
+}
+
+/* ================================= */
+/* EC RAM Access with port-mapped IO */
+/* ================================= */
+
+/*
+ * See datasheet of e.g. IT8502E/F/G, e.g.
+ * 6.2 Plug and Play Configuration (PNPCFG)
+ *
+ * Depending on configured BARDSEL register
+ * the ports
+ *   ECRAM_PORTIO_ADDR_PORT and
+ *   ECRAM_PORTIO_DATA_PORT
+ * are configured.
+ *
+ * By performing IO on these ports one can
+ * read/write to registers in the EC.
+ *
+ * "To access a register of PNPCFG, write target index to
+ *  address port and access this PNPCFG register via
+ *  data port" [datasheet, 6.2 Plug and Play Configuration]
+ */
+
+// IO ports used to write to communicate with embedded controller
+// Start of used ports
+#define ECRAM_PORTIO_START_PORT 0x4E
+// Number of used ports
+#define ECRAM_PORTIO_PORTS_SIZE 2
+// Port used to specify address in EC RAM to read/write
+// 0x4E/0x4F is the usual port for IO super controller
+// 0x2E/0x2F also common (ITE can also be configured to use these)
+#define ECRAM_PORTIO_ADDR_PORT 0x4E
+// Port to send/receive the value to write/read
+#define ECRAM_PORTIO_DATA_PORT 0x4F
+// Name used to request ports
+#define ECRAM_PORTIO_NAME "legion"
+
+struct ecram_portio {
+	/* protects read/write to EC RAM performed
+	 * as a certain sequence of outb, inb
+	 * commands on the IO ports. There can
+	 * be at most one.
+	 */
+	struct mutex io_port_mutex;
+};
+
+static ssize_t ecram_portio_init(struct ecram_portio *ec_portio)
+{
+	if (!request_region(ECRAM_PORTIO_START_PORT, ECRAM_PORTIO_PORTS_SIZE,
+			    ECRAM_PORTIO_NAME)) {
+		pr_info("Cannot init ecram_portio the %x ports starting at %x\n",
+			ECRAM_PORTIO_PORTS_SIZE, ECRAM_PORTIO_START_PORT);
+		return -ENODEV;
+	}
+	//pr_info("Reserved %x ports starting at %x\n", ECRAM_PORTIO_PORTS_SIZE, ECRAM_PORTIO_START_PORT);
+	mutex_init(&ec_portio->io_port_mutex);
+	return 0;
+}
+
+static void ecram_portio_exit(struct ecram_portio *ec_portio)
+{
+	release_region(ECRAM_PORTIO_START_PORT, ECRAM_PORTIO_PORTS_SIZE);
+}
+
+/* Read a byte from the EC RAM.
+ *
+ * Return status because of commong signature for alle
+ * methods to access EC RAM.
+ */
+static ssize_t ecram_portio_read(struct ecram_portio *ec_portio, u16 offset,
+				 u8 *value)
+{
+	mutex_lock(&ec_portio->io_port_mutex);
+
+	outb(0x2E, ECRAM_PORTIO_ADDR_PORT);
+	outb(0x11, ECRAM_PORTIO_DATA_PORT);
+	outb(0x2F, ECRAM_PORTIO_ADDR_PORT);
+	// TODO: no explicit cast between types seems to be sometimes
+	// done and sometimes not
+	outb((u8)((offset >> 8) & 0xFF), ECRAM_PORTIO_DATA_PORT);
+
+	outb(0x2E, ECRAM_PORTIO_ADDR_PORT);
+	outb(0x10, ECRAM_PORTIO_DATA_PORT);
+	outb(0x2F, ECRAM_PORTIO_ADDR_PORT);
+	outb((u8)(offset & 0xFF), ECRAM_PORTIO_DATA_PORT);
+
+	outb(0x2E, ECRAM_PORTIO_ADDR_PORT);
+	outb(0x12, ECRAM_PORTIO_DATA_PORT);
+	outb(0x2F, ECRAM_PORTIO_ADDR_PORT);
+	*value = inb(ECRAM_PORTIO_DATA_PORT);
+
+	mutex_unlock(&ec_portio->io_port_mutex);
+	return 0;
+}
+
+/* Write a byte to the EC RAM.
+ *
+ * Return status because of commong signature for alle
+ * methods to access EC RAM.
+ */
+static ssize_t ecram_portio_write(struct ecram_portio *ec_portio, u16 offset,
+				  u8 value)
+{
+	mutex_lock(&ec_portio->io_port_mutex);
+
+	outb(0x2E, ECRAM_PORTIO_ADDR_PORT);
+	outb(0x11, ECRAM_PORTIO_DATA_PORT);
+	outb(0x2F, ECRAM_PORTIO_ADDR_PORT);
+	// TODO: no explicit cast between types seems to be sometimes
+	// done and sometimes not
+	outb((u8)((offset >> 8) & 0xFF), ECRAM_PORTIO_DATA_PORT);
+
+	outb(0x2E, ECRAM_PORTIO_ADDR_PORT);
+	outb(0x10, ECRAM_PORTIO_DATA_PORT);
+	outb(0x2F, ECRAM_PORTIO_ADDR_PORT);
+	outb((u8)(offset & 0xFF), ECRAM_PORTIO_DATA_PORT);
+
+	outb(0x2E, ECRAM_PORTIO_ADDR_PORT);
+	outb(0x12, ECRAM_PORTIO_DATA_PORT);
+	outb(0x2F, ECRAM_PORTIO_ADDR_PORT);
+	outb(value, ECRAM_PORTIO_DATA_PORT);
+
+	mutex_unlock(&ec_portio->io_port_mutex);
+	// TODO: remove this
+	//pr_info("Writing %d to addr %x\n", value, offset);
+	return 0;
+}
+
+/* =================================== */
+/* EC RAM Access                       */
+/* =================================== */
+
+struct ecram {
+	struct ecram_portio portio;
+};
+
+static ssize_t ecram_init(struct ecram *ecram,
+			  phys_addr_t memoryio_ec_physical_start,
+			  size_t region_size)
+{
+	ssize_t err;
+
+	err = ecram_portio_init(&ecram->portio);
+	if (err) {
+		pr_info("Failed ecram_portio_init\n");
+		goto err_ecram_portio_init;
+	}
+
+	return 0;
+
+err_ecram_portio_init:
+	return err;
+}
+
+static void ecram_exit(struct ecram *ecram)
+{
+	pr_info("Unloading legion ecram\n");
+	ecram_portio_exit(&ecram->portio);
+	pr_info("Unloading legion ecram done\n");
+}
+
+/** Read from EC RAM
+ * ecram_offset address on the EC
+ */
+static u8 ecram_read(struct ecram *ecram, u16 ecram_offset)
+{
+	u8 value;
+	int err;
+
+	err = ecram_portio_read(&ecram->portio, ecram_offset, &value);
+	if (err)
+		pr_info("Error reading EC RAM at 0x%x.\n", ecram_offset);
+	return value;
+}
+
+static void ecram_write(struct ecram *ecram, u16 ecram_offset, u8 value)
+{
+	int err;
+
+	if (ec_readonly) {
+		pr_info("Skipping writing EC RAM to 0x%x: Read-Only.\n",
+			ecram_offset);
+		return;
+	}
+	err = ecram_portio_write(&ecram->portio, ecram_offset, value);
+	if (err)
+		pr_info("Error writing EC RAM to 0x%x: Read-Only.\n", ecram_offset);
+}
+
+/* =============================== */
+/* Reads from EC  */
+/* ===============================  */
+
+static u16 read_ec_id(struct ecram *ecram, const struct model_config *model)
+{
+	u8 id1 = ecram_read(ecram, model->registers->ECHIPID1);
+	u8 id2 = ecram_read(ecram, model->registers->ECHIPID2);
+
+	return (id1 << 8) + id2;
+}
+
+static u16 read_ec_version(struct ecram *ecram,
+			   const struct model_config *model)
+{
+	u8 vers = ecram_read(ecram, model->registers->ECHIPVER);
+	u8 debug = ecram_read(ecram, model->registers->ECDEBUG);
+
+	return (vers << 8) + debug;
+}
+
+/* ============================= */
+/* Data model for sensor values  */
+/* ============================= */
+
+struct sensor_values {
+	u16 fan1_rpm; // current speed in rpm of fan 1
+	u16 fan2_rpm; // current speed in rpm of fan2
+	u16 fan1_target_rpm; // target speed in rpm of fan 1
+	u16 fan2_target_rpm; // target speed in rpm of fan 2
+	u8 cpu_temp_celsius; // cpu temperature in celcius
+	u8 gpu_temp_celsius; // gpu temperature in celcius
+	u8 ic_temp_celsius; // ic temperature in celcius
+};
+
+enum SENSOR_ATTR {
+	SENSOR_CPU_TEMP_ID = 1,
+	SENSOR_GPU_TEMP_ID = 2,
+	SENSOR_IC_TEMP_ID = 3,
+	SENSOR_FAN1_RPM_ID = 4,
+	SENSOR_FAN2_RPM_ID = 5,
+	SENSOR_FAN1_TARGET_RPM_ID = 6,
+	SENSOR_FAN2_TARGET_RPM_ID = 7
+};
+
+/* ============================= */
+/* Data model for fan curve      */
+/* ============================= */
+
+struct fancurve_point {
+	// rpm1 devided by 100
+	u8 rpm1_raw;
+	// rpm2 devided by 100
+	u8 rpm2_raw;
+	// >=2 , <=5 (lower is faster); must increase by level
+	u8 accel;
+	// >=2 , <=5 (lower is faster); must increase by level
+	u8 decel;
+
+	// min must be lower than or equal to max
+	// last level max must be 127
+	// <=127 cpu max temp for this level; must increase by level
+	u8 cpu_max_temp_celsius;
+	// <=127 cpu min temp for this level; must increase by level
+	u8 cpu_min_temp_celsius;
+	// <=127 gpu min temp for this level; must increase by level
+	u8 gpu_max_temp_celsius;
+	// <=127 gpu max temp for this level; must increase by level
+	u8 gpu_min_temp_celsius;
+	// <=127 ic max temp for this level; must increase by level
+	u8 ic_max_temp_celsius;
+	// <=127 ic max temp for this level; must increase by level
+	u8 ic_min_temp_celsius;
+};
+
+enum FANCURVE_ATTR {
+	FANCURVE_ATTR_PWM1 = 1,
+	FANCURVE_ATTR_PWM2 = 2,
+	FANCURVE_ATTR_CPU_TEMP = 3,
+	FANCURVE_ATTR_CPU_HYST = 4,
+	FANCURVE_ATTR_GPU_TEMP = 5,
+	FANCURVE_ATTR_GPU_HYST = 6,
+	FANCURVE_ATTR_IC_TEMP = 7,
+	FANCURVE_ATTR_IC_HYST = 8,
+	FANCURVE_ATTR_ACCEL = 9,
+	FANCURVE_ATTR_DECEL = 10,
+	FANCURVE_SIZE = 11,
+	FANCURVE_MINIFANCURVE_ON_COOL = 12
+};
+
+// used for clearing table entries
+static const struct fancurve_point fancurve_point_zero = { 0, 0, 0, 0, 0,
+							   0, 0, 0, 0, 0 };
+
+struct fancurve {
+	struct fancurve_point points[MAXFANCURVESIZE];
+	// number of points used; must be <= MAXFANCURVESIZE
+	size_t size;
+	// the point at which fans are run currently
+	size_t current_point_i;
+};
+
+// validation functions
+
+static bool fancurve_is_valid_min_temp(int min_temp)
+{
+	return min_temp >= 0 && min_temp <= 127;
+}
+
+static bool fancurve_is_valid_max_temp(int max_temp)
+{
+	return max_temp >= 0 && max_temp <= 127;
+}
+
+// setters with validation
+// - make hwmon implementation easier
+// - keep fancurve valid, otherwise EC will not properly control fan
+
+static bool fancurve_set_rpm1(struct fancurve *fancurve, int point_id, int rpm)
+{
+	bool valid = point_id == 0 ? rpm == 0 : (rpm >= 0 && rpm <= 4500);
+
+	if (valid)
+		fancurve->points[point_id].rpm1_raw = rpm / 100;
+	return valid;
+}
+
+static bool fancurve_set_rpm2(struct fancurve *fancurve, int point_id, int rpm)
+{
+	bool valid = point_id == 0 ? rpm == 0 : (rpm >= 0 && rpm <= 4500);
+
+	if (valid)
+		fancurve->points[point_id].rpm2_raw = rpm / 100;
+	return valid;
+}
+
+// TODO: remove { ... } from single line if body
+
+static bool fancurve_set_accel(struct fancurve *fancurve, int point_id,
+			       int accel)
+{
+	bool valid = accel >= 2 && accel <= 5;
+
+	if (valid)
+		fancurve->points[point_id].accel = accel;
+	return valid;
+}
+
+static bool fancurve_set_decel(struct fancurve *fancurve, int point_id,
+			       int decel)
+{
+	bool valid = decel >= 2 && decel <= 5;
+
+	if (valid)
+		fancurve->points[point_id].decel = decel;
+	return valid;
+}
+
+static bool fancurve_set_cpu_temp_max(struct fancurve *fancurve, int point_id,
+				      int value)
+{
+	bool valid = fancurve_is_valid_max_temp(value);
+
+	if (valid)
+		fancurve->points[point_id].cpu_max_temp_celsius = value;
+
+	return valid;
+}
+
+static bool fancurve_set_gpu_temp_max(struct fancurve *fancurve, int point_id,
+				      int value)
+{
+	bool valid = fancurve_is_valid_max_temp(value);
+
+	if (valid)
+		fancurve->points[point_id].gpu_max_temp_celsius = value;
+	return valid;
+}
+
+static bool fancurve_set_ic_temp_max(struct fancurve *fancurve, int point_id,
+				     int value)
+{
+	bool valid = fancurve_is_valid_max_temp(value);
+
+	if (valid)
+		fancurve->points[point_id].ic_max_temp_celsius = value;
+	return valid;
+}
+
+static bool fancurve_set_cpu_temp_min(struct fancurve *fancurve, int point_id,
+				      int value)
+{
+	bool valid = fancurve_is_valid_max_temp(value);
+
+	if (valid)
+		fancurve->points[point_id].cpu_min_temp_celsius = value;
+	return valid;
+}
+
+static bool fancurve_set_gpu_temp_min(struct fancurve *fancurve, int point_id,
+				      int value)
+{
+	bool valid = fancurve_is_valid_min_temp(value);
+
+	if (valid)
+		fancurve->points[point_id].gpu_min_temp_celsius = value;
+	return valid;
+}
+
+static bool fancurve_set_ic_temp_min(struct fancurve *fancurve, int point_id,
+				     int value)
+{
+	bool valid = fancurve_is_valid_min_temp(value);
+
+	if (valid)
+		fancurve->points[point_id].ic_min_temp_celsius = value;
+	return valid;
+}
+
+static bool fancurve_set_size(struct fancurve *fancurve, int size,
+			      bool init_values)
+{
+	bool valid = size >= 1 && size <= MAXFANCURVESIZE;
+
+	if (!valid)
+		return false;
+	if (init_values && size < fancurve->size) {
+		// fancurve size is decreased, but last entry always needs 127 temperatures
+		// Note: size >=1
+		fancurve->points[size - 1].cpu_max_temp_celsius = 127;
+		fancurve->points[size - 1].ic_max_temp_celsius = 127;
+		fancurve->points[size - 1].gpu_max_temp_celsius = 127;
+	}
+	if (init_values && size > fancurve->size) {
+		// fancurve increased, so new entries need valid values
+		int i;
+		int last = fancurve->size > 0 ? fancurve->size - 1 : 0;
+
+		for (i = fancurve->size; i < size; ++i)
+			fancurve->points[i] = fancurve->points[last];
+	}
+	return true;
+}
+
+static ssize_t fancurve_print_seqfile(const struct fancurve *fancurve,
+				      struct seq_file *s)
+{
+	int i;
+
+	seq_printf(
+		s,
+		"rpm1|rpm2|acceleration|deceleration|cpu_min_temp|cpu_max_temp|gpu_min_temp|gpu_max_temp|ic_min_temp|ic_max_temp\n");
+	for (i = 0; i < fancurve->size; ++i) {
+		const struct fancurve_point *point = &fancurve->points[i];
+
+		seq_printf(
+			s, "%d\t %d\t %d\t %d\t %d\t %d\t %d\t %d\t %d\t %d\n",
+			point->rpm1_raw * 100, point->rpm2_raw * 100,
+			point->accel, point->decel, point->cpu_min_temp_celsius,
+			point->cpu_max_temp_celsius,
+			point->gpu_min_temp_celsius,
+			point->gpu_max_temp_celsius, point->ic_min_temp_celsius,
+			point->ic_max_temp_celsius);
+	}
+	return 0;
+}
+
+struct light {
+	bool initialized;
+	struct led_classdev led;
+	unsigned int last_brightness;
+	u8 light_id;
+	unsigned int lower_limit;
+	unsigned int upper_limit;
+};
+
+/* =============================  */
+/* Global and shared data between */
+/* all calls to this module       */
+/* =============================  */
+// Implemented like ideapad-laptop.c but currently still
+// without dynamic memory allocation (instead global _priv)
+struct legion_private {
+	struct platform_device *platform_device;
+	// TODO: remove or keep? init?
+	struct acpi_device *adev;
+
+	// Method to access ECRAM
+	struct ecram ecram;
+	// Configuration with registers and ECRAM access method
+	const struct model_config *conf;
+
+	// TODO: maybe refactor and keep only local to each function
+	// last known fan curve
+	struct fancurve fancurve;
+	// configured fan curve from user space
+	struct fancurve fancurve_configured;
+
+	// update lock, when partial values of fancurve are changed
+	struct mutex fancurve_mutex;
+
+	//interfaces
+	struct dentry *debugfs_dir;
+	struct device *hwmon_dev;
+	struct platform_profile_handler platform_profile_handler;
+
+	struct light kbd_bl;
+	struct light ylogo_light;
+	struct light iport_light;
+
+	// TODO: remove?
+	bool loaded;
+
+	// TODO: remove, only for reverse enginnering
+	struct ecram_memoryio ec_memoryio;
+};
+
+// shared between different drivers: WMI, platform and protected by mutex
+static struct legion_private *legion_shared;
+static struct legion_private _priv;
+static DEFINE_MUTEX(legion_shared_mutex);
+
+static int legion_shared_init(struct legion_private *priv)
+{
+	int ret;
+
+	mutex_lock(&legion_shared_mutex);
+
+	if (!legion_shared) {
+		legion_shared = priv;
+		mutex_init(&legion_shared->fancurve_mutex);
+		ret = 0;
+	} else {
+		pr_warn("Found multiple platform devices\n");
+		ret = -EINVAL;
+	}
+
+	priv->loaded = true;
+	mutex_unlock(&legion_shared_mutex);
+
+	return ret;
+}
+
+static void legion_shared_exit(struct legion_private *priv)
+{
+	pr_info("Unloading legion shared\n");
+	mutex_lock(&legion_shared_mutex);
+
+	if (legion_shared == priv)
+		legion_shared = NULL;
+
+	mutex_unlock(&legion_shared_mutex);
+	pr_info("Unloading legion shared done\n");
+}
+
+static int get_simple_wmi_attribute(struct legion_private *priv,
+				    const char *guid, u8 instance,
+				    u32 method_id, bool invert,
+				    unsigned long scale, unsigned long *value)
+{
+	unsigned long state = 0;
+	int err;
+
+	if (scale == 0) {
+		pr_info("Scale cannot be 0\n");
+		return -EINVAL;
+	}
+	err = wmi_exec_noarg_int(guid, instance, method_id, &state);
+	if (err)
+		return -EINVAL;
+
+	// TODO: remove later
+	pr_info("%swith raw value: %ld\n", __func__, state);
+
+	state = state * scale;
+
+	if (invert)
+		state = !state;
+	*value = state;
+	return 0;
+}
+
+static int get_simple_wmi_attribute_bool(struct legion_private *priv,
+					 const char *guid, u8 instance,
+					 u32 method_id, bool invert,
+					 unsigned long scale, bool *value)
+{
+	unsigned long int_val = *value;
+	int err = get_simple_wmi_attribute(priv, guid, instance, method_id,
+					   invert, scale, &int_val);
+	*value = int_val;
+	return err;
+}
+
+static int set_simple_wmi_attribute(struct legion_private *priv,
+				    const char *guid, u8 instance,
+				    u32 method_id, bool invert, int scale,
+				    int state)
+{
+	int err;
+	u8 in_param;
+
+	if (scale == 0) {
+		pr_info("Scale cannot be 0\n");
+		return -EINVAL;
+	}
+
+	if (invert)
+		state = !state;
+
+	in_param = state / scale;
+
+	err = wmi_exec_arg(guid, instance, method_id, &in_param,
+			   sizeof(in_param));
+	return err;
+}
+
+/* ============================= */
+/* Sensor value reading/writing */
+/* ============================= */
+
+static int ec_read_sensor_values(struct ecram *ecram,
+				 const struct model_config *model,
+				 struct sensor_values *values)
+{
+	values->fan1_target_rpm =
+		100 * ecram_read(ecram, model->registers->EXT_FAN1_TARGET_RPM);
+	values->fan2_target_rpm =
+		100 * ecram_read(ecram, model->registers->EXT_FAN2_TARGET_RPM);
+
+	values->fan1_rpm =
+		ecram_read(ecram, model->registers->EXT_FAN1_RPM_LSB) +
+		(((int)ecram_read(ecram, model->registers->EXT_FAN1_RPM_MSB))
+		 << 8);
+	values->fan2_rpm =
+		ecram_read(ecram, model->registers->EXT_FAN2_RPM_LSB) +
+		(((int)ecram_read(ecram, model->registers->EXT_FAN2_RPM_MSB))
+		 << 8);
+
+	values->cpu_temp_celsius =
+		ecram_read(ecram, model->registers->EXT_CPU_TEMP_INPUT);
+	values->gpu_temp_celsius =
+		ecram_read(ecram, model->registers->EXT_GPU_TEMP_INPUT);
+	values->ic_temp_celsius =
+		ecram_read(ecram, model->registers->EXT_IC_TEMP_INPUT);
+
+	values->cpu_temp_celsius = ecram_read(ecram, 0xC5E6);
+	values->gpu_temp_celsius = ecram_read(ecram, 0xC5E7);
+	values->ic_temp_celsius = ecram_read(ecram, 0xC5E8);
+
+	return 0;
+}
+
+static ssize_t ec_read_temperature(struct ecram *ecram,
+				   const struct model_config *model,
+				   int sensor_id, int *temperature)
+{
+	int err = 0;
+	unsigned long res;
+
+	if (sensor_id == 0) {
+		res = ecram_read(ecram, 0xC5E6);
+	} else if (sensor_id == 1) {
+		res = ecram_read(ecram, 0xC5E7);
+	} else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+	if (!err)
+		*temperature = res;
+	return err;
+}
+
+static ssize_t ec_read_fanspeed(struct ecram *ecram,
+				const struct model_config *model, int fan_id,
+				int *fanspeed_rpm)
+{
+	int err = 0;
+	unsigned long res;
+
+	if (fan_id == 0) {
+		res = ecram_read(ecram, model->registers->EXT_FAN1_RPM_LSB) +
+		      (((int)ecram_read(ecram,
+					model->registers->EXT_FAN1_RPM_MSB))
+		       << 8);
+	} else if (fan_id == 1) {
+		res = ecram_read(ecram, model->registers->EXT_FAN2_RPM_LSB) +
+		      (((int)ecram_read(ecram,
+					model->registers->EXT_FAN2_RPM_MSB))
+		       << 8);
+	} else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+	if (!err)
+		*fanspeed_rpm = res;
+	return err;
+}
+
+// '\_SB.PCI0.LPC0.EC0.FANS
+#define ACPI_PATH_FAN_SPEED1 "FANS"
+// '\_SB.PCI0.LPC0.EC0.FA2S
+#define ACPI_PATH_FAN_SPEED2 "FA2S"
+
+static ssize_t acpi_read_fanspeed(struct legion_private *priv, int fan_id,
+				  int *value)
+{
+	int err;
+	unsigned long acpi_value;
+	const char *acpi_path;
+
+	if (fan_id == 0) {
+		acpi_path = ACPI_PATH_FAN_SPEED1;
+	} else if (fan_id == 1) {
+		acpi_path = ACPI_PATH_FAN_SPEED2;
+	} else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+	err = eval_int(priv->adev->handle, acpi_path, &acpi_value);
+	if (!err)
+		*value = (int)acpi_value * 100;
+	return err;
+}
+
+// '\_SB.PCI0.LPC0.EC0.CPUT
+#define ACPI_PATH_CPU_TEMP "CPUT"
+// '\_SB.PCI0.LPC0.EC0.GPUT
+#define ACPI_PATH_GPU_TEMP "GPUT"
+
+static ssize_t acpi_read_temperature(struct legion_private *priv, int fan_id,
+				     int *value)
+{
+	int err;
+	unsigned long acpi_value;
+	const char *acpi_path;
+
+	if (fan_id == 0) {
+		acpi_path = ACPI_PATH_CPU_TEMP;
+	} else if (fan_id == 1) {
+		acpi_path = ACPI_PATH_GPU_TEMP;
+	} else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+	err = eval_int(priv->adev->handle, acpi_path, &acpi_value);
+	if (!err)
+		*value = (int)acpi_value;
+	return err;
+}
+
+// fan_id: 0 or 1
+static ssize_t wmi_read_fanspeed(int fan_id, int *fanspeed_rpm)
+{
+	int err;
+	unsigned long res;
+	struct acpi_buffer params;
+
+	params.length = 1;
+	params.pointer = &fan_id;
+
+	err = wmi_exec_int(WMI_GUID_LENOVO_FAN_METHOD, 0,
+			   WMI_METHOD_ID_FAN_GETCURRENTFANSPEED, &params, &res);
+
+	if (!err)
+		*fanspeed_rpm = res;
+	return err;
+}
+
+//sensor_id: cpu = 0, gpu = 1
+static ssize_t wmi_read_temperature(int sensor_id, int *temperature)
+{
+	int err;
+	unsigned long res;
+	struct acpi_buffer params;
+
+	if (sensor_id == 0)
+		sensor_id = 0x03;
+	else if (sensor_id == 1)
+		sensor_id = 0x04;
+	else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+
+	params.length = 1;
+	params.pointer = &sensor_id;
+
+	err = wmi_exec_int(WMI_GUID_LENOVO_FAN_METHOD, 0,
+			   WMI_METHOD_ID_FAN_GETCURRENTSENSORTEMPERATURE,
+			   &params, &res);
+
+	if (!err)
+		*temperature = res;
+	return err;
+}
+
+// fan_id: 0 or 1
+static ssize_t wmi_read_fanspeed_gz(int fan_id, int *fanspeed_rpm)
+{
+	int err;
+	u32 method_id;
+	unsigned long res;
+
+	if (fan_id == 0)
+		method_id = WMI_METHOD_ID_GETFAN1SPEED;
+	else if (fan_id == 1)
+		method_id = WMI_METHOD_ID_GETFAN2SPEED;
+	else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+	err = wmi_exec_noarg_int(LEGION_WMI_GAMEZONE_GUID, 0, method_id, &res);
+
+	if (!err)
+		*fanspeed_rpm = res;
+	return err;
+}
+
+//sensor_id: cpu = 0, gpu = 1
+static ssize_t wmi_read_temperature_gz(int sensor_id, int *temperature)
+{
+	int err;
+	u32 method_id;
+	unsigned long res;
+
+	if (sensor_id == 0)
+		method_id = WMI_METHOD_ID_GETCPUTEMP;
+	else if (sensor_id == 1)
+		method_id = WMI_METHOD_ID_GETGPUTEMP;
+	else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+
+	err = wmi_exec_noarg_int(LEGION_WMI_GAMEZONE_GUID, 0, method_id, &res);
+
+	if (!err)
+		*temperature = res;
+	return err;
+}
+
+// fan_id: 0 or 1
+static ssize_t wmi_read_fanspeed_other(int fan_id, int *fanspeed_rpm)
+{
+	int err;
+	enum OtherMethodFeature featured_id;
+	int res;
+
+	if (fan_id == 0)
+		featured_id = OtherMethodFeature_FAN_SPEED_1;
+	else if (fan_id == 1)
+		featured_id = OtherMethodFeature_FAN_SPEED_2;
+	else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+
+	err = wmi_other_method_get_value(featured_id, &res);
+
+	if (!err)
+		*fanspeed_rpm = res;
+	return err;
+}
+
+//sensor_id: cpu = 0, gpu = 1
+static ssize_t wmi_read_temperature_other(int sensor_id, int *temperature)
+{
+	int err;
+	enum OtherMethodFeature featured_id;
+	int res;
+
+	if (sensor_id == 0)
+		featured_id = OtherMethodFeature_TEMP_CPU;
+	else if (sensor_id == 1)
+		featured_id = OtherMethodFeature_TEMP_GPU;
+	else {
+		// TODO: use all correct error codes
+		return -EEXIST;
+	}
+
+	err = wmi_other_method_get_value(featured_id, &res);
+	if (!err)
+		*temperature = res;
+	return err;
+}
+
+static ssize_t read_fanspeed(struct legion_private *priv, int fan_id,
+			     int *speed_rpm)
+{
+	// TODO: use enums or function pointers?
+	switch (priv->conf->access_method_fanspeed) {
+	case ACCESS_METHOD_EC:
+		return ec_read_fanspeed(&priv->ecram, priv->conf, fan_id,
+					speed_rpm);
+	case ACCESS_METHOD_ACPI:
+		return acpi_read_fanspeed(priv, fan_id, speed_rpm);
+	case ACCESS_METHOD_WMI:
+		return wmi_read_fanspeed_gz(fan_id, speed_rpm);
+	case ACCESS_METHOD_WMI2:
+		return wmi_read_fanspeed(fan_id, speed_rpm);
+	case ACCESS_METHOD_WMI3:
+		return wmi_read_fanspeed_other(fan_id, speed_rpm);
+	default:
+		pr_info("No access method for fanspeed: %d\n",
+			priv->conf->access_method_fanspeed);
+		return -EINVAL;
+	}
+}
+
+static ssize_t read_temperature(struct legion_private *priv, int sensor_id,
+				int *temperature)
+{
+	// TODO: use enums or function pointers?
+	switch (priv->conf->access_method_temperature) {
+	case ACCESS_METHOD_EC:
+		return ec_read_temperature(&priv->ecram, priv->conf, sensor_id,
+					   temperature);
+	case ACCESS_METHOD_ACPI:
+		return acpi_read_temperature(priv, sensor_id, temperature);
+	case ACCESS_METHOD_WMI:
+		return wmi_read_temperature_gz(sensor_id, temperature);
+	case ACCESS_METHOD_WMI2:
+		return wmi_read_temperature(sensor_id, temperature);
+	case ACCESS_METHOD_WMI3:
+		return wmi_read_temperature_other(sensor_id, temperature);
+	default:
+		pr_info("No access method for temperature: %d\n",
+			priv->conf->access_method_temperature);
+		return -EINVAL;
+	}
+}
+
+/* ============================= */
+/* Fancurve reading/writing      */
+/* ============================= */
+
+/* Fancurve from WMI
+ * This allows changing fewer parameters.
+ * It is only available on newer models.
+ */
+
+struct WMIFanTable {
+	u8 FSTM; //FSMD
+	u8 FSID;
+	u32 FSTL; //FSST
+	u16 FSS0;
+	u16 FSS1;
+	u16 FSS2;
+	u16 FSS3;
+	u16 FSS4;
+	u16 FSS5;
+	u16 FSS6;
+	u16 FSS7;
+	u16 FSS8;
+	u16 FSS9;
+} __packed;
+
+struct WMIFanTableRead {
+	u32 FSFL;
+	u32 FSS0;
+	u32 FSS1;
+	u32 FSS2;
+	u32 FSS3;
+	u32 FSS4;
+	u32 FSS5;
+	u32 FSS6;
+	u32 FSS7;
+	u32 FSS8;
+	u32 FSS9;
+	u32 FSSA;
+} __packed;
+
+static ssize_t wmi_read_fancurve_custom(const struct model_config *model,
+					struct fancurve *fancurve)
+{
+	u8 buffer[88];
+	int err;
+
+	// The output buffer from the ACPI call is 88 bytes and larger
+	// than the returned object
+	pr_info("Size of object: %lu\n", sizeof(struct WMIFanTableRead));
+	err = wmi_exec_noarg_ints(WMI_GUID_LENOVO_FAN_METHOD, 0,
+				  WMI_METHOD_ID_FAN_GET_TABLE, buffer,
+				  sizeof(buffer));
+	print_hex_dump(KERN_INFO, "legion_laptop fan table wmi buffer",
+		       DUMP_PREFIX_ADDRESS, 16, 1, buffer, sizeof(buffer),
+		       true);
+	if (!err) {
+		struct WMIFanTableRead *fantable =
+			(struct WMIFanTableRead *)&buffer[0];
+		fancurve->current_point_i = 0;
+		fancurve->size = 10;
+		fancurve->points[0].rpm1_raw = fantable->FSS0;
+		fancurve->points[1].rpm1_raw = fantable->FSS1;
+		fancurve->points[2].rpm1_raw = fantable->FSS2;
+		fancurve->points[3].rpm1_raw = fantable->FSS3;
+		fancurve->points[4].rpm1_raw = fantable->FSS4;
+		fancurve->points[5].rpm1_raw = fantable->FSS5;
+		fancurve->points[6].rpm1_raw = fantable->FSS6;
+		fancurve->points[7].rpm1_raw = fantable->FSS7;
+		fancurve->points[8].rpm1_raw = fantable->FSS8;
+		fancurve->points[9].rpm1_raw = fantable->FSS9;
+		//fancurve->points[10].rpm1_raw = fantable->FSSA;
+	}
+	return err;
+}
+
+static ssize_t wmi_write_fancurve_custom(const struct model_config *model,
+					 const struct fancurve *fancurve)
+{
+	u8 buffer[0x20];
+	int err;
+
+	// The buffer is read like this in ACPI firmware
+	//
+	// CreateByteField (Arg2, Zero, FSTM)
+	// CreateByteField (Arg2, One, FSID)
+	// CreateDWordField (Arg2, 0x02, FSTL)
+	// CreateByteField (Arg2, 0x06, FSS0)
+	// CreateByteField (Arg2, 0x08, FSS1)
+	// CreateByteField (Arg2, 0x0A, FSS2)
+	// CreateByteField (Arg2, 0x0C, FSS3)
+	// CreateByteField (Arg2, 0x0E, FSS4)
+	// CreateByteField (Arg2, 0x10, FSS5)
+	// CreateByteField (Arg2, 0x12, FSS6)
+	// CreateByteField (Arg2, 0x14, FSS7)
+	// CreateByteField (Arg2, 0x16, FSS8)
+	// CreateByteField (Arg2, 0x18, FSS9)
+
+	memset(buffer, 0, sizeof(buffer));
+	buffer[0x06] = fancurve->points[0].rpm1_raw;
+	buffer[0x08] = fancurve->points[1].rpm1_raw;
+	buffer[0x0A] = fancurve->points[2].rpm1_raw;
+	buffer[0x0C] = fancurve->points[3].rpm1_raw;
+	buffer[0x0E] = fancurve->points[4].rpm1_raw;
+	buffer[0x10] = fancurve->points[5].rpm1_raw;
+	buffer[0x12] = fancurve->points[6].rpm1_raw;
+	buffer[0x14] = fancurve->points[7].rpm1_raw;
+	buffer[0x16] = fancurve->points[8].rpm1_raw;
+	buffer[0x18] = fancurve->points[9].rpm1_raw;
+
+	print_hex_dump(KERN_INFO, "legion_laptop fan table wmi write buffer",
+		       DUMP_PREFIX_ADDRESS, 16, 1, buffer, sizeof(buffer),
+		       true);
+	err = wmi_exec_arg(WMI_GUID_LENOVO_FAN_METHOD, 0,
+			   WMI_METHOD_ID_FAN_SET_TABLE, buffer, sizeof(buffer));
+	return err;
+}
+
+/* Read the fan curve from the EC.
+ *
+ * In newer models (>=2022) there is an ACPI/WMI to read fan curve as
+ * a whole. So read/write fan table as a whole to use the
+ * same interface for both cases.
+ *
+ * It reads all points from EC memory, even if stored fancurve is smaller, so
+ * it can contain 0 entries.
+ */
+static int ec_read_fancurve_legion(struct ecram *ecram,
+				   const struct model_config *model,
+				   struct fancurve *fancurve)
+{
+	size_t i = 0;
+
+	for (i = 0; i < MAXFANCURVESIZE; ++i) {
+		struct fancurve_point *point = &fancurve->points[i];
+
+		point->rpm1_raw =
+			ecram_read(ecram, model->registers->EXT_FAN1_BASE + i);
+		point->rpm2_raw =
+			ecram_read(ecram, model->registers->EXT_FAN2_BASE + i);
+
+		point->accel = ecram_read(
+			ecram, model->registers->EXT_FAN_ACC_BASE + i);
+		point->decel = ecram_read(
+			ecram, model->registers->EXT_FAN_DEC_BASE + i);
+		point->cpu_max_temp_celsius =
+			ecram_read(ecram, model->registers->EXT_CPU_TEMP + i);
+		point->cpu_min_temp_celsius = ecram_read(
+			ecram, model->registers->EXT_CPU_TEMP_HYST + i);
+		point->gpu_max_temp_celsius =
+			ecram_read(ecram, model->registers->EXT_GPU_TEMP + i);
+		point->gpu_min_temp_celsius = ecram_read(
+			ecram, model->registers->EXT_GPU_TEMP_HYST + i);
+		point->ic_max_temp_celsius =
+			ecram_read(ecram, model->registers->EXT_VRM_TEMP + i);
+		point->ic_min_temp_celsius = ecram_read(
+			ecram, model->registers->EXT_VRM_TEMP_HYST + i);
+	}
+
+	// Do not trust that hardware; It might suddenly report
+	// a larger size, so clamp it.
+	fancurve->size =
+		ecram_read(ecram, model->registers->EXT_FAN_POINTS_SIZE);
+	fancurve->size =
+		min(fancurve->size, (typeof(fancurve->size))(MAXFANCURVESIZE));
+	fancurve->current_point_i =
+		ecram_read(ecram, model->registers->EXT_FAN_CUR_POINT);
+	fancurve->current_point_i =
+		min(fancurve->current_point_i, fancurve->size);
+	return 0;
+}
+
+static int ec_write_fancurve_legion(struct ecram *ecram,
+				    const struct model_config *model,
+				    const struct fancurve *fancurve,
+				    bool write_size)
+{
+	size_t i;
+
+	// Reset fan update counters (try to avoid any race conditions)
+	ecram_write(ecram, 0xC5FE, 0);
+	ecram_write(ecram, 0xC5FF, 0);
+	for (i = 0; i < MAXFANCURVESIZE; ++i) {
+		// Entries for points larger than fancurve size should be cleared
+		// to 0
+		const struct fancurve_point *point =
+			i < fancurve->size ? &fancurve->points[i] :
+					     &fancurve_point_zero;
+
+		ecram_write(ecram, model->registers->EXT_FAN1_BASE + i,
+			    point->rpm1_raw);
+		ecram_write(ecram, model->registers->EXT_FAN2_BASE + i,
+			    point->rpm2_raw);
+
+		ecram_write(ecram, model->registers->EXT_FAN_ACC_BASE + i,
+			    point->accel);
+		ecram_write(ecram, model->registers->EXT_FAN_DEC_BASE + i,
+			    point->decel);
+
+		ecram_write(ecram, model->registers->EXT_CPU_TEMP + i,
+			    point->cpu_max_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_CPU_TEMP_HYST + i,
+			    point->cpu_min_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_GPU_TEMP + i,
+			    point->gpu_max_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_GPU_TEMP_HYST + i,
+			    point->gpu_min_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_VRM_TEMP + i,
+			    point->ic_max_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_VRM_TEMP_HYST + i,
+			    point->ic_min_temp_celsius);
+	}
+
+	if (write_size) {
+		ecram_write(ecram, model->registers->EXT_FAN_POINTS_SIZE,
+			    fancurve->size);
+	}
+
+	// Reset current fan level to 0, so algorithm in EC
+	// selects fan curve point again and resetting hysterisis
+	// effects
+	ecram_write(ecram, model->registers->EXT_FAN_CUR_POINT, 0);
+
+	// Reset internal fan levels
+	ecram_write(ecram, 0xC634, 0); // CPU
+	ecram_write(ecram, 0xC635, 0); // GPU
+	ecram_write(ecram, 0xC636, 0); // SENSOR
+
+	return 0;
+}
+
+#define FANCURVESIZE_IDEAPDAD 8
+
+static int ec_read_fancurve_ideapad(struct ecram *ecram,
+				    const struct model_config *model,
+				    struct fancurve *fancurve)
+{
+	size_t i = 0;
+
+	for (i = 0; i < FANCURVESIZE_IDEAPDAD; ++i) {
+		struct fancurve_point *point = &fancurve->points[i];
+
+		point->rpm1_raw =
+			ecram_read(ecram, model->registers->EXT_FAN1_BASE + i);
+		point->rpm2_raw =
+			ecram_read(ecram, model->registers->EXT_FAN2_BASE + i);
+
+		point->accel = 0;
+		point->decel = 0;
+		point->cpu_max_temp_celsius =
+			ecram_read(ecram, model->registers->EXT_CPU_TEMP + i);
+		point->cpu_min_temp_celsius = ecram_read(
+			ecram, model->registers->EXT_CPU_TEMP_HYST + i);
+		point->gpu_max_temp_celsius =
+			ecram_read(ecram, model->registers->EXT_GPU_TEMP + i);
+		point->gpu_min_temp_celsius = ecram_read(
+			ecram, model->registers->EXT_GPU_TEMP_HYST + i);
+		point->ic_max_temp_celsius = 0;
+		point->ic_min_temp_celsius = 0;
+	}
+
+	// Do not trust that hardware; It might suddenly report
+	// a larger size, so clamp it.
+	fancurve->size = FANCURVESIZE_IDEAPDAD;
+	fancurve->current_point_i =
+		ecram_read(ecram, model->registers->EXT_FAN_CUR_POINT);
+	fancurve->current_point_i =
+		min(fancurve->current_point_i, fancurve->size);
+	return 0;
+}
+
+static int ec_write_fancurve_ideapad(struct ecram *ecram,
+				     const struct model_config *model,
+				     const struct fancurve *fancurve)
+{
+	size_t i;
+	int valr1;
+	int valr2;
+
+	// add this later: maybe other addresses needed
+	// therefore, fan curve might not be effective immediately but
+	// only after temp change
+	// Reset fan update counters (try to avoid any race conditions)
+	ecram_write(ecram, 0xC5FE, 0);
+	ecram_write(ecram, 0xC5FF, 0);
+	for (i = 0; i < FANCURVESIZE_IDEAPDAD; ++i) {
+		const struct fancurve_point *point = &fancurve->points[i];
+
+		ecram_write(ecram, model->registers->EXT_FAN1_BASE + i,
+			    point->rpm1_raw);
+		valr1 = ecram_read(ecram, model->registers->EXT_FAN1_BASE + i);
+		ecram_write(ecram, model->registers->EXT_FAN2_BASE + i,
+			    point->rpm2_raw);
+		valr2 = ecram_read(ecram, model->registers->EXT_FAN2_BASE + i);
+		pr_info("Writing fan1: %d; reading fan1: %d\n", point->rpm1_raw,
+			valr1);
+		pr_info("Writing fan2: %d; reading fan2: %d\n", point->rpm2_raw,
+			valr2);
+
+		// write to memory and repeat 8 bytes later again
+		ecram_write(ecram, model->registers->EXT_CPU_TEMP + i,
+			    point->cpu_max_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_CPU_TEMP + 8 + i,
+			    point->cpu_max_temp_celsius);
+		// write to memory and repeat 8 bytes later again
+		ecram_write(ecram, model->registers->EXT_CPU_TEMP_HYST + i,
+			    point->cpu_min_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_CPU_TEMP_HYST + 8 + i,
+			    point->cpu_min_temp_celsius);
+		// write to memory and repeat 8 bytes later again
+		ecram_write(ecram, model->registers->EXT_GPU_TEMP + i,
+			    point->gpu_max_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_GPU_TEMP + 8 + i,
+			    point->gpu_max_temp_celsius);
+		// write to memory and repeat 8 bytes later again
+		ecram_write(ecram, model->registers->EXT_GPU_TEMP_HYST + i,
+			    point->gpu_min_temp_celsius);
+		ecram_write(ecram, model->registers->EXT_GPU_TEMP_HYST + 8 + i,
+			    point->gpu_min_temp_celsius);
+	}
+
+	// add this later: maybe other addresses needed
+	// therefore, fan curve might not be effective immediately but
+	// only after temp change
+	// // Reset current fan level to 0, so algorithm in EC
+	// // selects fan curve point again and resetting hysterisis
+	// // effects
+	// ecram_write(ecram, model->registers->EXT_FAN_CUR_POINT, 0);
+
+	// // Reset internal fan levels
+	// ecram_write(ecram, 0xC634, 0); // CPU
+	// ecram_write(ecram, 0xC635, 0); // GPU
+	// ecram_write(ecram, 0xC636, 0); // SENSOR
+
+	return 0;
+}
+
+#define FANCURVESIZE_LOQ 10
+
+static int ec_read_fancurve_loq(struct ecram *ecram,
+				    const struct model_config *model,
+				    struct fancurve *fancurve)
+{
+	size_t i = 0;
+	size_t struct_offset = 3; // {cpu_temp: u8, rpm: u8, gpu_temp?: u8}
+
+	for (i = 0; i < FANCURVESIZE_LOQ; ++i) {
+		struct fancurve_point *point = &fancurve->points[i];
+
+		point->rpm1_raw =
+			ecram_read(ecram, model->registers->EXT_FAN1_BASE + (i * struct_offset));
+		point->rpm2_raw =
+			ecram_read(ecram, model->registers->EXT_FAN2_BASE + (i * struct_offset));
+
+		point->accel = 0;
+		point->decel = 0;
+		point->cpu_max_temp_celsius =
+			ecram_read(ecram, model->registers->EXT_CPU_TEMP + (i * struct_offset));
+		point->gpu_max_temp_celsius =
+			ecram_read(ecram, model->registers->EXT_GPU_TEMP + (i * struct_offset));
+		point->cpu_min_temp_celsius = 0;
+		point->gpu_min_temp_celsius = 0;
+		point->ic_max_temp_celsius = 0;
+		point->ic_min_temp_celsius = 0;
+	}
+
+	fancurve->size = FANCURVESIZE_LOQ;
+	fancurve->current_point_i =
+		ecram_read(ecram, model->registers->EXT_FAN_CUR_POINT);
+	fancurve->current_point_i =
+		min(fancurve->current_point_i, fancurve->size);
+	return 0;
+}
+
+static int ec_write_fancurve_loq(struct ecram *ecram,
+				     const struct model_config *model,
+				     const struct fancurve *fancurve)
+{
+	size_t i;
+	int valr1;
+	int valr2;
+	size_t struct_offset = 3; // {cpu_temp: u8, rpm: u8, gpu_temp?: u8}
+
+	for (i = 0; i < FANCURVESIZE_LOQ; ++i) {
+		const struct fancurve_point *point = &fancurve->points[i];
+
+		ecram_write(ecram, model->registers->EXT_FAN1_BASE + (i * struct_offset),
+			    point->rpm1_raw);
+		valr1 = ecram_read(ecram, model->registers->EXT_FAN1_BASE + (i * struct_offset));
+		ecram_write(ecram, model->registers->EXT_FAN2_BASE + (i * struct_offset),
+			    point->rpm2_raw);
+		valr2 = ecram_read(ecram, model->registers->EXT_FAN2_BASE + (i * struct_offset));
+		pr_info("Writing fan1: %d; reading fan1: %d\n", point->rpm1_raw,
+			valr1);
+		pr_info("Writing fan2: %d; reading fan2: %d\n", point->rpm2_raw,
+			valr2);
+
+		// write to memory and repeat 8 bytes later again
+		ecram_write(ecram, model->registers->EXT_CPU_TEMP + (i * struct_offset),
+			    point->cpu_max_temp_celsius);
+		// write to memory and repeat 8 bytes later again
+		ecram_write(ecram, model->registers->EXT_GPU_TEMP + (i * struct_offset),
+			    point->gpu_max_temp_celsius);
+	}
+
+	return 0;
+}
+
+static int read_fancurve(struct legion_private *priv, struct fancurve *fancurve)
+{
+	// TODO: use enums or function pointers?
+	switch (priv->conf->access_method_fancurve) {
+	case ACCESS_METHOD_EC:
+		return ec_read_fancurve_legion(&priv->ecram, priv->conf,
+					       fancurve);
+	case ACCESS_METHOD_EC2:
+		return ec_read_fancurve_ideapad(&priv->ecram, priv->conf,
+						fancurve);
+	case ACCESS_METHOD_EC3:
+		return ec_read_fancurve_loq(&priv->ecram, priv->conf,
+						fancurve);
+	case ACCESS_METHOD_WMI3:
+		return wmi_read_fancurve_custom(priv->conf, fancurve);
+	default:
+		pr_info("No access method for fancurve: %d\n",
+			priv->conf->access_method_fancurve);
+		return -EINVAL;
+	}
+}
+
+static int write_fancurve(struct legion_private *priv,
+			  const struct fancurve *fancurve, bool write_size)
+{
+	// TODO: use enums or function pointers?
+	switch (priv->conf->access_method_fancurve) {
+	case ACCESS_METHOD_EC:
+		return ec_write_fancurve_legion(&priv->ecram, priv->conf,
+						fancurve, write_size);
+	case ACCESS_METHOD_EC2:
+		return ec_write_fancurve_ideapad(&priv->ecram, priv->conf,
+						 fancurve);
+	case ACCESS_METHOD_EC3:
+		return ec_write_fancurve_loq(&priv->ecram, priv->conf,
+						 fancurve);
+	case ACCESS_METHOD_WMI3:
+		return wmi_write_fancurve_custom(priv->conf, fancurve);
+	default:
+		pr_info("No access method for fancurve: %d\n",
+			priv->conf->access_method_fancurve);
+		return -EINVAL;
+	}
+}
+
+#define MINIFANCUVE_ON_COOL_ON 0x04
+#define MINIFANCUVE_ON_COOL_OFF 0xA0
+
+static int ec_read_minifancurve(struct ecram *ecram,
+				const struct model_config *model, bool *state)
+{
+	int value =
+		ecram_read(ecram, model->registers->EXT_MINIFANCURVE_ON_COOL);
+
+	switch (value) {
+	case MINIFANCUVE_ON_COOL_ON:
+		*state = true;
+		break;
+	case MINIFANCUVE_ON_COOL_OFF:
+		*state = false;
+		break;
+	default:
+		pr_info("Unexpected value in MINIFANCURVE register: %d\n",
+			value);
+		return -1;
+	}
+	return 0;
+}
+
+static ssize_t ec_write_minifancurve(struct ecram *ecram,
+				     const struct model_config *model,
+				     bool state)
+{
+	u8 val = state ? MINIFANCUVE_ON_COOL_ON : MINIFANCUVE_ON_COOL_OFF;
+
+	ecram_write(ecram, model->registers->EXT_MINIFANCURVE_ON_COOL, val);
+	return 0;
+}
+
+#define EC_LOCKFANCONTROLLER_ON 8
+#define EC_LOCKFANCONTROLLER_OFF 0
+
+static ssize_t ec_write_lockfancontroller(struct ecram *ecram,
+					  const struct model_config *model,
+					  bool state)
+{
+	u8 val = state ? EC_LOCKFANCONTROLLER_ON : EC_LOCKFANCONTROLLER_OFF;
+
+	ecram_write(ecram, model->registers->EXT_LOCKFANCONTROLLER, val);
+	return 0;
+}
+
+static int ec_read_lockfancontroller(struct ecram *ecram,
+				     const struct model_config *model,
+				     bool *state)
+{
+	int value = ecram_read(ecram, model->registers->EXT_LOCKFANCONTROLLER);
+
+	switch (value) {
+	case EC_LOCKFANCONTROLLER_ON:
+		*state = true;
+		break;
+	case EC_LOCKFANCONTROLLER_OFF:
+		*state = false;
+		break;
+	default:
+		pr_info("Unexpected value in lockfanspeed register: %d\n",
+			value);
+		return -1;
+	}
+	return 0;
+}
+
+#define EC_FANFULLSPEED_ON 0x40
+#define EC_FANFULLSPEED_OFF 0x00
+
+static int ec_read_fanfullspeed(struct ecram *ecram,
+				const struct model_config *model, bool *state)
+{
+	int value = ecram_read(ecram, model->registers->EXT_MAXIMUMFANSPEED);
+
+	switch (value) {
+	case EC_FANFULLSPEED_ON:
+		*state = true;
+		break;
+	case EC_FANFULLSPEED_OFF:
+		*state = false;
+		break;
+	default:
+		pr_info("Unexpected value in maximumfanspeed register: %d\n",
+			value);
+		return -1;
+	}
+	return 0;
+}
+
+static ssize_t ec_write_fanfullspeed(struct ecram *ecram,
+				     const struct model_config *model,
+				     bool state)
+{
+	u8 val = state ? EC_FANFULLSPEED_ON : EC_FANFULLSPEED_OFF;
+
+	ecram_write(ecram, model->registers->EXT_MAXIMUMFANSPEED, val);
+	return 0;
+}
+
+static ssize_t wmi_read_fanfullspeed(struct legion_private *priv, bool *state)
+{
+	return get_simple_wmi_attribute_bool(priv, WMI_GUID_LENOVO_FAN_METHOD,
+					     0, WMI_METHOD_ID_FAN_GET_FULLSPEED,
+					     false, 1, state);
+}
+
+static ssize_t wmi_write_fanfullspeed(struct legion_private *priv, bool state)
+{
+	return set_simple_wmi_attribute(priv, WMI_GUID_LENOVO_FAN_METHOD, 0,
+					WMI_METHOD_ID_FAN_SET_FULLSPEED, false,
+					1, state);
+}
+
+static ssize_t read_fanfullspeed(struct legion_private *priv, bool *state)
+{
+	// TODO: use enums or function pointers?
+	switch (priv->conf->access_method_fanfullspeed) {
+	case ACCESS_METHOD_EC:
+		return ec_read_fanfullspeed(&priv->ecram, priv->conf, state);
+	case ACCESS_METHOD_WMI:
+		return wmi_read_fanfullspeed(priv, state);
+	default:
+		pr_info("No access method for fan full speed: %d\n",
+			priv->conf->access_method_fanfullspeed);
+		return -EINVAL;
+	}
+}
+
+static ssize_t write_fanfullspeed(struct legion_private *priv, bool state)
+{
+	ssize_t res;
+
+	switch (priv->conf->access_method_fanfullspeed) {
+	case ACCESS_METHOD_EC:
+		res = ec_write_fanfullspeed(&priv->ecram, priv->conf, state);
+		return res;
+	case ACCESS_METHOD_WMI:
+		return wmi_write_fanfullspeed(priv, state);
+	default:
+		pr_info("No access method for fan full speed: %d\n",
+			priv->conf->access_method_fanfullspeed);
+		return -EINVAL;
+	}
+}
+
+/* ============================= */
+/* Power mode reading/writing    */
+/* ============================= */
+
+enum legion_ec_powermode {
+	LEGION_EC_POWERMODE_QUIET = 2,
+	LEGION_EC_POWERMODE_BALANCED = 0,
+	LEGION_EC_POWERMODE_PERFORMANCE = 1,
+	LEGION_EC_POWERMODE_CUSTOM = 3
+};
+
+enum legion_wmi_powermode {
+	LEGION_WMI_POWERMODE_QUIET = 1,
+	LEGION_WMI_POWERMODE_BALANCED = 2,
+	LEGION_WMI_POWERMODE_PERFORMANCE = 3,
+	LEGION_WMI_POWERMODE_CUSTOM = 255
+};
+
+enum legion_wmi_powermode ec_to_wmi_powermode(int ec_mode)
+{
+	switch (ec_mode) {
+	case LEGION_EC_POWERMODE_QUIET:
+		return LEGION_WMI_POWERMODE_QUIET;
+	case LEGION_EC_POWERMODE_BALANCED:
+		return LEGION_WMI_POWERMODE_BALANCED;
+	case LEGION_EC_POWERMODE_PERFORMANCE:
+		return LEGION_WMI_POWERMODE_PERFORMANCE;
+	case LEGION_EC_POWERMODE_CUSTOM:
+		return LEGION_WMI_POWERMODE_CUSTOM;
+	default:
+		return LEGION_WMI_POWERMODE_BALANCED;
+	}
+}
+
+enum legion_ec_powermode wmi_to_ec_powermode(enum legion_wmi_powermode wmi_mode)
+{
+	switch (wmi_mode) {
+	case LEGION_WMI_POWERMODE_QUIET:
+		return LEGION_EC_POWERMODE_QUIET;
+	case LEGION_WMI_POWERMODE_BALANCED:
+		return LEGION_EC_POWERMODE_BALANCED;
+	case LEGION_WMI_POWERMODE_PERFORMANCE:
+		return LEGION_EC_POWERMODE_PERFORMANCE;
+	case LEGION_WMI_POWERMODE_CUSTOM:
+		return LEGION_EC_POWERMODE_CUSTOM;
+	default:
+		return LEGION_EC_POWERMODE_BALANCED;
+	}
+}
+
+static ssize_t ec_read_powermode(struct legion_private *priv, int *powermode)
+{
+	*powermode =
+		ecram_read(&priv->ecram, priv->conf->registers->EXT_POWERMODE);
+	return 0;
+}
+
+static ssize_t ec_write_powermode(struct legion_private *priv, u8 value)
+{
+	if (!((value >= 0 && value <= 2) || value == 255)) {
+		pr_info("Unexpected power mode value ignored: %d\n", value);
+		return -ENOMEM;
+	}
+	ecram_write(&priv->ecram, priv->conf->registers->EXT_POWERMODE, value);
+	return 0;
+}
+
+static ssize_t acpi_read_powermode(struct legion_private *priv, int *powermode)
+{
+	unsigned long acpi_powermode;
+	int err;
+
+	// spmo method not always available
+	// \_SB.PCI0.LPC0.EC0.SPMO
+	err = eval_spmo(priv->adev->handle, &acpi_powermode);
+	*powermode = (int)acpi_powermode;
+	return err;
+}
+
+static ssize_t wmi_read_powermode(int *powermode)
+{
+	int err;
+	unsigned long res;
+
+	err = wmi_exec_noarg_int(LEGION_WMI_GAMEZONE_GUID, 0,
+				 WMI_METHOD_ID_GETSMARTFANMODE, &res);
+
+	if (!err)
+		*powermode = res;
+	return err;
+}
+
+static ssize_t wmi_write_powermode(u8 value)
+{
+	if (!((value >= LEGION_WMI_POWERMODE_QUIET &&
+	       value <= LEGION_WMI_POWERMODE_PERFORMANCE) ||
+	      value == LEGION_WMI_POWERMODE_CUSTOM)) {
+		pr_info("Unexpected power mode value ignored: %d\n", value);
+		return -ENOMEM;
+	}
+	return wmi_exec_arg(LEGION_WMI_GAMEZONE_GUID, 0,
+			    WMI_METHOD_ID_SETSMARTFANMODE, &value,
+			    sizeof(value));
+}
+
+static ssize_t read_powermode(struct legion_private *priv, int *powermode)
+{
+	ssize_t res;
+
+	switch (priv->conf->access_method_powermode) {
+	case ACCESS_METHOD_EC:
+		res = ec_read_powermode(priv, powermode);
+		*powermode = ec_to_wmi_powermode(*powermode);
+		return res;
+	case ACCESS_METHOD_ACPI:
+		return acpi_read_powermode(priv, powermode);
+	case ACCESS_METHOD_WMI:
+		return wmi_read_powermode(powermode);
+	default:
+		pr_info("No access method for powermode: %d\n",
+			priv->conf->access_method_powermode);
+		return -EINVAL;
+	}
+}
+
+static ssize_t write_powermode(struct legion_private *priv,
+			       enum legion_wmi_powermode value)
+{
+	ssize_t res;
+
+	//TODO: remove again
+	pr_info("Set powermode\n");
+
+	switch (priv->conf->access_method_powermode) {
+	case ACCESS_METHOD_EC:
+		res = ec_write_powermode(priv, wmi_to_ec_powermode(value));
+		return res;
+	case ACCESS_METHOD_WMI:
+		return wmi_write_powermode(value);
+	default:
+		pr_info("No access method for powermode: %d\n",
+			priv->conf->access_method_powermode);
+		return -EINVAL;
+	}
+}
+
+/**
+ * Shortly toggle powermode to a different mode
+ * and switch back, e.g. to reset fan curve.
+ */
+static void toggle_powermode(struct legion_private *priv)
+{
+	int old_powermode;
+	int next_powermode;
+
+	read_powermode(priv, &old_powermode);
+	next_powermode = old_powermode == 0 ? 1 : 0;
+
+	write_powermode(priv, next_powermode);
+	mdelay(1500);
+	write_powermode(priv, old_powermode);
+}
+
+/* ============================= */
+/* Charging mode reading/writing */
+/* ============================- */
+
+#define FCT_RAPID_CHARGE_ON 0x07
+#define FCT_RAPID_CHARGE_OFF 0x08
+#define RAPID_CHARGE_ON 0x0
+#define RAPID_CHARGE_OFF 0x1
+
+static int acpi_read_rapidcharge(struct acpi_device *adev, bool *state)
+{
+	unsigned long result;
+	int err;
+
+	//also works? which one is better?
+	/*
+	 * err = eval_qcho(adev->handle, &result);
+	 * if (err)
+	 *  return err;
+	 * state = result;
+	 * return 0;
+	 */
+
+	err = eval_gbmd(adev->handle, &result);
+	if (err)
+		return err;
+
+	*state = result & 0x04;
+	return 0;
+}
+
+static int acpi_write_rapidcharge(struct acpi_device *adev, bool state)
+{
+	int err;
+	unsigned long fct_nr = state > 0 ? FCT_RAPID_CHARGE_ON :
+					   FCT_RAPID_CHARGE_OFF;
+
+	err = exec_sbmc(adev->handle, fct_nr);
+	pr_info("Set rapidcharge to %d by calling %lu: result: %d\n", state,
+		fct_nr, err);
+	return err;
+}
+
+/* ============================= */
+/* Keyboard backlight read/write */
+/* ============================= */
+
+static ssize_t legion_kbd_bl2_brightness_get(struct legion_private *priv)
+{
+	unsigned long state = 0;
+	int err;
+
+	err = wmi_exec_noarg_int(LEGION_WMI_GAMEZONE_GUID, 0,
+				 WMI_METHOD_ID_GETKEYBOARDLIGHT, &state);
+	if (err)
+		return -EINVAL;
+
+	return state;
+}
+
+//static int legion_kbd_bl2_brightness_set(struct legion_private *priv,
+//					 unsigned int brightness)
+//{
+//	u8 in_param = brightness;
+
+//	return wmi_exec_arg(LEGION_WMI_GAMEZONE_GUID, 0,
+//			    WMI_METHOD_ID_SETKEYBOARDLIGHT, &in_param,
+//			    sizeof(in_param));
+//}
+
+//min: 1, max: 3
+#define LIGHT_ID_KEYBOARD 0x00
+//min: 0, max: 1
+#define LIGHT_ID_YLOGO 0x03
+//min: 1, max: 2
+#define LIGHT_ID_IOPORT 0x05
+
+static int legion_wmi_light_get(struct legion_private *priv, u8 light_id,
+				unsigned int min_value, unsigned int max_value)
+{
+	struct acpi_buffer params;
+	u8 in;
+	u8 result[2];
+	u8 value;
+	int err;
+
+	params.length = 1;
+	params.pointer = &in;
+	in = light_id;
+	err = wmi_exec_ints(LEGION_WMI_KBBACKLIGHT_GUID, 0,
+			    WMI_METHOD_ID_KBBACKLIGHTGET, &params, result,
+			    ARRAY_SIZE(result));
+	if (err) {
+		pr_info("Error for WMI method call to get brightness\n");
+		return -EIO;
+	}
+
+	value = result[1];
+	if (!(value >= min_value && value <= max_value)) {
+		pr_info("Error WMI call for reading brightness: expected a value between %u and %u, but got %d\n",
+			min_value, max_value, value);
+		return -EFAULT;
+	}
+
+	return value - min_value;
+}
+
+static int legion_wmi_light_set(struct legion_private *priv, u8 light_id,
+				unsigned int min_value, unsigned int max_value,
+				unsigned int brightness)
+{
+	struct acpi_buffer buffer;
+	u8 in_buffer_param[8];
+	unsigned long result;
+	int err;
+
+	buffer.length = 3;
+	buffer.pointer = &in_buffer_param[0];
+	in_buffer_param[0] = light_id;
+	in_buffer_param[1] = 0x01;
+	in_buffer_param[2] =
+		clamp(brightness + min_value, min_value, max_value);
+
+	err = wmi_exec_int(LEGION_WMI_KBBACKLIGHT_GUID, 0,
+			   WMI_METHOD_ID_KBBACKLIGHTSET, &buffer, &result);
+	if (err) {
+		pr_info("Error for WMI method call to set brightness on light: %d\n",
+			light_id);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static int legion_kbd_bl_brightness_get(struct legion_private *priv)
+{
+	return legion_wmi_light_get(priv, LIGHT_ID_KEYBOARD, 1, 3);
+}
+
+static int legion_kbd_bl_brightness_set(struct legion_private *priv,
+					unsigned int brightness)
+{
+	return legion_wmi_light_set(priv, LIGHT_ID_KEYBOARD, 1, 3, brightness);
+}
+
+/* =============================  */
+/* debugfs interface              */
+/* ============================   */
+
+static int debugfs_ecmemory_show(struct seq_file *s, void *unused)
+{
+	struct legion_private *priv = s->private;
+	size_t offset;
+
+	for (offset = 0; offset < priv->conf->memoryio_size; ++offset) {
+		char value = ecram_read(&priv->ecram,
+					priv->conf->memoryio_physical_ec_start +
+						offset);
+
+		seq_write(s, &value, 1);
+	}
+	return 0;
+}
+
+DEFINE_SHOW_ATTRIBUTE(debugfs_ecmemory);
+
+static int debugfs_ecmemoryram_show(struct seq_file *s, void *unused)
+{
+	struct legion_private *priv = s->private;
+	size_t offset;
+	ssize_t err;
+	u8 value;
+
+	for (offset = 0; offset < priv->conf->ramio_size; ++offset) {
+		err = ecram_memoryio_read(&priv->ec_memoryio, offset, &value);
+		if (!err)
+			seq_write(s, &value, 1);
+		else
+			return -EACCES;
+	}
+	return 0;
+}
+
+DEFINE_SHOW_ATTRIBUTE(debugfs_ecmemoryram);
+
+//TODO: make (almost) all methods static
+
+static void seq_file_print_with_error(struct seq_file *s, const char *name,
+				      ssize_t err, int value)
+{
+	seq_printf(s, "%s error: %ld\n", name, err);
+	seq_printf(s, "%s: %d\n", name, value);
+}
+
+static int debugfs_fancurve_show(struct seq_file *s, void *unused)
+{
+	struct legion_private *priv = s->private;
+	bool is_minifancurve;
+	bool is_lockfancontroller;
+	bool is_maximumfanspeed;
+	bool is_rapidcharge = false;
+	int powermode;
+	int temperature;
+	int fanspeed;
+	int err;
+	unsigned long cfg;
+	struct fancurve wmi_fancurve;
+	//int kb_backlight;
+
+	mutex_lock(&priv->fancurve_mutex);
+
+	seq_printf(s, "EC Chip ID: %x\n", read_ec_id(&priv->ecram, priv->conf));
+	seq_printf(s, "EC Chip Version: %x\n",
+		   read_ec_version(&priv->ecram, priv->conf));
+	seq_printf(s, "legion_laptop features: %s\n", LEGIONFEATURES);
+	seq_printf(s, "legion_laptop ec_readonly: %d\n", ec_readonly);
+
+	err = eval_int(priv->adev->handle, "VPC0._CFG", &cfg);
+	seq_printf(s, "ACPI CFG error: %d\n", err);
+	seq_printf(s, "ACPI CFG: %lu\n", cfg);
+
+	seq_printf(s, "temperature access method: %d\n",
+		   priv->conf->access_method_temperature);
+	err = read_temperature(priv, 0, &temperature);
+	seq_file_print_with_error(s, "CPU temperature", err, temperature);
+	err = ec_read_temperature(&priv->ecram, priv->conf, 0, &temperature);
+	seq_file_print_with_error(s, "CPU temperature EC", err, temperature);
+	err = acpi_read_temperature(priv, 0, &temperature);
+	seq_file_print_with_error(s, "CPU temperature ACPI", err, temperature);
+	err = wmi_read_temperature_gz(0, &temperature);
+	seq_file_print_with_error(s, "CPU temperature WMI", err, temperature);
+	err = wmi_read_temperature(0, &temperature);
+	seq_file_print_with_error(s, "CPU temperature WMI2", err, temperature);
+	err = wmi_read_temperature_other(0, &temperature);
+	seq_file_print_with_error(s, "CPU temperature WMI3", err, temperature);
+
+	err = read_temperature(priv, 1, &temperature);
+	seq_file_print_with_error(s, "GPU temperature", err, temperature);
+	err = ec_read_temperature(&priv->ecram, priv->conf, 1, &temperature);
+	seq_file_print_with_error(s, "GPU temperature EC", err, temperature);
+	err = acpi_read_temperature(priv, 1, &temperature);
+	seq_file_print_with_error(s, "GPU temperature ACPI", err, temperature);
+	err = wmi_read_temperature_gz(1, &temperature);
+	seq_file_print_with_error(s, "GPU temperature WMI", err, temperature);
+	err = wmi_read_temperature(1, &temperature);
+	seq_file_print_with_error(s, "GPU temperature WMI2", err, temperature);
+	err = wmi_read_temperature_other(1, &temperature);
+	seq_file_print_with_error(s, "GPU temperature WMI3", err, temperature);
+
+	seq_printf(s, "fan speed access method: %d\n",
+		   priv->conf->access_method_fanspeed);
+	err = read_fanspeed(priv, 0, &fanspeed);
+	seq_file_print_with_error(s, "1 fanspeed", err, fanspeed);
+	err = ec_read_fanspeed(&priv->ecram, priv->conf, 0, &fanspeed);
+	seq_file_print_with_error(s, "1 fanspeed EC", err, fanspeed);
+	err = acpi_read_fanspeed(priv, 0, &fanspeed);
+	seq_file_print_with_error(s, "1 fanspeed ACPI", err, fanspeed);
+	err = wmi_read_fanspeed_gz(0, &fanspeed);
+	seq_file_print_with_error(s, "1 fanspeed WMI", err, fanspeed);
+	err = wmi_read_fanspeed(0, &fanspeed);
+	seq_file_print_with_error(s, "1 fanspeed WMI2", err, fanspeed);
+	err = wmi_read_fanspeed_other(0, &fanspeed);
+	seq_file_print_with_error(s, "1 fanspeed WMI3", err, fanspeed);
+
+	err = read_fanspeed(priv, 1, &fanspeed);
+	seq_file_print_with_error(s, "2 fanspeed", err, fanspeed);
+	err = ec_read_fanspeed(&priv->ecram, priv->conf, 1, &fanspeed);
+	seq_file_print_with_error(s, "2 fanspeed EC", err, fanspeed);
+	err = acpi_read_fanspeed(priv, 1, &fanspeed);
+	seq_file_print_with_error(s, "2 fanspeed ACPI", err, fanspeed);
+	err = wmi_read_fanspeed_gz(1, &fanspeed);
+	seq_file_print_with_error(s, "2 fanspeed WMI", err, fanspeed);
+	err = wmi_read_fanspeed(1, &fanspeed);
+	seq_file_print_with_error(s, "2 fanspeed WMI2", err, fanspeed);
+	err = wmi_read_fanspeed_other(1, &fanspeed);
+	seq_file_print_with_error(s, "2 fanspeed WMI3", err, fanspeed);
+
+	seq_printf(s, "powermode access method: %d\n",
+		   priv->conf->access_method_powermode);
+	err = read_powermode(priv, &powermode);
+	seq_file_print_with_error(s, "powermode", err, powermode);
+	err = ec_read_powermode(priv, &powermode);
+	seq_file_print_with_error(s, "powermode EC", err, powermode);
+	err = acpi_read_powermode(priv, &powermode);
+	seq_file_print_with_error(s, "powermode ACPI", err, powermode);
+	err = wmi_read_powermode(&powermode);
+	seq_file_print_with_error(s, "powermode WMI", err, powermode);
+	seq_printf(s, "has custom powermode: %d\n",
+		   priv->conf->has_custom_powermode);
+
+	err = acpi_read_rapidcharge(priv->adev, &is_rapidcharge);
+	seq_printf(s, "ACPI rapidcharge error: %d\n", err);
+	seq_printf(s, "ACPI rapidcharge: %d\n", is_rapidcharge);
+
+	seq_printf(s, "WMI backlight 2 state: %ld\n",
+		   legion_kbd_bl2_brightness_get(priv));
+	seq_printf(s, "WMI backlight 3 state: %d\n",
+		   legion_kbd_bl_brightness_get(priv));
+
+	seq_printf(s, "WMI light IO port: %d\n",
+		   legion_wmi_light_get(priv, LIGHT_ID_IOPORT, 0, 4));
+
+	seq_printf(s, "WMI light Y logo/lid: %d\n",
+		   legion_wmi_light_get(priv, LIGHT_ID_YLOGO, 0, 4));
+
+	seq_printf(s, "EC minifancurve feature enabled: %d\n",
+		   priv->conf->has_minifancurve);
+	err = ec_read_minifancurve(&priv->ecram, priv->conf, &is_minifancurve);
+	seq_printf(s, "EC minifancurve on cool: %s\n",
+		   err ? "error" : (is_minifancurve ? "true" : "false"));
+
+	err = ec_read_lockfancontroller(&priv->ecram, priv->conf,
+					&is_lockfancontroller);
+	seq_printf(s, "EC lockfancontroller error: %d\n", err);
+	seq_printf(s, "EC lockfancontroller: %s\n",
+		   err ? "error" : (is_lockfancontroller ? "true" : "false"));
+
+	err = read_fanfullspeed(priv, &is_maximumfanspeed);
+	seq_file_print_with_error(s, "fanfullspeed", err, is_maximumfanspeed);
+
+	err = ec_read_fanfullspeed(&priv->ecram, priv->conf,
+				   &is_maximumfanspeed);
+	seq_file_print_with_error(s, "fanfullspeed EC", err,
+				  is_maximumfanspeed);
+
+	read_fancurve(priv, &priv->fancurve);
+	seq_printf(s, "EC fan curve current point id: %ld\n",
+		   priv->fancurve.current_point_i);
+	seq_printf(s, "EC fan curve points size: %ld\n", priv->fancurve.size);
+
+	seq_puts(s, "Current fan curve in hardware:\n");
+	fancurve_print_seqfile(&priv->fancurve, s);
+	seq_puts(s, "=====================\n");
+	mutex_unlock(&priv->fancurve_mutex);
+
+	seq_puts(s, "Current fan curve in hardware (WMI; might be empty)\n");
+	wmi_fancurve.size = 0;
+	err = wmi_read_fancurve_custom(priv->conf, &wmi_fancurve);
+	fancurve_print_seqfile(&wmi_fancurve, s);
+	seq_puts(s, "=====================\n");
+	return 0;
+}
+
+DEFINE_SHOW_ATTRIBUTE(debugfs_fancurve);
+
+static void legion_debugfs_init(struct legion_private *priv)
+{
+	struct dentry *dir;
+
+	// TODO: remove this note
+	// Note: like other kernel modules, do not catch errors here
+	// because if kernel is build without debugfs this
+	// will return an error but module still has to
+	// work, just without debugfs
+	// TODO: what permissions; some modules do 400
+	// other do 444
+	dir = debugfs_create_dir(LEGION_DRVR_SHORTNAME, NULL);
+	debugfs_create_file("fancurve", 0444, dir, priv,
+			    &debugfs_fancurve_fops);
+	debugfs_create_file("ecmemory", 0444, dir, priv,
+			    &debugfs_ecmemory_fops);
+	debugfs_create_file("ecmemoryram", 0444, dir, priv,
+			    &debugfs_ecmemoryram_fops);
+
+	priv->debugfs_dir = dir;
+}
+
+static void legion_debugfs_exit(struct legion_private *priv)
+{
+	pr_info("Unloading legion dubugfs\n");
+	// The following is does nothing if pointer is NULL
+	debugfs_remove_recursive(priv->debugfs_dir);
+	priv->debugfs_dir = NULL;
+	pr_info("Unloading legion dubugfs done\n");
+}
+
+/* =============================  */
+/* sysfs interface                */
+/* ============================   */
+
+static int show_simple_wmi_attribute(struct device *dev,
+				     struct device_attribute *attr, char *buf,
+				     const char *guid, u8 instance,
+				     u32 method_id, bool invert,
+				     unsigned long scale)
+{
+	unsigned long state = 0;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = get_simple_wmi_attribute(priv, guid, instance, method_id, invert,
+				       scale, &state);
+	mutex_unlock(&priv->fancurve_mutex);
+
+	if (err)
+		return -EINVAL;
+
+	return sysfs_emit(buf, "%lu\n", state);
+}
+
+static int show_simple_wmi_attribute_from_buffer(struct device *dev,
+						 struct device_attribute *attr,
+						 char *buf, const char *guid,
+						 u8 instance, u32 method_id,
+						 size_t ressize, size_t i,
+						 int scale)
+{
+	u8 res[16];
+	int err;
+	int out;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	if (ressize > ARRAY_SIZE(res)) {
+		pr_info("Buffer too small for WMI result\n");
+		return -EINVAL;
+	}
+	if (i >= ressize) {
+		pr_info("Index not within buffer size\n");
+		return -EINVAL;
+	}
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = wmi_exec_noarg_ints(guid, instance, method_id, res, ressize);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	out = scale * res[i];
+	return sysfs_emit(buf, "%d\n", out);
+}
+
+static int store_simple_wmi_attribute(struct device *dev,
+				      struct device_attribute *attr,
+				      const char *buf, size_t count,
+				      const char *guid, u8 instance,
+				      u32 method_id, bool invert, int scale)
+{
+	int state;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	err = kstrtouint(buf, 0, &state);
+	if (err)
+		return err;
+	err = set_simple_wmi_attribute(priv, guid, instance, method_id, invert,
+				       scale, state);
+	if (err)
+		return err;
+	return count;
+}
+
+static ssize_t lockfancontroller_show(struct device *dev,
+				      struct device_attribute *attr, char *buf)
+{
+	struct legion_private *priv = dev_get_drvdata(dev);
+	bool is_lockfancontroller;
+	int err;
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = ec_read_lockfancontroller(&priv->ecram, priv->conf,
+					&is_lockfancontroller);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	return sysfs_emit(buf, "%d\n", is_lockfancontroller);
+}
+
+static ssize_t lockfancontroller_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	struct legion_private *priv = dev_get_drvdata(dev);
+	bool is_lockfancontroller;
+	int err;
+
+	err = kstrtobool(buf, &is_lockfancontroller);
+	if (err)
+		return err;
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = ec_write_lockfancontroller(&priv->ecram, priv->conf,
+					 is_lockfancontroller);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(lockfancontroller);
+
+static ssize_t rapidcharge_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	bool state = false;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = acpi_read_rapidcharge(priv->adev, &state);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	return sysfs_emit(buf, "%d\n", state);
+}
+
+static ssize_t rapidcharge_store(struct device *dev,
+				 struct device_attribute *attr, const char *buf,
+				 size_t count)
+{
+	struct legion_private *priv = dev_get_drvdata(dev);
+	int state;
+	int err;
+
+	err = kstrtouint(buf, 0, &state);
+	if (err)
+		return err;
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = acpi_write_rapidcharge(priv->adev, state);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(rapidcharge);
+
+static ssize_t issupportgpuoc_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_ISSUPPORTGPUOC, false,
+					 1);
+}
+
+static DEVICE_ATTR_RO(issupportgpuoc);
+
+static ssize_t aslcodeversion_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETVERSION, false, 1);
+}
+
+static DEVICE_ATTR_RO(aslcodeversion);
+
+static ssize_t issupportcpuoc_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_ISSUPPORTCPUOC, false,
+					 1);
+}
+
+static DEVICE_ATTR_RO(issupportcpuoc);
+
+static ssize_t winkey_show(struct device *dev, struct device_attribute *attr,
+			   char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETWINKEYSTATUS, true,
+					 1);
+}
+
+static ssize_t winkey_store(struct device *dev, struct device_attribute *attr,
+			    const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  LEGION_WMI_GAMEZONE_GUID, 0,
+					  WMI_METHOD_ID_SETWINKEYSTATUS, true,
+					  1);
+}
+
+static DEVICE_ATTR_RW(winkey);
+
+// on newer models the touchpad feature in ideapad does not work anymore, so
+// we need this
+static ssize_t touchpad_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETTPSTATUS, true, 1);
+}
+
+static ssize_t touchpad_store(struct device *dev, struct device_attribute *attr,
+			      const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  LEGION_WMI_GAMEZONE_GUID, 0,
+					  WMI_METHOD_ID_SETTPSTATUS, true, 1);
+}
+
+static DEVICE_ATTR_RW(touchpad);
+
+static ssize_t gsync_show(struct device *dev, struct device_attribute *attr,
+			  char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETGSYNCSTATUS, true, 1);
+}
+
+static ssize_t gsync_store(struct device *dev, struct device_attribute *attr,
+			   const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  LEGION_WMI_GAMEZONE_GUID, 0,
+					  WMI_METHOD_ID_SETGSYNCSTATUS, true,
+					  1);
+}
+
+static DEVICE_ATTR_RW(gsync);
+
+static ssize_t powerchargemode_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETPOWERCHARGEMODE,
+					 false, 1);
+}
+static DEVICE_ATTR_RO(powerchargemode);
+
+static ssize_t overdrive_show(struct device *dev, struct device_attribute *attr,
+			      char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETODSTATUS, false, 1);
+}
+
+static ssize_t overdrive_store(struct device *dev,
+			       struct device_attribute *attr, const char *buf,
+			       size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  LEGION_WMI_GAMEZONE_GUID, 0,
+					  WMI_METHOD_ID_SETODSTATUS, false, 1);
+}
+
+static DEVICE_ATTR_RW(overdrive);
+
+static ssize_t thermalmode_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETTHERMALMODE, false,
+					 1);
+}
+static DEVICE_ATTR_RO(thermalmode);
+
+// TOOD: probably remove again because provided by other means; only useful for overclocking
+static ssize_t cpumaxfrequency_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETCPUMAXFREQUENCY,
+					 false, 1);
+}
+static DEVICE_ATTR_RO(cpumaxfrequency);
+
+static ssize_t isacfitforoc_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_ISACFITFOROC, false, 1);
+}
+static DEVICE_ATTR_RO(isacfitforoc);
+
+static ssize_t igpumode_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 LEGION_WMI_GAMEZONE_GUID, 0,
+					 WMI_METHOD_ID_GETIGPUMODESTATUS, false,
+					 1);
+}
+
+static ssize_t igpumode_store(struct device *dev, struct device_attribute *attr,
+			      const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  LEGION_WMI_GAMEZONE_GUID, 0,
+					  WMI_METHOD_ID_SETIGPUMODESTATUS,
+					  false, 1);
+}
+
+static DEVICE_ATTR_RW(igpumode);
+
+static ssize_t cpu_oc_show(struct device *dev, struct device_attribute *attr,
+			   char *buf)
+{
+	return show_simple_wmi_attribute_from_buffer(
+		dev, attr, buf, WMI_GUID_LENOVO_CPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_GET_OC_STATUS, 16, 0, 1);
+}
+
+static ssize_t cpu_oc_store(struct device *dev, struct device_attribute *attr,
+			    const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  WMI_GUID_LENOVO_CPU_METHOD, 0,
+					  WMI_METHOD_ID_CPU_SET_OC_STATUS,
+					  false, 1);
+}
+
+static DEVICE_ATTR_RW(cpu_oc);
+
+static ssize_t cpu_shortterm_powerlimit_show(struct device *dev,
+					     struct device_attribute *attr,
+					     char *buf)
+{
+	return show_simple_wmi_attribute_from_buffer(
+		dev, attr, buf, WMI_GUID_LENOVO_CPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_GET_SHORTTERM_POWERLIMIT, 16, 0, 1);
+}
+
+static ssize_t cpu_shortterm_powerlimit_store(struct device *dev,
+					      struct device_attribute *attr,
+					      const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(
+		dev, attr, buf, count, WMI_GUID_LENOVO_CPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_SET_SHORTTERM_POWERLIMIT, false, 1);
+}
+
+static DEVICE_ATTR_RW(cpu_shortterm_powerlimit);
+
+static ssize_t cpu_longterm_powerlimit_show(struct device *dev,
+					    struct device_attribute *attr,
+					    char *buf)
+{
+	return show_simple_wmi_attribute_from_buffer(
+		dev, attr, buf, WMI_GUID_LENOVO_CPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_GET_LONGTERM_POWERLIMIT, 16, 0, 1);
+}
+
+static ssize_t cpu_longterm_powerlimit_store(struct device *dev,
+					     struct device_attribute *attr,
+					     const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(
+		dev, attr, buf, count, WMI_GUID_LENOVO_CPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_SET_LONGTERM_POWERLIMIT, false, 1);
+}
+
+static DEVICE_ATTR_RW(cpu_longterm_powerlimit);
+
+static ssize_t cpu_default_powerlimit_show(struct device *dev,
+					   struct device_attribute *attr,
+					   char *buf)
+{
+	return show_simple_wmi_attribute(
+		dev, attr, buf, WMI_GUID_LENOVO_CPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_GET_DEFAULT_POWERLIMIT, false, 1);
+}
+
+static DEVICE_ATTR_RO(cpu_default_powerlimit);
+
+static ssize_t cpu_peak_powerlimit_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 WMI_GUID_LENOVO_GPU_METHOD, 0,
+					 WMI_METHOD_ID_CPU_GET_PEAK_POWERLIMIT,
+					 false, 1);
+}
+
+static ssize_t cpu_peak_powerlimit_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  WMI_GUID_LENOVO_GPU_METHOD, 0,
+					  WMI_METHOD_ID_CPU_SET_PEAK_POWERLIMIT,
+					  false, 1);
+}
+
+static DEVICE_ATTR_RW(cpu_peak_powerlimit);
+
+static ssize_t cpu_apu_sppt_powerlimit_show(struct device *dev,
+					    struct device_attribute *attr,
+					    char *buf)
+{
+	return show_simple_wmi_attribute(
+		dev, attr, buf, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_GET_APU_SPPT_POWERLIMIT, false, 1);
+}
+
+static ssize_t cpu_apu_sppt_powerlimit_store(struct device *dev,
+					     struct device_attribute *attr,
+					     const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(
+		dev, attr, buf, count, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_SET_APU_SPPT_POWERLIMIT, false, 1);
+}
+
+static DEVICE_ATTR_RW(cpu_apu_sppt_powerlimit);
+
+static ssize_t cpu_cross_loading_powerlimit_show(struct device *dev,
+						 struct device_attribute *attr,
+						 char *buf)
+{
+	return show_simple_wmi_attribute(
+		dev, attr, buf, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_GET_CROSS_LOADING_POWERLIMIT, false, 1);
+}
+
+static ssize_t cpu_cross_loading_powerlimit_store(struct device *dev,
+						  struct device_attribute *attr,
+						  const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(
+		dev, attr, buf, count, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_CPU_SET_CROSS_LOADING_POWERLIMIT, false, 1);
+}
+
+static DEVICE_ATTR_RW(cpu_cross_loading_powerlimit);
+
+static ssize_t gpu_oc_show(struct device *dev, struct device_attribute *attr,
+			   char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 WMI_GUID_LENOVO_GPU_METHOD, 0,
+					 WMI_METHOD_ID_GPU_GET_OC_STATUS, false,
+					 1);
+}
+
+static ssize_t gpu_oc_store(struct device *dev, struct device_attribute *attr,
+			    const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  WMI_GUID_LENOVO_GPU_METHOD, 0,
+					  WMI_METHOD_ID_GPU_SET_OC_STATUS,
+					  false, 1);
+}
+
+static DEVICE_ATTR_RW(gpu_oc);
+
+static ssize_t gpu_ppab_powerlimit_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	return show_simple_wmi_attribute_from_buffer(
+		dev, attr, buf, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_GPU_GET_PPAB_POWERLIMIT, 16, 0, 1);
+}
+
+static ssize_t gpu_ppab_powerlimit_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  WMI_GUID_LENOVO_GPU_METHOD, 0,
+					  WMI_METHOD_ID_GPU_SET_PPAB_POWERLIMIT,
+					  false, 1);
+}
+
+static DEVICE_ATTR_RW(gpu_ppab_powerlimit);
+
+static ssize_t gpu_ctgp_powerlimit_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	return show_simple_wmi_attribute_from_buffer(
+		dev, attr, buf, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_GPU_GET_CTGP_POWERLIMIT, 16, 0, 1);
+}
+
+static ssize_t gpu_ctgp_powerlimit_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  WMI_GUID_LENOVO_GPU_METHOD, 0,
+					  WMI_METHOD_ID_GPU_SET_CTGP_POWERLIMIT,
+					  false, 1);
+}
+
+static DEVICE_ATTR_RW(gpu_ctgp_powerlimit);
+
+static ssize_t gpu_ctgp2_powerlimit_show(struct device *dev,
+					 struct device_attribute *attr,
+					 char *buf)
+{
+	return show_simple_wmi_attribute_from_buffer(
+		dev, attr, buf, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_GPU_GET_CTGP_POWERLIMIT, 16, 0x0C, 1);
+}
+
+static DEVICE_ATTR_RO(gpu_ctgp2_powerlimit);
+
+// TOOD: probably remove again because provided by other means; only useful for overclocking
+static ssize_t
+gpu_default_ppab_ctrgp_powerlimit_show(struct device *dev,
+				       struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(
+		dev, attr, buf, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_GPU_GET_DEFAULT_PPAB_CTGP_POWERLIMIT, false, 1);
+}
+static DEVICE_ATTR_RO(gpu_default_ppab_ctrgp_powerlimit);
+
+static ssize_t gpu_temperature_limit_show(struct device *dev,
+					  struct device_attribute *attr,
+					  char *buf)
+{
+	return show_simple_wmi_attribute(
+		dev, attr, buf, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_GPU_GET_TEMPERATURE_LIMIT, false, 1);
+}
+
+static ssize_t gpu_temperature_limit_store(struct device *dev,
+					   struct device_attribute *attr,
+					   const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(
+		dev, attr, buf, count, WMI_GUID_LENOVO_GPU_METHOD, 0,
+		WMI_METHOD_ID_GPU_SET_TEMPERATURE_LIMIT, false, 1);
+}
+
+static DEVICE_ATTR_RW(gpu_temperature_limit);
+
+// TOOD: probably remove again because provided by other means; only useful for overclocking
+static ssize_t gpu_boost_clock_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 WMI_GUID_LENOVO_GPU_METHOD, 0,
+					 WMI_METHOD_ID_GPU_GET_BOOST_CLOCK,
+					 false, 1);
+}
+static DEVICE_ATTR_RO(gpu_boost_clock);
+
+static ssize_t fan_fullspeed_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	bool state = false;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = read_fanfullspeed(priv, &state);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	return sysfs_emit(buf, "%d\n", state);
+}
+
+static ssize_t fan_fullspeed_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count)
+{
+	struct legion_private *priv = dev_get_drvdata(dev);
+	int state;
+	int err;
+
+	err = kstrtouint(buf, 0, &state);
+	if (err)
+		return err;
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = write_fanfullspeed(priv, state);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(fan_fullspeed);
+
+static ssize_t fan_maxspeed_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	return show_simple_wmi_attribute(dev, attr, buf,
+					 WMI_GUID_LENOVO_FAN_METHOD, 0,
+					 WMI_METHOD_ID_FAN_GET_MAXSPEED, false,
+					 1);
+}
+
+static ssize_t fan_maxspeed_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t count)
+{
+	return store_simple_wmi_attribute(dev, attr, buf, count,
+					  WMI_GUID_LENOVO_FAN_METHOD, 0,
+					  WMI_METHOD_ID_FAN_SET_MAXSPEED, false,
+					  1);
+}
+
+static DEVICE_ATTR_RW(fan_maxspeed);
+
+static ssize_t powermode_show(struct device *dev, struct device_attribute *attr,
+			      char *buf)
+{
+	struct legion_private *priv = dev_get_drvdata(dev);
+	int power_mode;
+
+	mutex_lock(&priv->fancurve_mutex);
+	read_powermode(priv, &power_mode);
+	mutex_unlock(&priv->fancurve_mutex);
+	return sysfs_emit(buf, "%d\n", power_mode);
+}
+
+static void legion_platform_profile_notify(void);
+
+static ssize_t powermode_store(struct device *dev,
+			       struct device_attribute *attr, const char *buf,
+			       size_t count)
+{
+	struct legion_private *priv = dev_get_drvdata(dev);
+	int powermode;
+	int err;
+
+	err = kstrtouint(buf, 0, &powermode);
+	if (err)
+		return err;
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = write_powermode(priv, powermode);
+	mutex_unlock(&priv->fancurve_mutex);
+	if (err)
+		return -EINVAL;
+
+	// TODO: better?
+	// we have to wait a bit before change is done in hardware and
+	// readback done after notifying returns correct value, otherwise
+	// the notified reader will read old value
+	msleep(500);
+	legion_platform_profile_notify();
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(powermode);
+
+static struct attribute *legion_sysfs_attributes[] = {
+	&dev_attr_powermode.attr,
+	&dev_attr_lockfancontroller.attr,
+	&dev_attr_rapidcharge.attr,
+	&dev_attr_winkey.attr,
+	&dev_attr_touchpad.attr,
+	&dev_attr_gsync.attr,
+	&dev_attr_powerchargemode.attr,
+	&dev_attr_overdrive.attr,
+	&dev_attr_cpumaxfrequency.attr,
+	&dev_attr_isacfitforoc.attr,
+	&dev_attr_cpu_oc.attr,
+	&dev_attr_cpu_shortterm_powerlimit.attr,
+	&dev_attr_cpu_longterm_powerlimit.attr,
+	&dev_attr_cpu_apu_sppt_powerlimit.attr,
+	&dev_attr_cpu_default_powerlimit.attr,
+	&dev_attr_cpu_peak_powerlimit.attr,
+	&dev_attr_cpu_cross_loading_powerlimit.attr,
+	&dev_attr_gpu_oc.attr,
+	&dev_attr_gpu_ppab_powerlimit.attr,
+	&dev_attr_gpu_ctgp_powerlimit.attr,
+	&dev_attr_gpu_ctgp2_powerlimit.attr,
+	&dev_attr_gpu_default_ppab_ctrgp_powerlimit.attr,
+	&dev_attr_gpu_temperature_limit.attr,
+	&dev_attr_gpu_boost_clock.attr,
+	&dev_attr_fan_fullspeed.attr,
+	&dev_attr_fan_maxspeed.attr,
+	&dev_attr_thermalmode.attr,
+	&dev_attr_issupportcpuoc.attr,
+	&dev_attr_issupportgpuoc.attr,
+	&dev_attr_aslcodeversion.attr,
+	&dev_attr_igpumode.attr,
+	NULL
+};
+
+static const struct attribute_group legion_attribute_group = {
+	.attrs = legion_sysfs_attributes
+};
+
+static int legion_sysfs_init(struct legion_private *priv)
+{
+	return device_add_group(&priv->platform_device->dev,
+				&legion_attribute_group);
+}
+
+static void legion_sysfs_exit(struct legion_private *priv)
+{
+	pr_info("Unloading legion sysfs\n");
+	device_remove_group(&priv->platform_device->dev,
+			    &legion_attribute_group);
+	pr_info("Unloading legion sysfs done\n");
+}
+
+/* =============================  */
+/* WMI + ACPI                     */
+/* ============================   */
+// heavily based on ideapad_laptop.c
+
+// TODO: proper names if meaning of all events is clear
+enum LEGION_WMI_EVENT {
+	LEGION_WMI_EVENT_GAMEZONE = 1,
+	LEGION_EVENT_A,
+	LEGION_EVENT_B,
+	LEGION_EVENT_C,
+	LEGION_EVENT_D,
+	LEGION_EVENT_E,
+	LEGION_EVENT_F,
+	LEGION_EVENT_G
+};
+
+struct legion_wmi_private {
+	enum LEGION_WMI_EVENT event;
+};
+
+//static void legion_wmi_notify2(u32 value, void *context)
+//    {
+//	pr_info("WMI notify\n" );
+//    }
+
+static void legion_wmi_notify(struct wmi_device *wdev, union acpi_object *data)
+{
+	struct legion_wmi_private *wpriv;
+	struct legion_private *priv;
+
+	mutex_lock(&legion_shared_mutex);
+	priv = legion_shared;
+	if ((!priv) && (priv->loaded)) {
+		pr_info("Received WMI event while not initialized!\n");
+		goto unlock;
+	}
+
+	wpriv = dev_get_drvdata(&wdev->dev);
+	switch (wpriv->event) {
+	case LEGION_EVENT_A:
+		pr_info("Fan event: legion type: %d;  acpi type: %d (%d=integer)",
+			wpriv->event, data->type, ACPI_TYPE_INTEGER);
+		// TODO: here it is too early (first unlock mutext, then wait a bit)
+		//legion_platform_profile_notify();
+		break;
+	default:
+		pr_info("Event: legion type: %d;  acpi type: %d (%d=integer)",
+			wpriv->event, data->type, ACPI_TYPE_INTEGER);
+		break;
+	}
+
+unlock:
+	mutex_unlock(&legion_shared_mutex);
+	// todo; fix that!
+	// problem: we get an event just before the powermode change (from the key?),
+	// so if we notify too early, it will read the old power mode/platform profile
+	msleep(500);
+	legion_platform_profile_notify();
+}
+
+static int legion_wmi_probe(struct wmi_device *wdev, const void *context)
+{
+	struct legion_wmi_private *wpriv;
+
+	wpriv = devm_kzalloc(&wdev->dev, sizeof(*wpriv), GFP_KERNEL);
+	if (!wpriv)
+		return -ENOMEM;
+
+	*wpriv = *(const struct legion_wmi_private *)context;
+
+	dev_set_drvdata(&wdev->dev, wpriv);
+	dev_info(&wdev->dev, "Register after probing for WMI.\n");
+	return 0;
+}
+
+static const struct legion_wmi_private legion_wmi_context_gamezone = {
+	.event = LEGION_WMI_EVENT_GAMEZONE
+};
+static const struct legion_wmi_private legion_wmi_context_a = {
+	.event = LEGION_EVENT_A
+};
+static const struct legion_wmi_private legion_wmi_context_b = {
+	.event = LEGION_EVENT_B
+};
+static const struct legion_wmi_private legion_wmi_context_c = {
+	.event = LEGION_EVENT_C
+};
+static const struct legion_wmi_private legion_wmi_context_d = {
+	.event = LEGION_EVENT_D
+};
+static const struct legion_wmi_private legion_wmi_context_e = {
+	.event = LEGION_EVENT_E
+};
+static const struct legion_wmi_private legion_wmi_context_f = {
+	.event = LEGION_EVENT_F
+};
+
+#define LEGION_WMI_GUID_FAN_EVENT "D320289E-8FEA-41E0-86F9-611D83151B5F"
+#define LEGION_WMI_GUID_FAN2_EVENT "bc72a435-e8c1-4275-b3e2-d8b8074aba59"
+#define LEGION_WMI_GUID_GAMEZONE_KEY_EVENT \
+	"10afc6d9-ea8b-4590-a2e7-1cd3c84bb4b1"
+#define LEGION_WMI_GUID_GAMEZONE_GPU_EVENT \
+	"bfd42481-aee3-4502-a107-afb68425c5f8"
+#define LEGION_WMI_GUID_GAMEZONE_OC_EVENT "d062906b-12d4-4510-999d-4831ee80e985"
+#define LEGION_WMI_GUID_GAMEZONE_TEMP_EVENT \
+	"bfd42481-aee3-4501-a107-afb68425c5f8"
+//#define LEGION_WMI_GUID_GAMEZONE_DATA_EVENT  "887b54e3-dddc-4b2c-8b88-68a26a8835d0"
+
+static const struct wmi_device_id legion_wmi_ids[] = {
+	{ LEGION_WMI_GAMEZONE_GUID, &legion_wmi_context_gamezone },
+	{ LEGION_WMI_GUID_FAN_EVENT, &legion_wmi_context_a },
+	{ LEGION_WMI_GUID_FAN2_EVENT, &legion_wmi_context_b },
+	{ LEGION_WMI_GUID_GAMEZONE_KEY_EVENT, &legion_wmi_context_c },
+	{ LEGION_WMI_GUID_GAMEZONE_GPU_EVENT, &legion_wmi_context_d },
+	{ LEGION_WMI_GUID_GAMEZONE_OC_EVENT, &legion_wmi_context_e },
+	{ LEGION_WMI_GUID_GAMEZONE_TEMP_EVENT, &legion_wmi_context_f },
+	{ "8FC0DE0C-B4E4-43FD-B0F3-8871711C1294",
+	  &legion_wmi_context_gamezone }, /* Legion 5 */
+	{},
+};
+MODULE_DEVICE_TABLE(wmi, legion_wmi_ids);
+
+static struct wmi_driver legion_wmi_driver = {
+	.driver = {
+		.name = "legion_wmi",
+	},
+	.id_table = legion_wmi_ids,
+	.probe = legion_wmi_probe,
+	.notify = legion_wmi_notify,
+};
+
+//acpi_status status = wmi_install_notify_handler(LEGION_WMI_GAMEZONE_GUID,
+//				legion_wmi_notify2, NULL);
+//if (ACPI_FAILURE(status)) {
+//    return -ENODEV;
+//}
+//return 0;
+
+static int legion_wmi_init(void)
+{
+	return wmi_driver_register(&legion_wmi_driver);
+}
+
+static void legion_wmi_exit(void)
+{
+	// TODO: remove this
+	pr_info("Unloading legion WMI\n");
+
+	//wmi_remove_notify_handler(LEGION_WMI_GAMEZONE_GUID);
+	wmi_driver_unregister(&legion_wmi_driver);
+	pr_info("Unloading legion WMI done\n");
+}
+
+/* =============================  */
+/* Platform profile               */
+/* ============================   */
+
+static void legion_platform_profile_notify(void)
+{
+	if (!enable_platformprofile)
+		pr_info("Skipping platform_profile_notify because enable_platformprofile is false\n");
+
+	platform_profile_notify();
+}
+
+static int legion_platform_profile_get(struct platform_profile_handler *pprof,
+				       enum platform_profile_option *profile)
+{
+	int powermode;
+	struct legion_private *priv;
+
+	priv = container_of(pprof, struct legion_private,
+			    platform_profile_handler);
+	read_powermode(priv, &powermode);
+
+	switch (powermode) {
+	case LEGION_WMI_POWERMODE_BALANCED:
+		*profile = PLATFORM_PROFILE_BALANCED;
+		break;
+	case LEGION_WMI_POWERMODE_PERFORMANCE:
+		*profile = PLATFORM_PROFILE_PERFORMANCE;
+		break;
+	case LEGION_WMI_POWERMODE_QUIET:
+		*profile = PLATFORM_PROFILE_QUIET;
+		break;
+	case LEGION_WMI_POWERMODE_CUSTOM:
+		*profile = PLATFORM_PROFILE_BALANCED_PERFORMANCE;
+		break;
+	default:
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int legion_platform_profile_set(struct platform_profile_handler *pprof,
+				       enum platform_profile_option profile)
+{
+	int powermode;
+	struct legion_private *priv;
+
+	priv = container_of(pprof, struct legion_private,
+			    platform_profile_handler);
+
+	switch (profile) {
+	case PLATFORM_PROFILE_BALANCED:
+		powermode = LEGION_WMI_POWERMODE_BALANCED;
+		break;
+	case PLATFORM_PROFILE_PERFORMANCE:
+		powermode = LEGION_WMI_POWERMODE_PERFORMANCE;
+		break;
+	case PLATFORM_PROFILE_QUIET:
+		powermode = LEGION_WMI_POWERMODE_QUIET;
+		break;
+	case PLATFORM_PROFILE_BALANCED_PERFORMANCE:
+		powermode = LEGION_WMI_POWERMODE_CUSTOM;
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return write_powermode(priv, powermode);
+}
+
+static int legion_platform_profile_init(struct legion_private *priv)
+{
+	int err;
+
+	if (!enable_platformprofile) {
+		pr_info("Skipping creating platform profile support because enable_platformprofile is false\n");
+		return 0;
+	}
+
+	priv->platform_profile_handler.profile_get =
+		legion_platform_profile_get;
+	priv->platform_profile_handler.profile_set =
+		legion_platform_profile_set;
+
+	set_bit(PLATFORM_PROFILE_QUIET, priv->platform_profile_handler.choices);
+	set_bit(PLATFORM_PROFILE_BALANCED,
+		priv->platform_profile_handler.choices);
+	set_bit(PLATFORM_PROFILE_PERFORMANCE,
+		priv->platform_profile_handler.choices);
+	if (priv->conf->has_custom_powermode &&
+	    priv->conf->access_method_powermode == ACCESS_METHOD_WMI) {
+		set_bit(PLATFORM_PROFILE_BALANCED_PERFORMANCE,
+			priv->platform_profile_handler.choices);
+	}
+
+	err = platform_profile_register(&priv->platform_profile_handler);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static void legion_platform_profile_exit(struct legion_private *priv)
+{
+	if (!enable_platformprofile) {
+		pr_info("Skipping unloading platform profile support because enable_platformprofile is false\n");
+		return;
+	}
+	pr_info("Unloading legion platform profile\n");
+	platform_profile_remove();
+	pr_info("Unloading legion platform profile done\n");
+}
+
+/* =============================  */
+/* hwom interface              */
+/* ============================   */
+
+// hw-mon interface
+
+// todo: register_group or register_info?
+
+// TODO: use one common function (like here) or one function per attribute?
+static ssize_t sensor_label_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	int sensor_id = (to_sensor_dev_attr(attr))->index;
+	const char *label;
+
+	switch (sensor_id) {
+	case SENSOR_CPU_TEMP_ID:
+		label = "CPU Temperature\n";
+		break;
+	case SENSOR_GPU_TEMP_ID:
+		label = "GPU Temperature\n";
+		break;
+	case SENSOR_IC_TEMP_ID:
+		label = "IC Temperature\n";
+		break;
+	case SENSOR_FAN1_RPM_ID:
+		label = "Fan 1\n";
+		break;
+	case SENSOR_FAN2_RPM_ID:
+		label = "Fan 2\n";
+		break;
+	case SENSOR_FAN1_TARGET_RPM_ID:
+		label = "Fan 1 Target\n";
+		break;
+	case SENSOR_FAN2_TARGET_RPM_ID:
+		label = "Fan 2 Target\n";
+		break;
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return sprintf(buf, label);
+}
+
+// TODO: use one common function (like here) or one function per attribute?
+static ssize_t sensor_show(struct device *dev, struct device_attribute *devattr,
+			   char *buf)
+{
+	struct legion_private *priv = dev_get_drvdata(dev);
+	int sensor_id = (to_sensor_dev_attr(devattr))->index;
+	struct sensor_values values;
+	int outval;
+	int err = -EIO;
+
+	switch (sensor_id) {
+	case SENSOR_CPU_TEMP_ID:
+		err = read_temperature(priv, 0, &outval);
+		outval *= 1000;
+		break;
+	case SENSOR_GPU_TEMP_ID:
+		err = read_temperature(priv, 1, &outval);
+		outval *= 1000;
+		break;
+	case SENSOR_IC_TEMP_ID:
+		ec_read_sensor_values(&priv->ecram, priv->conf, &values);
+		outval = 1000 * values.ic_temp_celsius;
+		err = 0;
+		break;
+	case SENSOR_FAN1_RPM_ID:
+		err = read_fanspeed(priv, 0, &outval);
+		break;
+	case SENSOR_FAN2_RPM_ID:
+		err = read_fanspeed(priv, 1, &outval);
+		break;
+	case SENSOR_FAN1_TARGET_RPM_ID:
+		ec_read_sensor_values(&priv->ecram, priv->conf, &values);
+		outval = values.fan1_target_rpm;
+		err = 0;
+		break;
+	case SENSOR_FAN2_TARGET_RPM_ID:
+		ec_read_sensor_values(&priv->ecram, priv->conf, &values);
+		outval = values.fan2_target_rpm;
+		err = 0;
+		break;
+	default:
+		pr_info("Error reading sensor value with id %d\n", sensor_id);
+		return -EOPNOTSUPP;
+	}
+	if (err)
+		return err;
+
+	return sprintf(buf, "%d\n", outval);
+}
+
+static SENSOR_DEVICE_ATTR_RO(temp1_input, sensor, SENSOR_CPU_TEMP_ID);
+static SENSOR_DEVICE_ATTR_RO(temp1_label, sensor_label, SENSOR_CPU_TEMP_ID);
+static SENSOR_DEVICE_ATTR_RO(temp2_input, sensor, SENSOR_GPU_TEMP_ID);
+static SENSOR_DEVICE_ATTR_RO(temp2_label, sensor_label, SENSOR_GPU_TEMP_ID);
+static SENSOR_DEVICE_ATTR_RO(temp3_input, sensor, SENSOR_IC_TEMP_ID);
+static SENSOR_DEVICE_ATTR_RO(temp3_label, sensor_label, SENSOR_IC_TEMP_ID);
+static SENSOR_DEVICE_ATTR_RO(fan1_input, sensor, SENSOR_FAN1_RPM_ID);
+static SENSOR_DEVICE_ATTR_RO(fan1_label, sensor_label, SENSOR_FAN1_RPM_ID);
+static SENSOR_DEVICE_ATTR_RO(fan2_input, sensor, SENSOR_FAN2_RPM_ID);
+static SENSOR_DEVICE_ATTR_RO(fan2_label, sensor_label, SENSOR_FAN2_RPM_ID);
+static SENSOR_DEVICE_ATTR_RO(fan1_target, sensor, SENSOR_FAN1_TARGET_RPM_ID);
+static SENSOR_DEVICE_ATTR_RO(fan2_target, sensor, SENSOR_FAN2_TARGET_RPM_ID);
+
+static struct attribute *sensor_hwmon_attributes[] = {
+	&sensor_dev_attr_temp1_input.dev_attr.attr,
+	&sensor_dev_attr_temp1_label.dev_attr.attr,
+	&sensor_dev_attr_temp2_input.dev_attr.attr,
+	&sensor_dev_attr_temp2_label.dev_attr.attr,
+	&sensor_dev_attr_temp3_input.dev_attr.attr,
+	&sensor_dev_attr_temp3_label.dev_attr.attr,
+	&sensor_dev_attr_fan1_input.dev_attr.attr,
+	&sensor_dev_attr_fan1_label.dev_attr.attr,
+	&sensor_dev_attr_fan2_input.dev_attr.attr,
+	&sensor_dev_attr_fan2_label.dev_attr.attr,
+	&sensor_dev_attr_fan1_target.dev_attr.attr,
+	&sensor_dev_attr_fan2_target.dev_attr.attr,
+	NULL
+};
+
+static ssize_t autopoint_show(struct device *dev,
+			      struct device_attribute *devattr, char *buf)
+{
+	struct fancurve fancurve;
+	int err;
+	int value;
+	struct legion_private *priv = dev_get_drvdata(dev);
+	int fancurve_attr_id = to_sensor_dev_attr_2(devattr)->nr;
+	int point_id = to_sensor_dev_attr_2(devattr)->index;
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = read_fancurve(priv, &fancurve);
+	mutex_unlock(&priv->fancurve_mutex);
+
+	if (err) {
+		pr_info("Failed to read fancurve\n");
+		return -EOPNOTSUPP;
+	}
+	if (!(point_id >= 0 && point_id < MAXFANCURVESIZE)) {
+		pr_info("Failed to read fancurve due to wrong point id: %d\n",
+			point_id);
+		return -EOPNOTSUPP;
+	}
+
+	switch (fancurve_attr_id) {
+	case FANCURVE_ATTR_PWM1:
+		value = fancurve.points[point_id].rpm1_raw * 100;
+		break;
+	case FANCURVE_ATTR_PWM2:
+		value = fancurve.points[point_id].rpm2_raw * 100;
+		break;
+	case FANCURVE_ATTR_CPU_TEMP:
+		value = fancurve.points[point_id].cpu_max_temp_celsius;
+		break;
+	case FANCURVE_ATTR_CPU_HYST:
+		value = fancurve.points[point_id].cpu_min_temp_celsius;
+		break;
+	case FANCURVE_ATTR_GPU_TEMP:
+		value = fancurve.points[point_id].gpu_max_temp_celsius;
+		break;
+	case FANCURVE_ATTR_GPU_HYST:
+		value = fancurve.points[point_id].gpu_min_temp_celsius;
+		break;
+	case FANCURVE_ATTR_IC_TEMP:
+		value = fancurve.points[point_id].ic_max_temp_celsius;
+		break;
+	case FANCURVE_ATTR_IC_HYST:
+		value = fancurve.points[point_id].ic_min_temp_celsius;
+		break;
+	case FANCURVE_ATTR_ACCEL:
+		value = fancurve.points[point_id].accel;
+		break;
+	case FANCURVE_ATTR_DECEL:
+		value = fancurve.points[point_id].decel;
+		break;
+	case FANCURVE_SIZE:
+		value = fancurve.size;
+		break;
+	default:
+		pr_info("Failed to read fancurve due to wrong attribute id: %d\n",
+			fancurve_attr_id);
+		return -EOPNOTSUPP;
+	}
+
+	return sprintf(buf, "%d\n", value);
+}
+
+static ssize_t autopoint_store(struct device *dev,
+			       struct device_attribute *devattr,
+			       const char *buf, size_t count)
+{
+	struct fancurve fancurve;
+	int err;
+	int value;
+	bool valid;
+	struct legion_private *priv = dev_get_drvdata(dev);
+	int fancurve_attr_id = to_sensor_dev_attr_2(devattr)->nr;
+	int point_id = to_sensor_dev_attr_2(devattr)->index;
+	bool write_fancurve_size = false;
+
+	if (!(point_id >= 0 && point_id < MAXFANCURVESIZE)) {
+		pr_info("Failed to read fancurve due to wrong point id: %d\n",
+			point_id);
+		err = -EOPNOTSUPP;
+		goto error;
+	}
+
+	err = kstrtoint(buf, 0, &value);
+	if (err) {
+		pr_info("Parsing hwmon store failed: error: %d; point_id: %d; fancurve_attr_id: %d\\n",
+			err, point_id, fancurve_attr_id);
+		goto error;
+	}
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = read_fancurve(priv, &fancurve);
+
+	if (err) {
+		pr_info("Failed to read fancurve\n");
+		err = -EOPNOTSUPP;
+		goto error_mutex;
+	}
+
+	switch (fancurve_attr_id) {
+	case FANCURVE_ATTR_PWM1:
+		valid = fancurve_set_rpm1(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_PWM2:
+		valid = fancurve_set_rpm2(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_CPU_TEMP:
+		valid = fancurve_set_cpu_temp_max(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_CPU_HYST:
+		valid = fancurve_set_cpu_temp_min(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_GPU_TEMP:
+		valid = fancurve_set_gpu_temp_max(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_GPU_HYST:
+		valid = fancurve_set_gpu_temp_min(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_IC_TEMP:
+		valid = fancurve_set_ic_temp_max(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_IC_HYST:
+		valid = fancurve_set_ic_temp_min(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_ACCEL:
+		valid = fancurve_set_accel(&fancurve, point_id, value);
+		break;
+	case FANCURVE_ATTR_DECEL:
+		valid = fancurve_set_decel(&fancurve, point_id, value);
+		break;
+	case FANCURVE_SIZE:
+		valid = fancurve_set_size(&fancurve, value, true);
+		write_fancurve_size = true;
+		break;
+	default:
+		pr_info("Failed to write fancurve due to wrong attribute id: %d\n",
+			fancurve_attr_id);
+		err = -EOPNOTSUPP;
+		goto error_mutex;
+	}
+
+	if (!valid) {
+		pr_info("Ignoring invalid fancurve value %d for attribute %d at point %d\n",
+			value, fancurve_attr_id, point_id);
+		err = -EOPNOTSUPP;
+		goto error_mutex;
+	}
+
+	err = write_fancurve(priv, &fancurve, write_fancurve_size);
+	if (err) {
+		pr_info("Failed to write fancurve for accessing hwmon at point_id: %d\n",
+			point_id);
+		err = -EOPNOTSUPP;
+		goto error_mutex;
+	}
+
+	mutex_unlock(&priv->fancurve_mutex);
+	return count;
+
+error_mutex:
+	mutex_unlock(&priv->fancurve_mutex);
+error:
+	return count;
+}
+
+// rpm1
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point1_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point2_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point3_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point4_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point5_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point6_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point7_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point8_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point9_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point10_pwm, autopoint,
+			       FANCURVE_ATTR_PWM1, 9);
+// rpm2
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point1_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point2_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point3_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point4_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point5_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point6_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point7_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point8_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point9_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point10_pwm, autopoint,
+			       FANCURVE_ATTR_PWM2, 9);
+// CPU temp
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point1_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point2_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point3_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point4_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point5_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point6_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point7_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point8_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point9_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point10_temp, autopoint,
+			       FANCURVE_ATTR_CPU_TEMP, 9);
+// CPU temp hyst
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point1_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point2_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point3_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point4_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point5_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point6_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point7_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point8_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point9_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point10_temp_hyst, autopoint,
+			       FANCURVE_ATTR_CPU_HYST, 9);
+// GPU temp
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point1_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point2_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point3_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point4_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point5_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point6_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point7_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point8_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point9_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point10_temp, autopoint,
+			       FANCURVE_ATTR_GPU_TEMP, 9);
+// GPU temp hyst
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point1_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point2_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point3_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point4_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point5_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point6_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point7_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point8_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point9_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm2_auto_point10_temp_hyst, autopoint,
+			       FANCURVE_ATTR_GPU_HYST, 9);
+// IC temp
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point1_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point2_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point3_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point4_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point5_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point6_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point7_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point8_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point9_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point10_temp, autopoint,
+			       FANCURVE_ATTR_IC_TEMP, 9);
+// IC temp hyst
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point1_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point2_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point3_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point4_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point5_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point6_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point7_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point8_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point9_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm3_auto_point10_temp_hyst, autopoint,
+			       FANCURVE_ATTR_IC_HYST, 9);
+// accel
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point1_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point2_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point3_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point4_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point5_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point6_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point7_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point8_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point9_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point10_accel, autopoint,
+			       FANCURVE_ATTR_ACCEL, 9);
+// decel
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point1_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 0);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point2_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 1);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point3_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 2);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point4_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 3);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point5_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 4);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point6_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 5);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point7_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 6);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point8_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 7);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point9_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 8);
+static SENSOR_DEVICE_ATTR_2_RW(pwm1_auto_point10_decel, autopoint,
+			       FANCURVE_ATTR_DECEL, 9);
+//size
+static SENSOR_DEVICE_ATTR_2_RW(auto_points_size, autopoint, FANCURVE_SIZE, 0);
+
+static ssize_t minifancurve_show(struct device *dev,
+				 struct device_attribute *devattr, char *buf)
+{
+	bool value;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = ec_read_minifancurve(&priv->ecram, priv->conf, &value);
+	if (err) {
+		err = -1;
+		pr_info("Failed to read minifancurve\n");
+		goto error_unlock;
+	}
+	mutex_unlock(&priv->fancurve_mutex);
+	return sprintf(buf, "%d\n", value);
+
+error_unlock:
+	mutex_unlock(&priv->fancurve_mutex);
+	return -1;
+}
+
+static ssize_t minifancurve_store(struct device *dev,
+				  struct device_attribute *devattr,
+				  const char *buf, size_t count)
+{
+	int value;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	err = kstrtoint(buf, 0, &value);
+	if (err) {
+		err = -1;
+		pr_info("Parsing hwmon store failed: error:%d\n",
+			err);
+		goto error;
+	}
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = ec_write_minifancurve(&priv->ecram, priv->conf, value);
+	if (err) {
+		err = -1;
+		pr_info("Failed to write minifancurve\n");
+		goto error_unlock;
+	}
+	mutex_unlock(&priv->fancurve_mutex);
+	return count;
+
+error_unlock:
+	mutex_unlock(&priv->fancurve_mutex);
+error:
+	return err;
+}
+
+static SENSOR_DEVICE_ATTR_RW(minifancurve, minifancurve, 0);
+
+static ssize_t pwm1_mode_show(struct device *dev,
+			      struct device_attribute *devattr, char *buf)
+{
+	bool value;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = ec_read_fanfullspeed(&priv->ecram, priv->conf, &value);
+	if (err) {
+		err = -1;
+		pr_info("Failed to pwm1_mode/maximumfanspeed\n");
+		goto error_unlock;
+	}
+	mutex_unlock(&priv->fancurve_mutex);
+	return sprintf(buf, "%d\n", value ? 0 : 2);
+
+error_unlock:
+	mutex_unlock(&priv->fancurve_mutex);
+	return -1;
+}
+
+// TODO: remove? or use WMI method?
+static ssize_t pwm1_mode_store(struct device *dev,
+			       struct device_attribute *devattr,
+			       const char *buf, size_t count)
+{
+	int value;
+	int is_maximumfanspeed;
+	int err;
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	err = kstrtoint(buf, 0, &value);
+	if (err) {
+		err = -1;
+		pr_info("Parsing hwmon store failed: error:%d\n",
+			err);
+		goto error;
+	}
+	is_maximumfanspeed = value == 0;
+
+	mutex_lock(&priv->fancurve_mutex);
+	err = ec_write_fanfullspeed(&priv->ecram, priv->conf,
+				    is_maximumfanspeed);
+	if (err) {
+		err = -1;
+		pr_info("Failed to write pwm1_mode/maximumfanspeed\n");
+		goto error_unlock;
+	}
+	mutex_unlock(&priv->fancurve_mutex);
+	return count;
+
+error_unlock:
+	mutex_unlock(&priv->fancurve_mutex);
+error:
+	return err;
+}
+
+static SENSOR_DEVICE_ATTR_RW(pwm1_mode, pwm1_mode, 0);
+
+static struct attribute *fancurve_hwmon_attributes[] = {
+	&sensor_dev_attr_pwm1_auto_point1_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point2_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point3_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point4_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point5_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point6_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point7_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point8_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point9_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point10_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point1_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point2_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point3_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point4_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point5_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point6_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point7_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point8_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point9_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point10_pwm.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point1_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point2_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point3_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point4_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point5_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point6_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point7_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point8_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point9_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point10_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point1_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point2_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point3_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point4_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point5_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point6_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point7_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point8_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point9_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point10_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point1_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point2_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point3_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point4_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point5_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point6_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point7_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point8_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point9_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point10_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point1_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point2_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point3_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point4_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point5_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point6_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point7_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point8_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point9_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm2_auto_point10_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point1_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point2_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point3_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point4_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point5_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point6_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point7_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point8_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point9_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point10_temp.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point1_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point2_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point3_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point4_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point5_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point6_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point7_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point8_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point9_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm3_auto_point10_temp_hyst.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point1_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point2_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point3_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point4_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point5_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point6_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point7_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point8_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point9_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point10_accel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point1_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point2_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point3_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point4_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point5_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point6_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point7_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point8_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point9_decel.dev_attr.attr,
+	&sensor_dev_attr_pwm1_auto_point10_decel.dev_attr.attr,
+	//
+	&sensor_dev_attr_auto_points_size.dev_attr.attr,
+	&sensor_dev_attr_minifancurve.dev_attr.attr,
+	&sensor_dev_attr_pwm1_mode.dev_attr.attr, NULL
+};
+
+static umode_t legion_hwmon_is_visible(struct kobject *kobj,
+				       struct attribute *attr, int idx)
+{
+	bool supported = true;
+	struct device *dev = kobj_to_dev(kobj);
+	struct legion_private *priv = dev_get_drvdata(dev);
+
+	if (attr == &sensor_dev_attr_minifancurve.dev_attr.attr)
+		supported = priv->conf->has_minifancurve;
+
+	supported = supported && (priv->conf->access_method_fancurve !=
+				  ACCESS_METHOD_NO_ACCESS);
+
+	return supported ? attr->mode : 0;
+}
+
+static const struct attribute_group legion_hwmon_sensor_group = {
+	.attrs = sensor_hwmon_attributes,
+	.is_visible = NULL
+};
+
+static const struct attribute_group legion_hwmon_fancurve_group = {
+	.attrs = fancurve_hwmon_attributes,
+	.is_visible = legion_hwmon_is_visible,
+};
+
+static const struct attribute_group *legion_hwmon_groups[] = {
+	&legion_hwmon_sensor_group, &legion_hwmon_fancurve_group, NULL
+};
+
+static ssize_t legion_hwmon_init(struct legion_private *priv)
+{
+	//TODO: use hwmon_device_register_with_groups or
+	// hwmon_device_register_with_info (latter means all hwmon functions have to be
+	// changed)
+	// some laptop driver do it in one way, some in the other
+	// TODO: Use devm_hwmon_device_register_with_groups ?
+	// some laptop drivers use this, some
+	struct device *hwmon_dev = hwmon_device_register_with_groups(
+		&priv->platform_device->dev, "legion_hwmon", priv,
+		legion_hwmon_groups);
+	if (IS_ERR_OR_NULL(hwmon_dev)) {
+		pr_err("hwmon_device_register failed!\n");
+		return PTR_ERR(hwmon_dev);
+	}
+	dev_set_drvdata(hwmon_dev, priv);
+	priv->hwmon_dev = hwmon_dev;
+	return 0;
+}
+
+static void legion_hwmon_exit(struct legion_private *priv)
+{
+	pr_info("Unloading legion hwon\n");
+	if (priv->hwmon_dev) {
+		hwmon_device_unregister(priv->hwmon_dev);
+		priv->hwmon_dev = NULL;
+	}
+	pr_info("Unloading legion hwon done\n");
+}
+
+/* ACPI*/
+
+static int acpi_init(struct legion_private *priv, struct acpi_device *adev)
+{
+	int err;
+	unsigned long cfg;
+	bool skip_acpi_sta_check;
+	struct device *dev = &priv->platform_device->dev;
+
+	priv->adev = adev;
+	if (!priv->adev) {
+		dev_info(dev, "Could not get ACPI handle\n");
+		goto err_acpi_init;
+	}
+
+	skip_acpi_sta_check = force || (!priv->conf->acpi_check_dev);
+	if (!skip_acpi_sta_check) {
+		err = eval_int(priv->adev->handle, "_STA", &cfg);
+		if (err) {
+			dev_info(dev, "Could not evaluate ACPI _STA\n");
+			goto err_acpi_init;
+		}
+
+		err = eval_int(priv->adev->handle, "VPC0._CFG", &cfg);
+		if (err) {
+			dev_info(dev, "Could not evaluate ACPI _CFG\n");
+			goto err_acpi_init;
+		}
+		dev_info(dev, "ACPI CFG: %lu\n", cfg);
+	} else {
+		dev_info(dev, "Skipping ACPI _STA check");
+	}
+
+	return 0;
+
+err_acpi_init:
+	return err;
+}
+
+/* =============================  */
+/* White Keyboard Backlight       */
+/* ============================   */
+// In style of ideapad-driver and with code modified from ideapad-driver.
+
+static enum led_brightness
+legion_kbd_bl_led_cdev_brightness_get(struct led_classdev *led_cdev)
+{
+	struct legion_private *priv =
+		container_of(led_cdev, struct legion_private, kbd_bl.led);
+
+	return legion_kbd_bl_brightness_get(priv);
+}
+
+static int legion_kbd_bl_led_cdev_brightness_set(struct led_classdev *led_cdev,
+						 enum led_brightness brightness)
+{
+	struct legion_private *priv =
+		container_of(led_cdev, struct legion_private, kbd_bl.led);
+
+	return legion_kbd_bl_brightness_set(priv, brightness);
+}
+
+static int legion_kbd_bl_init(struct legion_private *priv)
+{
+	int brightness, err;
+
+	if (WARN_ON(priv->kbd_bl.initialized)) {
+		pr_info("Keyboard backlight already initialized\n");
+		return -EEXIST;
+	}
+
+	if (priv->conf->access_method_keyboard == ACCESS_METHOD_NO_ACCESS) {
+		pr_info("Keyboard backlight handling disabled by this driver\n");
+		return -ENODEV;
+	}
+
+	brightness = legion_kbd_bl_brightness_get(priv);
+	if (brightness < 0) {
+		pr_info("Error reading keyboard brightness\n");
+		return brightness;
+	}
+
+	priv->kbd_bl.last_brightness = brightness;
+
+	// will be renamed to "platform::kbd_backlight_1" if it exists already
+	priv->kbd_bl.led.name = "platform::" LED_FUNCTION_KBD_BACKLIGHT;
+	priv->kbd_bl.led.max_brightness = 2;
+	priv->kbd_bl.led.brightness_get = legion_kbd_bl_led_cdev_brightness_get;
+	priv->kbd_bl.led.brightness_set_blocking =
+		legion_kbd_bl_led_cdev_brightness_set;
+	priv->kbd_bl.led.flags = LED_BRIGHT_HW_CHANGED;
+
+	err = led_classdev_register(&priv->platform_device->dev,
+				    &priv->kbd_bl.led);
+	if (err)
+		return err;
+
+	priv->kbd_bl.initialized = true;
+
+	return 0;
+}
+
+/**
+ * Deinit keyboard backlight.
+ *
+ * Can also be called if init was not successful.
+ *
+ */
+static void legion_kbd_bl_exit(struct legion_private *priv)
+{
+	if (!priv->kbd_bl.initialized)
+		return;
+
+	priv->kbd_bl.initialized = false;
+
+	led_classdev_unregister(&priv->kbd_bl.led);
+}
+
+/* =============================  */
+/* Additional light driver        */
+/* ============================   */
+
+static enum led_brightness
+legion_wmi_cdev_brightness_get(struct led_classdev *led_cdev)
+{
+	struct legion_private *priv =
+		container_of(led_cdev, struct legion_private, kbd_bl.led);
+	struct light *light_ins = container_of(led_cdev, struct light, led);
+
+	return legion_wmi_light_get(priv, light_ins->light_id,
+				    light_ins->lower_limit,
+				    light_ins->upper_limit);
+}
+
+static int legion_wmi_cdev_brightness_set(struct led_classdev *led_cdev,
+					  enum led_brightness brightness)
+{
+	struct legion_private *priv =
+		container_of(led_cdev, struct legion_private, kbd_bl.led);
+	struct light *light_ins = container_of(led_cdev, struct light, led);
+
+	return legion_wmi_light_set(priv, light_ins->light_id,
+				    light_ins->lower_limit,
+				    light_ins->upper_limit, brightness);
+}
+
+static int legion_light_init(struct legion_private *priv,
+			     struct light *light_ins, u8 light_id,
+			     u8 lower_limit, u8 upper_limit, const char *name)
+{
+	int brightness, err;
+
+	if (WARN_ON(light_ins->initialized)) {
+		pr_info("Light already initialized for light: %u\n",
+			light_ins->light_id);
+		return -EEXIST;
+	}
+
+	light_ins->light_id = light_id;
+	light_ins->lower_limit = lower_limit;
+	light_ins->upper_limit = upper_limit;
+
+	brightness = legion_wmi_light_get(priv, light_ins->light_id,
+					  light_ins->lower_limit,
+					  light_ins->upper_limit);
+	if (brightness < 0) {
+		pr_info("Error reading brightness for light: %u\n",
+			light_ins->light_id);
+		return brightness;
+	}
+
+	light_ins->led.name = name;
+	light_ins->led.max_brightness =
+		light_ins->upper_limit - light_ins->lower_limit;
+	light_ins->led.brightness_get = legion_wmi_cdev_brightness_get;
+	light_ins->led.brightness_set_blocking = legion_wmi_cdev_brightness_set;
+	light_ins->led.flags = LED_BRIGHT_HW_CHANGED;
+
+	err = led_classdev_register(&priv->platform_device->dev,
+				    &light_ins->led);
+	if (err)
+		return err;
+
+	light_ins->initialized = true;
+
+	return 0;
+}
+
+/**
+ * Deinit light.
+ *
+ * Can also be called if init was not successful.
+ *
+ */
+static void legion_light_exit(struct legion_private *priv,
+			      struct light *light_ins)
+{
+	if (!light_ins->initialized)
+		return;
+
+	light_ins->initialized = false;
+
+	led_classdev_unregister(&light_ins->led);
+}
+
+/* =============================  */
+/* Platform driver                */
+/* ============================   */
+
+static int legion_add(struct platform_device *pdev)
+{
+	struct legion_private *priv;
+	const struct dmi_system_id *dmi_sys;
+	int err;
+	u16 ec_read_id;
+	bool skip_ec_id_check;
+	bool is_ec_id_valid;
+	bool is_denied = true;
+	bool is_allowed = false;
+	bool do_load_by_list = false;
+	bool do_load = false;
+	//struct legion_private *priv = dev_get_drvdata(&pdev->dev);
+	dev_info(&pdev->dev, "legion_laptop platform driver probing\n");
+
+	dev_info(
+		&pdev->dev,
+		"Read identifying information: DMI_SYS_VENDOR: %s; DMI_PRODUCT_NAME: %s; DMI_BIOS_VERSION:%s\n",
+		dmi_get_system_info(DMI_SYS_VENDOR),
+		dmi_get_system_info(DMI_PRODUCT_NAME),
+		dmi_get_system_info(DMI_BIOS_VERSION));
+
+	// TODO: allocate?
+	priv = &_priv;
+	priv->platform_device = pdev;
+	err = legion_shared_init(priv);
+	if (err) {
+		dev_info(&pdev->dev, "legion_laptop is forced to load.\n");
+		goto err_legion_shared_init;
+	}
+	dev_set_drvdata(&pdev->dev, priv);
+
+	// TODO: remove
+	pr_info("Read identifying information: DMI_SYS_VENDOR: %s; DMI_PRODUCT_NAME: %s; DMI_BIOS_VERSION:%s\n",
+		dmi_get_system_info(DMI_SYS_VENDOR),
+		dmi_get_system_info(DMI_PRODUCT_NAME),
+		dmi_get_system_info(DMI_BIOS_VERSION));
+
+	dmi_sys = dmi_first_match(optimistic_allowlist);
+	is_allowed = dmi_sys != NULL;
+	is_denied = dmi_check_system(denylist);
+	do_load_by_list = is_allowed && !is_denied;
+	do_load = do_load_by_list || force;
+
+	dev_info(
+		&pdev->dev,
+		"is_denied: %d; is_allowed: %d; do_load_by_list: %d; do_load: %d\n",
+		is_denied, is_allowed, do_load_by_list, do_load);
+
+	if (!(do_load)) {
+		dev_info(
+			&pdev->dev,
+			"Module not usable for this laptop because it is not in allowlist. Notify the maintainer if you want to add your device or force load with param force.\n");
+		err = -ENOMEM;
+		goto err_model_mismtach;
+	}
+
+	if (force)
+		dev_info(&pdev->dev, "legion_laptop is forced to load.\n");
+
+	if (!do_load_by_list && do_load) {
+		dev_info(
+			&pdev->dev,
+			"legion_laptop is forced to load and would otherwise not be loaded\n");
+	}
+
+	// if forced and no module found, use config for first model
+	if (dmi_sys == NULL)
+		dmi_sys = &optimistic_allowlist[0];
+	dev_info(&pdev->dev, "Using configuration for system: %s\n",
+		 dmi_sys->ident);
+
+	priv->conf = dmi_sys->driver_data;
+
+	err = acpi_init(priv, ACPI_COMPANION(&pdev->dev));
+	if (err) {
+		dev_info(&pdev->dev, "Could not init ACPI access: %d\n", err);
+		goto err_acpi_init;
+	}
+
+	// TODO: remove; only used for reverse engineering
+	pr_info("Creating RAM access to embedded controller\n");
+	err = ecram_memoryio_init(&priv->ec_memoryio,
+				  priv->conf->ramio_physical_start, 0,
+				  priv->conf->ramio_size);
+	if (err) {
+		dev_info(
+			&pdev->dev,
+			"Could not init RAM access to embedded controller: %d\n",
+			err);
+		goto err_ecram_memoryio_init;
+	}
+
+	err = ecram_init(&priv->ecram, priv->conf->memoryio_physical_ec_start,
+			 priv->conf->memoryio_size);
+	if (err) {
+		dev_info(&pdev->dev,
+			 "Could not init access to embedded controller: %d\n",
+			 err);
+		goto err_ecram_init;
+	}
+
+	ec_read_id = read_ec_id(&priv->ecram, priv->conf);
+	dev_info(&pdev->dev, "Read embedded controller ID 0x%x\n", ec_read_id);
+	skip_ec_id_check = force || (!priv->conf->check_embedded_controller_id);
+	is_ec_id_valid = skip_ec_id_check ||
+			 (ec_read_id == priv->conf->embedded_controller_id);
+	if (!is_ec_id_valid) {
+		err = -ENOMEM;
+		dev_info(&pdev->dev, "Expected EC chip id 0x%x but read 0x%x\n",
+			 priv->conf->embedded_controller_id, ec_read_id);
+		goto err_ecram_id;
+	}
+	if (skip_ec_id_check) {
+		dev_info(&pdev->dev,
+			 "Skipped checking embedded controller id\n");
+	}
+
+	dev_info(&pdev->dev, "Creating debugfs interface\n");
+	legion_debugfs_init(priv);
+
+	pr_info("Creating sysfs interface\n");
+	err = legion_sysfs_init(priv);
+	if (err) {
+		dev_info(&pdev->dev, "Failed to create sysfs interface: %d\n",
+			 err);
+		goto err_sysfs_init;
+	}
+
+	pr_info("Creating hwmon interface");
+	err = legion_hwmon_init(priv);
+	if (err) {
+		dev_info(&pdev->dev, "Failed to create hwmon interface: %d\n",
+			 err);
+		goto err_hwmon_init;
+	}
+
+	pr_info("Creating platform profile support\n");
+	err = legion_platform_profile_init(priv);
+	if (err) {
+		dev_info(&pdev->dev, "Failed to create platform profile: %d\n",
+			 err);
+		goto err_platform_profile;
+	}
+
+	pr_info("Init WMI driver support\n");
+	err = legion_wmi_init();
+	if (err) {
+		dev_info(&pdev->dev, "Failed to init WMI driver: %d\n", err);
+		goto err_wmi;
+	}
+
+	pr_info("Init keyboard backlight LED driver\n");
+	err = legion_kbd_bl_init(priv);
+	if (err) {
+		dev_info(
+			&pdev->dev,
+			"Failed to init keyboard backlight LED driver. Skipping ...\n");
+	}
+
+	pr_info("Init Y-Logo LED driver\n");
+	err = legion_light_init(priv, &priv->ylogo_light, LIGHT_ID_YLOGO, 0, 1,
+				"platform::ylogo");
+	if (err) {
+		dev_info(&pdev->dev,
+			 "Failed to init Y-Logo LED driver. Skipping ...\n");
+	}
+
+	pr_info("Init IO-Port LED driver\n");
+	err = legion_light_init(priv, &priv->iport_light, LIGHT_ID_IOPORT, 1, 2,
+				"platform::ioport");
+	if (err) {
+		dev_info(&pdev->dev,
+			 "Failed to init IO-Port LED driver. Skipping ...\n");
+	}
+
+	dev_info(&pdev->dev, "legion_laptop loaded for this device\n");
+	return 0;
+
+	// TODO: remove eventually
+	legion_light_exit(priv, &priv->iport_light);
+	legion_light_exit(priv, &priv->ylogo_light);
+	legion_kbd_bl_exit(priv);
+	legion_wmi_exit();
+err_wmi:
+	legion_platform_profile_exit(priv);
+err_platform_profile:
+	legion_hwmon_exit(priv);
+err_hwmon_init:
+	legion_sysfs_exit(priv);
+err_sysfs_init:
+	legion_debugfs_exit(priv);
+err_ecram_id:
+	ecram_exit(&priv->ecram);
+err_ecram_init:
+	ecram_memoryio_exit(&priv->ec_memoryio);
+err_ecram_memoryio_init:
+err_acpi_init:
+	legion_shared_exit(priv);
+err_legion_shared_init:
+err_model_mismtach:
+	dev_info(&pdev->dev, "legion_laptop not loaded for this device\n");
+	return err;
+}
+
+static void legion_remove(struct platform_device *pdev)
+{
+	struct legion_private *priv = dev_get_drvdata(&pdev->dev);
+
+	mutex_lock(&legion_shared_mutex);
+	priv->loaded = false;
+	mutex_unlock(&legion_shared_mutex);
+
+	legion_light_exit(priv, &priv->iport_light);
+	legion_light_exit(priv, &priv->ylogo_light);
+	legion_kbd_bl_exit(priv);
+	// first unregister wmi, so toggling powermode does not
+	// generate events anymore that even might be delayed
+	legion_wmi_exit();
+	legion_platform_profile_exit(priv);
+
+	// toggle power mode to load default setting from embedded controller
+	// again
+	toggle_powermode(priv);
+
+	legion_hwmon_exit(priv);
+	legion_sysfs_exit(priv);
+	legion_debugfs_exit(priv);
+	ecram_exit(&priv->ecram);
+	ecram_memoryio_exit(&priv->ec_memoryio);
+	legion_shared_exit(priv);
+
+	pr_info("Legion platform unloaded\n");
+}
+
+static int legion_resume(struct platform_device *pdev)
+{
+	//struct legion_private *priv = dev_get_drvdata(&pdev->dev);
+	dev_info(&pdev->dev, "Resumed in legion-laptop\n");
+
+	return 0;
+}
+
+#ifdef CONFIG_PM_SLEEP
+static int legion_pm_resume(struct device *dev)
+{
+	//struct legion_private *priv = dev_get_drvdata(dev);
+	dev_info(dev, "Resumed PM in legion-laptop\n");
+
+	return 0;
+}
+#endif
+static SIMPLE_DEV_PM_OPS(legion_pm, NULL, legion_pm_resume);
+
+// same as ideapad
+static const struct acpi_device_id legion_device_ids[] = {
+	// todo: change to "VPC2004", and also ACPI paths
+	{ "PNP0C09", 0 },
+	{ "", 0 },
+};
+MODULE_DEVICE_TABLE(acpi, legion_device_ids);
+
+static struct platform_driver legion_driver = {
+	.probe = legion_add,
+	.remove = legion_remove,
+	.resume = legion_resume,
+	.driver = {
+		.name   = "legion",
+		.pm     = &legion_pm,
+		.acpi_match_table = ACPI_PTR(legion_device_ids),
+	},
+};
+
+static int __init legion_init(void)
+{
+	int err;
+
+	pr_info("Loading legion_laptop\n");
+	err = platform_driver_register(&legion_driver);
+	if (err) {
+		pr_info("legion_laptop: platform_driver_register failed\n");
+		return err;
+	}
+
+	return 0;
+}
+
+module_init(legion_init);
+
+static void __exit legion_exit(void)
+{
+	platform_driver_unregister(&legion_driver);
+	pr_info("legion_laptop exit\n");
+}
+
+module_exit(legion_exit);
-- 
2.47.0


From 90c0d2fad58199812732c33cf76e3e77a4a6ccfa Mon Sep 17 00:00:00 2001
From: Antheas Kapenekakis <git@antheas.dev>
Date: Mon, 21 Oct 2024 19:29:25 +0200
Subject: [PATCH v1.4 120/120] Revert "drm/amdgpu: use GTT only as fallback for
 VRAM|GTT"

This reverts commit 216c1282dde38ca87ebdf1ccacee5a0682901574.

Most promising suspect for the AutoUMA regression, see:
https://gitlab.freedesktop.org/drm/amd/-/issues/3704
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_object.c | 6 ------
 1 file changed, 6 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
index e32161f6b67a..53b488312e49 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c
@@ -176,12 +176,6 @@ void amdgpu_bo_placement_from_domain(struct amdgpu_bo *abo, u32 domain)
 			abo->flags & AMDGPU_GEM_CREATE_PREEMPTIBLE ?
 			AMDGPU_PL_PREEMPT : TTM_PL_TT;
 		places[c].flags = 0;
-		/*
-		 * When GTT is just an alternative to VRAM make sure that we
-		 * only use it as fallback and still try to fill up VRAM first.
-		 */
-		if (domain & abo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM)
-			places[c].flags |= TTM_PL_FLAG_FALLBACK;
 		c++;
 	}
 
-- 
2.47.0

